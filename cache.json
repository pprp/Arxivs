{"2024-10-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.01805v1","updated":"2024-10-02T17:59:52Z","published":"2024-10-02T17:59:52Z","title":"Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads","summary":"  Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.\n","authors":["Yuxiang Huang","Binhang Yuan","Xu Han","Chaojun Xiao","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01805v1.pdf","comment":"Preprints"},{"id":"http://arxiv.org/abs/2410.01795v1","updated":"2024-10-02T17:53:08Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01794v1","updated":"2024-10-02T17:52:41Z","published":"2024-10-02T17:52:41Z","title":"Loki: An Open-Source Tool for Fact Verification","summary":"  We introduce Loki, an open-source tool designed to address the growing\nproblem of misinformation. Loki adopts a human-centered approach, striking a\nbalance between the quality of fact-checking and the cost of human involvement.\nIt decomposes the fact-checking task into a five-step pipeline: breaking down\nlong texts into individual claims, assessing their check-worthiness, generating\nqueries, retrieving evidence, and verifying the claims. Instead of fully\nautomating the claim verification process, Loki provides essential information\nat each step to assist human judgment, especially for general users such as\njournalists and content moderators. Moreover, it has been optimized for\nlatency, robustness, and cost efficiency at a commercially usable level. Loki\nis released under an MIT license and is available on GitHub. We also provide a\nvideo presenting the system and its capabilities.\n","authors":["Haonan Li","Xudong Han","Hao Wang","Yuxia Wang","Minghan Wang","Rui Xing","Yilin Geng","Zenan Zhai","Preslav Nakov","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2410.01794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01792v1","updated":"2024-10-02T17:50:19Z","published":"2024-10-02T17:50:19Z","title":"When a language model is optimized for reasoning, does it still show\n  embers of autoregression? An analysis of OpenAI o1","summary":"  In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 -\nlike previous LLMs - is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.\n","authors":["R. Thomas McCoy","Shunyu Yao","Dan Friedman","Mathew D. Hardy","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2410.01792v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.01791v1","updated":"2024-10-02T17:49:07Z","published":"2024-10-02T17:49:07Z","title":"DreamGarden: A Designer Assistant for Growing Games from a Single Prompt","summary":"  Coding assistants are increasingly leveraged in game design, both generating\ncode and making high-level plans. To what degree can these tools align with\ndeveloper workflows, and what new modes of human-computer interaction can\nemerge from their use? We present DreamGarden, an AI system capable of\nassisting with the development of diverse game environments in Unreal Engine.\nAt the core of our method is an LLM-driven planner, capable of breaking down a\nsingle, high-level prompt -- a dream, memory, or imagined scenario provided by\na human user -- into a hierarchical action plan, which is then distributed\nacross specialized submodules facilitating concrete implementation. This system\nis presented to the user as a garden of plans and actions, both growing\nindependently and responding to user intervention via seed prompts, pruning,\nand feedback. Through a user study, we explore design implications of this\nsystem, charting courses for future work in semi-autonomous assistants and\nopen-ended simulation design.\n","authors":["Sam Earle","Samyak Parajuli","Andrzej Banburski-Fahey"],"pdf_url":"https://arxiv.org/pdf/2410.01791v1.pdf","comment":"21 pages + appendix, 11 figures"},{"id":"http://arxiv.org/abs/2406.00314v3","updated":"2024-10-02T17:44:46Z","published":"2024-06-01T06:17:32Z","title":"CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models","summary":"  The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.\n","authors":["Sarthak Harne","Monjoy Narayan Choudhury","Madhav Rao","TK Srikanth","Seema Mehrotra","Apoorva Vashisht","Aarushi Basu","Manjit Sodhi"],"pdf_url":"https://arxiv.org/pdf/2406.00314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01784v1","updated":"2024-10-02T17:40:44Z","published":"2024-10-02T17:40:44Z","title":"OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic\n  Foundation Models","summary":"  The advancements in artificial intelligence in recent years, such as Large\nLanguage Models (LLMs), have fueled expectations for breakthroughs in genomic\nfoundation models (GFMs). The code of nature, hidden in diverse genomes since\nthe very beginning of life's evolution, holds immense potential for impacting\nhumans and ecosystems through genome modeling. Recent breakthroughs in GFMs,\nsuch as Evo, have attracted significant investment and attention to genomic\nmodeling, as they address long-standing challenges and transform in-silico\ngenomic studies into automated, reliable, and efficient paradigms. In the\ncontext of this flourishing era of consecutive technological revolutions in\ngenomics, GFM studies face two major challenges: the lack of GFM benchmarking\ntools and the absence of open-source software for diverse genomics. These\nchallenges hinder the rapid evolution of GFMs and their wide application in\ntasks such as understanding and synthesizing genomes, problems that have\npersisted for decades. To address these challenges, we introduce GFMBench, a\nframework dedicated to GFM-oriented benchmarking. GFMBench standardizes\nbenchmark suites and automates benchmarking for a wide range of open-source\nGFMs. It integrates millions of genomic sequences across hundreds of genomic\ntasks from four large-scale benchmarks, democratizing GFMs for a wide range of\nin-silico genomic applications. Additionally, GFMBench is released as\nopen-source software, offering user-friendly interfaces and diverse tutorials,\napplicable for AutoBench and complex tasks like RNA design and structure\nprediction. To facilitate further advancements in genome modeling, we have\nlaunched a public leaderboard showcasing the benchmark performance derived from\nAutoBench. GFMBench represents a step toward standardizing GFM benchmarking and\ndemocratizing GFM applications.\n","authors":["Heng Yang","Jack Cole","Ke Li"],"pdf_url":"https://arxiv.org/pdf/2410.01784v1.pdf","comment":"https://github.com/yangheng95/OmniGenomeBench"},{"id":"http://arxiv.org/abs/2409.02449v2","updated":"2024-10-02T17:40:25Z","published":"2024-09-04T05:08:23Z","title":"What is lost in Normalization? Exploring Pitfalls in Multilingual ASR\n  Model Evaluations","summary":"  This paper explores the pitfalls in evaluating multilingual automatic speech\nrecognition (ASR) models, with a particular focus on Indic language scripts. We\ninvestigate the text normalization routine employed by leading ASR models,\nincluding OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,\nand their unintended consequences on performance metrics. Our research reveals\nthat current text normalization practices, while aiming to standardize ASR\noutputs for fair comparison, by removing inconsistencies such as variations in\nspelling, punctuation, and special characters, are fundamentally flawed when\napplied to Indic scripts. Through empirical analysis using text similarity\nscores and in-depth linguistic examination, we demonstrate that these flaws\nlead to artificially improved performance metrics for Indic languages. We\nconclude by proposing a shift towards developing text normalization routines\nthat leverage native linguistic expertise, ensuring more robust and accurate\nevaluations of multilingual ASR models.\n","authors":["Kavya Manohar","Leena G Pillai"],"pdf_url":"https://arxiv.org/pdf/2409.02449v2.pdf","comment":"Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01782v1","updated":"2024-10-02T17:37:18Z","published":"2024-10-02T17:37:18Z","title":"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/\n","authors":["Shayekh Bin Islam","Md Asib Rahman","K S M Tozammel Hossain","Enamul Hoque","Shafiq Joty","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2410.01782v1.pdf","comment":"Accepted to EMNLP 2024 Findings. Website:\n  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.00274v2","updated":"2024-10-02T17:34:41Z","published":"2024-09-30T23:02:51Z","title":"Social Conjuring: Multi-User Runtime Collaboration with AI in Building\n  Virtual 3D Worlds","summary":"  Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.\n","authors":["Amina Kobenova","Cyan DeVeaux","Samyak Parajuli","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2410.00274v2.pdf","comment":"27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding\n  issues in arXiv compilation"},{"id":"http://arxiv.org/abs/2410.01779v1","updated":"2024-10-02T17:33:26Z","published":"2024-10-02T17:33:26Z","title":"Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets","summary":"  We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable.\n","authors":["Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01772v1","updated":"2024-10-02T17:29:34Z","published":"2024-10-02T17:29:34Z","title":"DeFine: Enhancing LLM Decision-Making with Factor Profiles and\n  Analogical Reasoning","summary":"  LLMs are ideal for decision-making due to their ability to reason over long\ncontexts and identify critical factors. However, challenges arise when\nprocessing transcripts of spoken speech describing complex scenarios. These\ntranscripts often contain ungrammatical or incomplete sentences, repetitions,\nhedging, and vagueness. For example, during a company's earnings call, an\nexecutive might project a positive revenue outlook to reassure investors,\ndespite significant uncertainty regarding future earnings. It is crucial for\nLLMs to incorporate this uncertainty systematically when making decisions. In\nthis paper, we introduce DeFine, a new framework that constructs probabilistic\nfactor profiles from complex scenarios. DeFine then integrates these profiles\nwith analogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in novel situations. Our framework\nseparates the tasks of quantifying uncertainty in complex scenarios and\nincorporating it into LLM decision-making. This approach is particularly useful\nin fields such as medical consultations, negotiations, and political debates,\nwhere making decisions under uncertainty is vital.\n","authors":["Yebowen Hu","Xiaoyang Wang","Wenlin Yao","Yiming Lu","Daoan Zhang","Hassan Foroosh","Dong Yu","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01100v2","updated":"2024-10-02T17:09:53Z","published":"2024-07-01T09:06:57Z","title":"Eliminating Position Bias of Language Models: A Mechanistic Approach","summary":"  Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.\n","authors":["Ziqi Wang","Hanlin Zhang","Xiner Li","Kuan-Hao Huang","Chi Han","Shuiwang Ji","Sham M. Kakade","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.01100v2.pdf","comment":"26 pages, 6 figures, 15 tables"},{"id":"http://arxiv.org/abs/2409.19913v2","updated":"2024-10-02T17:03:25Z","published":"2024-09-30T03:32:02Z","title":"Scaling Optimal LR Across Token Horizons","summary":"  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.\n","authors":["Johan Bjorck","Alon Benhaim","Vishrav Chaudhary","Furu Wei","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2409.19913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19085v2","updated":"2024-10-02T16:54:33Z","published":"2024-02-29T12:12:30Z","title":"Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment","summary":"  Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nPareto improvements in multi-objective alignment.\n","authors":["Yiju Guo","Ganqu Cui","Lifan Yuan","Ning Ding","Zexu Sun","Bowen Sun","Huimin Chen","Ruobing Xie","Jie Zhou","Yankai Lin","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19085v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2312.15561v4","updated":"2024-10-02T16:52:30Z","published":"2023-12-24T23:01:00Z","title":"README: Bridging Medical Jargon and Lay Understanding for Patient\n  Education through Data-Centric NLP","summary":"  The advancement in healthcare has shifted focus toward patient-centric\napproaches, particularly in self-care and patient education, facilitated by\naccess to Electronic Health Records (EHR). However, medical jargon in EHRs\nposes significant challenges in patient comprehension. To address this, we\nintroduce a new task of automatically generating lay definitions, aiming to\nsimplify complex medical terms into patient-friendly lay language. We first\ncreated the README dataset, an extensive collection of over 50,000 unique\n(medical term, lay definition) pairs and 300,000 mentions, each offering\ncontext-aware lay definitions manually annotated by domain experts. We have\nalso engineered a data-centric Human-AI pipeline that synergizes data\nfiltering, augmentation, and selection to improve data quality. We then used\nREADME as the training data for models and leveraged a Retrieval-Augmented\nGeneration method to reduce hallucinations and improve the quality of model\noutputs. Our extensive automatic and human evaluations demonstrate that\nopen-source mobile-friendly models, when fine-tuned with high-quality data, are\ncapable of matching or even surpassing the performance of state-of-the-art\nclosed-source large language models like ChatGPT. This research represents a\nsignificant stride in closing the knowledge gap in patient education and\nadvancing patient-centric healthcare solutions.\n","authors":["Zonghai Yao","Nandyala Siddharth Kantu","Guanghao Wei","Hieu Tran","Zhangqi Duan","Sunjae Kwon","Zhichao Yang","README annotation team","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.15561v4.pdf","comment":"To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01736v1","updated":"2024-10-02T16:47:35Z","published":"2024-10-02T16:47:35Z","title":"Recursive Abstractive Processing for Retrieval in Dynamic Datasets","summary":"  Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.\n","authors":["Charbel Chucri","Rami Azouz","Joachim Ott"],"pdf_url":"https://arxiv.org/pdf/2410.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10490v2","updated":"2024-10-02T16:47:30Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10882v5","updated":"2024-10-02T16:46:54Z","published":"2024-06-16T10:10:37Z","title":"SCAR: Efficient Instruction-Tuning for Large Language Models via Style\n  Consistency-Aware Response Ranking","summary":"  Recent studies have shown that maintaining a consistent response style by\nhuman experts and enhancing data quality in training sets can significantly\nimprove the performance of fine-tuned Large Language Models (LLMs) while\nreducing the number of training examples needed. However, the precise\ndefinition of style and the relationship between style, data quality, and LLM\nperformance remains unclear. This research identifies two key stylistic\nelements in responses: linguistic form and semantic surprisal. We find that,\namong training data of comparable quality, higher consistency in these response\nelements leads to better LLM performance. Inspired by this, we introduce Style\nConsistency-Aware Response Ranking (SCAR), which automatically prioritizes\ninstruction-response pairs in the training set based on their response\nstylistic consistency. By selecting the most style-consistent examples,\nsometimes as few as 0.7% of the full dataset, the fine-tuned LLMs can match or\neven surpass the performance of models trained on the entire dataset in coding\nand open-ended question-answering benchmarks. Code and data are available at\nhttps://github.com/zhuang-li/SCAR .\n","authors":["Zhuang Li","Yuncheng Hua","Thuy-Trang Vu","Haolan Zhan","Lizhen Qu","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.10882v5.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2410.01735v1","updated":"2024-10-02T16:46:38Z","published":"2024-10-02T16:46:38Z","title":"LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits","summary":"  Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.01735v1.pdf","comment":"20 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB"},{"id":"http://arxiv.org/abs/2410.01733v1","updated":"2024-10-02T16:46:01Z","published":"2024-10-02T16:46:01Z","title":"Visual Perception in Text Strings","summary":"  Understanding visual semantics embedded in consecutive characters is a\ncrucial capability for both large language models (LLMs) and multi-modal large\nlanguage models (MLLMs). This type of artifact possesses the unique\ncharacteristic that identical information can be readily formulated in both\ntexts and images, making them a significant proxy for analyzing modern LLMs'\nand MLLMs' capabilities in modality-agnostic vision understanding. In this\nwork, we select ASCII art as a representative artifact, where the lines and\nbrightness used to depict each concept are rendered by characters, and we frame\nthe problem as an ASCII art recognition task. We benchmark model performance on\nthis task by constructing an evaluation dataset with an elaborate\ncategorization tree and also collect a training set to elicit the models'\nvisual perception ability. Through a comprehensive analysis of dozens of\nmodels, results reveal that although humans can achieve nearly 100% accuracy,\nthe state-of-the-art LLMs and MLLMs lag far behind. Models are capable of\nrecognizing concepts depicted in the ASCII arts given only text inputs\nindicated by over 60% accuracy for some concepts, but most of them achieves\nmerely around 30% accuracy when averaged across all categories. When provided\nwith images as inputs, GPT-4o gets 82.68%, outperforming the strongest\nopen-source MLLM by 21.95%. Although models favor different kinds of ASCII art\ndepending on the modality provided, none of the MLLMs successfully benefit when\nboth modalities are supplied simultaneously. Moreover, supervised fine-tuning\nhelps improve models' accuracy especially when provided with the image\nmodality, but also highlights the need for better training techniques to\nenhance the information fusion among modalities.\n","authors":["Qi Jia","Xiang Yue","Shanshan Huang","Ziheng Qin","Yizhu Liu","Bill Yuchen Lin","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.01733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01731v1","updated":"2024-10-02T16:43:24Z","published":"2024-10-02T16:43:24Z","title":"ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation","summary":"  The practical use of text-to-image generation has evolved from simple,\nmonolithic models to complex workflows that combine multiple specialized\ncomponents. While workflow-based approaches can lead to improved image quality,\ncrafting effective workflows requires significant expertise, owing to the large\nnumber of available components, their complex inter-dependence, and their\ndependence on the generation prompt. Here, we introduce the novel task of\nprompt-adaptive workflow generation, where the goal is to automatically tailor\na workflow to each user prompt. We propose two LLM-based approaches to tackle\nthis task: a tuning-based method that learns from user-preference data, and a\ntraining-free method that uses the LLM to select existing flows. Both\napproaches lead to improved image quality when compared to monolithic models or\ngeneric, prompt-independent workflows. Our work shows that prompt-dependent\nflow prediction offers a new pathway to improving text-to-image generation\nquality, complementing existing research directions in the field.\n","authors":["Rinon Gal","Adi Haviv","Yuval Alaluf","Amit H. Bermano","Daniel Cohen-Or","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2410.01731v1.pdf","comment":"Project website: https://comfygen-paper.github.io/"},{"id":"http://arxiv.org/abs/2410.01729v1","updated":"2024-10-02T16:39:58Z","published":"2024-10-02T16:39:58Z","title":"Evaluating Robustness of Reward Models for Mathematical Reasoning","summary":"  Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.\n","authors":["Sunghwan Kim","Dongjin Kang","Taeyoon Kwon","Hyungjoo Chae","Jungsoo Won","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.01729v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01727v1","updated":"2024-10-02T16:37:19Z","published":"2024-10-02T16:37:19Z","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","summary":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.01727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01724v1","updated":"2024-10-02T16:34:40Z","published":"2024-10-02T16:34:40Z","title":"Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for\n  Enhanced Batch Prompting","summary":"  Batch prompting is a common technique in large language models (LLMs) used to\nprocess multiple inputs simultaneously, aiming to improve computational\nefficiency. However, as batch sizes increase, performance degradation often\noccurs due to the model's difficulty in handling lengthy context inputs.\nExisting methods that attempt to mitigate these issues rely solely on batch\ndata arrangement and majority voting rather than improving the design of the\nbatch prompt itself. In this paper, we address these limitations by proposing\n\"Auto-Demo Prompting,\" a novel approach that leverages the question-output\npairs from earlier questions within a batch as demonstrations for subsequent\nanswer inference. We provide a formal theoretical analysis of how Auto-Demo\nPrompting functions within the autoregressive generation process of LLMs,\nillustrating how it utilizes prior outputs to optimize the model's internal\nrepresentations. Our method effectively bridges the gap between batch prompting\nand few-shot prompting, enhancing performance with only a slight compromise in\ntoken usage. Experimental results across five NLP tasks demonstrate its\neffectiveness in mitigating performance degradation and occasionally\noutperforming single prompts. Furthermore, it opens new avenues for applying\nfew-shot learning techniques, such as demonstration selection, within batch\nprompting, making it a robust solution for real-world applications.\n","authors":["Longyu Feng","Mengze Hong","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01720v1","updated":"2024-10-02T16:32:05Z","published":"2024-10-02T16:32:05Z","title":"Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective","summary":"  Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open source\nour code through an anonymous GitHub repository at\nhttps://anonymous.4open.science/r/Understanding-Synthetic.\n","authors":["Zeyu Gan","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00757v2","updated":"2024-10-02T16:30:34Z","published":"2024-01-01T13:53:53Z","title":"LogicAsker: Evaluating and Improving the Logical Reasoning Ability of\n  Large Language Models","summary":"  We introduce LogicAsker, a novel approach for evaluating and enhancing the\nlogical reasoning capabilities of large language models (LLMs) such as ChatGPT\nand GPT-4. Despite LLMs' prowess in tasks like writing assistance, code\ngeneration, and machine translation, assessing their ability to reason has been\nchallenging. Traditional evaluations often prioritize accuracy on downstream\ntasks over direct assessments of reasoning processes. LogicAsker addresses this\ngap by employing a set of atomic reasoning skills grounded in propositional and\npredicate logic to systematically examine and improve the reasoning prowess of\nLLMs. Our methodology reveals significant gaps in LLMs' learning of logical\nrules, with identified reasoning failures ranging from 29\\% to 90\\% across\ndifferent models. Moreover, we leverage these findings to construct targeted\ndemonstration examples and fine-tune data, notably enhancing logical reasoning\nin models like GPT-4o by up to 5\\%. To our knowledge, this is the first effort\nto utilize test case outcomes to effectively refine LLMs' formal reasoning\ncapabilities. We make our code, data, and results publicly available\n(https://github.com/yxwan123/LogicAsker) to facilitate further research and\nreplication of our findings.\n","authors":["Yuxuan Wan","Wenxuan Wang","Yiliu Yang","Youliang Yuan","Jen-tse Huang","Pinjia He","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00757v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.19653v2","updated":"2024-10-02T16:23:12Z","published":"2024-05-30T03:12:04Z","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems","summary":"  Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call \"system captions\" or\nSysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.\n","authors":["Patrick Emami","Zhaonan Li","Saumya Sinha","Truc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2405.19653v2.pdf","comment":"21 pages. Under review"},{"id":"http://arxiv.org/abs/2410.01708v1","updated":"2024-10-02T16:16:02Z","published":"2024-10-02T16:16:02Z","title":"Examining the Role of Relationship Alignment in Large Language Models","summary":"  The rapid development and deployment of Generative AI in social settings\nraise important questions about how to optimally personalize them for users\nwhile maintaining accuracy and realism. Based on a Facebook public post-comment\ndataset, this study evaluates the ability of Llama 3.0 (70B) to predict the\nsemantic tones across different combinations of a commenter's and poster's\ngender, age, and friendship closeness and to replicate these differences in\nLLM-generated comments.\n  The study consists of two parts: Part I assesses differences in semantic\ntones across social relationship categories, and Part II examines the\nsimilarity between comments generated by Llama 3.0 (70B) and human comments\nfrom Part I given public Facebook posts as input. Part I results show that\nincluding social relationship information improves the ability of a model to\npredict the semantic tone of human comments. However, Part II results show that\neven without including social context information in the prompt, LLM-generated\ncomments and human comments are equally sensitive to social context, suggesting\nthat LLMs can comprehend semantics from the original post alone. When we\ninclude all social relationship information in the prompt, the similarity\nbetween human comments and LLM-generated comments decreases. This inconsistency\nmay occur because LLMs did not include social context information as part of\ntheir training data. Together these results demonstrate the ability of LLMs to\ncomprehend semantics from the original post and respond similarly to human\ncomments, but also highlights their limitations in generalizing personalized\ncomments through prompting alone.\n","authors":["Kristen M. Altenburger","Hongda Jiang","Robert E. Kraut","Yi-Chia Wang","Jane Dwivedi-Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01707v1","updated":"2024-10-02T16:15:31Z","published":"2024-10-02T16:15:31Z","title":"Interpretable Contrastive Monte Carlo Tree Search Reasoning","summary":"  We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning\nalgorithm for Large Language Models (LLMs), significantly improves both\nreasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM\nreasoning works often overlooked its biggest drawback--slower speed compared to\nCoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on\nvarious tasks with limited quantitative analysis or ablation studies of its\ncomponents from reasoning interpretability perspective. 3. The reward model is\nthe most crucial component in MCTS, however previous work has rarely conducted\nin-depth study or improvement of MCTS's reward models. Thus, we conducted\nextensive ablation studies and quantitative analysis on components of MCTS,\nrevealing the impact of each component on the MCTS reasoning performance of\nLLMs. Building on this, (i) we designed a highly interpretable reward model\nbased on the principle of contrastive decoding and (ii) achieved an average\nspeed improvement of 51.9% per node using speculative decoding. Additionally,\n(iii) we improved UCT node selection strategy and backpropagation used in\nprevious works, resulting in significant performance improvement. We\noutperformed o1-mini by an average of 17.4% on the Blocksworld multi-step\nreasoning dataset using Llama-3.1-70B with SC-MCTS*.\n","authors":["Zitian Gao","Boye Niu","Xuzheng He","Haotian Xu","Hongzhang Liu","Aiwei Liu","Xuming Hu","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2410.01707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01704v1","updated":"2024-10-02T16:15:04Z","published":"2024-10-02T16:15:04Z","title":"An Exploration of Self-Supervised Mutual Information Alignment for\n  Multi-Task Settings","summary":"  There is a growing need for pluralistic alignment methods that can steer\nlanguage models towards individual attributes and preferences. One such method,\nSelf-Supervised Alignment with Mutual Information (SAMI), uses conditional\nmutual information to encourage the connection between behavioral preferences\nand model responses. We conduct two experiments exploring SAMI in multi-task\nsettings. First, we compare SAMI to Direct Preference Optimization (DPO) on a\nmulti-task benchmark (MT-Bench), using a stronger model to generate training\ndata for a weaker one across diverse categories (humanities, STEM, extraction,\ncoding, math, reasoning, and roleplay). Our results indicate that one iteration\nof SAMI has a 57% win rate against DPO, with significant variation in\nperformance between task categories. Second, we examine SAMI's impact on\nmathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While\nSAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2%\nboost. However, SAMI shows interesting scaling trends. When given 10 attempts,\nSAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining\nSAMI with SFT yields an additional improvement of 1.3% in multi-attempt\nsettings, though single-attempt accuracy remains unchanged.\n","authors":["Soham Govande"],"pdf_url":"https://arxiv.org/pdf/2410.01704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09722v2","updated":"2024-10-02T16:14:09Z","published":"2024-07-12T23:29:54Z","title":"Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM\n  Inference","summary":"  Large language models (LLMs) have achieved remarkable success across diverse\ntasks, yet their inference processes are hindered by substantial time and\nenergy demands due to single-token generation at each decoding step. While\nprevious methods such as speculative decoding mitigate these inefficiencies by\nproducing multiple tokens per step, each token is still generated by its\nsingle-token distribution, thereby enhancing speed without improving\neffectiveness. In contrast, our work simultaneously enhances inference speed\nand improves the output effectiveness. We consider multi-token joint decoding\n(MTJD), which generates multiple tokens from their joint distribution at each\niteration, theoretically reducing perplexity and enhancing task performance.\nHowever, MTJD suffers from the high cost of sampling from the joint\ndistribution of multiple tokens. Inspired by speculative decoding, we introduce\nmulti-token assisted decoding (MTAD), a novel framework designed to accelerate\nMTJD. MTAD leverages a smaller auxiliary model to approximate the joint\ndistribution of a larger model, incorporating a verification mechanism that not\nonly ensures the accuracy of this approximation, but also improves the decoding\nefficiency over conventional speculative decoding. Theoretically, we\ndemonstrate that MTAD closely approximates exact MTJD with bounded error.\nEmpirical evaluations using Llama-2 and OPT models ranging from 13B to 70B\nparameters across various tasks reveal that MTAD reduces perplexity by 21.2%\nand improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than\nconventional speculative decoding methods. These results highlight MTAD's\nability to make multi-token joint decoding both effective and efficient,\npromoting more sustainable and high-performance deployment of LLMs.\n","authors":["Zongyue Qin","Ziniu Hu","Zifan He","Neha Prakriya","Jason Cong","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01696v1","updated":"2024-10-02T16:05:01Z","published":"2024-10-02T16:05:01Z","title":"CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving\n  Long-Range Reasoning Problems using LLMs","summary":"  Large language models (LLMs) have demonstrated limitations in handling\ncombinatorial optimization problems involving long-range reasoning, partially\ndue to causal hallucinations and huge search space. As for causal\nhallucinations, i.e., the inconsistency between reasoning and corresponding\nstate transition, this paper introduces the Causal Relationship Enhancement\n(CRE) mechanism combining cause-effect interventions and the Individual\nTreatment Effect (ITE) to guarantee the solid causal rightness between each\nstep of reasoning and state transition. As for the long causal range and huge\nsearch space limiting the performances of existing models featuring\nsingle-direction search, a Dual-End Searching (DES) approach is proposed to\nseek solutions by simultaneously starting from both the initial and goal states\non the causal probability tree. By integrating CRE and DES (CreDes), our model\nhas realized simultaneous multi-step reasoning, circumventing the\ninefficiencies from cascading multiple one-step reasoning like the\nChain-of-Thought (CoT). Experiments demonstrate that CreDes significantly\noutperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning\ntasks in terms of both accuracy and time efficiency.\n","authors":["Kangsheng Wang","Xiao Zhang","Hao Liu","Songde Han","Huimin Ma","Tianyu Hu"],"pdf_url":"https://arxiv.org/pdf/2410.01696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01692v1","updated":"2024-10-02T16:03:49Z","published":"2024-10-02T16:03:49Z","title":"U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models","summary":"  Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where performance seems to stagnate at first and then\nimprove sharply and unpredictably with scale beyond a threshold. By dividing\nquestions in the datasets according to difficulty level by average performance,\nwe observe U-shaped scaling for hard questions, and inverted-U scaling followed\nby steady improvement for easy questions. Moreover, the emergence threshold\nroughly coincides with the point at which performance on easy questions reverts\nfrom inverse scaling to standard scaling. Capitalizing on the observable though\nopposing scaling trend on easy and hard questions, we propose a simple yet\neffective pipeline, called Slice-and-Sandwich, to predict both the emergence\nthreshold and model performance beyond the threshold.\n","authors":["Tung-Yu Wu","Pei-Yu Lo"],"pdf_url":"https://arxiv.org/pdf/2410.01692v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.01691v1","updated":"2024-10-02T16:03:13Z","published":"2024-10-02T16:03:13Z","title":"FactAlign: Long-form Factuality Alignment of Large Language Models","summary":"  Large language models have demonstrated significant potential as the\nnext-generation information access engines. However, their reliability is\nhindered by issues of hallucination and generating non-factual content. This is\nparticularly problematic in long-form responses, where assessing and ensuring\nfactual accuracy is complex. In this paper, we address this gap by proposing\nFactAlign, a novel alignment framework designed to enhance the factuality of\nLLMs' long-form responses while maintaining their helpfulness. We introduce\nfKTO, a fine-grained, sentence-level alignment algorithm that extends the\nKahneman-Tversky Optimization (KTO) alignment method. Leveraging recent\nadvances in automatic factuality evaluation, FactAlign utilizes fine-grained\nfactuality assessments to guide the alignment process. Our experiments on\nopen-domain prompts and information-seeking questions demonstrate that\nFactAlign significantly improves the factual accuracy of LLM responses while\nalso improving their helpfulness. Further analyses identify that FactAlign is\ncapable of training LLMs to provide more information without losing factual\nprecision, thus improving the factual F1 score. Our source code, datasets, and\ntrained models are publicly available at https://github.com/MiuLab/FactAlign\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01691v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.03807v2","updated":"2024-10-02T16:00:39Z","published":"2024-06-06T07:30:14Z","title":"Tool-Planner: Task Planning with Clusters across Multiple Tools","summary":"  Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\n\\url{https://github.com/OceannTwT/Tool-Planner}\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yuwei Zhang","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2406.03807v2.pdf","comment":"48pages second version"},{"id":"http://arxiv.org/abs/2410.01679v1","updated":"2024-10-02T15:49:30Z","published":"2024-10-02T15:49:30Z","title":"VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment","summary":"  Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.\n","authors":["Amirhossein Kazemnejad","Milad Aghajohari","Eva Portelance","Alessandro Sordoni","Siva Reddy","Aaron Courville","Nicolas Le Roux"],"pdf_url":"https://arxiv.org/pdf/2410.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19597v2","updated":"2024-10-02T15:47:40Z","published":"2024-04-30T14:43:57Z","title":"TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning","summary":"  The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.\n","authors":["Xuanli He","Jun Wang","Qiongkai Xu","Pasquale Minervini","Pontus Stenetorp","Benjamin I. P. Rubinstein","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2404.19597v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.01675v1","updated":"2024-10-02T15:46:40Z","published":"2024-10-02T15:46:40Z","title":"Trying to be human: Linguistic traces of stochastic empathy in language\n  models","summary":"  Differentiating between generated and human-written content is important for\nnavigating the modern world. Large language models (LLMs) are crucial drivers\nbehind the increased quality of computer-generated content. Reportedly, humans\nfind it increasingly difficult to identify whether an AI model generated a\npiece of text. Our work tests how two important factors contribute to the human\nvs AI race: empathy and an incentive to appear human. We address both aspects\nin two experiments: human participants and a state-of-the-art LLM wrote\nrelationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610),\neither instructed to be as human as possible or not. New samples of humans\n(n=428 and n=408) then judged the texts' source. Our findings show that when\nempathy is required, humans excel. Contrary to expectations, instructions to\nappear human were only effective for the LLM, so the human advantage\ndiminished. Computational text analysis revealed that LLMs become more human\nbecause they may have an implicit representation of what makes a text human and\neffortlessly apply these heuristics. The model resorts to a conversational,\nself-referential, informal tone with a simpler vocabulary to mimic stochastic\nempathy. We discuss these findings in light of recent claims on the on-par\nperformance of LLMs.\n","authors":["Bennett Kleinberg","Jari Zegers","Jonas Festor","Stefana Vida","Julian Prsent","Riccardo Loconte","Sanne Peereboom"],"pdf_url":"https://arxiv.org/pdf/2410.01675v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.01671v1","updated":"2024-10-02T15:39:55Z","published":"2024-10-02T15:39:55Z","title":"Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding","summary":"  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering.\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yanxin Shen","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2410.01671v1.pdf","comment":"Underreview version of LQCA, Bridge context gap for long context"},{"id":"http://arxiv.org/abs/2410.00907v2","updated":"2024-10-02T15:34:12Z","published":"2024-10-01T17:53:28Z","title":"Addition is All You Need for Energy-efficient Language Models","summary":"  Large neural networks spend most computation on floating point tensor\nmultiplications. In this work, we find that a floating point multiplier can be\napproximated by one integer adder with high precision. We propose the\nlinear-complexity multiplication L-Mul algorithm that approximates floating\npoint number multiplication with integer addition operations. The new algorithm\ncosts significantly less computation resource than 8-bit floating point\nmultiplication but achieves higher precision. Compared to 8-bit floating point\nmultiplications, the proposed method achieves higher precision but consumes\nsignificantly less bit-level computation. Since multiplying floating point\nnumbers requires substantially higher energy compared to integer addition\noperations, applying the L-Mul operation in tensor processing hardware can\npotentially reduce 95% energy cost by element-wise floating point tensor\nmultiplications and 80% energy cost of dot products. We calculated the\ntheoretical error expectation of L-Mul, and evaluated the algorithm on a wide\nrange of textual, visual, and symbolic tasks, including natural language\nunderstanding, structural reasoning, mathematics, and commonsense question\nanswering. Our numerical analysis experiments agree with the theoretical error\nestimation, which indicates that L-Mul with 4-bit mantissa achieves comparable\nprecision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa\noutperforms float8_e5m2. Evaluation results on popular benchmarks show that\ndirectly applying L-Mul to the attention mechanism is almost lossless. We\nfurther show that replacing all floating point multiplications with 3-bit\nmantissa L-Mul in a transformer model achieves equivalent precision as using\nfloat8_e4m3 as accumulation precision in both fine-tuning and inference.\n","authors":["Hongyin Luo","Wei Sun"],"pdf_url":"https://arxiv.org/pdf/2410.00907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04222v4","updated":"2024-10-02T15:27:36Z","published":"2024-02-06T18:29:39Z","title":"What is \"Typological Diversity\" in NLP?","summary":"  The NLP research community has devoted increased attention to languages\nbeyond English, resulting in considerable improvements for multilingual NLP.\nHowever, these improvements only apply to a small subset of the world's\nlanguages. Aiming to extend this, an increasing number of papers aspires to\nenhance generalizable multilingual performance across languages. To this end,\nlinguistic typology is commonly used to motivate language selection, on the\nbasis that a broad typological sample ought to imply generalization across a\nbroad range of languages. These selections are often described as being\n'typologically diverse'. In this work, we systematically investigate NLP\nresearch that includes claims regarding 'typological diversity'. We find there\nare no set definitions or criteria for such claims. We introduce metrics to\napproximate the diversity of language selection along several axes and find\nthat the results vary considerably across papers. Crucially, we show that\nskewed language selection can lead to overestimated multilingual performance.\nWe recommend future work to include an operationalization of 'typological\ndiversity' that empirically justifies the diversity of language samples.\n","authors":["Esther Ploeger","Wessel Poelman","Miryam de Lhoneux","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2402.04222v4.pdf","comment":"EMNLP 2024: Main Conference"},{"id":"http://arxiv.org/abs/2408.00118v3","updated":"2024-10-02T15:22:49Z","published":"2024-07-31T19:13:07Z","title":"Gemma 2: Improving Open Language Models at a Practical Size","summary":"  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n","authors":[" Gemma Team","Morgane Riviere","Shreya Pathak","Pier Giuseppe Sessa","Cassidy Hardin","Surya Bhupatiraju","Lonard Hussenot","Thomas Mesnard","Bobak Shahriari","Alexandre Ram","Johan Ferret","Peter Liu","Pouya Tafti","Abe Friesen","Michelle Casbon","Sabela Ramos","Ravin Kumar","Charline Le Lan","Sammy Jerome","Anton Tsitsulin","Nino Vieillard","Piotr Stanczyk","Sertan Girgin","Nikola Momchev","Matt Hoffman","Shantanu Thakoor","Jean-Bastien Grill","Behnam Neyshabur","Olivier Bachem","Alanna Walton","Aliaksei Severyn","Alicia Parrish","Aliya Ahmad","Allen Hutchison","Alvin Abdagic","Amanda Carl","Amy Shen","Andy Brock","Andy Coenen","Anthony Laforge","Antonia Paterson","Ben Bastian","Bilal Piot","Bo Wu","Brandon Royal","Charlie Chen","Chintu Kumar","Chris Perry","Chris Welty","Christopher A. Choquette-Choo","Danila Sinopalnikov","David Weinberger","Dimple Vijaykumar","Dominika Rogoziska","Dustin Herbison","Elisa Bandy","Emma Wang","Eric Noland","Erica Moreira","Evan Senter","Evgenii Eltyshev","Francesco Visin","Gabriel Rasskin","Gary Wei","Glenn Cameron","Gus Martins","Hadi Hashemi","Hanna Klimczak-Pluciska","Harleen Batra","Harsh Dhand","Ivan Nardini","Jacinda Mein","Jack Zhou","James Svensson","Jeff Stanway","Jetha Chan","Jin Peng Zhou","Joana Carrasqueira","Joana Iljazi","Jocelyn Becker","Joe Fernandez","Joost van Amersfoort","Josh Gordon","Josh Lipschultz","Josh Newlan","Ju-yeong Ji","Kareem Mohamed","Kartikeya Badola","Kat Black","Katie Millican","Keelin McDonell","Kelvin Nguyen","Kiranbir Sodhia","Kish Greene","Lars Lowe Sjoesund","Lauren Usui","Laurent Sifre","Lena Heuermann","Leticia Lago","Lilly McNealus","Livio Baldini Soares","Logan Kilpatrick","Lucas Dixon","Luciano Martins","Machel Reid","Manvinder Singh","Mark Iverson","Martin Grner","Mat Velloso","Mateo Wirth","Matt Davidow","Matt Miller","Matthew Rahtz","Matthew Watson","Meg Risdal","Mehran Kazemi","Michael Moynihan","Ming Zhang","Minsuk Kahng","Minwoo Park","Mofi Rahman","Mohit Khatwani","Natalie Dao","Nenshad Bardoliwalla","Nesh Devanathan","Neta Dumai","Nilay Chauhan","Oscar Wahltinez","Pankil Botarda","Parker Barnes","Paul Barham","Paul Michel","Pengchong Jin","Petko Georgiev","Phil Culliton","Pradeep Kuppala","Ramona Comanescu","Ramona Merhej","Reena Jana","Reza Ardeshir Rokni","Rishabh Agarwal","Ryan Mullins","Samaneh Saadat","Sara Mc Carthy","Sarah Cogan","Sarah Perrin","Sbastien M. R. Arnold","Sebastian Krause","Shengyang Dai","Shruti Garg","Shruti Sheth","Sue Ronstrom","Susan Chan","Timothy Jordan","Ting Yu","Tom Eccles","Tom Hennigan","Tomas Kocisky","Tulsee Doshi","Vihan Jain","Vikas Yadav","Vilobh Meshram","Vishal Dharmadhikari","Warren Barkley","Wei Wei","Wenming Ye","Woohyun Han","Woosuk Kwon","Xiang Xu","Zhe Shen","Zhitao Gong","Zichuan Wei","Victor Cotruta","Phoebe Kirk","Anand Rao","Minh Giang","Ludovic Peran","Tris Warkentin","Eli Collins","Joelle Barral","Zoubin Ghahramani","Raia Hadsell","D. Sculley","Jeanine Banks","Anca Dragan","Slav Petrov","Oriol Vinyals","Jeff Dean","Demis Hassabis","Koray Kavukcuoglu","Clement Farabet","Elena Buchatskaya","Sebastian Borgeaud","Noah Fiedel","Armand Joulin","Kathleen Kenealy","Robert Dadashi","Alek Andreev"],"pdf_url":"https://arxiv.org/pdf/2408.00118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01651v1","updated":"2024-10-02T15:18:34Z","published":"2024-10-02T15:18:34Z","title":"Efficient Long-range Language Modeling with Self-supervised Causal\n  Retrieval","summary":"  Recently, retrieval-based language models (RLMs) have received much\nattention. However, most of them leverage a pre-trained retriever with fixed\nparameters, which may not adapt well to causal language models. In this work,\nwe propose Grouped Cross-Attention, a novel module enabling joint pre-training\nof the retriever and causal LM, and apply it to long-context modeling. For a\ngiven input sequence, we split it into chunks and use the current chunk to\nretrieve past chunks for subsequent text generation. Our innovation allows the\nretriever to learn how to retrieve past chunks that better minimize the\nauto-regressive loss of subsequent tokens in an end-to-end manner. By\nintegrating top-$k$ retrieval, our model can be pre-trained efficiently from\nscratch with context lengths up to 64K tokens. Our experiments show our model,\ncompared with long-range LM baselines, can achieve lower perplexity with\ncomparable or lower pre-training and inference costs.\n","authors":["Xiang Hu","Zhihao Teng","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.01651v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2409.14302v2","updated":"2024-10-02T15:17:35Z","published":"2024-09-22T03:13:38Z","title":"Reliable and diverse evaluation of LLM medical knowledge mastery","summary":"  Mastering medical knowledge is crucial for medical-specific LLMs. However,\ndespite the existence of medical benchmarks like MedQA, a unified framework\nthat fully leverages existing knowledge bases to evaluate LLMs' mastery of\nmedical knowledge is still lacking. In the study, we propose a novel framework\nPretexEval that dynamically generates reliable and diverse test samples to\nevaluate LLMs for any given medical knowledge base. We notice that test samples\nproduced directly from knowledge bases by templates or LLMs may introduce\nfactual errors and also lack diversity. To address these issues, we introduce a\nnovel schema into our proposed evaluation framework that employs predicate\nequivalence transformations to produce a series of variants for any given\nmedical knowledge point. Finally, these produced predicate variants are\nconverted into textual language, resulting in a series of reliable and diverse\ntest samples to evaluate whether LLMs fully master the given medical factual\nknowledge point. Here, we use our proposed framework to systematically\ninvestigate the mastery of medical factual knowledge of 12 well-known LLMs,\nbased on two knowledge bases that are crucial for clinical diagnosis and\ntreatment. The evaluation results illustrate that current LLMs still exhibit\nsignificant deficiencies in fully mastering medical knowledge, despite\nachieving considerable success on some famous public benchmarks. These new\nfindings provide valuable insights for developing medical-specific LLMs,\nhighlighting that current LLMs urgently need to strengthen their comprehensive\nand in-depth mastery of medical knowledge before being applied to real-world\nmedical scenarios.\n","authors":["Yuxuan Zhou","Xien Liu","Chen Ning","Xiao Zhang","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2409.14302v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.01648v1","updated":"2024-10-02T15:16:02Z","published":"2024-10-02T15:16:02Z","title":"DeIDClinic: A Multi-Layered Framework for De-identification of Clinical\n  Free-text Data","summary":"  De-identification is important in protecting patients' privacy for healthcare\ntext analytics. The MASK framework is one of the best on the de-identification\nshared task organised by n2c2/i2b2 challenges. This work enhances the MASK\nframework by integrating ClinicalBERT, a deep learning model specifically\nfine-tuned on clinical texts, alongside traditional de-identification methods\nlike dictionary lookup and rule-based approaches. The system effectively\nidentifies and either redacts or replaces sensitive identifiable entities\nwithin clinical documents, while also allowing users to customise the masked\ndocuments according to their specific needs. The integration of ClinicalBERT\nsignificantly improves the performance of entity recognition, achieving 0.9732\nF1-score, especially for common entities such as names, dates, and locations.\n  A risk assessment feature has also been developed, which analyses the\nuniqueness of context within documents to classify them into risk levels,\nguiding further de-identification efforts. While the system demonstrates strong\noverall performance, this work highlights areas for future improvement,\nincluding handling more complex entity occurrences and enhancing the system's\nadaptability to different clinical settings.\n","authors":["Angel Paul","Dhivin Shaji","Lifeng Han","Warren Del-Pinto","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2410.01648v1.pdf","comment":"ongoing work"},{"id":"http://arxiv.org/abs/2410.01637v1","updated":"2024-10-02T15:08:12Z","published":"2024-10-02T15:08:12Z","title":"On The Adaptation of Unlimiformer for Decoder-Only Transformers","summary":"  One of the prominent issues stifling the current generation of large language\nmodels is their limited context length. Recent proprietary models such as GPT-4\nand Claude 2 have introduced longer context lengths, 8k/32k and 100k,\nrespectively; however, despite the efforts in the community, most common\nmodels, such as LLama-2, have a context length of 4k or less. Unlimiformer\n(Bertsch et al., 2023) is a recently popular vector-retrieval augmentation\nmethod that offloads cross-attention computations to a kNN index. However, its\nmain limitation is incompatibility with decoder-only transformers out of the\nbox. In this work, we explore practical considerations of adapting Unlimiformer\nto decoder-only transformers and introduce a series of modifications to\novercome this limitation. Moreover, we expand the original experimental setup\non summarization to include a new task (i.e., free-form Q&A) and an\ninstruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase\nthe effectiveness of these modifications on summarization, performing on par\nwith a model with 2x the context length. Moreover, we discuss limitations and\nfuture directions for free-form Q&A and instruction-tuned models.\n","authors":["Kian Ahrabian","Alon Benhaim","Barun Patra","Jay Pujara","Saksham Singhal","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2410.01637v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.04701v2","updated":"2024-10-02T15:07:09Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.\n","authors":["Michael Gnther","Isabelle Mohr","Daniel James Williams","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v2.pdf","comment":"11 pages, 3rd draft"},{"id":"http://arxiv.org/abs/2309.12934v3","updated":"2024-10-02T15:04:59Z","published":"2023-09-22T15:32:49Z","title":"TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with\n  Diverse Writing Styles","summary":"  Recent advances in Large Language Models (LLMs) have enabled the generation\nof open-ended high-quality texts, that are non-trivial to distinguish from\nhuman-written texts. We refer to such LLM-generated texts as deepfake texts.\nThere are currently over 72K text generation models in the huggingface model\nrepo. As such, users with malicious intent can easily use these open-sourced\nLLMs to generate harmful texts and dis/misinformation at scale. To mitigate\nthis problem, a computational method to determine if a given text is a deepfake\ntext or not is desired--i.e., Turing Test (TT). In particular, in this work, we\ninvestigate the more general version of the problem, known as Authorship\nAttribution (AA), in a multi-class setting--i.e., not only determining if a\ngiven text is a deepfake text or not but also being able to pinpoint which LLM\nis the author. We propose TopFormer to improve existing AA solutions by\ncapturing more linguistic patterns in deepfake texts by including a Topological\nData Analysis (TDA) layer in the Transformer-based model. We show the benefits\nof having a TDA layer when dealing with imbalanced, and multi-style datasets,\nby extracting TDA features from the reshaped $pooled\\_output$ of our backbone\nas input. This Transformer-based model captures contextual representations\n(i.e., semantic and syntactic linguistic features), while TDA captures the\nshape and structure of data (i.e., linguistic structures). Finally, TopFormer,\noutperforms all baselines in all 3 datasets, achieving up to 7\\% increase in\nMacro F1 score. Our code and datasets are available at:\nhttps://github.com/AdaUchendu/topformer\n","authors":["Adaku Uchendu","Thai Le","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2309.12934v3.pdf","comment":"Accepted at The 27th European Conference on Artificial Intelligence\n  (ECAI 2024)"},{"id":"http://arxiv.org/abs/2410.01633v1","updated":"2024-10-02T15:04:21Z","published":"2024-10-02T15:04:21Z","title":"A Thematic Framework for Analyzing Large-scale Self-reported Social\n  Media Data on Opioid Use Disorder Treatment Using Buprenorphine Product","summary":"  Background: One of the key FDA-approved medications for Opioid Use Disorder\n(OUD) is buprenorphine. Despite its popularity, individuals often report\nvarious information needs regarding buprenorphine treatment on social media\nplatforms like Reddit. However, the key challenge is to characterize these\nneeds. In this study, we propose a theme-based framework to curate and analyze\nlarge-scale data from social media to characterize self-reported treatment\ninformation needs (TINs).\n  Methods: We collected 15,253 posts from r/Suboxone, one of the largest Reddit\nsub-community for buprenorphine products. Following the standard protocol, we\nfirst identified and defined five main themes from the data and then coded\n6,000 posts based on these themes, where one post can be labeled with\napplicable one to three themes. Finally, we determined the most frequently\nappearing sub-themes (topics) for each theme by analyzing samples from each\ngroup.\n  Results: Among the 6,000 posts, 40.3% contained a single theme, 36% two\nthemes, and 13.9% three themes. The most frequent topics for each theme or\ntheme combination came with several key findings - prevalent reporting of\npsychological and physical effects during recovery, complexities in accessing\nbuprenorphine, and significant information gaps regarding medication\nadministration, tapering, and usage of substances during different stages of\nrecovery. Moreover, self-treatment strategies and peer-driven advice reveal\nvaluable insights and potential misconceptions.\n  Conclusions: The findings obtained using our proposed framework can inform\nbetter patient education and patient-provider communication, design systematic\ninterventions to address treatment-related misconceptions and rumors, and\nstreamline the generation of hypotheses for future research.\n","authors":["Madhusudan Basak","Omar Sharif","Sarah E. Lord","Jacob T. Borodovsky","Lisa A. Marsch","Sandra A. Springer","Edward Nunes","Charlie D. Brackett","Luke J. ArchiBald","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2410.01633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01627v1","updated":"2024-10-02T15:01:55Z","published":"2024-10-02T15:01:55Z","title":"Intent Detection in the Age of LLMs","summary":"  Intent detection is a critical component of task-oriented dialogue systems\n(TODS) which enables the identification of suitable actions to address user\nutterances at each dialog turn. Traditional approaches relied on\ncomputationally efficient supervised sentence transformer encoder models, which\nrequire substantial training data and struggle with out-of-scope (OOS)\ndetection. The emergence of generative large language models (LLMs) with\nintrinsic world knowledge presents new opportunities to address these\nchallenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context\nlearning and chain-of-thought prompting for intent detection, and compare their\nperformance with contrastively fine-tuned sentence transformer (SetFit) models\nto highlight prediction quality and latency tradeoff. We propose a hybrid\nsystem using uncertainty based routing strategy to combine the two approaches\nthat along with negative data augmentation results in achieving the best of\nboth worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To\nbetter understand LLM OOS detection capabilities, we perform controlled\nexperiments revealing that this capability is significantly influenced by the\nscope of intent labels and the size of the label space. We also introduce a\ntwo-step approach utilizing internal LLM representations, demonstrating\nempirical gains in OOS detection accuracy and F1-score by >5% for the\nMistral-7B model.\n","authors":["Gaurav Arora","Shreya Jain","Srujana Merugu"],"pdf_url":"https://arxiv.org/pdf/2410.01627v1.pdf","comment":"Accepted at EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01610v1","updated":"2024-10-02T14:48:22Z","published":"2024-10-02T14:48:22Z","title":"Upcycling Instruction Tuning from Dense to Mixture-of-Experts via\n  Parameter Merging","summary":"  Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training.\nIn this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient\napproach for tuning a dense pre-trained model into a MoE instruction model.\nSpecifically, we first point out that intermediate checkpoints during\ninstruction tuning of the dense model are naturally suitable for specialized\nexperts, and then propose an expert expansion stage to flexibly achieve models\nwith flexible numbers of experts, where genetic algorithm and parameter merging\nare introduced to ensure sufficient diversity of new extended experts. To\nensure that each specialized expert in the MoE model works as expected, we\nselect a small amount of seed data that each expert excels to pre-optimize the\nrouter. Extensive experiments with various data scales and upcycling settings\ndemonstrate the outstanding performance and data efficiency of UpIT, as well as\nstable improvement in expert or data scaling. Further analysis reveals the\nimportance of ensuring expert diversity in upcycling.\n","authors":["Tingfeng Hui","Zhenyu Zhang","Shuohuan Wang","Yu Sun","Hua Wu","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2410.01610v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2405.16869v2","updated":"2024-10-02T14:42:10Z","published":"2024-05-27T06:36:17Z","title":"Multiple Heads are Better than One: Mixture of Modality Knowledge\n  Experts for Entity Representation Learning","summary":"  Learning high-quality multi-modal entity representations is an important goal\nof multi-modal knowledge graph (MMKG) representation learning, which can\nenhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The\nmain challenge is to collaboratively model the structural information concealed\nin massive triples and the multi-modal features of the entities. Existing\nmethods focus on crafting elegant entity-wise multi-modal fusion strategies,\nyet they overlook the utilization of multi-perspective features concealed\nwithin the modalities under diverse relational contexts. To address this issue,\nwe introduce a novel framework with Mixture of Modality Knowledge experts\n(MoMoK for short) to learn adaptive multi-modal entity representations for\nbetter MMKGC. We design relation-guided modality knowledge experts to acquire\nrelation-aware modality embeddings and integrate the predictions from\nmulti-modalities to achieve joint decisions. Additionally, we disentangle the\nexperts by minimizing their mutual information. Experiments on four public MMKG\nbenchmarks demonstrate the outstanding performance of MoMoK under complex\nscenarios.\n","authors":["Yichi Zhang","Zhuo Chen","Lingbing Guo","Yajing Xu","Binbin Hu","Ziqi Liu","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.16869v2.pdf","comment":"Work in progress. Code and data will be released at\n  https://github.com/zjukg/MoMoK"},{"id":"http://arxiv.org/abs/2410.01600v1","updated":"2024-10-02T14:39:13Z","published":"2024-10-02T14:39:13Z","title":"ENTP: Encoder-only Next Token Prediction","summary":"  Next-token prediction models have predominantly relied on decoder-only\nTransformers with causal attention, driven by the common belief that causal\nattention is essential to prevent \"cheating\" by masking future tokens. We\nchallenge this widely accepted notion and argue that this design choice is\nabout efficiency rather than necessity. While decoder-only Transformers are\nstill a good choice for practical reasons, they are not the only viable option.\nIn this work, we introduce Encoder-only Next Token Prediction (ENTP). We\nexplore the differences between ENTP and decoder-only Transformers in\nexpressive power and complexity, highlighting potential advantages of ENTP. We\nintroduce the Triplet-Counting task and show, both theoretically and\nexperimentally, that while ENTP can perform this task easily, a decoder-only\nTransformer cannot. Finally, we empirically demonstrate ENTP's superior\nperformance across various realistic tasks, such as length generalization and\nin-context learning.\n","authors":["Ethan Ewer","Daewon Chae","Thomas Zeng","Jinkyu Kim","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2410.01600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15452v2","updated":"2024-10-02T14:35:40Z","published":"2024-09-23T18:27:03Z","title":"CUTE: Measuring LLMs' Understanding of Their Tokens","summary":"  Large Language Models (LLMs) show remarkable performance on a wide variety of\ntasks. Most LLMs split text into multi-character tokens and process them as\natomic units without direct access to individual characters. This raises the\nquestion: To what extent can LLMs learn orthographic information? To answer\nthis, we propose a new benchmark, CUTE, which features a collection of tasks\ndesigned to test the orthographic knowledge of LLMs. We evaluate popular LLMs\non CUTE, finding that most of them seem to know the spelling of their tokens,\nyet fail to use this information effectively to manipulate text, calling into\nquestion how much of this knowledge is generalizable.\n","authors":["Lukas Edman","Helmut Schmid","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2409.15452v2.pdf","comment":"Accepted to EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2409.13385v2","updated":"2024-10-02T14:30:28Z","published":"2024-09-20T10:36:49Z","title":"Contextual Compression in Retrieval-Augmented Generation for Large\n  Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle\nwith limitations such as hallucinations, outdated knowledge, opacity, and\ninexplicable reasoning. To address these challenges, Retrieval-Augmented\nGeneration (RAG) has proven to be a viable solution, leveraging external\ndatabases to improve the consistency and coherence of generated content,\nespecially valuable for complex, knowledge-rich tasks, and facilitates\ncontinuous improvement by leveraging domain-specific insights. By combining the\nintrinsic knowledge of LLMs with the vast, dynamic repositories of external\ndatabases, RAG achieves a synergistic effect. However, RAG is not without its\nlimitations, including a limited context window, irrelevant information, and\nthe high processing overhead for extensive contextual data. In this\ncomprehensive work, we explore the evolution of Contextual Compression\nparadigms, providing an in-depth examination of the field. Finally, we outline\nthe current challenges and suggest potential research and development\ndirections, paving the way for future advancements in this area.\n","authors":["Sourav Verma"],"pdf_url":"https://arxiv.org/pdf/2409.13385v2.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2312.11062v2","updated":"2024-10-02T14:26:30Z","published":"2023-12-18T09:58:19Z","title":"Entity or Relation Embeddings? An Analysis of Encoding Strategies for\n  Relation Extraction","summary":"  Relation extraction is essentially a text classification problem, which can\nbe tackled by fine-tuning a pre-trained language model (LM). However, a key\nchallenge arises from the fact that relation extraction cannot\nstraightforwardly be reduced to sequence or token classification. Existing\napproaches therefore solve the problem in an indirect way: they fine-tune an LM\nto learn embeddings of the head and tail entities, and then predict the\nrelationship from these entity embeddings. Our hypothesis in this paper is that\nrelation extraction models can be improved by capturing relationships in a more\ndirect way. In particular, we experiment with appending a prompt with a [MASK]\ntoken, whose contextualised representation is treated as a relation embedding.\nWhile, on its own, this strategy significantly underperforms the aforementioned\napproach, we find that the resulting relation embeddings are highly\ncomplementary to what is captured by embeddings of the head and tail entity. By\njointly considering both types of representations, we end up with a simple\nmodel that outperforms the state-of-the-art across several relation extraction\nbenchmarks.\n","authors":["Frank Mtumbuka","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2312.11062v2.pdf","comment":"Accepted in the Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.10421v3","updated":"2024-10-02T14:20:50Z","published":"2024-06-14T21:52:21Z","title":"SciEx: Benchmarking Large Language Models on Scientific Exams with Human\n  Expert Grading and Automatic Grading","summary":"  With the rapid development of Large Language Models (LLMs), it is crucial to\nhave benchmarks which can evaluate the ability of LLMs on different domains.\nOne common use of LLMs is performing tasks on scientific topics, such as\nwriting algorithms, querying databases or giving mathematical proofs. Inspired\nby the way university students are evaluated on such tasks, in this paper, we\npropose SciEx - a benchmark consisting of university computer science exam\nquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)\nmultilingual, containing both English and German exams, and (2) multi-modal,\ncontaining questions that involve images, and (3) contains various types of\nfreeform questions with different difficulty levels, due to the nature of\nuniversity exams. We evaluate the performance of various state-of-the-art LLMs\non our new benchmark. Since SciEx questions are freeform, it is not\nstraightforward to evaluate LLM performance. Therefore, we provide human expert\ngrading of the LLM outputs on SciEx. We show that the free-form exams in SciEx\nremain challenging for the current LLMs, where the best LLM only achieves\n59.4\\% exam grade on average. We also provide detailed comparisons between LLM\nperformance and student performance on SciEx. To enable future evaluation of\nnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.\nOur experiments show that, although they do not perform perfectly on solving\nthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with\nexpert grading.\n","authors":["Tu Anh Dinh","Carlos Mullov","Leonard Brmann","Zhaolin Li","Danni Liu","Simon Rei","Jueun Lee","Nathan Lerzer","Fabian Ternava","Jianfeng Gao","Tobias Rddiger","Alexander Waibel","Tamim Asfour","Michael Beigl","Rainer Stiefelhagen","Carsten Dachsbacher","Klemens Bhm","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2406.10421v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.11176v3","updated":"2024-10-02T14:20:29Z","published":"2024-02-17T02:54:32Z","title":"KnowTuning: Knowledge-aware Fine-tuning for Large Language Models","summary":"  Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.\n","authors":["Yougang Lyu","Lingyong Yan","Shuaiqiang Wang","Haibo Shi","Dawei Yin","Pengjie Ren","Zhumin Chen","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2402.11176v3.pdf","comment":"EMNLP 2024 main paper"},{"id":"http://arxiv.org/abs/2410.01579v1","updated":"2024-10-02T14:15:13Z","published":"2024-10-02T14:15:13Z","title":"Spoken Grammar Assessment Using LLM","summary":"  Spoken language assessment (SLA) systems restrict themselves to evaluating\nthe pronunciation and oral fluency of a speaker by analysing the read and\nspontaneous spoken utterances respectively. The assessment of language grammar\nor vocabulary is relegated to written language assessment (WLA) systems. Most\nWLA systems present a set of sentences from a curated finite-size database of\nsentences thereby making it possible to anticipate the test questions and train\noneself. In this paper, we propose a novel end-to-end SLA system to assess\nlanguage grammar from spoken utterances thus making WLA systems redundant;\nadditionally, we make the assessment largely unteachable by employing a large\nlanguage model (LLM) to bring in variations in the test. We further demonstrate\nthat a hybrid automatic speech recognition (ASR) with a custom-built language\nmodel outperforms the state-of-the-art ASR engine for spoken grammar\nassessment.\n","authors":["Sunil Kumar Kopparapu","Chitralekha Bhat","Ashish Panda"],"pdf_url":"https://arxiv.org/pdf/2410.01579v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.01560v1","updated":"2024-10-02T14:00:09Z","published":"2024-10-02T14:00:09Z","title":"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data","summary":"  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n","authors":["Shubham Toshniwal","Wei Du","Ivan Moshkov","Branislav Kisacanin","Alexan Ayrapetyan","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2410.01560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v2","updated":"2024-10-02T13:59:40Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companisonship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01555v1","updated":"2024-10-02T13:52:09Z","published":"2024-10-02T13:52:09Z","title":"ACE: A LLM-based Negotiation Coaching System","summary":"  The growing prominence of LLMs has led to an increase in the development of\nAI tutoring systems. These systems are crucial in providing underrepresented\npopulations with improved access to valuable education. One important area of\neducation that is unavailable to many learners is strategic bargaining related\nto negotiation. To address this, we develop a LLM-based Assistant for Coaching\nnEgotiation (ACE). ACE not only serves as a negotiation partner for users but\nalso provides them with targeted feedback for improvement. To build our system,\nwe collect a dataset of negotiation transcripts between MBA students. These\ntranscripts come from trained negotiators and emulate realistic bargaining\nscenarios. We use the dataset, along with expert consultations, to design an\nannotation scheme for detecting negotiation mistakes. ACE employs this scheme\nto identify mistakes and provide targeted feedback to users. To test the\neffectiveness of ACE-generated feedback, we conducted a user experiment with\ntwo consecutive trials of negotiation and found that it improves negotiation\nperformances significantly compared to a system that doesn't provide feedback\nand one which uses an alternative method of providing feedback.\n","authors":["Ryan Shea","Aymen Kallala","Xin Lucy Liu","Michael W. Morris","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01555v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01553v1","updated":"2024-10-02T13:47:17Z","published":"2024-10-02T13:47:17Z","title":"MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an\n  AI-SCE Framework","summary":"  Artificial intelligence (AI) and large language models (LLMs) in healthcare\nrequire advanced clinical skills (CS), yet current benchmarks fail to evaluate\nthese comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by\nmedical education's Objective Structured Clinical Examinations (OSCEs), to\naddress this gap. MedQA-CS evaluates LLMs through two instruction-following\ntasks, LLM-as-medical-student and LLM-as-CS-examiner, designed to reflect real\nclinical scenarios. Our contributions include developing MedQA-CS, a\ncomprehensive evaluation framework with publicly available data and expert\nannotations, and providing the quantitative and qualitative assessment of LLMs\nas reliable judges in CS evaluation. Our experiments show that MedQA-CS is a\nmore challenging benchmark for evaluating clinical skills than traditional\nmultiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks,\nMedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities\nfor both open- and closed-source LLMs.\n","authors":["Zonghai Yao","Zihao Zhang","Chaolong Tang","Xingyu Bian","Youxia Zhao","Zhichao Yang","Junda Wang","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11062v2","updated":"2024-10-02T13:44:30Z","published":"2024-07-10T17:53:30Z","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","summary":"  Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","authors":["Mengzhao Chen","Wenqi Shao","Peng Xu","Jiahao Wang","Peng Gao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11062v2.pdf","comment":"An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs"},{"id":"http://arxiv.org/abs/2406.04886v2","updated":"2024-10-02T13:40:10Z","published":"2024-06-07T12:32:44Z","title":"Unveiling the Invisible: Captioning Videos with Metaphors","summary":"  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n","authors":["Abisek Rajakumar Kalarani","Pushpak Bhattacharyya","Sumit Shekhar"],"pdf_url":"https://arxiv.org/pdf/2406.04886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01548v1","updated":"2024-10-02T13:37:54Z","published":"2024-10-02T13:37:54Z","title":"In-Context Transfer Learning: Demonstration Synthesis by Transferring\n  Similar Tasks","summary":"  In-context learning (ICL) is an effective approach to help large language\nmodels (LLMs) adapt to various tasks by providing demonstrations of the target\ntask. Considering the high cost of labeling demonstrations, many methods\npropose synthesizing demonstrations from scratch using LLMs. However, the\nquality of the demonstrations synthesized from scratch is limited by the\ncapabilities and knowledge of LLMs. To address this, inspired by transfer\nlearning, we propose In-Context Transfer Learning (ICTL), which synthesizes\ntarget task demonstrations by transferring labeled demonstrations from similar\nsource tasks. ICTL consists of two steps: source sampling and target transfer.\nFirst, we define an optimization objective, which minimizes transfer error to\nsample source demonstrations similar to the target task. Then, we employ LLMs\nto transfer the sampled source demonstrations to the target task, matching the\ndefinition and format of the target task. Experiments on Super-NI show that\nICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the\neffectiveness of our method.\n","authors":["Dingzirui Wang","Xuangliang Zhang","Qiguang Chen","Longxu Dou","Xiao Xu","Rongyu Cao","Yingwei Ma","Qingfu Zhu","Wanxiang Che","Binhua Li","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.01548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01532v1","updated":"2024-10-02T13:24:56Z","published":"2024-10-02T13:24:56Z","title":"Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for\n  Large Language Models","summary":"  Advancements in Natural Language Processing (NLP), have led to the emergence\nof Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which\nexcel across a range of tasks but require extensive fine-tuning to align their\noutputs with human expectations. A widely used method for achieving this\nalignment is Reinforcement Learning from Human Feedback (RLHF), which, despite\nits success, faces challenges in accurately modelling human preferences. In\nthis paper, we introduce GazeReward, a novel framework that integrates implicit\nfeedback -- and specifically eye-tracking (ET) data -- into the Reward Model\n(RM). In addition, we explore how ET-based features can provide insights into\nuser preferences. Through ablation studies we test our framework with different\nintegration methods, LLMs, and ET generator models, demonstrating that our\napproach significantly improves the accuracy of the RM on established human\npreference datasets. This work advances the ongoing discussion on optimizing AI\nalignment with human values, exploring the potential of cognitive data for\nshaping future NLP research.\n","authors":["Angela Lopez-Cardona","Carlos Segura","Alexandros Karatzoglou","Sergi Abadal","Ioannis Arapakis"],"pdf_url":"https://arxiv.org/pdf/2410.01532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01524v1","updated":"2024-10-02T13:12:13Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01518v1","updated":"2024-10-02T13:09:41Z","published":"2024-10-02T13:09:41Z","title":"InfiniPot: Infinite Context Processing on Memory-Constrained LLMs","summary":"  Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.\n","authors":["Minsoo Kim","Kyuhong Shim","Jungwook Choi","Simyung Chang"],"pdf_url":"https://arxiv.org/pdf/2410.01518v1.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2406.07791v5","updated":"2024-10-02T13:08:43Z","published":"2024-06-12T01:12:28Z","title":"Judging the Judges: A Systematic Investigation of Position Bias in\n  Pairwise Comparative Assessments by LLMs","summary":"  LLM-as-a-Judge presents a promising alternative to human evaluators across\nvarious tasks, but inherent biases, especially position bias - a tendency to\nfavor solutions based on their position in the prompt - have compromised its\neffectiveness. Our study introduces a systematic framework to examine position\nbias in pairwise comparisons, focusing on repetition stability, position\nconsistency, and preference fairness. This research significantly contributes\nto the field by introducing new concepts for understanding position bias and\nproviding a multi-dimensional framework for evaluations. We conducted\nexperiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks\nand approximately 40 solution-generating models - candidates, resulting in over\n100,000 evaluation instances. Our findings confirm that position bias in\ncapable LLM judges is not due to random chances, along with notable variations\nobserved across judges and tasks. Moreover, position bias is weakly influenced\nby the length of prompt components but significantly impacted by the quality\ngap between solutions. These insights can help optimize judge model selections,\nimprove benchmark design, and inform future research on debiasing strategies,\nultimately enhancing the reliability of LLM judges.\n","authors":["Lin Shi","Chiyu Ma","Wenhua Liang","Weicheng Ma","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2406.07791v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01512v1","updated":"2024-10-02T13:02:23Z","published":"2024-10-02T13:02:23Z","title":"InstaTrans: An Instruction-Aware Translation Framework for Non-English\n  Instruction Datasets","summary":"  It is challenging to generate high-quality instruction datasets for\nnon-English languages due to tail phenomena, which limit performance on less\nfrequently observed data. To mitigate this issue, we propose translating\nexisting high-quality English instruction datasets as a solution, emphasizing\nthe need for complete and instruction-aware translations to maintain the\ninherent attributes of these datasets. We claim that fine-tuning LLMs with\ndatasets translated in this way can improve their performance in the target\nlanguage. To this end, we introduces a new translation framework tailored for\ninstruction datasets, named InstaTrans (INSTruction-Aware TRANSlation). Through\nextensive experiments, we demonstrate the superiority of InstaTrans over other\ncompetitors in terms of completeness and instruction-awareness of translation,\nhighlighting its potential to broaden the accessibility of LLMs across diverse\nlanguages at a relatively low cost. Furthermore, we have validated that\nfine-tuning LLMs with datasets translated by InstaTrans can effectively improve\ntheir performance in the target language.\n","authors":["Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.01512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01508v1","updated":"2024-10-02T13:00:21Z","published":"2024-10-02T13:00:21Z","title":"Disentangling Latent Shifts of In-Context Learning Through Self-Training","summary":"  In-context learning (ICL) has become essential in natural language\nprocessing, particularly with autoregressive large language models capable of\nlearning from demonstrations provided within the prompt. However, ICL faces\nchallenges with stability and long contexts, especially as the number of\ndemonstrations grows, leading to poor generalization and inefficient inference.\nTo address these issues, we introduce STICL (Self-Training ICL), an approach\nthat disentangles the latent shifts of demonstrations from the latent shift of\nthe query through self-training. STICL employs a teacher model to generate\npseudo-labels and trains a student model using these labels, encoded in an\nadapter module. The student model exhibits weak-to-strong generalization,\nprogressively refining its predictions over time. Our empirical results show\nthat STICL improves generalization and stability, consistently outperforming\ntraditional ICL methods and other disentangling strategies across both\nin-domain and out-of-domain data.\n","authors":["Josip Juki","Jan najder"],"pdf_url":"https://arxiv.org/pdf/2410.01508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01504v1","updated":"2024-10-02T12:57:12Z","published":"2024-10-02T12:57:12Z","title":"PersonaMath: Enhancing Math Reasoning through Persona-Driven Data\n  Augmentation","summary":"  While closed-source Large Language Models (LLMs) demonstrate strong\nmathematical problem-solving abilities, open-source models continue to struggle\nwith such tasks. To bridge this gap, we propose a data augmentation approach\nand introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we\ntrain the PersonaMath models. Our approach consists of two stages: the first\nstage is learning from Persona Diversification, and the second stage is\nlearning from Reflection. In the first stage, we regenerate detailed\nchain-of-thought (CoT) solutions as instructions using a closed-source LLM and\nintroduce a novel persona-driven data augmentation technique to enhance the\ndataset's quantity and diversity. In the second stage, we incorporate\nreflection to fully leverage more challenging and valuable questions.\nEvaluation of our PersonaMath models on MATH and GSM8K reveals that the\nPersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on\nMATH and 68.7% on GSM8K, surpassing all baseline methods and achieving\nstate-of-the-art performance. Notably, our dataset contains only 70.3K data\npoints-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model\noutperforms these baselines, demonstrating the high quality and diversity of\nour dataset, which enables more efficient model training. We open-source the\nPersonaMathQA dataset, PersonaMath models, and our code for public usage.\n","authors":["Jing Luo","Run Luo","Longze Chen","Liang Zhu","Chang Ao","Jiaming Li","Yukun Chen","Xin Cheng","Wen Yang","Jiayuan Su","Chengming Li","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2410.01504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04370v2","updated":"2024-10-02T12:49:18Z","published":"2024-06-01T02:08:44Z","title":"Large Language Model Confidence Estimation via Black-Box Access","summary":"  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b and Mistral-7b on four benchmark Q\\&A tasks as well as of\nPegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n","authors":["Tejaswini Pedapati","Amit Dhurandhar","Soumya Ghosh","Soham Dan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2406.04370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17678v3","updated":"2024-10-02T12:47:50Z","published":"2024-07-25T00:27:07Z","title":"S2-Attention: Hardware-Aware Context Sharding Among Attention Heads","summary":"  Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Liliang Ren","Barun Patra","Vishrav Chaudhary","Hao Peng","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.01497v1","updated":"2024-10-02T12:45:52Z","published":"2024-10-02T12:45:52Z","title":"DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.\n","authors":["Yuxuan Zhang","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2410.01497v1.pdf","comment":"Preprint under review, 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01487v1","updated":"2024-10-02T12:36:08Z","published":"2024-10-02T12:36:08Z","title":"Small Language Models Like Small Vocabularies: Probing the Linguistic\n  Abilities of Grapheme- and Phoneme-Based Baby Llamas","summary":"  Current language models use subword-based tokenization algorithms like Byte\nPair Encoding, which put their validity as models of linguistic representations\ninto question. In this paper, we explore the potential of tokenization-free,\nphoneme- and grapheme-based language models. We demonstrate that small models\nbased on the Llama architecture can achieve strong linguistic performance on\nstandard syntactic and novel lexical/phonetic benchmarks when trained with\ncharacter-level vocabularies. We further show that phoneme-based models without\nany graphemic biases almost match grapheme-based models in standard tasks and\nnovel evaluations. Our findings suggest a promising direction for creating more\nlinguistically plausible language models that are better suited for\ncomputational studies of language acquisition and processing.\n","authors":["Bastian Bunzeck","Daniel Duran","Leonie Schade","Sina Zarrie"],"pdf_url":"https://arxiv.org/pdf/2410.01487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01485v1","updated":"2024-10-02T12:35:53Z","published":"2024-10-02T12:35:53Z","title":"A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts","summary":"  Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.\n","authors":["Suyu Ge","Xihui Lin","Yunan Zhang","Jiawei Han","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18120v3","updated":"2024-10-02T12:34:25Z","published":"2024-02-28T07:18:39Z","title":"Exploring Multilingual Concepts of Human Value in Large Language Models:\n  Is Value Alignment Consistent, Transferable and Controllable across\n  Languages?","summary":"  Prior research has revealed that certain abstract concepts are linearly\nrepresented as directions in the representation space of LLMs, predominantly\ncentered around English. In this paper, we extend this investigation to a\nmultilingual context, with a specific focus on human values-related concepts\n(i.e., value concepts) due to their significance for AI safety. Through our\ncomprehensive exploration covering 7 types of human values, 16 languages and 3\nLLM series with distinct multilinguality (e.g., monolingual, bilingual and\nmultilingual), we first empirically confirm the presence of value concepts\nwithin LLMs in a multilingual format. Further analysis on the cross-lingual\ncharacteristics of these concepts reveals 3 traits arising from language\nresource disparities: cross-lingual inconsistency, distorted linguistic\nrelationships, and unidirectional cross-lingual transfer between high- and\nlow-resource languages, all in terms of value concepts. Moreover, we validate\nthe feasibility of cross-lingual control over value alignment capabilities of\nLLMs, leveraging the dominant language as a source language. Ultimately,\nrecognizing the significant impact of LLMs' multilinguality on our results, we\nconsolidate our findings and provide prudent suggestions on the composition of\nmultilingual data for LLMs pre-training.\n","authors":["Shaoyang Xu","Weilong Dong","Zishan Guo","Xinwei Wu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.18120v3.pdf","comment":"EMNLP 2024 findings, code&dataset:\n  https://github.com/shaoyangxu/Multilingual-Human-Value-Concepts"},{"id":"http://arxiv.org/abs/2404.01569v2","updated":"2024-10-02T12:31:11Z","published":"2024-04-02T02:03:28Z","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental\n  Approach","summary":"  In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.\n","authors":["Manish Sanwal"],"pdf_url":"https://arxiv.org/pdf/2404.01569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06670v2","updated":"2024-10-02T12:26:54Z","published":"2024-04-10T01:14:12Z","title":"What's Mine becomes Yours: Defining, Annotating and Detecting\n  Context-Dependent Paraphrases in News Interview Dialogs","summary":"  Best practices for high conflict conversations like counseling or customer\nsupport almost always include recommendations to paraphrase the previous\nspeaker. Although paraphrase classification has received widespread attention\nin NLP, paraphrases are usually considered independent from context, and common\nmodels and datasets are not applicable to dialog settings. In this work, we\ninvestigate paraphrases in dialog (e.g., Speaker 1: \"That book is mine.\"\nbecomes Speaker 2: \"That book is yours.\"). We provide an operationalization of\ncontext-dependent paraphrases, and develop a training for crowd-workers to\nclassify paraphrases in dialog. We introduce a dataset with utterance pairs\nfrom NPR and CNN news interviews annotated for context-dependent paraphrases.\nTo enable analyses on label variation, the dataset contains 5,581 annotations\non 600 utterance pairs. We present promising results with in-context learning\nand with token classification models for automatic paraphrase detection in\ndialog.\n","authors":["Anna Wegmann","Tijs van den Broek","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.06670v2.pdf","comment":"Accepted as main conference paper to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01450v1","updated":"2024-10-02T12:01:32Z","published":"2024-10-02T12:01:32Z","title":"Agent-Driven Large Language Models for Mandarin Lyric Generation","summary":"  Generative Large Language Models have shown impressive in-context learning\nabilities, performing well across various tasks with just a prompt. Previous\nmelody-to-lyric research has been limited by scarce high-quality aligned data\nand unclear standard for creativeness. Most efforts focused on general themes\nor emotions, which are less valuable given current language model capabilities.\nIn tonal contour languages like Mandarin, pitch contours are influenced by both\nmelody and tone, leading to variations in lyric-melody fit. Our study,\nvalidated by the Mpop600 dataset, confirms that lyricists and melody writers\nconsider this fit during their composition process. In this research, we\ndeveloped a multi-agent system that decomposes the melody-to-lyric task into\nsub-tasks, with each agent controlling rhyme, syllable count, lyric-melody\nalignment, and consistency. Listening tests were conducted via a\ndiffusion-based singing voice synthesizer to evaluate the quality of lyrics\ngenerated by different agent groups.\n","authors":["Hong-Hsiang Liu","Yi-Wen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01450v1.pdf","comment":"6 pages, figures, Accepted at O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2410.01448v1","updated":"2024-10-02T11:59:58Z","published":"2024-10-02T11:59:58Z","title":"Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation","summary":"  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller"],"pdf_url":"https://arxiv.org/pdf/2410.01448v1.pdf","comment":"Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,\n  co-located with ISMIR 2024)"},{"id":"http://arxiv.org/abs/2404.03887v4","updated":"2024-10-02T11:56:35Z","published":"2024-04-05T04:25:47Z","title":"SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical\n  Reasoning in Large Language Models","summary":"  This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.\n","authors":["Hyeonwoo Kim","Gyoungjin Gim","Yungi Kim","Jihoo Kim","Byungju Kim","Wonseok Lee","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.03887v4.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.01444v1","updated":"2024-10-02T11:54:06Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.13443v2","updated":"2024-10-02T11:46:10Z","published":"2024-06-19T11:08:56Z","title":"Dual-Phase Accelerated Prompt Optimization","summary":"  Gradient-free prompt optimization methods have made significant strides in\nenhancing the performance of closed-source Large Language Models (LLMs) across\na wide range of tasks. However, existing approaches make light of the\nimportance of high-quality prompt initialization and the identification of\neffective optimization directions, thus resulting in substantial optimization\nsteps to obtain satisfactory performance. In this light, we aim to accelerate\nprompt optimization process to tackle the challenge of low convergence rate. We\npropose a dual-phase approach which starts with generating high-quality initial\nprompts by adopting a well-designed meta-instruction to delve into\ntask-specific information, and iteratively optimize the prompts at the sentence\nlevel, leveraging previous tuning experience to expand prompt candidates and\naccept effective ones. Extensive experiments on eight datasets demonstrate the\neffectiveness of our proposed method, achieving a consistent accuracy gain over\nbaselines with less than five optimization steps.\n","authors":["Muchen Yang","Moxin Li","Yongle Li","Zijun Chen","Chongming Gao","Junqi Zhang","Yangyang Li","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.13443v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.09549v2","updated":"2024-10-02T11:44:26Z","published":"2024-06-13T19:30:32Z","title":"Urdu Dependency Parsing and Treebank Development: A Syntactic and\n  Morphological Perspective","summary":"  Parsing is the process of analyzing a sentence's syntactic structure by\nbreaking it down into its grammatical components. and is critical for various\nlinguistic applications. Urdu is a low-resource, free word-order language and\nexhibits complex morphology. Literature suggests that dependency parsing is\nwell-suited for such languages. Our approach begins with a basic feature model\nencompassing word location, head word identification, and dependency relations,\nfollowed by a more advanced model integrating part-of-speech (POS) tags and\nmorphological attributes (e.g., suffixes, gender). We manually annotated a\ncorpus of news articles of varying complexity. Using Maltparser and the\nNivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an\nunlabeled attachment score (UAS) of 84%, demonstrating the feasibility of\ndependency parsing for Urdu.\n","authors":["Nudrat Habib"],"pdf_url":"https://arxiv.org/pdf/2406.09549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10341v2","updated":"2024-10-02T11:38:51Z","published":"2024-09-16T14:56:59Z","title":"Detecting Sexism in German Online Newspaper Comments with Open-Source\n  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks\n  1 and 2, Closed Track)","summary":"  Sexism in online media comments is a pervasive challenge that often manifests\nsubtly, complicating moderation efforts as interpretations of what constitutes\nsexism can vary among individuals. We study monolingual and multilingual\nopen-source text embeddings to reliably detect sexism and misogyny in\nGerman-language online comments from an Austrian newspaper. We observed\nclassifiers trained on text embeddings to mimic closely the individual\njudgements of human annotators. Our method showed robust performance in the\nGermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1\nscore of 0.597 (4th place, as reported on Codabench). It also accurately\npredicted the distribution of human annotations in GerMS-Detect Subtask 2, with\nan average Jensen-Shannon distance of 0.301 (2nd place). The computational\nefficiency of our approach suggests potential for scalable applications across\nvarious languages and linguistic contexts.\n","authors":["Florian Bremm","Patrick Gustav Blaneck","Tobias Bornheim","Niklas Grieger","Stephan Bialonski"],"pdf_url":"https://arxiv.org/pdf/2409.10341v2.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.01434v1","updated":"2024-10-02T11:36:45Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions via\nsubnetworks that can be composed to perform more complex tasks. Recent\ndevelopments in mechanistic interpretability have made progress in identifying\nsubnetworks, often referred to as circuits, which represent the minimal\ncomputational subgraph responsible for a model's behavior on specific tasks.\nHowever, most studies focus on identifying circuits for individual tasks\nwithout investigating how functionally similar circuits relate to each other.\nTo address this gap, we examine the modularity of neural networks by analyzing\ncircuits for highly compositional subtasks within a transformer-based language\nmodel. Specifically, given a probabilistic context-free grammar, we identify\nand compare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through subnetwork set operations to\nrepresent more complex functional capabilities of the model.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v1.pdf","comment":"24 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.11085v4","updated":"2024-10-02T11:35:45Z","published":"2023-10-17T09:10:27Z","title":"Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models","summary":"  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02076v5","updated":"2024-10-02T11:29:18Z","published":"2024-09-03T17:25:54Z","title":"LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs","summary":"  In evaluating the long-context capabilities of large language models (LLMs),\nbenchmarks such as \"Needle-in-a-Haystack\" (NIAH), Ruler, and Needlebench are\ncommonly used. While these benchmarks measure how well models understand\nlong-context input sequences, they do not effectively gauge the quality of\nlong-form text generation--a critical aspect for applications such as design\nproposals and creative writing. To address this gap, we have introduced a new\nlong-form text evaluation benchmark, LongGenBench, which tests models' ability\nto identify specific events within generated long text sequences. In this\nbenchmark, we prompt long-context LMs to create long-form text that must\ninclude particular events or constraints and evaluate their ability to\nincorporate these elements. We evaluated ten long-context LMs across four\ndistinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenBench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.\n","authors":["Yuhao Wu","Ming Shan Hee","Zhiqing Hu","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2409.02076v5.pdf","comment":"work in progress; Github: https://github.com/mozhu621/LongGenBench/"},{"id":"http://arxiv.org/abs/2410.01428v1","updated":"2024-10-02T11:26:02Z","published":"2024-10-02T11:26:02Z","title":"Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with\n  Retrieval-Augmentation for Solving Challenging Tasks","summary":"  State-of-the-art large language models (LLMs) exhibit impressive\nproblem-solving capabilities but may struggle with complex reasoning and\nfactual correctness. Existing methods harness the strengths of chain-of-thought\nand retrieval-augmented generation (RAG) to decompose a complex problem into\nsimpler steps and apply retrieval to improve factual correctness. These methods\nwork well on straightforward reasoning tasks but often falter on challenging\ntasks such as competitive programming and mathematics, due to frequent\nreasoning errors and irrelevant knowledge retrieval. To address this, we\nintroduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a\nnovel framework that leverages fine-tuned critic models to guide both reasoning\nand retrieval processes through planning. CR-Planner solves a problem by\niteratively selecting and executing sub-goals. Initially, it identifies the\nmost promising sub-goal from reasoning, query generation, and retrieval, guided\nby rewards given by a critic model named sub-goal critic. It then executes this\nsub-goal through sampling and selecting the optimal output based on evaluations\nfrom another critic model named execution critic. This iterative process,\ninformed by retrieved information and critic models, enables CR-Planner to\neffectively navigate the solution space towards the final answer. We employ\nMonte Carlo Tree Search to collect the data for training the critic models,\nallowing for a systematic exploration of action sequences and their long-term\nimpacts. We validate CR-Planner on challenging domain-knowledge-intensive and\nreasoning-heavy tasks, including competitive programming, theorem-driven math\nreasoning, and complex domain retrieval problems. Our experiments demonstrate\nthat CR-Planner significantly outperforms baselines, highlighting its\neffectiveness in addressing challenging problems by improving both reasoning\nand retrieval.\n","authors":["Xingxuan Li","Weiwen Xu","Ruochen Zhao","Fangkai Jiao","Shafiq Joty","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.01428v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.18618v3","updated":"2024-10-02T11:08:29Z","published":"2024-09-27T10:35:45Z","title":"Model-based Preference Optimization in Abstractive Summarization without\n  Human Feedback","summary":"  In abstractive summarization, the challenge of producing concise and accurate\nsummaries arises from the vast amount of information contained in the source\ndocument. Consequently, although Large Language Models (LLMs) can generate\nfluent text, they often introduce inaccuracies by hallucinating content not\nfound in the original source. While supervised fine-tuning methods that\nmaximize likelihood contribute to this issue, they do not consistently enhance\nthe faithfulness of the summaries. Preference-based optimization methods, such\nas Direct Preference Optimization (DPO), can further refine the model to align\nwith human preferences. However, these methods still heavily depend on costly\nhuman feedback. In this work, we introduce a novel and straightforward approach\ncalled Model-based Preference Optimization (MPO) to fine-tune LLMs for improved\nsummarization abilities without any human feedback. By leveraging the model's\ninherent summarization capabilities, we create a preference dataset that is\nfully generated by the model using different decoding strategies. Our\nexperiments on standard summarization datasets and various metrics demonstrate\nthat our proposed MPO significantly enhances the quality of generated summaries\nwithout relying on human feedback.\n","authors":["Jaepill Choi","Kyubyung Chae","Jiwoo Song","Yohan Jo","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18618v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.11239v2","updated":"2024-10-02T11:07:14Z","published":"2024-09-17T14:40:02Z","title":"LLM-as-a-Judge & Reward Model: What They Can and Cannot Do","summary":"  LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\nin diverse contexts, such as non-English prompts, factual verification, or\nchallenging questions, remains unexplored. In this paper, we conduct a\ncomprehensive analysis of automated evaluators, reporting several key findings\non their behavior. First, we discover that English evaluation capabilities\nsignificantly influence language-specific evaluation capabilities, often more\nthan the language proficiency itself, enabling evaluators trained in English to\neasily transfer their skills to other languages. Second, we identify critical\nshortcomings, where LLMs fail to detect and penalize errors, such as factual\ninaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we find that state-of-the-art evaluators struggle with\nchallenging prompts, in either English or Korean, underscoring their\nlimitations in assessing or generating complex reasoning questions. We release\nthe dataset and codes used.\n","authors":["Guijin Son","Hyunwoo Ko","Hoyoung Lee","Yewon Kim","Seunghyeok Hong"],"pdf_url":"https://arxiv.org/pdf/2409.11239v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2406.14760v2","updated":"2024-10-02T11:03:16Z","published":"2024-06-20T22:10:52Z","title":"An LLM Feature-based Framework for Dialogue Constructiveness Assessment","summary":"  Research on dialogue constructiveness assessment focuses on (i) analysing\nconversational factors that influence individuals to take specific actions, win\ndebates, change their perspectives or broaden their open-mindedness and (ii)\npredicting constructiveness outcomes following dialogues for such use cases.\nThese objectives can be achieved by training either interpretable feature-based\nmodels (which often involve costly human annotations) or neural models such as\npre-trained language models (which have empirically shown higher task accuracy\nbut lack interpretability). In this paper we propose an LLM feature-based\nframework for dialogue constructiveness assessment that combines the strengths\nof feature-based and neural approaches, while mitigating their downsides. The\nframework first defines a set of dataset-independent and interpretable\nlinguistic features, which can be extracted by both prompting an LLM and simple\nheuristics. Such features are then used to train LLM feature-based models. We\napply this framework to three datasets of dialogue constructiveness and find\nthat our LLM feature-based models outperform or performs at least as well as\nstandard feature-based models and neural models. We also find that the LLM\nfeature-based model learns more robust prediction rules instead of relying on\nsuperficial shortcuts, which often trouble neural models.\n","authors":["Lexin Zhou","Youmna Farag","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2406.14760v2.pdf","comment":"Paper accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01417v1","updated":"2024-10-02T10:58:54Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09177v2","updated":"2024-10-02T10:43:07Z","published":"2024-02-14T13:45:19Z","title":"Leveraging the Context through Multi-Round Interactions for Jailbreaking\n  Attacks","summary":"  Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As\ndefense mechanisms evolve, directly obtaining harmful information becomes\nincreasingly challenging for Jailbreaking attacks. In this work, inspired from\nChomsky's transformational-generative grammar theory and human practices of\nindirect context to elicit harmful information, we focus on a new attack form,\ncalled Contextual Interaction Attack. We contend that the prior\ncontext\\u2014the information preceding the attack query\\u2014plays a pivotal\nrole in enabling strong Jailbreaking attacks. Specifically, we propose a first\nmulti-turn approach that leverages benign preliminary questions to interact\nwith the LLM. Due to the autoregressive nature of LLMs, which use previous\nconversation rounds as context during generation, we guide the model's\nquestion-response pair to construct a context that is semantically aligned with\nthe attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box\nand can also transfer across LLMs. We believe this can lead to further\ndevelopments and understanding of security in LLMs.\n","authors":["Yixin Cheng","Markos Georgopoulos","Volkan Cevher","Grigorios G. Chrysos"],"pdf_url":"https://arxiv.org/pdf/2402.09177v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2409.17171v2","updated":"2024-10-02T10:28:02Z","published":"2024-09-19T21:45:13Z","title":"Cross-Domain Content Generation with Domain-Specific Small Language\n  Models","summary":"  Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.\n","authors":["Ankit Maloo","Abhinav Garg"],"pdf_url":"https://arxiv.org/pdf/2409.17171v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.01401v1","updated":"2024-10-02T10:27:07Z","published":"2024-10-02T10:27:07Z","title":"Question-guided Knowledge Graph Re-scoring and Injection for Knowledge\n  Graph Question Answering","summary":"  Knowledge graph question answering (KGQA) involves answering natural language\nquestions by leveraging structured information stored in a knowledge graph.\nTypically, KGQA initially retrieve a targeted subgraph from a large-scale\nknowledge graph, which serves as the basis for reasoning models to address\nqueries. However, the retrieved subgraph inevitably brings distraction\ninformation for knowledge utilization, impeding the model's ability to perform\naccurate reasoning. To address this issue, we propose a Question-guided\nKnowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the\ninput question, thereby focusing specifically on pertinent factual knowledge.\nMoreover, we introduce Knowformer, a parameter-efficient method for injecting\nthe re-scored knowledge graph into large language models to enhance their\nability to perform factual reasoning. Extensive experiments on multiple KGQA\nbenchmarks demonstrate the superiority of our method over existing systems.\n","authors":["Yu Zhang","Kehai Chen","Xuefeng Bai","zhao kang","Quanjiang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01401v1.pdf","comment":"findings of EMNLP2024"},{"id":"http://arxiv.org/abs/2410.01400v1","updated":"2024-10-02T10:24:51Z","published":"2024-10-02T10:24:51Z","title":"CrowdCounter: A benchmark type-specific multi-target counterspeech\n  dataset","summary":"  Counterspeech presents a viable alternative to banning or suspending users\nfor hate speech while upholding freedom of expression. However, writing\neffective counterspeech is challenging for moderators/users. Hence, developing\nsuggestion tools for writing counterspeech is the need of the hour. One\ncritical challenge in developing such a tool is the lack of quality and\ndiversity of the responses in the existing datasets. Hence, we introduce a new\ndataset - CrowdCounter containing 3,425 hate speech-counterspeech pairs\nspanning six different counterspeech types (empathy, humor, questioning,\nwarning, shaming, contradiction), which is the first of its kind. The design of\nour annotation platform itself encourages annotators to write type-specific,\nnon-redundant and high-quality counterspeech. We evaluate two frameworks for\ngenerating counterspeech responses - vanilla and type-controlled prompts -\nacross four large language models. In terms of metrics, we evaluate the\nresponses using relevance, diversity and quality. We observe that Flan-T5 is\nthe best model in the vanilla framework across different models. Type-specific\nprompts enhance the relevance of the responses, although they might reduce the\nlanguage quality. DialoGPT proves to be the best at following the instructions\nand generating the type-specific counterspeech accurately.\n","authors":["Punyajoy Saha","Abhilash Datta","Abhik Jana","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.01400v1.pdf","comment":"19 pages, 1 figure, 14 tables, Code available\n  https://github.com/hate-alert/CrowdCounter"},{"id":"http://arxiv.org/abs/2410.01383v1","updated":"2024-10-02T09:51:42Z","published":"2024-10-02T09:51:42Z","title":"PairDistill: Pairwise Relevance Distillation for Dense Retrieval","summary":"  Effective information retrieval (IR) from vast datasets relies on advanced\ntechniques to extract relevant information in response to queries. Recent\nadvancements in dense retrieval have showcased remarkable efficacy compared to\ntraditional sparse retrieval methods. To further enhance retrieval performance,\nknowledge distillation techniques, often leveraging robust cross-encoder\nrerankers, have been extensively explored. However, existing approaches\nprimarily distill knowledge from pointwise rerankers, which assign absolute\nrelevance scores to documents, thus facing challenges related to inconsistent\ncomparisons. This paper introduces Pairwise Relevance Distillation\n(PairDistill) to leverage pairwise reranking, offering fine-grained\ndistinctions between similarly relevant documents to enrich the training of\ndense retrieval models. Our experiments demonstrate that PairDistill\noutperforms existing methods, achieving new state-of-the-art results across\nmultiple benchmarks. This highlights the potential of PairDistill in advancing\ndense retrieval techniques effectively. Our source code and trained models are\nreleased at https://github.com/MiuLab/PairDistill\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01383v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.01380v1","updated":"2024-10-02T09:49:45Z","published":"2024-10-02T09:49:45Z","title":"Knowledge Entropy Decay during Language Model Pretraining Hinders New\n  Knowledge Acquisition","summary":"  In this work, we investigate how a model's tendency to broadly integrate its\nparametric knowledge evolves throughout pretraining, and how this behavior\naffects overall performance, particularly in terms of knowledge acquisition and\nforgetting. We introduce the concept of knowledge entropy, which quantifies the\nrange of memory sources the model engages with; high knowledge entropy\nindicates that the model utilizes a wide range of memory sources, while low\nknowledge entropy suggests reliance on specific sources with greater certainty.\nOur analysis reveals a consistent decline in knowledge entropy as pretraining\nadvances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude\nthat diminishing knowledge entropy (smaller number of active memory sources)\nimpairs the model's knowledge acquisition and retention capabilities. We find\nfurther support for this by demonstrating that increasing the activity of\ninactive memory sources enhances the model's capacity for knowledge acquisition\nand retention.\n","authors":["Jiyeon Kim","Hyunji Lee","Hyowon Cho","Joel Jang","Hyeonbin Hwang","Seungpil Won","Youbin Ahn","Dohaeng Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.01380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01363v1","updated":"2024-10-02T09:23:07Z","published":"2024-10-02T09:23:07Z","title":"PCQPR: Proactive Conversational Question Planning with Reflection","summary":"  Conversational Question Generation (CQG) enhances the interactivity of\nconversational question-answering systems in fields such as education, customer\nservice, and entertainment. However, traditional CQG, focusing primarily on the\nimmediate context, lacks the conversational foresight necessary to guide\nconversations toward specified conclusions. This limitation significantly\nrestricts their ability to achieve conclusion-oriented conversational outcomes.\nIn this work, we redefine the CQG task as Conclusion-driven Conversational\nQuestion Generation (CCQG) by focusing on proactivity, not merely reacting to\nthe unfolding conversation but actively steering it towards a\nconclusion-oriented question-answer pair. To address this, we propose a novel\napproach, called Proactive Conversational Question Planning with self-Refining\n(PCQPR). Concretely, by integrating a planning algorithm inspired by Monte\nCarlo Tree Search (MCTS) with the analytical capabilities of large language\nmodels (LLMs), PCQPR predicts future conversation turns and continuously\nrefines its questioning strategies. This iterative self-refining mechanism\nensures the generation of contextually relevant questions strategically devised\nto reach a specified outcome. Our extensive evaluations demonstrate that PCQPR\nsignificantly surpasses existing CQG methods, marking a paradigm shift towards\nconclusion-oriented conversational question-answering systems.\n","authors":["Shasha Guo","Lizi Liao","Jing Zhang","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01363v1.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2409.00997v2","updated":"2024-10-02T09:18:55Z","published":"2024-09-02T07:23:13Z","title":"DataSculpt: Crafting Data Landscapes for Long-Context LLMs through\n  Multi-Objective Partitioning","summary":"  In recent years, Large Language Models (LLMs) have demonstrated significant\nimprovements across a variety of tasks, one of which is the long-context\ncapability. The key to improving long-context performance lies in effective\ndata organization and management strategies that integrate data from multiple\ndomains and optimize the context window during training. Through extensive\nexperimental analysis, we identified three key challenges in designing\neffective data management strategies that enable the model to achieve\nlong-context capability without sacrificing performance in other tasks: (1) a\nshortage of long documents across multiple domains, (2) effective construction\nof context windows, and (3) efficient organization of large-scale datasets. To\naddress these challenges, we introduce DataSculpt, a novel data management\nframework designed for long-context training. We first formulate the\norganization of training data as a multi-objective combinatorial optimization\nproblem, focusing on attributes including relevance, homogeneity, integrity,\nand efficiency. Specifically, our approach utilizes a coarse-to-fine\nmethodology to optimize training data organization both efficiently and\neffectively. We begin by clustering the data based on semantic similarity\n(coarse), followed by a multi-objective greedy search within each cluster to\nscore and concatenate documents into various context windows (fine). Our\ncomprehensive evaluations demonstrate that DataSculpt significantly enhances\nlong-context training performance, resulting in improvements of 18.09% in\nretrieval augmentation, 21.23% in summarization, 21.27% in reading\ncomprehension, and a 3.81% increase in code completion, while also maintaining\noverall model proficiency with a 4.88% improvement.\n","authors":["Keer Lu","Xiaonan Nie","Zheng Liang","Da Pan","Shusen Zhang","Keshi Zhao","Weipeng Chen","Zenan Zhou","Guosheng Dong","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.00997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10770v4","updated":"2024-10-02T09:18:47Z","published":"2024-02-16T15:48:33Z","title":"How Reliable Are Automatic Evaluation Methods for Instruction-Tuned\n  LLMs?","summary":"  Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we perform a meta-evaluation of such\nmethods and assess their reliability across a broad range of tasks. In\nevaluating how well automatic methods align with human evaluations, correlation\nmetrics are the most commonly employed method despite their inherent\nlimitations when dealing with ties and different scales. To address these\nshortcomings, we use Pairwise Accuracy as an alternative to standard\ncorrelation measures. We observe that while automatic evaluation methods can\napproximate human ratings under specific conditions, their validity is highly\ncontext-dependent. Specifically, the simple ROUGE-L metric correlates very well\nwith human ratings for short-answer English tasks but is unreliable in\nfree-form generation tasks and cross-lingual scenarios. The effectiveness of\nthe more advanced method of using GPT-4 as a judge diminishes significantly if\nreference answers are not included in the prompt, which is the scenario where\nthis method has the potential to provide the most value compared to other\nmetrics. Our findings enhance the understanding of how automatic methods should\nbe applied and interpreted when developing and evaluating instruction-tuned\nLLMs.\n","authors":["Ehsan Doostmohammadi","Oskar Holmstrm","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2402.10770v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01356v1","updated":"2024-10-02T09:14:39Z","published":"2024-10-02T09:14:39Z","title":"Assisted Data Annotation for Business Process Information Extraction\n  from Textual Documents","summary":"  Machine-learning based generation of process models from natural language\ntext process descriptions provides a solution for the time-intensive and\nexpensive process discovery phase. Many organizations have to carry out this\nphase, before they can utilize business process management and its benefits.\nYet, research towards this is severely restrained by an apparent lack of large\nand high-quality datasets. This lack of data can be attributed to, among other\nthings, an absence of proper tool assistance for dataset creation, resulting in\nhigh workloads and inferior data quality. We explore two assistance features to\nsupport dataset creation, a recommendation system for identifying process\ninformation in the text and visualization of the current state of already\nidentified process information as a graphical business process model. A\ncontrolled user study with 31 participants shows that assisting dataset\ncreators with recommendations lowers all aspects of workload, up to $-51.0\\%$,\nand significantly improves annotation quality, up to $+38.9\\%$. We make all\ndata and code available to encourage further research on additional novel\nassistance strategies.\n","authors":["Julian Neuberger","Han van der Aa","Lars Ackermann","Daniel Buschek","Jannic Herrmann","Stefan Jablonski"],"pdf_url":"https://arxiv.org/pdf/2410.01356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00037v2","updated":"2024-10-02T09:11:45Z","published":"2024-09-17T17:55:39Z","title":"Moshi: a speech-text foundation model for real-time dialogue","summary":"  We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.\n","authors":["Alexandre Dfossez","Laurent Mazar","Manu Orsini","Amlie Royer","Patrick Prez","Herv Jgou","Edouard Grave","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2410.00037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11184v3","updated":"2024-10-02T09:07:15Z","published":"2024-04-17T09:01:02Z","title":"FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out\n  Document","summary":"  Through the advent of pre-trained language models, there have been notable\nadvancements in abstractive summarization systems. Simultaneously, a\nconsiderable number of novel methods for evaluating factual consistency in\nabstractive summarization systems has been developed. But these evaluation\napproaches incorporate substantial limitations, especially on refinement and\ninterpretability. In this work, we propose highly effective and interpretable\nfactual inconsistency detection method metric Factual Inconsistency Detection\nby Zoom-in Summary and Zoom-out Document for abstractive summarization systems\nthat is based on fine-grained atomic facts decomposition. Moreover, we align\natomic facts decomposed from the summary with the source document through\nadaptive granularity expansion. These atomic facts represent a more\nfine-grained unit of information, facilitating detailed understanding and\ninterpretability of the summary's factual inconsistency. Experimental results\ndemonstrate that our proposed factual consistency checking system significantly\noutperforms existing systems.\n","authors":["Joonho Yang","Seunghyun Yoon","Byeongjeong Kim","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2404.11184v3.pdf","comment":"Published as a main conference paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01335v1","updated":"2024-10-02T08:53:07Z","published":"2024-10-02T08:53:07Z","title":"Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models","summary":"  Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.\n","authors":["Lucas Bandarkar","Benjamin Muller","Pritish Yuvraj","Rui Hou","Nayan Singhal","Hongjiang Lv","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01335v1.pdf","comment":"11 main pages, 23 pages total, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.01334v1","updated":"2024-10-02T08:52:58Z","published":"2024-10-02T08:52:58Z","title":"Unveiling Language Skills under Circuits","summary":"  The exploration of language skills in language models (LMs) has always been\none of the central goals in mechanistic interpretability. However, existing\ncircuit analyses often fall short in representing the full functional scope of\nthese models, primarily due to the exclusion of Feed-Forward layers.\nAdditionally, isolating the effect of a single language skill from a text,\nwhich inherently involves multiple entangled skills, poses a significant\nchallenge. To address these gaps, we introduce a novel concept, Memory Circuit,\na minimum unit that fully and independently manipulates the memory-reading\nfunctionality of a language model, and disentangle the transformer model\nprecisely into a circuit graph which is an ensemble of paths connecting\ndifferent memory circuits. Based on this disentanglement, we identify salient\ncircuit paths, named as skill paths, responsible for three crucial language\nskills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning\n(ICL) Skill, leveraging causal effect estimation through interventions and\ncounterfactuals. Our experiments on various datasets confirm the correspondence\nbetween our identified skill paths and language skills, and validate three\nlongstanding hypotheses: 1) Language skills are identifiable through circuit\ndissection; 2) Simple language skills reside in shallow layers, whereas complex\nlanguage skills are found in deeper layers; 3) Complex language skills are\nformed on top of simpler language skills. Our codes are available at:\nhttps://github.com/Zodiark-ch/Language-Skill-of-LLMs.\n","authors":["Hang Chen","Jiaying Zhu","Xinyu Yang","Wenya Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13979v3","updated":"2024-10-02T08:51:45Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03199v2","updated":"2024-10-02T08:45:32Z","published":"2024-05-24T13:33:11Z","title":"Bayesian WeakS-to-Strong from Text Classification to Generation","summary":"  Advances in large language models raise the question of how alignment\ntechniques will adapt as models become increasingly complex and humans will\nonly be able to supervise them weakly. Weak-to-Strong mimics such a scenario\nwhere weak model supervision attempts to harness the full capabilities of a\nmuch stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by\nexploring an ensemble of weak models which simulate the variability in human\nopinions. Confidence scores are estimated using a Bayesian approach to guide\nthe WeakS-to-Strong generalization. Furthermore, we extend the application of\nWeakS-to-Strong from text classification tasks to text generation tasks where\nmore advanced strategies are investigated for supervision. Moreover, direct\npreference optimization is applied to advance the student model's preference\nlearning, beyond the basic learning framework of teacher forcing. Results\ndemonstrate the effectiveness of the proposed approach for the reliability of a\nstrong student model, showing potential for superalignment.\n","authors":["Ziyun Cui","Ziyang Zhang","Wen Wu","Guangzhi Sun","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03199v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13550v2","updated":"2024-10-02T08:32:31Z","published":"2024-02-21T06:11:03Z","title":"Are LLMs Effective Negotiators? Systematic Evaluation of the\n  Multifaceted Capabilities of LLMs in Negotiation Dialogues","summary":"  A successful negotiation requires a range of capabilities, including\ncomprehension of the conversation context, Theory-of-Mind (ToM) skills to infer\nthe partner's motives, strategic reasoning, and effective communication, making\nit challenging for automated systems. Despite the remarkable performance of\nLLMs in various NLP tasks, there is no systematic evaluation of their\ncapabilities in negotiation. Such an evaluation is critical for advancing AI\nnegotiation agents and negotiation research, ranging from designing dialogue\nsystems to providing pedagogical feedback and scaling up data collection\npractices. This work aims to systematically analyze the multifaceted\ncapabilities of LLMs across diverse dialogue scenarios throughout the stages of\na typical negotiation interaction. Our analysis highlights GPT-4's superior\nperformance in many tasks while identifying specific challenges, such as making\nsubjective assessments and generating contextually appropriate, strategically\nadvantageous responses.\n","authors":["Deuksin Kwon","Emily Weiss","Tara Kulshrestha","Kushal Chawla","Gale M. Lucas","Jonathan Gratch"],"pdf_url":"https://arxiv.org/pdf/2402.13550v2.pdf","comment":"Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.20135v3","updated":"2024-10-02T08:32:02Z","published":"2024-09-30T09:34:31Z","title":"Federated Instruction Tuning of LLMs with Domain Coverage Augmentation","summary":"  Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with server-side public data for instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. For client-side computational efficiency and system scalability,\nFedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with\nserver-side feature alignment. Extensive experiments across four distinct\ndomains (code, medical, financial, and mathematical) substantiate the\neffectiveness of both methods. Additionally, we investigate privacy\npreservation against memory extraction attacks utilizing various amounts of\npublic data. Results show that there is no significant correlation between the\nvolume of public data and the privacy-preserving capability. However, as the\nfine-tuning rounds increase, the risk of privacy leakage reduces or converges.\n","authors":["Zezhou Wang","Yaxin Du","Zhuzhong Qian","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2409.20135v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00727v2","updated":"2024-10-02T08:08:45Z","published":"2024-10-01T14:16:10Z","title":"Show Me What's Wrong!: Combining Charts and Text to Guide Data Analysis","summary":"  Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome\nbut vital task across different domains. In the context of financial fraud\ndetection, analysts must quickly identify suspicious activity among\ntransactional data. This is an iterative process made of complex exploratory\ntasks such as recognizing patterns, grouping, and comparing. To mitigate the\ninformation overload inherent to these steps, we present a tool combining\nautomated information highlights, Large Language Model generated textual\ninsights, and visual analytics, facilitating exploration at different levels of\ndetail. We perform a segmentation of the data per analysis area and visually\nrepresent each one, making use of automated visual cues to signal which require\nmore attention. Upon user selection of an area, our system provides textual and\ngraphical summaries. The text, acting as a link between the high-level and\ndetailed views of the chosen segment, allows for a quick understanding of\nrelevant details. A thorough exploration of the data comprising the selection\ncan be done through graphical representations. The feedback gathered in a study\nperformed with seven domain experts suggests our tool effectively supports and\nguides exploratory analysis, easing the identification of suspicious\ninformation.\n","authors":["Beatriz Feliciano","Rita Costa","Jean Alves","Javier Libana","Diogo Duarte","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2410.00727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01306v1","updated":"2024-10-02T08:01:05Z","published":"2024-10-02T08:01:05Z","title":"Emotion-Aware Response Generation Using Affect-Enriched Embeddings with\n  LLMs","summary":"  There is a need for empathetic and coherent responses in automated\nchatbot-facilitated psychotherapy sessions. This study addresses the challenge\nof enhancing the emotional and contextual understanding of large language\nmodels (LLMs) in psychiatric applications. We introduce a novel framework that\nintegrates multiple emotion lexicons, including NRC Emotion Lexicon, VADER,\nWordNet, and SentiWordNet, with state-of-the-art LLMs such as LLAMA 2, Flan-T5,\nChatGPT 3.0, and ChatGPT 4.0. The primary dataset comprises over 2,000 therapy\nsession transcripts from the Counseling and Psychotherapy database, covering\ndiscussions on anxiety, depression, trauma, and addiction. We segment the\ntranscripts into smaller chunks, enhancing them with lexical features and\ncomputing embeddings using BERT, GPT-3, and RoBERTa to capture semantic and\nemotional nuances. These embeddings are stored in a FAISS vector database,\nenabling efficient similarity search and clustering based on cosine similarity.\nUpon user query, the most relevant segments are retrieved and provided as\ncontext to the LLMs, significantly improving the models' ability to generate\nempathetic and contextually appropriate responses. Experimental evaluations\ndemonstrate that in-corporating emotion lexicons enhances empathy, coherence,\ninformativeness, and fluency scores. Our findings highlight the critical role\nof emotional embeddings in improving LLM performance for psychotherapy.\n","authors":["Abdur Rasool","Muhammad Irfan Shahzad","Hafsa Aslam","Vincent Chan"],"pdf_url":"https://arxiv.org/pdf/2410.01306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04927v3","updated":"2024-10-02T07:58:56Z","published":"2024-09-07T22:54:47Z","title":"Just ASR + LLM? A Study on Speech Large Language Models' Ability to\n  Identify and Understand Speaker in Spoken Dialogue","summary":"  In recent years, we have observed a rapid advancement in speech language\nmodels (SpeechLLMs), catching up with humans' listening and reasoning\nabilities. SpeechLLMs have demonstrated impressive spoken dialog\nquestion-answering (SQA) performance in benchmarks like Gaokao, the English\nlistening test of the college entrance exam in China, which seemingly requires\nunderstanding both the spoken content and voice characteristics of speakers in\na conversation. However, after carefully examining Gaokao's questions, we find\nthe correct answers to many questions can be inferred from the conversation\ntranscript alone, i.e.\\ without speaker segmentation and identification. Our\nevaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and\nour proposed \"What Do You Like?\" dataset shows a significantly higher accuracy\nin these context-based questions than in identity-critical questions, which can\nonly be answered reliably with correct speaker identification. The results and\nanalysis suggest that when solving SQA, the current SpeechLLMs exhibit limited\nspeaker awareness from the audio and behave similarly to an LLM reasoning from\nthe conversation transcription without sound. We propose that tasks focused on\nidentity-critical questions could offer a more accurate evaluation framework of\nSpeechLLMs in SQA.\n","authors":["Junkai Wu","Xulin Fan","Bo-Ru Lu","Xilin Jiang","Nima Mesgarani","Mark Hasegawa-Johnson","Mari Ostendorf"],"pdf_url":"https://arxiv.org/pdf/2409.04927v3.pdf","comment":"Accepted to IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2410.01305v1","updated":"2024-10-02T07:57:33Z","published":"2024-10-02T07:57:33Z","title":"Revisiting Hierarchical Text Classification: Inference and Metrics","summary":"  Hierarchical text classification (HTC) is the task of assigning labels to a\ntext within a structured space organized as a hierarchy. Recent works treat HTC\nas a conventional multilabel classification problem, therefore evaluating it as\nsuch. We instead propose to evaluate models based on specifically designed\nhierarchical metrics and we demonstrate the intricacy of metric choice and\nprediction inference method. We introduce a new challenging dataset and we\nevaluate fairly, recent sophisticated models, comparing them with a range of\nsimple but strong baselines, including a new theoretically motivated loss.\nFinally, we show that those baselines are very often competitive with the\nlatest models. This highlights the importance of carefully considering the\nevaluation methodology when proposing new methods for HTC. Code implementation\nand dataset are available at \\url{https://github.com/RomanPlaud/revisitingHTC}.\n","authors":["Roman Plaud","Matthieu Labeau","Antoine Saillenfest","Thomas Bonald"],"pdf_url":"https://arxiv.org/pdf/2410.01305v1.pdf","comment":"Accepted at CoNLL 2024"},{"id":"http://arxiv.org/abs/2409.16911v2","updated":"2024-10-02T07:52:56Z","published":"2024-09-25T13:15:50Z","title":"Pruning Multilingual Large Language Models for Multilingual Inference","summary":"  Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language.\n","authors":["Hwichan Kim","Jun Suzuki","Tosho Hirasawa","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2409.16911v2.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.04507v2","updated":"2024-10-02T07:52:37Z","published":"2024-09-06T16:32:46Z","title":"3D Data Long-Term Preservation in Cultural Heritage","summary":"  The report explores the challenges and strategies for preserving 3D digital\ndata in cultural heritage. It discusses the issue of technological\nobsolescence, emphasising the need for ustainable storage solutions and ongoing\ndata management strategies. Key topics include understanding technological\nobsolescence, the lifecycle of digital content, digital continuity, data\nmanagement plans (DMP), FAIR principles, and the use of public repositories.\nThe report also covers the importance of metadata in long-term digital\npreservation, including types of metadata and strategies for building valuable\nmetadata. It examines the evolving standards and interoperability in 3D format\npreservation and the importance of managing metadata and paradata. The document\nprovides a comprehensive overview of the challenges and solutions for\npreserving 3D cultural heritage data in the long term.\n","authors":["Nicola Amico","Achille Felicetti"],"pdf_url":"https://arxiv.org/pdf/2409.04507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16508v3","updated":"2024-10-02T07:51:47Z","published":"2024-02-26T11:42:29Z","title":"Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision","summary":"  Cross-lingual open domain question answering (CLQA) is a complex problem,\ncomprising cross-lingual retrieval from a multilingual knowledge base, followed\nby answer generation in the query language. Both steps are usually tackled by\nseparate models, requiring substantial annotated datasets, and typically\nauxiliary resources, like machine translation systems to bridge between\nlanguages. In this paper, we show that CLQA can be addressed using a single\nencoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural questions to supervise answer\ngeneration. Together, we show our approach, \\texttt{CLASS}, outperforms\ncomparable methods on both supervised and zero-shot language adaptation\nsettings, including those using machine translation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2402.16508v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01294v1","updated":"2024-10-02T07:40:56Z","published":"2024-10-02T07:40:56Z","title":"Endless Jailbreaks with Bijection Learning","summary":"  Despite extensive safety training, LLMs are vulnerable to adversarial inputs.\nIn this work, we introduce a simple but powerful attack paradigm, bijection\nlearning, that yields a practically endless set of jailbreak prompts. We\nexploit language models' advanced reasoning capabilities to teach them\ninvertible languages (bijections) in context, pass encoded queries to the model\nto bypass built-in safety mechanisms, and finally decode responses back into\nEnglish, yielding helpful replies to harmful requests. Our approach proves\neffective on a wide range of frontier language models and harm categories.\nBijection learning is an automated and universal attack that grows stronger\nwith scale: larger models with more advanced reasoning capabilities are more\nsusceptible to bijection learning jailbreaks despite stronger safety\nmechanisms.\n","authors":["Brian R. Y. Huang","Maximilian Li","Leonard Tang"],"pdf_url":"https://arxiv.org/pdf/2410.01294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15371v3","updated":"2024-10-02T07:38:02Z","published":"2024-09-19T10:26:42Z","title":"Bone: Block Affine Transformation as Parameter Efficient Fine-tuning\n  Methods for Large Language Models","summary":"  Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. However, these improvements\ncomplicate the initial setup of model training and increase initialization\ntime. More importantly, they overlook the internal interactions of the original\nweight information. To address these issues, we introduce a novel theory,\n``Weight Guide'' aimed at continuously guiding trainable matrices through the\noriginal weights during training to enhance the utilization of weight\ninformation. Based on this theory, we designed a new PEFT technique called Bone\n(\\textbf{B}l\\textbf{o}ck Affi\\textbf{ne}), which not only enhances the\nutilization of original weight information but also emphasizes the internal\nconnections between weights, leading to faster convergence and better data\nfitting. Experimental comparisons across two different LLM architectures\n(LLaMA2, RWKV6) and various parameter scales demonstrate that the Bone\nstructure can achieve rapid convergence and superior data fitting without the\nneed for complex initialization. For example, when fine-tuning LLaMA2-7B on the\nMetaMathQA dataset and validating on GSM8k and math benchmarks, Bone achieved\nfine-tuning scores of 49.36 and 8.8, respectively, outperforming PISSA by\n5.84\\% and 1.96\\%.\n","authors":["Jiale Kang"],"pdf_url":"https://arxiv.org/pdf/2409.15371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08700v3","updated":"2024-10-02T07:26:40Z","published":"2024-04-10T18:08:59Z","title":"DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs","summary":"  LLMs acquire knowledge from massive data snapshots collected at different\ntimestamps. Their knowledge is then commonly evaluated using static benchmarks.\nHowever, factual knowledge is generally subject to time-sensitive changes, and\nstatic benchmarks cannot address those cases. We present an approach to\ndynamically evaluate the knowledge in LLMs and their time-sensitiveness against\nWikidata, a publicly available up-to-date knowledge graph. We evaluate the\ntime-sensitive knowledge in twenty-four private and open-source LLMs, as well\nas the effectiveness of four editing methods in updating the outdated facts.\nOur results show that 1) outdatedness is a critical problem across\nstate-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with\nslight variations of the question prompt; and 3) the performance of the\nstate-of-the-art knowledge editing algorithms is very limited, as they can not\nreduce the cases of outdatedness and output inconsistency.\n","authors":["Seyed Mahed Mousavi","Simone Alghisi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2404.08700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01285v1","updated":"2024-10-02T07:14:26Z","published":"2024-10-02T07:14:26Z","title":"Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration","summary":"  The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.\n","authors":["Kangxi Wu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01285v1.pdf","comment":"Accepted to the EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2305.03977v3","updated":"2024-10-02T06:35:36Z","published":"2023-05-06T08:43:33Z","title":"Unlocking the Power of GANs in Non-Autoregressive Text Generation","summary":"  Generative Adversarial Networks (GANs) have been studied in text generation\nto tackle the exposure bias problem. Despite their remarkable development, they\nadopt autoregressive structures so suffering from high latency in both training\nand inference stages. Although GANs have potential to support efficient\ngeneration by adopting non-autoregressive (NAR) structures, their explorations\nin NAR models are extremely limited. In this work, we conduct pioneering study\nof building language GANs based on NAR structures. We identify two issues that\nconstrain the performance of GAN-based NAR models. Firstly, existing methods of\nincorporating latent variables provide highly similar representations which\ncannot describe the diversity of different words in sentences. We tackle this\nproblem by proposing Position-Aware Self-Modulation, providing more diverse and\neffective representations. Secondly, the attention mechanism in Transformer\ncannot accurately build word dependencies in the unstable training of GANs, and\nwe adopt Dependency Feed Forward Network to enhance the model capacity in\ndependency modeling. Armed with these two facilities, we propose a GAN-based\nNAR model, Adversarial Non-autoregressive Transformer (ANT). The experimental\nresults demonstrate that ANT can achieve comparable performance with mainstream\nmodels in a single forward pass and has great potential in various applications\nlike latent interpolation and semi-supervised learning.\n","authors":["Da Ren","Yi Cai","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2305.03977v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02118v2","updated":"2024-10-02T06:32:10Z","published":"2024-07-02T10:06:41Z","title":"Breaking Language Barriers: Cross-Lingual Continual Pre-Training at\n  Scale","summary":"  In recent years, Large Language Models (LLMs) have made significant strides\ntowards Artificial General Intelligence. However, training these models from\nscratch requires substantial computational resources and vast amounts of text\ndata. In this paper, we explore an alternative approach to constructing an LLM\nfor a new language by continually pretraining (CPT) from existing pretrained\nLLMs, instead of using randomly initialized parameters. Based on parallel\nexperiments on 40 model sizes ranging from 40M to 5B parameters, we find that\n1) CPT converges faster and saves significant resources in a scalable manner;\n2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)\nwith a joint data-parameter scaling term; 3) The compute-optimal data-parameter\nallocation for CPT markedly differs based on our estimated scaling factors; 4)\nThe effectiveness of transfer at scale is influenced by training duration and\nlinguistic properties, while robust to data replaying, a method that\neffectively mitigates catastrophic forgetting in CPT. We hope our findings\nprovide deeper insights into the transferability of LLMs at scale for the\nresearch community.\n","authors":["Wenzhen Zheng","Wenbo Pan","Xu Xu","Libo Qin","Li Yue","Ming Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.02118v2.pdf","comment":"8 pages. Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01268v1","updated":"2024-10-02T06:24:51Z","published":"2024-10-02T06:24:51Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Unveiling AI's Potential Through Tools, Techniques, and\n  Applications","summary":"  This book serves as an introduction to deep learning and machine learning,\nfocusing on their applications in big data analytics. It covers essential\nconcepts, tools like ChatGPT and Claude, hardware recommendations, and\npractical guidance on setting up development environments using libraries like\nPyTorch and TensorFlow. Designed for beginners and advanced users alike, it\nprovides step-by-step instructions, hands-on projects, and insights into AI's\nfuture, including AutoML and edge computing.\n","authors":["Pohsun Feng","Ziqian Bi","Yizhu Wen","Xuanhe Pan","Benji Peng","Ming Liu","Jiawei Xu","Keyu Chen","Junyu Liu","Caitlyn Heqi Yin","Sen Zhang","Jinlang Wang","Qian Niu","Ming Li","Tianyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01268v1.pdf","comment":"This book contains 156 pages and 9 figures"},{"id":"http://arxiv.org/abs/2407.02834v3","updated":"2024-10-02T06:18:02Z","published":"2024-07-03T06:21:07Z","title":"Aspect-Based Sentiment Analysis Techniques: A Comparative Study","summary":"  Since the dawn of the digitalisation era, customer feedback and online\nreviews are unequivocally major sources of insights for businesses.\nConsequently, conducting comparative analyses of such sources has become the de\nfacto modus operandi of any business that wishes to give itself a competitive\nedge over its peers and improve customer loyalty. Sentiment analysis is one\nsuch method instrumental in gauging public interest, exposing market trends,\nand analysing competitors. While traditional sentiment analysis focuses on\noverall sentiment, as the needs advance with time, it has become important to\nexplore public opinions and sentiments on various specific subjects, products\nand services mentioned in the reviews on a finer-granular level. To this end,\nAspect-based Sentiment Analysis (ABSA), supported by advances in Artificial\nIntelligence (AI) techniques which have contributed to a paradigm shift from\nsimple word-level analysis to tone and context-aware analyses, focuses on\nidentifying specific aspects within the text and determining the sentiment\nassociated with each aspect. In this study, we compare several deep-NN methods\nfor ABSA on two benchmark datasets (Restaurant14 and Laptop-14) and found that\nFAST LSA obtains the best overall results of 87.6% and 82.6% accuracy but does\nnot pass LSA+DeBERTa which reports 90.33% and 86.21% accuracy respectively.\n","authors":["Dineth Jayakody","Koshila Isuranda","A V A Malkith","Nisansa de Silva","Sachintha Rajith Ponnamperuma","G G N Sandamali","K L K Sudheera"],"pdf_url":"https://arxiv.org/pdf/2407.02834v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13621v2","updated":"2024-10-02T06:14:17Z","published":"2024-09-20T16:32:54Z","title":"Advancing Event Causality Identification via Heuristic Semantic\n  Dependency Inquiry Network","summary":"  Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.\n","authors":["Haoran Li","Qiang Gao","Hongmei Wu","Li Huang"],"pdf_url":"https://arxiv.org/pdf/2409.13621v2.pdf","comment":"EMNLP 2024 camera-ready version. Code is released at\n  https://github.com/hrlics/SemDI"},{"id":"http://arxiv.org/abs/2410.01257v1","updated":"2024-10-02T06:05:52Z","published":"2024-10-02T06:05:52Z","title":"HelpSteer2-Preference: Complementing Ratings with Preferences","summary":"  Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We\nalso demonstrate the effectiveness of this reward model at aligning models to\nfollow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the\ntrained Reward Model at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward\n","authors":["Zhilin Wang","Alexander Bukharin","Olivier Delalleau","Daniel Egert","Gerald Shen","Jiaqi Zeng","Oleksii Kuchaiev","Yi Dong"],"pdf_url":"https://arxiv.org/pdf/2410.01257v1.pdf","comment":"26 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.03868v2","updated":"2024-10-02T05:51:53Z","published":"2024-04-05T02:53:51Z","title":"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge\n  Graph Construction","summary":"  In this work, we are interested in automated methods for knowledge graph\ncreation (KGC) from input text. Progress on large language models (LLMs) has\nprompted a series of recent works applying them to KGC, e.g., via zero/few-shot\nprompting. Despite successes on small domain-specific datasets, these models\nface difficulties scaling up to text common in many real-world applications. A\nprincipal issue is that, in prior methods, the KG schema has to be included in\nthe LLM prompt to generate valid triplets; larger and more complex schemas\neasily exceed the LLMs' context window length. Furthermore, there are scenarios\nwhere a fixed pre-defined schema is not available and we would like the method\nto construct a high-quality KG with a succinct self-generated schema. To\naddress these problems, we propose a three-phase framework named\nExtract-Define-Canonicalize (EDC): open information extraction followed by\nschema definition and post-hoc canonicalization. EDC is flexible in that it can\nbe applied to settings where a pre-defined target schema is available and when\nit is not; in the latter case, it constructs a schema automatically and applies\nself-canonicalization. To further improve performance, we introduce a trained\ncomponent that retrieves schema elements relevant to the input text; this\nimproves the LLMs' extraction performance in a retrieval-augmented\ngeneration-like manner. We demonstrate on three KGC benchmarks that EDC is able\nto extract high-quality triplets without any parameter tuning and with\nsignificantly larger schemas compared to prior works. Code for EDC is available\nat https://github.com/clear-nus/edc.\n","authors":["Bowen Zhang","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2404.03868v2.pdf","comment":"18 pages, 3 figures, Proceedings of the 2024 Conference on Empirical\n  Methods in Natural Language Processing"},{"id":"http://arxiv.org/abs/2410.01246v1","updated":"2024-10-02T05:22:07Z","published":"2024-10-02T05:22:07Z","title":"AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended\n  Responses","summary":"  Question answering (QA) tasks have been extensively studied in the field of\nnatural language processing (NLP). Answers to open-ended questions are highly\ndiverse and difficult to quantify, and cannot be simply evaluated as correct or\nincorrect, unlike close-ended questions with definitive answers. While large\nlanguage models (LLMs) have demonstrated strong capabilities across various\ntasks, they exhibit relatively weaker performance in evaluating answers to\nopen-ended questions. In this study, we propose a method that leverages LLMs\nand the analytic hierarchy process (AHP) to assess answers to open-ended\nquestions. We utilized LLMs to generate multiple evaluation criteria for a\nquestion. Subsequently, answers were subjected to pairwise comparisons under\neach criterion with LLMs, and scores for each answer were calculated in the\nAHP. We conducted experiments on four datasets using both ChatGPT-3.5-turbo and\nGPT-4. Our results indicate that our approach more closely aligns with human\njudgment compared to the four baselines. Additionally, we explored the impact\nof the number of criteria, variations in models, and differences in datasets on\nthe results.\n","authors":["Xiaotian Lu","Jiyi Li","Koh Takeuchi","Hisashi Kashima"],"pdf_url":"https://arxiv.org/pdf/2410.01246v1.pdf","comment":"Accepted for EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.01240v1","updated":"2024-10-02T05:04:06Z","published":"2024-10-02T05:04:06Z","title":"Automatic deductive coding in discourse analysis: an application of\n  large language models in learning analytics","summary":"  Deductive coding is a common discourse analysis method widely used by\nlearning science and learning analytics researchers for understanding teaching\nand learning interactions. It often requires researchers to manually label all\ndiscourses to be analyzed according to a theoretically guided coding scheme,\nwhich is time-consuming and labor-intensive. The emergence of large language\nmodels such as GPT has opened a new avenue for automatic deductive coding to\novercome the limitations of traditional deductive coding. To evaluate the\nusefulness of large language models in automatic deductive coding, we employed\nthree different classification methods driven by different artificial\nintelligence technologies, including the traditional text classification method\nwith text feature engineering, BERT-like pretrained language model and GPT-like\npretrained large language model (LLM). We applied these methods to two\ndifferent datasets and explored the potential of GPT and prompt engineering in\nautomatic deductive coding. By analyzing and comparing the accuracy and Kappa\nvalues of these three classification methods, we found that GPT with prompt\nengineering outperformed the other two methods on both datasets with limited\nnumber of training samples. By providing detailed prompt structures, the\nreported work demonstrated how large language models can be used in the\nimplementation of automatic deductive coding.\n","authors":["Lishan Zhang","Han Wu","Xiaoshan Huang","Tengfei Duan","Hanxiang Du"],"pdf_url":"https://arxiv.org/pdf/2410.01240v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2409.05152v2","updated":"2024-10-02T05:02:02Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v2.pdf","comment":"EMNLP 2024 Findings; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2409.04081v3","updated":"2024-10-02T05:00:57Z","published":"2024-09-06T07:44:44Z","title":"UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity","summary":"  Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.\n","authors":["Yicheng Fu","Raviteja Anantha","Prabal Vashisht","Jianpeng Cheng","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2409.04081v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01035v2","updated":"2024-10-02T04:20:31Z","published":"2024-09-02T08:10:51Z","title":"Unleashing the Power of Task-Specific Directions in Parameter Efficient\n  Fine-tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet requiring extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs)-critical for transitioning large\nmodels from pretrained states to task-specific enhancements in PEFT. We propose\na framework to clearly define these directions and explore their properties,\nand practical utilization challenges. We then introduce a novel approach,\nLoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning\nprocess, thereby enhancing model performance on targeted tasks. Extensive\nexperiments have conclusively demonstrated the effectiveness of LoRA-Dash, and\nin-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The\ncode is available at https://github.com/Chongjie-Si/Subspace-Tuning.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v2.pdf","comment":"Revisions ongoing. Codes in\n  https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2401.05967v3","updated":"2024-10-02T04:17:36Z","published":"2024-01-11T15:13:00Z","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding","summary":"  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n","authors":["Yihua Zhu","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.05967v3.pdf","comment":"EMNLP2024 findings (Long)"},{"id":"http://arxiv.org/abs/2409.19541v3","updated":"2024-10-02T04:15:11Z","published":"2024-09-29T03:56:50Z","title":"Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance\n  Regularization","summary":"  Language models frequently inherit societal biases from their training data.\nNumerous techniques have been proposed to mitigate these biases during both the\npre-training and fine-tuning stages. However, fine-tuning a pre-trained\ndebiased language model on a downstream task can reintroduce biases into the\nmodel. Additionally, existing debiasing methods for downstream tasks either (i)\nrequire labels of protected attributes (e.g., age, race, or political views)\nthat are often not available or (ii) rely on indicators of bias, which\nrestricts their applicability to gender debiasing since they rely on\ngender-specific words. To address this, we introduce a novel debiasing\nregularization technique based on the class-wise variance of embeddings.\nCrucially, our method does not require attribute labels and targets any\nattribute, thus addressing the shortcomings of existing debiasing methods. Our\nexperiments on encoder language models and three datasets demonstrate that our\nmethod outperforms existing strong debiasing baselines that rely on target\nattribute labels while maintaining performance on the target task.\n","authors":["Shahed Masoudian","Markus Frohmann","Navid Rekabsaz","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2409.19541v3.pdf","comment":"Accepted to EMNLP 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.01806v1","updated":"2024-10-02T17:59:57Z","published":"2024-10-02T17:59:57Z","title":"Samba: Synchronized Set-of-Sequences Modeling for Multiple Object\n  Tracking","summary":"  Multiple object tracking in complex scenarios - such as coordinated dance\nperformances, team sports, or dynamic animal groups - presents unique\nchallenges. In these settings, objects frequently move in coordinated patterns,\nocclude each other, and exhibit long-term dependencies in their trajectories.\nHowever, it remains a key open research question on how to model long-range\ndependencies within tracklets, interdependencies among tracklets, and the\nassociated temporal occlusions. To this end, we introduce Samba, a novel\nlinear-time set-of-sequences model designed to jointly process multiple\ntracklets by synchronizing the multiple selective state-spaces used to model\neach tracklet. Samba autoregressively predicts the future track query for each\nsequence while maintaining synchronized long-term memory representations across\ntracklets. By integrating Samba into a tracking-by-propagation framework, we\npropose SambaMOTR, the first tracker effectively addressing the aforementioned\nissues, including long-range dependencies, tracklet interdependencies, and\ntemporal occlusions. Additionally, we introduce an effective technique for\ndealing with uncertain observations (MaskObs) and an efficient training recipe\nto scale SambaMOTR to longer sequences. By modeling long-range dependencies and\ninteractions among tracked objects, SambaMOTR implicitly learns to track\nobjects accurately through occlusions without any hand-crafted heuristics. Our\napproach significantly surpasses prior state-of-the-art on the DanceTrack, BFT,\nand SportsMOT datasets.\n","authors":["Mattia Segu","Luigi Piccinelli","Siyuan Li","Yung-Hsu Yang","Bernt Schiele","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2410.01806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01804v1","updated":"2024-10-02T17:59:09Z","published":"2024-10-02T17:59:09Z","title":"EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis","summary":"  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n","authors":["Alexander Mai","Peter Hedman","George Kopanas","Dor Verbin","David Futschik","Qiangeng Xu","Falko Kuester","Jon Barron","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01804v1.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/ever"},{"id":"http://arxiv.org/abs/2410.01801v1","updated":"2024-10-02T17:57:12Z","published":"2024-10-02T17:57:12Z","title":"FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images","summary":"  We introduce FabricDiffusion, a method for transferring fabric textures from\na single clothing image to 3D garments of arbitrary shapes. Existing approaches\ntypically synthesize textures on the garment surface through 2D-to-3D texture\nmapping or depth-aware inpainting via generative models. Unfortunately, these\nmethods often struggle to capture and preserve texture details, particularly\ndue to challenging occlusions, distortions, or poses in the input image.\nInspired by the observation that in the fashion industry, most garments are\nconstructed by stitching sewing patterns with flat, repeatable textures, we\ncast the task of clothing texture transfer as extracting distortion-free,\ntileable texture materials that are subsequently mapped onto the UV space of\nthe garment. Building upon this insight, we train a denoising diffusion model\nwith a large-scale synthetic dataset to rectify distortions in the input\ntexture image. This process yields a flat texture map that enables a tight\ncoupling with existing Physically-Based Rendering (PBR) material generation\npipelines, allowing for realistic relighting of the garment under various\nlighting conditions. We show that FabricDiffusion can transfer various features\nfrom a single clothing image including texture patterns, material properties,\nand detailed prints and logos. Extensive experiments demonstrate that our model\nsignificantly outperforms state-to-the-art methods on both synthetic data and\nreal-world, in-the-wild clothing images while generalizing to unseen textures\nand garment shapes.\n","authors":["Cheng Zhang","Yuanhao Wang","Francisco Vicente Carrasco","Chenglei Wu","Jinlong Yang","Thabo Beeler","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2410.01801v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024. Project page:\n  https://humansensinglab.github.io/fabric-diffusion"},{"id":"http://arxiv.org/abs/2407.01445v3","updated":"2024-10-02T17:34:06Z","published":"2024-07-01T16:37:18Z","title":"FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training\n  with Limited Resources","summary":"  Existing studies of training state-of-the-art Contrastive Language-Image\nPretraining (CLIP) models on large-scale data involve hundreds of or even\nthousands of GPUs due to the requirement of a large batch size. However, such a\nlarge amount of resources is not accessible to most people. While advanced\ncompositional optimization techniques for optimizing global contrastive losses\nhave been demonstrated effective for removing the requirement of large batch\nsize, their performance on large-scale data remains underexplored and not\noptimized. To bridge the gap, this paper explores several aspects of CLIP\ntraining with limited resources (e.g., up to tens of GPUs). First, we introduce\nFastCLIP, a general CLIP training framework built on advanced compositional\noptimization techniques while designed and optimized for the distributed\nsetting. Our framework is equipped with an efficient gradient reduction\nstrategy to reduce communication overhead. Second, to further boost training\nefficiency, we investigate three components of the framework from an\noptimization perspective: the schedule of the inner learning rate, the update\nrules of the temperature parameter and the model parameters, respectively.\nExperiments on different strategies for each component shed light on how to\nconduct CLIP training more efficiently. Finally, we benchmark the performance\nof FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different\ncompute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7\nmillion, 9.1 million to 315 million image-text pairs to demonstrate the\nsignificant improvement of FastCLIP in the resource-limited setting. We release\nthe code of FastCLIP at https://github.com/Optimization-AI/fast_clip .\n","authors":["Xiyuan Wei","Fanjiang Ye","Ori Yonay","Xingyu Chen","Baixi Sun","Dingwen Tao","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01445v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2407.12492v2","updated":"2024-10-02T17:29:54Z","published":"2024-07-17T11:18:49Z","title":"Temporal Test-Time Adaptation with State-Space Models","summary":"  Distribution shifts between training and test data are inevitable over the\nlifecycle of a deployed model, leading to performance decay. Adapting a model\non test samples can help mitigate this drop in performance. However, most\ntest-time adaptation methods have focused on synthetic corruption shifts,\nleaving a variety of distribution shifts underexplored. In this paper, we focus\non distribution shifts that evolve gradually over time, which are common in the\nwild but challenging for existing methods, as we show. To address this, we\npropose STAD, a probabilistic state-space model that adapts a deployed model to\ntemporal distribution shifts by learning the time-varying dynamics in the last\nset of hidden features. Without requiring labels, our model infers\ntime-evolving class prototypes that act as a dynamic classification head.\nThrough experiments on real-world temporal distribution shifts, we show that\nour method excels in handling small batch sizes and label shift.\n","authors":["Mona Schirmer","Dan Zhang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2407.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01768v1","updated":"2024-10-02T17:25:31Z","published":"2024-10-02T17:25:31Z","title":"SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for\n  Remote Sensing Images","summary":"  Remote sensing image plays an irreplaceable role in fields such as\nagriculture, water resources, military, and disaster relief. Pixel-level\ninterpretation is a critical aspect of remote sensing image applications;\nhowever, a prevalent limitation remains the need for extensive manual\nannotation. For this, we try to introduce open-vocabulary semantic segmentation\n(OVSS) into the remote sensing context. However, due to the sensitivity of\nremote sensing images to low-resolution features, distorted target shapes and\nill-fitting boundaries are exhibited in the prediction mask. To tackle this\nissue, we propose a simple and general upsampler, SimFeatUp, to restore lost\nspatial information in deep features in a training-free style. Further, based\non the observation of the abnormal response of local patch tokens to [CLS]\ntoken in CLIP, we propose to execute a straightforward subtraction operation to\nalleviate the global bias in patch tokens. Extensive experiments are conducted\non 17 remote sensing datasets spanning semantic segmentation, building\nextraction, road detection, and flood detection tasks. Our method achieves an\naverage of 5.8%, 8.2%, 4%, and 15.3% improvement over state-of-the-art methods\non 4 tasks. All codes are released.\n\\url{https://earth-insights.github.io/SegEarth-OV}\n","authors":["Kaiyu Li","Ruixun Liu","Xiangyong Cao","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17253v2","updated":"2024-10-02T17:21:47Z","published":"2024-08-30T12:51:55Z","title":"VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters","summary":"  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either repurpose large language models\n(LLMs) or build large-scale time series datasets to develop TSF foundation\nmodels for universal forecasting. However, these methods face challenges due to\nthe severe cross-domain gap or in-domain heterogeneity. This paper explores a\nnew road to building a TSF foundation model from rich, high-quality natural\nimages. Our key insight is that a visual masked autoencoder, pre-trained on the\nImageNet dataset, can naturally be a numeric series forecaster. By\nreformulating TSF as an image reconstruction task, we bridge the gap between\nimage pre-training and TSF downstream tasks. Surprisingly, without further\nadaptation in the time-series domain, the proposed VisionTS could achieve\nsuperior zero-shot forecasting performance compared to existing TSF foundation\nmodels. With fine-tuning for one epoch, VisionTS could further improve the\nforecasting and achieve state-of-the-art performance in most cases. Extensive\nexperiments reveal intrinsic similarities between images and real-world time\nseries, suggesting visual models may offer a ``free lunch'' for TSF and\nhighlight the potential for future cross-modality research. Our code is\npublicly available at https://github.com/Keytoyze/VisionTS.\n","authors":["Mouxiang Chen","Lefei Shen","Zhuo Li","Xiaoyun Joy Wang","Jianling Sun","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17253v2.pdf","comment":"v2: add more experiments"},{"id":"http://arxiv.org/abs/2410.01766v1","updated":"2024-10-02T17:21:43Z","published":"2024-10-02T17:21:43Z","title":"SegHeD: Segmentation of Heterogeneous Data for Multiple Sclerosis\n  Lesions with Anatomical Constraints","summary":"  Assessment of lesions and their longitudinal progression from brain magnetic\nresonance (MR) images plays a crucial role in diagnosing and monitoring\nmultiple sclerosis (MS). Machine learning models have demonstrated a great\npotential for automated MS lesion segmentation. Training such models typically\nrequires large-scale high-quality datasets that are consistently annotated.\nHowever, MS imaging datasets are often small, segregated across multiple sites,\nwith different formats (cross-sectional or longitudinal), and diverse\nannotation styles. This poses a significant challenge to train a unified MS\nlesion segmentation model. To tackle this challenge, we present SegHeD, a novel\nmulti-dataset multi-task segmentation model that can incorporate heterogeneous\ndata as input and perform all-lesion, new-lesion, as well as vanishing-lesion\nsegmentation. Furthermore, we account for domain knowledge about MS lesions,\nincorporating longitudinal, spatial, and volumetric constraints into the\nsegmentation model. SegHeD is assessed on five MS datasets and achieves a high\nperformance in all, new, and vanishing-lesion segmentation, outperforming\nseveral state-of-the-art methods in this field.\n","authors":["Berke Doga Basaran","Xinru Zhang","Paul M. Matthews","Wenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2410.01766v1.pdf","comment":"13 pages, 4 figures, MICCAI, LDTM Workshop"},{"id":"http://arxiv.org/abs/2406.10995v2","updated":"2024-10-02T17:20:28Z","published":"2024-06-16T16:15:20Z","title":"Concept-skill Transferability-based Data Selection for Large\n  Vision-Language Models","summary":"  Instruction tuning, or supervised finetuning on extensive task-specific data,\nis necessary for Large Vision-Language Models (LVLMs) to generalize well across\na broad range of vision-language (VL) tasks. However, training on large VL\ndatasets can become prohibitively expensive. In this work, we introduce\nCOINCIDE, an effective and scalable data selection technique that uses a small\nmodel as a reference model to select visual instruction tuning data for\nefficient finetuning of a target LVLM, focusing on diversity and\ntransferability. Specifically, we cluster the training data using internal\nactivations from a small model, which identifies VL concept-skill compositions\nneeded by a target LVLM. We then sample data from these diverse clusters by\nconsidering their density and transferability, or the ability to transfer well\nto other concept-skill compositions. This approach ensures the diversity of\nthese compositions, which is vital for LVLM generalization. Extensive\nexperiments demonstrate that COINCIDE achieves superior performance and data\nselection efficiency against 8 strong baselines on two distinct datasets:\nLLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE\nachieves performance comparable to the LVLM finetuned on the whole dataset,\nwith 70% reduction of the wall-clock running time. On the Vision-Flan dataset,\nour method achieves superior results with only 16.7% of the training data.\n","authors":["Jaewoo Lee","Boyang Li","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.10995v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.15870v2","updated":"2024-10-02T17:07:13Z","published":"2024-02-24T17:22:15Z","title":"Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian\n  Splatting","summary":"  The recent advancements in 3D Gaussian splatting (3D-GS) have not only\nfacilitated real-time rendering through modern GPU rasterization pipelines but\nhave also attained state-of-the-art rendering quality. Nevertheless, despite\nits exceptional rendering quality and performance on standard datasets, 3D-GS\nfrequently encounters difficulties in accurately modeling specular and\nanisotropic components. This issue stems from the limited ability of spherical\nharmonics (SH) to represent high-frequency information. To overcome this\nchallenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic\nspherical Gaussian (ASG) appearance field instead of SH for modeling the\nview-dependent appearance of each 3D Gaussian. Additionally, we have developed\na coarse-to-fine training strategy to improve learning efficiency and eliminate\nfloaters caused by overfitting in real-world scenes. Our experimental results\ndemonstrate that our method surpasses existing approaches in terms of rendering\nquality. Thanks to ASG, we have significantly improved the ability of 3D-GS to\nmodel scenes with specular and anisotropic components without increasing the\nnumber of 3D Gaussians. This improvement extends the applicability of 3D GS to\nhandle intricate scenarios with specular and anisotropic surfaces. Project page\nis https://ingra14m.github.io/Spec-Gaussian-website/.\n","authors":["Ziyi Yang","Xinyu Gao","Yangtian Sun","Yihua Huang","Xiaoyang Lyu","Wen Zhou","Shaohui Jiao","Xiaojuan Qi","Xiaogang Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15870v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.01756v1","updated":"2024-10-02T17:06:39Z","published":"2024-10-02T17:06:39Z","title":"ImageFolder: Autoregressive Image Generation with Folded Tokens","summary":"  Image tokenizers are crucial for visual generative models, e.g., diffusion\nmodels (DMs) and autoregressive (AR) models, as they construct the latent\nrepresentation for modeling. Increasing token length is a common approach to\nimprove the image reconstruction quality. However, tokenizers with longer token\nlengths are not guaranteed to achieve better generation quality. There exists a\ntrade-off between reconstruction and generation quality regarding token length.\nIn this paper, we investigate the impact of token length on both image\nreconstruction and generation and provide a flexible solution to the tradeoff.\nWe propose ImageFolder, a semantic tokenizer that provides spatially aligned\nimage tokens that can be folded during autoregressive modeling to improve both\ngeneration efficiency and quality. To enhance the representative capability\nwithout increasing token length, we leverage dual-branch product quantization\nto capture different contexts of images. Specifically, semantic regularization\nis introduced in one branch to encourage compacted semantic information while\nanother branch is designed to capture the remaining pixel-level details.\nExtensive experiments demonstrate the superior quality of image generation and\nshorter token length with ImageFolder tokenizer.\n","authors":["Xiang Li","Hao Chen","Kai Qiu","Jason Kuen","Jiuxiang Gu","Bhiksha Raj","Zhe Lin"],"pdf_url":"https://arxiv.org/pdf/2410.01756v1.pdf","comment":"Code: https://github.com/lxa9867/ImageFolder"},{"id":"http://arxiv.org/abs/2410.01738v1","updated":"2024-10-02T16:48:47Z","published":"2024-10-02T16:48:47Z","title":"VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch\n  Diffusion Models","summary":"  Artistic typography is a technique to visualize the meaning of input\ncharacter in an imaginable and readable manner. With powerful text-to-image\ndiffusion models, existing methods directly design the overall geometry and\ntexture of input character, making it challenging to ensure both creativity and\nlegibility. In this paper, we introduce a dual-branch and training-free method,\nnamely VitaGlyph, enabling flexible artistic typography along with controllable\ngeometry change to maintain the readability. The key insight of VitaGlyph is to\ntreat input character as a scene composed of Subject and Surrounding, followed\nby rendering them under varying degrees of geometry transformation. The subject\nflexibly expresses the essential concept of input character, while the\nsurrounding enriches relevant background without altering the shape.\nSpecifically, we implement VitaGlyph through a three-phase framework: (i)\nKnowledge Acquisition leverages large language models to design text\ndescriptions of subject and surrounding. (ii) Regional decomposition detects\nthe part that most matches the subject description and divides input glyph\nimage into subject and surrounding regions. (iii) Typography Stylization\nfirstly refines the structure of subject region via Semantic Typography, and\nthen separately renders the textures of Subject and Surrounding regions through\nControllable Compositional Generation. Experimental results demonstrate that\nVitaGlyph not only achieves better artistry and readability, but also manages\nto depict multiple customize concepts, facilitating more creative and pleasing\nartistic typography generation. Our code will be made publicly at\nhttps://github.com/Carlofkl/VitaGlyph.\n","authors":["Kailai Feng","Yabo Zhang","Haodong Yu","Zhilong Ji","Jinfeng Bai","Hongzhi Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2410.01738v1.pdf","comment":"https://github.com/Carlofkl/VitaGlyph"},{"id":"http://arxiv.org/abs/2410.01737v1","updated":"2024-10-02T16:47:55Z","published":"2024-10-02T16:47:55Z","title":"RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection","summary":"  Multimodal Industrial Anomaly Detection (MIAD), utilizing 3D point clouds and\n2D RGB images to identify the abnormal region of products, plays a crucial role\nin industrial quality inspection. However, the conventional MIAD setting\npresupposes that all 2D and 3D modalities are paired, overlooking the fact that\nmultimodal data collected from the real world is often imperfect due to missing\nmodalities. Consequently, MIAD models that demonstrate robustness against\nmodal-incomplete data are highly desirable in practice. To address this\npractical challenge, we introduce a first-of-its-kind study that\ncomprehensively investigates Modality-Incomplete Industrial Anomaly Detection\n(MIIAD), to consider the imperfect learning environment in which the multimodal\ninformation may be incomplete. Not surprisingly, we discovered that most\nexisting MIAD approaches are inadequate for addressing MIIAD challenges,\nleading to significant performance degradation on the MIIAD benchmark we\ndeveloped. In this paper, we propose a novel two-stage Robust\nmodAlity-imcomplete fusing and Detecting frAmewoRk, abbreviated as RADAR. Our\nbootstrapping philosophy is to enhance two stages in MIIAD, improving the\nrobustness of the Multimodal Transformer: i) In feature fusion, we first\nexplore learning modality-incomplete instruction, guiding the pre-trained\nMultimodal Transformer to robustly adapt to various modality-incomplete\nscenarios, and implement adaptive parameter learning based on a HyperNetwork;\nii) In anomaly detection, we construct a real-pseudo hybrid module to highlight\nthe distinctiveness of modality combinations, further enhancing the robustness\nof the MIIAD model. Our experimental results demonstrate that the proposed\nRADAR significantly surpasses conventional MIAD methods in terms of\neffectiveness and robustness on our newly created MIIAD dataset, underscoring\nits practical application value.\n","authors":["Bingchen Miao","Wenqiao Zhang","Juncheng Li","Siliang Tang","Zhaocheng Li","Haochen Shi","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.01737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01731v1","updated":"2024-10-02T16:43:24Z","published":"2024-10-02T16:43:24Z","title":"ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation","summary":"  The practical use of text-to-image generation has evolved from simple,\nmonolithic models to complex workflows that combine multiple specialized\ncomponents. While workflow-based approaches can lead to improved image quality,\ncrafting effective workflows requires significant expertise, owing to the large\nnumber of available components, their complex inter-dependence, and their\ndependence on the generation prompt. Here, we introduce the novel task of\nprompt-adaptive workflow generation, where the goal is to automatically tailor\na workflow to each user prompt. We propose two LLM-based approaches to tackle\nthis task: a tuning-based method that learns from user-preference data, and a\ntraining-free method that uses the LLM to select existing flows. Both\napproaches lead to improved image quality when compared to monolithic models or\ngeneric, prompt-independent workflows. Our work shows that prompt-dependent\nflow prediction offers a new pathway to improving text-to-image generation\nquality, complementing existing research directions in the field.\n","authors":["Rinon Gal","Adi Haviv","Yuval Alaluf","Amit H. Bermano","Daniel Cohen-Or","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2410.01731v1.pdf","comment":"Project website: https://comfygen-paper.github.io/"},{"id":"http://arxiv.org/abs/2410.01723v1","updated":"2024-10-02T16:34:29Z","published":"2024-10-02T16:34:29Z","title":"HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration","summary":"  Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.\n","authors":["Yushi Huang","Zining Wang","Ruihao Gong","Jing Liu","Xinjie Zhang","Jinyang Guo","Xianglong Liu","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01723v1.pdf","comment":"Code will be released soon"},{"id":"http://arxiv.org/abs/2410.01719v1","updated":"2024-10-02T16:30:10Z","published":"2024-10-02T16:30:10Z","title":"OmniSR: Shadow Removal under Direct and Indirect Lighting","summary":"  Shadows can originate from occlusions in both direct and indirect\nillumination. Although most current shadow removal research focuses on shadows\ncaused by direct illumination, shadows from indirect illumination are often\njust as pervasive, particularly in indoor scenes. A significant challenge in\nremoving shadows from indirect illumination is obtaining shadow-free images to\ntrain the shadow removal network. To overcome this challenge, we propose a\nnovel rendering pipeline for generating shadowed and shadow-free images under\ndirect and indirect illumination, and create a comprehensive synthetic dataset\nthat contains over 30,000 image pairs, covering various object types and\nlighting conditions. We also propose an innovative shadow removal network that\nexplicitly integrates semantic and geometric priors through concatenation and\nattention mechanisms. The experiments show that our method outperforms\nstate-of-the-art shadow removal techniques and can effectively generalize to\nindoor and outdoor scenes under various lighting conditions, enhancing the\noverall effectiveness and applicability of shadow removal methods.\n","authors":["Jiamin Xu","Zelong Li","Yuxin Zheng","Chenyu Huang","Renshu Gu","Weiwei Xu","Gang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.01719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01718v1","updated":"2024-10-02T16:30:08Z","published":"2024-10-02T16:30:08Z","title":"COMUNI: Decomposing Common and Unique Video Signals for Diffusion-based\n  Video Generation","summary":"  Since videos record objects moving coherently, adjacent video frames have\ncommonness (similar object appearances) and uniqueness (slightly changed\npostures). To prevent redundant modeling of common video signals, we propose a\nnovel diffusion-based framework, named COMUNI, which decomposes the COMmon and\nUNIque video signals to enable efficient video generation. Our approach\nseparates the decomposition of video signals from the task of video generation,\nthus reducing the computation complexity of generative models. In particular,\nwe introduce CU-VAE to decompose video signals and encode them into latent\nfeatures. To train CU-VAE in a self-supervised manner, we employ a cascading\nmerge module to reconstitute video signals and a time-agnostic video decoder to\nreconstruct video frames. Then we propose CU-LDM to model latent features for\nvideo generation, which adopts two specific diffusion streams to simultaneously\nmodel the common and unique latent features. We further utilize additional\njoint modules for cross modeling of the common and unique latent features, and\na novel position embedding method to ensure the content consistency and motion\ncoherence of generated videos. The position embedding method incorporates\nspatial and temporal absolute position information into the joint modules.\nExtensive experiments demonstrate the necessity of decomposing common and\nunique video signals for video generation and the effectiveness and efficiency\nof our proposed method.\n","authors":["Mingzhen Sun","Weining Wang","Xinxin Zhu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14309v2","updated":"2024-10-02T16:28:38Z","published":"2024-04-22T16:10:38Z","title":"Towards Understanding the Robustness of Diffusion-Based Purification: A\n  Stochastic Perspective","summary":"  Diffusion-Based Purification (DBP) has emerged as an effective defense\nmechanism against adversarial attacks. The efficacy of DBP has been attributed\nto the forward diffusion process, which narrows the distribution gap between\nclean and adversarial images through the addition of Gaussian noise. Although\nthis explanation has some theoretical support, the significance of its\ncontribution to robustness remains unclear. In this paper, we argue that the\ninherent stochasticity in the DBP process is the primary driver of its\nrobustness. To explore this, we introduce a novel Deterministic White-Box\n(DW-box) evaluation protocol to assess robustness in the absence of\nstochasticity and to analyze the attack trajectories and loss landscapes. Our\nfindings suggest that DBP models primarily leverage stochasticity to evade\neffective attack directions, and their ability to purify adversarial\nperturbations can be weak. To further enhance the robustness of DBP models, we\nintroduce Adversarial Denoising Diffusion Training (ADDT), which incorporates\nclassifier-guided adversarial perturbations into diffusion training, thereby\nstrengthening the DBP models' ability to purify adversarial perturbations.\nAdditionally, we propose Rank-Based Gaussian Mapping (RBGM) to make\nperturbations more compatible with diffusion models. Experimental results\nvalidate the effectiveness of ADDT. In conclusion, our study suggests that\nfuture research on DBP can benefit from the perspective of decoupling the\nstochasticity-based and purification-based robustness.\n","authors":["Yiming Liu","Kezhao Liu","Yao Xiao","Ziyi Dong","Xiaogang Xu","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2404.14309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13465v2","updated":"2024-10-02T16:19:33Z","published":"2023-06-23T12:09:52Z","title":"3DSAM-adapter: Holistic adaptation of SAM from 2D to 3D for promptable\n  tumor segmentation","summary":"  Despite that the segment anything model (SAM) achieved impressive results on\ngeneral-purpose semantic segmentation with strong generalization ability on\ndaily images, its demonstrated performance on medical image segmentation is\nless precise and not stable, especially when dealing with tumor segmentation\ntasks that involve objects of small sizes, irregular shapes, and low contrast.\nNotably, the original SAM architecture is designed for 2D natural images,\ntherefore would not be able to extract the 3D spatial information from\nvolumetric medical data effectively. In this paper, we propose a novel\nadaptation method for transferring SAM from 2D to 3D for promptable medical\nimage segmentation. Through a holistically designed scheme for architecture\nmodification, we transfer the SAM to support volumetric inputs while retaining\nthe majority of its pre-trained parameters for reuse. The fine-tuning process\nis conducted in a parameter-efficient manner, wherein most of the pre-trained\nparameters remain frozen, and only a few lightweight spatial adapters are\nintroduced and tuned. Regardless of the domain gap between natural and medical\ndata and the disparity in the spatial arrangement between 2D and 3D, the\ntransformer trained on natural images can effectively capture the spatial\npatterns present in volumetric medical images with only lightweight\nadaptations. We conduct experiments on four open-source tumor segmentation\ndatasets, and with a single click prompt, our model can outperform domain\nstate-of-the-art medical image segmentation models on 3 out of 4 tasks,\nspecifically by 8.25%, 29.87%, and 10.11% for kidney tumor, pancreas tumor,\ncolon cancer segmentation, and achieve similar performance for liver tumor\nsegmentation. We also compare our adaptation method with existing popular\nadapters, and observed significant performance improvement on most datasets.\n","authors":["Shizhan Gong","Yuan Zhong","Wenao Ma","Jinpeng Li","Zhao Wang","Jingyang Zhang","Pheng-Ann Heng","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2306.13465v2.pdf","comment":"14 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.01699v1","updated":"2024-10-02T16:05:27Z","published":"2024-10-02T16:05:27Z","title":"Accelerating Auto-regressive Text-to-Image Generation with Training-free\n  Speculative Jacobi Decoding","summary":"  The current large auto-regressive models can generate high-quality,\nhigh-resolution images, but these models require hundreds or even thousands of\nsteps of next-token prediction during inference, resulting in substantial time\nconsumption. In existing studies, Jacobi decoding, an iterative parallel\ndecoding algorithm, has been used to accelerate the auto-regressive generation\nand can be executed without training. However, the Jacobi decoding relies on a\ndeterministic criterion to determine the convergence of iterations. Thus, it\nworks for greedy decoding but is incompatible with sampling-based decoding\nwhich is crucial for visual quality and diversity in the current\nauto-regressive text-to-image generation. In this paper, we propose a\ntraining-free probabilistic parallel decoding algorithm, Speculative Jacobi\nDecoding (SJD), to accelerate auto-regressive text-to-image generation. By\nintroducing a probabilistic convergence criterion, our SJD accelerates the\ninference of auto-regressive text-to-image generation while maintaining the\nrandomness in sampling-based token decoding and allowing the model to generate\ndiverse images. Specifically, SJD facilitates the model to predict multiple\ntokens at each step and accepts tokens based on the probabilistic criterion,\nenabling the model to generate images with fewer steps than the conventional\nnext-token-prediction paradigm. We also investigate the token initialization\nstrategies that leverage the spatial locality of visual data to further improve\nthe acceleration ratio under specific scenarios. We conduct experiments for our\nproposed SJD on multiple auto-regressive text-to-image generation models,\nshowing the effectiveness of model acceleration without sacrificing the visual\nquality.\n","authors":["Yao Teng","Han Shi","Xian Liu","Xuefei Ning","Guohao Dai","Yu Wang","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01698v1","updated":"2024-10-02T16:05:15Z","published":"2024-10-02T16:05:15Z","title":"COSMIC: Compress Satellite Images Efficiently via Diffusion Compensation","summary":"  With the rapidly increasing number of satellites in space and their enhanced\ncapabilities, the amount of earth observation images collected by satellites is\nexceeding the transmission limits of satellite-to-ground links. Although\nexisting learned image compression solutions achieve remarkable performance by\nusing a sophisticated encoder to extract fruitful features as compression and\nusing a decoder to reconstruct, it is still hard to directly deploy those\ncomplex encoders on current satellites' embedded GPUs with limited computing\ncapability and power supply to compress images in orbit. In this paper, we\npropose COSMIC, a simple yet effective learned compression solution to transmit\nsatellite images. We first design a lightweight encoder (i.e. reducing FLOPs by\n$2.6\\sim 5\\times $) on satellite to achieve a high image compression ratio to\nsave satellite-to-ground links. Then, for reconstructions on the ground, to\ndeal with the feature extraction ability degradation due to simplifying\nencoders, we propose a diffusion-based model to compensate image details when\ndecoding. Our insight is that satellite's earth observation photos are not just\nimages but indeed multi-modal data with a nature of Text-to-Image pairing since\nthey are collected with rich sensor data (e.g. coordinates, timestamp, etc.)\nthat can be used as the condition for diffusion generation. Extensive\nexperiments show that COSMIC outperforms state-of-the-art baselines on both\nperceptual and distortion metrics.\n","authors":["Ziyuan Zhang","Han Qiu","Maosen Zhang","Jun Liu","Bin Chen","Tianwei Zhang","Hewu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v3","updated":"2024-10-02T16:00:29Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel","Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:020. Expansion of\n  \"Dimensionality Reduction for Improving Out-of-Distribution Detection in\n  Medical Image Segmentation\" arXiv:2308.03723. Code available at\n  https://github.com/mckellwoodland/dimen_reduce_mahal\n  (https://zenodo.org/records/13881989)"},{"id":"http://arxiv.org/abs/2410.01680v1","updated":"2024-10-02T15:50:35Z","published":"2024-10-02T15:50:35Z","title":"PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation","summary":"  Various visual foundation models have distinct strengths and weaknesses, both\nof which can be improved through heterogeneous multi-teacher knowledge\ndistillation without labels, termed \"agglomerative models.\" We build upon this\nbody of work by studying the effect of the teachers' activation statistics,\nparticularly the impact of the loss function on the resulting student model\nquality. We explore a standard toolkit of statistical normalization techniques\nto better align the different distributions and assess their effects. Further,\nwe examine the impact on downstream teacher-matching metrics, which motivates\nthe use of Hadamard matrices. With these matrices, we demonstrate useful\nproperties, showing how they can be used for isotropic standardization, where\neach dimension of a multivariate distribution is standardized using the same\nscale. We call this technique \"PHI Standardization\" (PHI-S) and empirically\ndemonstrate that it produces the best student model across the suite of methods\nstudied.\n","authors":["Mike Ranzinger","Jon Barker","Greg Heinrich","Pavlo Molchanov","Bryan Catanzaro","Andrew Tao"],"pdf_url":"https://arxiv.org/pdf/2410.01680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01678v1","updated":"2024-10-02T15:48:42Z","published":"2024-10-02T15:48:42Z","title":"Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking","summary":"  3D multi-object tracking plays a critical role in autonomous driving by\nenabling the real-time monitoring and prediction of multiple objects'\nmovements. Traditional 3D tracking systems are typically constrained by\npredefined object categories, limiting their adaptability to novel, unseen\nobjects in dynamic environments. To address this limitation, we introduce\nopen-vocabulary 3D tracking, which extends the scope of 3D tracking to include\nobjects beyond predefined categories. We formulate the problem of\nopen-vocabulary 3D tracking and introduce dataset splits designed to represent\nvarious open-vocabulary scenarios. We propose a novel approach that integrates\nopen-vocabulary capabilities into a 3D tracking framework, allowing for\ngeneralization to unseen object classes. Our method effectively reduces the\nperformance gap between tracking known and novel objects through strategic\nadaptation. Experimental results demonstrate the robustness and adaptability of\nour method in diverse outdoor driving scenarios. To the best of our knowledge,\nthis work is the first to address open-vocabulary 3D tracking, presenting a\nsignificant advancement for autonomous systems in real-world settings. Code,\ntrained models, and dataset splits are available publicly.\n","authors":["Ayesha Ishaq","Mohamed El Amine Boudjoghra","Jean Lahoud","Fahad Shahbaz Khan","Salman Khan","Hisham Cholakkal","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2410.01678v1.pdf","comment":"7 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.11085v2","updated":"2024-10-02T15:35:15Z","published":"2024-08-20T17:58:23Z","title":"GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting","summary":"  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and\npropose a novel test-time camera pose refinement framework, GSLoc. This\nframework enhances the localization accuracy of state-of-the-art absolute pose\nregression and scene coordinate regression methods. The 3DGS model renders\nhigh-quality synthetic images and depth maps to facilitate the establishment of\n2D-3D correspondences. GSLoc obviates the need for training feature extractors\nor descriptors by operating directly on RGB images, utilizing the 3D foundation\nmodel, MASt3R, for precise 2D matching. To improve the robustness of our model\nin challenging outdoor environments, we incorporate an exposure-adaptive module\nwithin the 3DGS framework. Consequently, GSLoc enables efficient one-shot pose\nrefinement given a single RGB query and a coarse initial pose estimation. Our\nproposed approach surpasses leading NeRF-based optimization methods in both\naccuracy and runtime across indoor and outdoor visual localization benchmarks,\nachieving new state-of-the-art accuracy on two indoor datasets.\n","authors":["Changkun Liu","Shuai Chen","Yash Bhalgat","Siyan Hu","Ming Cheng","Zirui Wang","Victor Adrian Prisacariu","Tristan Braud"],"pdf_url":"https://arxiv.org/pdf/2408.11085v2.pdf","comment":"Fixed a small bug in the first version and achieved new\n  state-of-the-art accuracy. The project page is available at\n  https://gsloc.active.vision"},{"id":"http://arxiv.org/abs/2410.01665v1","updated":"2024-10-02T15:32:01Z","published":"2024-10-02T15:32:01Z","title":"Towards a vision foundation model for comprehensive assessment of\n  Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (CMR), considered the gold standard for\nnoninvasive cardiac assessment, is a diverse and complex modality requiring a\nwide variety of image processing tasks for comprehensive assessment of cardiac\nmorphology and function. Advances in deep learning have enabled the development\nof state-of-the-art (SoTA) models for these tasks. However, model training is\nchallenging due to data and label scarcity, especially in the less common\nimaging sequences. Moreover, each model is often trained for a specific task,\nwith no connection between related tasks. In this work, we introduce a vision\nfoundation model trained for CMR assessment, that is trained in a\nself-supervised fashion on 36 million CMR images. We then finetune the model in\nsupervised way for 9 clinical tasks typical to a CMR workflow, across\nclassification, segmentation, landmark localization, and pathology detection.\nWe demonstrate improved accuracy and robustness across all tasks, over a range\nof available labeled dataset sizes. We also demonstrate improved few-shot\nlearning with fewer labeled samples, a common challenge in medical image\nanalyses. We achieve an out-of-box performance comparable to SoTA for most\nclinical tasks. The proposed method thus presents a resource-efficient, unified\nframework for CMR assessment, with the potential to accelerate the development\nof deep learning-based solutions for image analysis tasks, even with few\nannotated data available.\n","authors":["Athira J Jacob","Indraneel Borgohain","Teodora Chitiboi","Puneet Sharma","Dorin Comaniciu","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2410.01665v1.pdf","comment":"11 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.19952v2","updated":"2024-10-02T15:26:06Z","published":"2024-09-30T05:14:07Z","title":"Image Copy Detection for Diffusion Models","summary":"  Images produced by diffusion models are increasingly popular in digital\nartwork and visual marketing. However, such generated images might replicate\ncontent from existing ones and pose the challenge of content originality.\nExisting Image Copy Detection (ICD) models, though accurate in detecting\nhand-crafted replicas, overlook the challenge from diffusion models. This\nmotivates us to introduce ICDiff, the first ICD specialized for diffusion\nmodels. To this end, we construct a Diffusion-Replication (D-Rep) dataset and\ncorrespondingly propose a novel deep embedding method. D-Rep uses a\nstate-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000\nimage-replica pairs, which are manually annotated into 6 replication levels\nranging from 0 (no replication) to 5 (total replication). Our method,\nPDF-Embedding, transforms the replication level of each image-replica pair into\na probability density function (PDF) as the supervision signal. The intuition\nis that the probability of neighboring replication levels should be continuous\nand smooth. Experimental results show that PDF-Embedding surpasses\nprotocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by\nutilizing PDF-Embedding, we find that the replication ratios of well-known\ndiffusion models against an open-source gallery range from 10% to 20%. The\nproject is publicly available at https://icdiff.github.io/.\n","authors":["Wenhao Wang","Yifan Sun","Zhentao Tan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.19952v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.13548v2","updated":"2024-10-02T15:25:31Z","published":"2024-09-20T14:47:58Z","title":"Data Diet: Can Trimming PET/CT Datasets Enhance Lesion Segmentation?","summary":"  In this work, we describe our approach to compete in the autoPET3 datacentric\ntrack. While conventional wisdom suggests that larger datasets lead to better\nmodel performance, recent studies indicate that excluding certain training\nsamples can enhance model accuracy. We find that in the autoPETIII dataset, a\nmodel that is trained on the entire dataset exhibits undesirable\ncharacteristics by producing a large number of false positives particularly for\nPSMA-PETs. We counteract this by removing the easiest samples from the training\ndataset as measured by the model loss before retraining from scratch. Using the\nproposed approach we manage to drive down the false negative volume and improve\nupon the baseline model in both false negative volume and dice score on the\npreliminary test set. Code and pre-trained models are available at\ngithub.com/alexanderjaus/autopet3_datadiet.\n","authors":["Alexander Jaus","Simon Rei","Jens Klesiek","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.13548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01647v1","updated":"2024-10-02T15:15:52Z","published":"2024-10-02T15:15:52Z","title":"3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and\n  Box-Focused Sampling for 3D Object Detection","summary":"  Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and\nhave been adapted for 3D Object Detection (3DOD), offering a promising approach\nto 3DOD through view-synthesis representation. However, NeRF faces inherent\nlimitations: (i) limited representational capacity for 3DOD due to its implicit\nnature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS)\nhas emerged as an explicit 3D representation that addresses these limitations.\nInspired by these advantages, this paper introduces 3DGS into 3DOD for the\nfirst time, identifying two main challenges: (i) Ambiguous spatial distribution\nof Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision,\nresulting in unclear 3D spatial distribution of Gaussian blobs and poor\ndifferentiation between objects and background, which hinders 3DOD; (ii)\nExcessive background blobs: 2D images often include numerous background pixels,\nleading to densely reconstructed 3DGS with many noisy Gaussian blobs\nrepresenting the background, negatively affecting detection. To tackle the\nchallenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D\nimages, and propose an elegant and efficient solution by incorporating 2D\nBoundary Guidance to significantly enhance the spatial distribution of Gaussian\nblobs, resulting in clearer differentiation between objects and their\nbackground. To address the challenge (ii), we propose a Box-Focused Sampling\nstrategy using 2D boxes to generate object probability distribution in 3D\nspaces, allowing effective probabilistic sampling in 3D to retain more object\nblobs and reduce noisy background blobs. Benefiting from our designs, our\n3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det,\nachieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet\ndataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.\n","authors":["Yang Cao","Yuanliang Jv","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2410.01647v1.pdf","comment":"Code Page: https://github.com/yangcaoai/3DGS-DET"},{"id":"http://arxiv.org/abs/2410.01638v1","updated":"2024-10-02T15:08:47Z","published":"2024-10-02T15:08:47Z","title":"Data Extrapolation for Text-to-image Generation on Small Datasets","summary":"  Text-to-image generation requires large amount of training data to\nsynthesizing high-quality images. For augmenting training data, previous\nmethods rely on data interpolations like cropping, flipping, and mixing up,\nwhich fail to introduce new information and yield only marginal improvements.\nIn this paper, we propose a new data augmentation method for text-to-image\ngeneration using linear extrapolation. Specifically, we apply linear\nextrapolation only on text feature, and new image data are retrieved from the\ninternet by search engines. For the reliability of new text-image pairs, we\ndesign two outlier detectors to purify retrieved images. Based on\nextrapolation, we construct training samples dozens of times larger than the\noriginal dataset, resulting in a significant improvement in text-to-image\nperformance. Moreover, we propose a NULL-guidance to refine score estimation,\nand apply recurrent affine transformation to fuse text information. Our model\nachieves FID scores of 7.91, 9.52 and 5.00 on the CUB, Oxford and COCO\ndatasets. The code and data will be available on GitHub\n(https://github.com/senmaoy/RAT-Diffusion).\n","authors":["Senmao Ye","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01618v1","updated":"2024-10-02T14:57:07Z","published":"2024-10-02T14:57:07Z","title":"SGBA: Semantic Gaussian Mixture Model-Based LiDAR Bundle Adjustment","summary":"  LiDAR bundle adjustment (BA) is an effective approach to reduce the drifts in\npose estimation from the front-end. Existing works on LiDAR BA usually rely on\npredefined geometric features for landmark representation. This reliance\nrestricts generalizability, as the system will inevitably deteriorate in\nenvironments where these specific features are absent. To address this issue,\nwe propose SGBA, a LiDAR BA scheme that models the environment as a semantic\nGaussian mixture model (GMM) without predefined feature types. This approach\nencodes both geometric and semantic information, offering a comprehensive and\ngeneral representation adaptable to various environments. Additionally, to\nlimit computational complexity while ensuring generalizability, we propose an\nadaptive semantic selection framework that selects the most informative\nsemantic clusters for optimization by evaluating the condition number of the\ncost function. Lastly, we introduce a probabilistic feature association scheme\nthat considers the entire probability density of assignments, which can manage\nuncertainties in measurement and initial pose estimation. We have conducted\nvarious experiments and the results demonstrate that SGBA can achieve accurate\nand robust pose refinement even in challenging scenarios with low-quality\ninitial pose estimation and limited geometric features. We plan to open-source\nthe work for the benefit of the community https://github.com/Ji1Xinyu/SGBA.\n","authors":["Xingyu Ji","Shenghai Yuan","Jianping Li","Pengyu Yin","Haozhi Cao","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.01618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01615v1","updated":"2024-10-02T14:53:45Z","published":"2024-10-02T14:53:45Z","title":"Saliency-Guided DETR for Moment Retrieval and Highlight Detection","summary":"  Existing approaches for video moment retrieval and highlight detection are\nnot able to align text and video features efficiently, resulting in\nunsatisfying performance and limited production usage. To address this, we\npropose a novel architecture that utilizes recent foundational video models\ndesigned for such alignment. Combined with the introduced Saliency-Guided Cross\nAttention mechanism and a hybrid DETR architecture, our approach significantly\nenhances performance in both moment retrieval and highlight detection tasks.\nFor even better improvement, we developed InterVid-MR, a large-scale and\nhigh-quality dataset for pretraining. Using it, our architecture achieves\nstate-of-the-art results on the QVHighlights, Charades-STA and TACoS\nbenchmarks. The proposed approach provides an efficient and scalable solution\nfor both zero-shot and fine-tuning scenarios in video-language tasks.\n","authors":["Aleksandr Gordeev","Vladimir Dokholyan","Irina Tolstykh","Maksim Kuprashevich"],"pdf_url":"https://arxiv.org/pdf/2410.01615v1.pdf","comment":"8 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2410.01614v1","updated":"2024-10-02T14:53:24Z","published":"2024-10-02T14:53:24Z","title":"Gaussian Splatting in Mirrors: Reflection-Aware Rendering via Virtual\n  Camera Optimization","summary":"  Recent advancements in 3D Gaussian Splatting (3D-GS) have revolutionized\nnovel view synthesis, facilitating real-time, high-quality image rendering.\nHowever, in scenarios involving reflective surfaces, particularly mirrors,\n3D-GS often misinterprets reflections as virtual spaces, resulting in blurred\nand inconsistent multi-view rendering within mirrors. Our paper presents a\nnovel method aimed at obtaining high-quality multi-view consistent reflection\nrendering by modelling reflections as physically-based virtual cameras. We\nestimate mirror planes with depth and normal estimates from 3D-GS and define\nvirtual cameras that are placed symmetrically about the mirror plane. These\nvirtual cameras are then used to explain mirror reflections in the scene. To\naddress imperfections in mirror plane estimates, we propose a straightforward\nyet effective virtual camera optimization method to enhance reflection quality.\nWe collect a new mirror dataset including three real-world scenarios for more\ndiverse evaluation. Experimental validation on both Mirror-Nerf and our\nreal-world dataset demonstrate the efficacy of our approach. We achieve\ncomparable or superior results while significantly reducing training time\ncompared to previous state-of-the-art.\n","authors":["Zihan Wang","Shuzhe Wang","Matias Turkulainen","Junyuan Fang","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2410.01614v1.pdf","comment":"To be published on 2024 British Machine Vision Conference"},{"id":"http://arxiv.org/abs/2410.01611v1","updated":"2024-10-02T14:49:05Z","published":"2024-10-02T14:49:05Z","title":"DRUPI: Dataset Reduction Using Privileged Information","summary":"  Dataset reduction (DR) seeks to select or distill samples from large datasets\ninto smaller subsets while preserving performance on target tasks. Existing\nmethods primarily focus on pruning or synthesizing data in the same format as\nthe original dataset, typically the input data and corresponding labels.\nHowever, in DR settings, we find it is possible to synthesize more information\nbeyond the data-label pair as an additional learning target to facilitate model\ntraining. In this paper, we introduce Dataset Reduction Using Privileged\nInformation (DRUPI), which enriches DR by synthesizing privileged information\nalongside the reduced dataset. This privileged information can take the form of\nfeature labels or attention labels, providing auxiliary supervision to improve\nmodel learning. Our findings reveal that effective feature labels must balance\nbetween being overly discriminative and excessively diverse, with a moderate\nlevel proving optimal for improving the reduced dataset's efficacy. Extensive\nexperiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI\nintegrates seamlessly with existing dataset reduction methods, offering\nsignificant performance gains.\n","authors":["Shaobo Wang","Yantai Yang","Shuaiyu Zhang","Chenghao Sun","Weiya Li","Xuming Hu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01609v1","updated":"2024-10-02T14:47:55Z","published":"2024-10-02T14:47:55Z","title":"DAViD: Domain Adaptive Visually-Rich Document Understanding with\n  Synthetic Insights","summary":"  Visually-Rich Documents (VRDs), encompassing elements like charts, tables,\nand references, convey complex information across various fields. However,\nextracting information from these rich documents is labor-intensive, especially\ngiven their inconsistent formats and domain-specific requirements. While\npretrained models for VRD Understanding have progressed, their reliance on\nlarge, annotated datasets limits scalability. This paper introduces the Domain\nAdaptive Visually-rich Document Understanding (DAViD) framework, which utilises\nmachine-generated synthetic data for domain adaptation. DAViD integrates\nfine-grained and coarse-grained document representation learning and employs\nsynthetic annotations to reduce the need for costly manual labelling. By\nleveraging pretrained models and synthetic data, DAViD achieves competitive\nperformance with minimal annotated datasets. Extensive experiments validate\nDAViD's effectiveness, demonstrating its ability to efficiently adapt to\ndomain-specific VRDU tasks.\n","authors":["Yihao Ding","Soyeon Caren Han","Zechuan Li","Hyunsuk Chung"],"pdf_url":"https://arxiv.org/pdf/2410.01609v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2303.14739v2","updated":"2024-10-02T14:43:17Z","published":"2023-03-26T14:38:42Z","title":"Geometry-Aware Attenuation Learning for Sparse-View CBCT Reconstruction","summary":"  Cone Beam Computed Tomography (CBCT) plays a vital role in clinical imaging.\nTraditional methods typically require hundreds of 2D X-ray projections to\nreconstruct a high-quality 3D CBCT image, leading to considerable radiation\nexposure. This has led to a growing interest in sparse-view CBCT reconstruction\nto reduce radiation doses. While recent advances, including deep learning and\nneural rendering algorithms, have made strides in this area, these methods\neither produce unsatisfactory results or suffer from time inefficiency of\nindividual optimization. In this paper, we introduce a novel geometry-aware\nencoder-decoder framework to solve this problem. Our framework starts by\nencoding multi-view 2D features from various 2D X-ray projections with a 2D CNN\nencoder. Leveraging the geometry of CBCT scanning, it then back-projects the\nmulti-view 2D features into the 3D space to formulate a comprehensive\nvolumetric feature map, followed by a 3D CNN decoder to recover 3D CBCT image.\nImportantly, our approach respects the geometric relationship between 3D CBCT\nimage and its 2D X-ray projections during feature back projection stage, and\nenjoys the prior knowledge learned from the data population. This ensures its\nadaptability in dealing with extremly sparse view inputs without individual\ntraining, such as scenarios with only 5 or 10 X-ray projections. Extensive\nevaluations on two simulated datasets and one real-world dataset demonstrate\nexceptional reconstruction quality and time efficiency of our method.\n","authors":["Zhentao Liu","Yu Fang","Changjian Li","Han Wu","Yuan Liu","Dinggang Shen","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2303.14739v2.pdf","comment":"15 pages, 15 figures, 10 tables"},{"id":"http://arxiv.org/abs/2410.01595v1","updated":"2024-10-02T14:33:12Z","published":"2024-10-02T14:33:12Z","title":"KnobGen: Controlling the Sophistication of Artwork in Sketch-Based\n  Diffusion Models","summary":"  Recent advances in diffusion models have significantly improved text-to-image\n(T2I) generation, but they often struggle to balance fine-grained precision\nwith high-level control. Methods like ControlNet and T2I-Adapter excel at\nfollowing sketches by seasoned artists but tend to be overly rigid, replicating\nunintentional flaws in sketches from novice users. Meanwhile, coarse-grained\nmethods, such as sketch-based abstraction frameworks, offer more accessible\ninput handling but lack the precise control needed for detailed, professional\nuse. To address these limitations, we propose KnobGen, a dual-pathway framework\nthat democratizes sketch-based image generation by seamlessly adapting to\nvarying levels of sketch complexity and user skill. KnobGen uses a\nCoarse-Grained Controller (CGC) module for high-level semantics and a\nFine-Grained Controller (FGC) module for detailed refinement. The relative\nstrength of these two modules can be adjusted through our knob inference\nmechanism to align with the user's specific needs. These mechanisms ensure that\nKnobGen can flexibly generate images from both novice sketches and those drawn\nby seasoned artists. This maintains control over the final output while\npreserving the natural appearance of the image, as evidenced on the\nMultiGen-20M dataset and a newly collected sketch dataset.\n","authors":["Pouyan Navard","Amin Karimi Monsefi","Mengxi Zhou","Wei-Lun Chao","Alper Yilmaz","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2410.01595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01594v1","updated":"2024-10-02T14:32:24Z","published":"2024-10-02T14:32:24Z","title":"MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation","summary":"  Sounding Video Generation (SVG) is an audio-video joint generation task\nchallenged by high-dimensional signal spaces, distinct data formats, and\ndifferent patterns of content information. To address these issues, we\nintroduce a novel multi-modal latent diffusion model (MM-LDM) for the SVG task.\nWe first unify the representation of audio and video data by converting them\ninto a single or a couple of images. Then, we introduce a hierarchical\nmulti-modal autoencoder that constructs a low-level perceptual latent space for\neach modality and a shared high-level semantic feature space. The former space\nis perceptually equivalent to the raw signal space of each modality but\ndrastically reduces signal dimensions. The latter space serves to bridge the\ninformation gap between modalities and provides more insightful cross-modal\nguidance. Our proposed method achieves new state-of-the-art results with\nsignificant quality and efficiency gains. Specifically, our method achieves a\ncomprehensive improvement on all evaluation metrics and a faster training and\nsampling speed on Landscape and AIST++ datasets. Moreover, we explore its\nperformance on open-domain sounding video generation, long sounding video\ngeneration, audio continuation, video continuation, and conditional\nsingle-modal generation tasks for a comprehensive evaluation, where our MM-LDM\ndemonstrates exciting adaptability and generalization ability.\n","authors":["Mingzhen Sun","Weining Wang","Yanyuan Qiao","Jiahui Sun","Zihan Qin","Longteng Guo","Xinxin Zhu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01594v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2401.12870v3","updated":"2024-10-02T14:32:19Z","published":"2024-01-23T16:04:19Z","title":"Unlocking the Potential: Multi-task Deep Learning for Spaceborne\n  Quantitative Monitoring of Fugitive Methane Plumes","summary":"  As global warming intensifies, increased attention is being paid to\nmonitoring fugitive methane emissions and detecting gas plumes from landfills.\nWe have divided methane emission monitoring into three subtasks: methane\nconcentration inversion, plume segmentation, and emission rate estimation.\nTraditional algorithms face certain limitations: methane concentration\ninversion typically employs the matched filter, which is sensitive to the\nglobal spectrum distribution and prone to significant noise. There is scant\nresearch on plume segmentation, with many studies depending on manual\nsegmentation, which can be subjective. The estimation of methane emission rate\nfrequently uses the IME algorithm, which necessitates meteorological\nmeasurement data. Utilizing the WENT landfill site in Hong Kong along with\nPRISMA hyperspectral satellite imagery, we introduce a novel deep\nlearning-based framework for quantitative methane emission monitoring from\nremote sensing images that is grounded in physical simulation. We create\nsimulated methane plumes using large eddy simulation (LES) and various\nconcentration maps of fugitive emissions using the radiative transfer equation\n(RTE), while applying augmentation techniques to construct a simulated PRISMA\ndataset. We train a U-Net network for methane concentration inversion, a Mask\nR-CNN network for methane plume segmentation, and a ResNet-50 network for\nmethane emission rate estimation. All three deep networks yield higher\nvalidation accuracy compared to traditional algorithms. Furthermore, we combine\nthe first two subtasks and the last two subtasks to design multi-task learning\nmodels, MTL-01 and MTL-02, both of which outperform single-task models in terms\nof accuracy. Our research exemplifies the application of multi-task deep\nlearning to quantitative methane monitoring and can be generalized to a wide\narray of methane monitoring tasks.\n","authors":["Guoxin Si","Shiliang Fu","Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2401.12870v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01591v1","updated":"2024-10-02T14:25:02Z","published":"2024-10-02T14:25:02Z","title":"Imaging foundation model for universal enhancement of non-ideal\n  measurement CT","summary":"  Non-ideal measurement computed tomography (NICT), which sacrifices optimal\nimaging standards for new advantages in CT imaging, is expanding the clinical\napplication scope of CT images. However, with the reduction of imaging\nstandards, the image quality has also been reduced, extremely limiting the\nclinical acceptability. Although numerous studies have demonstrated the\nfeasibility of deep learning for the NICT enhancement in specific scenarios,\ntheir high data cost and limited generalizability have become large obstacles.\nThe recent research on the foundation model has brought new opportunities for\nbuilding a universal NICT enhancement model - bridging the image quality\ndegradation with minimal data cost. However, owing to the challenges in the\ncollection of large pre-training datasets and the compatibility of data\nvariation, no success has been reported. In this paper, we propose a\nmulti-scale integrated Transformer AMPlifier (TAMP), the first imaging\nfoundation model for universal NICT enhancement. It has been pre-trained on a\nlarge-scale physical-driven simulation dataset with 3.6 million NICT-ICT image\npairs, and is able to directly generalize to the NICT enhancement tasks with\nvarious non-ideal settings and body regions. Via the adaptation with few data,\nit can further achieve professional performance in real-world specific\nscenarios. Our extensive experiments have demonstrated that the proposed TAMP\nhas significant potential for promoting the exploration and application of NICT\nand serving a wider range of medical scenarios.\n","authors":["Yuxin Liu","Rongjun Ge","Yuting He","Zhan Wu","Chenyu You","Shuo Li","Yang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01577v1","updated":"2024-10-02T14:13:06Z","published":"2024-10-02T14:13:06Z","title":"Coordinate-Based Neural Representation Enabling Zero-Shot Learning for\n  3D Multiparametric Quantitative MRI","summary":"  Quantitative magnetic resonance imaging (qMRI) offers tissue-specific\nphysical parameters with significant potential for neuroscience research and\nclinical practice. However, lengthy scan times for 3D multiparametric qMRI\nacquisition limit its clinical utility. Here, we propose SUMMIT, an innovative\nimaging methodology that includes data acquisition and an unsupervised\nreconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes\nmultiple important quantitative properties into highly undersampled k-space. It\nfurther leverages implicit neural representation incorporated with a dedicated\nphysics model to reconstruct the desired multiparametric maps without needing\nexternal training datasets. SUMMIT delivers co-registered T1, T2, T2*, and\nquantitative susceptibility mapping. Extensive simulations and phantom imaging\ndemonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised\napproach for qMRI reconstruction also introduces a novel zero-shot learning\nparadigm for multiparametric imaging applicable to various medical imaging\nmodalities.\n","authors":["Guoyan Lao","Ruimin Feng","Haikun Qi","Zhenfeng Lv","Qiangqiang Liu","Chunlei Liu","Yuyao Zhang","Hongjiang Wei"],"pdf_url":"https://arxiv.org/pdf/2410.01577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01573v1","updated":"2024-10-02T14:11:26Z","published":"2024-10-02T14:11:26Z","title":"PASS:Test-Time Prompting to Adapt Styles and Semantic Shapes in Medical\n  Image Segmentation","summary":"  Test-time adaptation (TTA) has emerged as a promising paradigm to handle the\ndomain shifts at test time for medical images from different institutions\nwithout using extra training data. However, existing TTA solutions for\nsegmentation tasks suffer from (1) dependency on modifying the source training\nstage and access to source priors or (2) lack of emphasis on shape-related\nsemantic knowledge that is crucial for segmentation tasks.Recent research on\nvisual prompt learning achieves source-relaxed adaptation by extended parameter\nspace but still neglects the full utilization of semantic features, thus\nmotivating our work on knowledge-enriched deep prompt learning. Beyond the\ngeneral concern of image style shifts, we reveal that shape variability is\nanother crucial factor causing the performance drop. To address this issue, we\npropose a TTA framework called PASS (Prompting to Adapt Styles and Semantic\nshapes), which jointly learns two types of prompts: the input-space prompt to\nreformulate the style of the test image to fit into the pretrained model and\nthe semantic-aware prompts to bridge high-level shape discrepancy across\ndomains. Instead of naively imposing a fixed prompt, we introduce an input\ndecorator to generate the self-regulating visual prompt conditioned on the\ninput data. To retrieve the knowledge representations and customize\ntarget-specific shape prompts for each test sample, we propose a\ncross-attention prompt modulator, which performs interaction between target\nrepresentations and an enriched shape prompt bank. Extensive experiments\ndemonstrate the superior performance of PASS over state-of-the-art methods on\nmultiple medical image segmentation datasets. The code is available at\nhttps://github.com/EndoluminalSurgicalVision-IMR/PASS.\n","authors":["Chuyan Zhang","Hao Zheng","Xin You","Yefeng Zheng","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2410.01573v1.pdf","comment":"Submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2410.00486v2","updated":"2024-10-02T14:07:56Z","published":"2024-10-01T08:18:12Z","title":"CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with\nphotorealistic scene reconstruction emerging as a key challenge. To address\nthis, we introduce Computational Alignment for Real-Time Gaussian Splatting\nSLAM (CaRtGS), a novel method enhancing the efficiency and quality of\nphotorealistic scene reconstruction in real-time environments. Leveraging 3D\nGaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and\nprocessing speed, which is crucial for scene photorealistic reconstruction. Our\napproach tackles computational misalignment in Gaussian Splatting SLAM\n(GS-SLAM) through an adaptive strategy that optimizes training, addresses\nlong-tail optimization, and refines densification. Experiments on Replica and\nTUM-RGBD datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity\nrendering with fewer Gaussian primitives. This work propels SLAM towards\nreal-time, photorealistic dense rendering, significantly advancing\nphotorealistic scene representation. For the benefit of the research community,\nwe release the code on our project website:\nhttps://dapengfeng.github.io/cartgs.\n","authors":["Dapeng Feng","Zhiqiang Chen","Yizhen Yin","Shipeng Zhong","Yuhua Qi","Hongbo Chen"],"pdf_url":"https://arxiv.org/pdf/2410.00486v2.pdf","comment":"Upon a thorough internal review, we have identified that our\n  manuscript lacks proper citation for a critical expression within the\n  methodology section. In this revised version, we add Taming-3DGS as a\n  citation in the splat-wise backpropagation statement"},{"id":"http://arxiv.org/abs/2404.02148v4","updated":"2024-10-02T14:07:04Z","published":"2024-04-02T17:58:03Z","title":"Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of\n  Video and Multi-view Diffusion Models","summary":"  Recent advancements in 3D generation are predominantly propelled by\nimprovements in 3D-aware image diffusion models. These models are pretrained on\nInternet-scale image data and fine-tuned on massive 3D data, offering the\ncapability of producing highly consistent multi-view images. However, due to\nthe scarcity of synchronized multi-view video data, it remains challenging to\nadapt this paradigm to 4D generation directly. Despite that, the available\nvideo and 3D data are adequate for training video and multi-view diffusion\nmodels separately that can provide satisfactory dynamic and geometric priors\nrespectively. To take advantage of both, this paper presents Diffusion$^2$, a\nnovel framework for dynamic 3D content creation that reconciles the knowledge\nabout geometric consistency and temporal smoothness from these models to\ndirectly sample dense multi-view multi-frame images which can be employed to\noptimize continuous 4D representation. Specifically, we design a simple yet\neffective denoising strategy via score composition of pretrained video and\nmulti-view diffusion models based on the probability structure of the target\nimage array. To alleviate the potential conflicts between two heterogeneous\nscores, we further introduce variance-reducing sampling via interpolated steps,\nfacilitating smooth and stable generation. Owing to the high parallelism of the\nproposed image generation process and the efficiency of the modern 4D\nreconstruction pipeline, our framework can generate 4D content within few\nminutes. Notably, our method circumvents the reliance on expensive and\nhard-to-scale 4D data, thereby having the potential to benefit from the scaling\nof the foundation video and multi-view diffusion models. Extensive experiments\ndemonstrate the efficacy of our proposed framework in generating highly\nseamless and consistent 4D assets under various types of conditions.\n","authors":["Zeyu Yang","Zijie Pan","Chun Gu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.02148v4.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2211.08007v3","updated":"2024-10-02T13:54:14Z","published":"2022-11-15T09:42:07Z","title":"Evidence-based Match-status-Aware Gait Recognition for Out-of-Gallery\n  Gait Identification","summary":"  Existing gait recognition methods typically identify individuals based on the\nsimilarity between probe and gallery samples. However, these methods often\nneglect the fact that the gallery may not contain identities corresponding to\nthe probes, leading to incorrect recognition.To identify Out-of-Gallery (OOG)\ngait queries, we propose an Evidence-based Match-status-Aware Gait Recognition\n(EMA-GR) framework. Inspired by Evidential Deep Learning (EDL), EMA-GR is\ndesigned to quantify the uncertainty associated with the match status of\nrecognition. Thus, EMA-GR identifies whether the probe has a counterpart in the\ngallery. Specifically, we adopt an evidence collector to gather match status\nevidence from a recognition result pair and parameterize a Dirichlet\ndistribution over the gathered evidence, following the Dempster-Shafer Theory\nof Evidence (DST). We measure the uncertainty and predict the match status of\nthe recognition results, and thus determine whether the probe is an OOG\nquery.To the best of our knowledge, our method is the first attempt to tackle\nOOG queries in gait recognition. Moreover, EMA-GR is agnostic against gait\nrecognition methods and improves the robustness against OOG queries. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non datasets with OOG queries, and can also generalize well to other\nidentity-retrieval tasks. Importantly, our method surpasses existing\nstate-of-the-art methods by a substantial margin, achieving a 51.26%\nimprovement when the OOG query rate is around 50% on OUMVLP.\n","authors":["Heming Du","Chen Liu","Ming Wang","Lincheng Li","Shunli Zhang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2211.08007v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04368v2","updated":"2024-10-02T13:51:37Z","published":"2024-09-06T15:59:30Z","title":"The Impact of Scanner Domain Shift on Deep Learning Performance in\n  Medical Imaging: an Experimental Study","summary":"  Purpose: Medical images acquired using different scanners and protocols can\ndiffer substantially in their appearance. This phenomenon, scanner domain\nshift, can result in a drop in the performance of deep neural networks which\nare trained on data acquired by one scanner and tested on another. This\nsignificant practical issue is well-acknowledged, however, no systematic study\nof the issue is available across different modalities and diagnostic tasks.\nMaterials and Methods: In this paper, we present a broad experimental study\nevaluating the impact of scanner domain shift on convolutional neural network\nperformance for different automated diagnostic tasks. We evaluate this\nphenomenon in common radiological modalities, including X-ray, CT, and MRI.\nResults: We find that network performance on data from a different scanner is\nalmost always worse than on same-scanner data, and we quantify the degree of\nperformance drop across different datasets. Notably, we find that this drop is\nmost severe for MRI, moderate for X-ray, and quite small for CT, on average,\nwhich we attribute to the standardized nature of CT acquisition systems which\nis not present in MRI or X-ray. We also study how injecting varying amounts of\ntarget domain data into the training set, as well as adding noise to the\ntraining data, helps with generalization. Conclusion: Our results provide\nextensive experimental evidence and quantification of the extent of performance\ndrop caused by scanner domain shift in deep learning across different\nmodalities, with the goal of guiding the future development of robust deep\nlearning models for medical image analysis.\n","authors":["Brian Guo","Darui Lu","Gregory Szumel","Rongze Gui","Tingyu Wang","Nicholas Konz","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2409.04368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00890v2","updated":"2024-10-02T13:47:34Z","published":"2024-10-01T17:29:43Z","title":"Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model\n  And Input View Curation","summary":"  Generating high-quality 3D content from text, single images, or sparse view\nimages remains a challenging task with broad applications. Existing methods\ntypically employ multi-view diffusion models to synthesize multi-view images,\nfollowed by a feed-forward process for 3D reconstruction. However, these\napproaches are often constrained by a small and fixed number of input views,\nlimiting their ability to capture diverse viewpoints and, even worse, leading\nto suboptimal generation results if the synthesized views are of poor quality.\nTo address these limitations, we propose Flex3D, a novel two-stage framework\ncapable of leveraging an arbitrary number of high-quality input views. The\nfirst stage consists of a candidate view generation and curation pipeline. We\nemploy a fine-tuned multi-view image diffusion model and a video diffusion\nmodel to generate a pool of candidate views, enabling a rich representation of\nthe target 3D object. Subsequently, a view selection pipeline filters these\nviews based on quality and consistency, ensuring that only the high-quality and\nreliable views are used for reconstruction. In the second stage, the curated\nviews are fed into a Flexible Reconstruction Model (FlexRM), built upon a\ntransformer architecture that can effectively process an arbitrary number of\ninputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane\nrepresentation, enabling efficient and detailed 3D generation. Through\nextensive exploration of design and training strategies, we optimize FlexRM to\nachieve superior performance in both reconstruction and generation tasks. Our\nresults demonstrate that Flex3D achieves state-of-the-art performance, with a\nuser study winning rate of over 92% in 3D generation tasks when compared to\nseveral of the latest feed-forward 3D generative models.\n","authors":["Junlin Han","Jianyuan Wang","Andrea Vedaldi","Philip Torr","Filippos Kokkinos"],"pdf_url":"https://arxiv.org/pdf/2410.00890v2.pdf","comment":"Project page: https://junlinhan.github.io/projects/flex3d/"},{"id":"http://arxiv.org/abs/2406.04886v2","updated":"2024-10-02T13:40:10Z","published":"2024-06-07T12:32:44Z","title":"Unveiling the Invisible: Captioning Videos with Metaphors","summary":"  Metaphors are a common communication tool used in our day-to-day life. The\ndetection and generation of metaphors in textual form have been studied\nextensively but metaphors in other forms have been under-explored. Recent\nstudies have shown that Vision-Language (VL) models cannot understand visual\nmetaphors in memes and adverts. As of now, no probing studies have been done\nthat involve complex language phenomena like metaphors with videos. Hence, we\nintroduce a new VL task of describing the metaphors present in the videos in\nour work. To facilitate this novel task, we construct and release a manually\ncreated dataset with 705 videos and 2115 human-written captions, along with a\nnew metric called Average Concept Distance (ACD), to automatically evaluate the\ncreativity of the metaphors generated. We also propose a novel low-resource\nvideo metaphor captioning system: GIT-LLaVA, which obtains comparable\nperformance to SoTA video language models on the proposed task. We perform a\ncomprehensive analysis of existing video language models on this task and\npublish our dataset, models, and benchmark results to enable further research.\n","authors":["Abisek Rajakumar Kalarani","Pushpak Bhattacharyya","Sumit Shekhar"],"pdf_url":"https://arxiv.org/pdf/2406.04886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03179v2","updated":"2024-10-02T13:32:56Z","published":"2024-07-03T14:59:46Z","title":"Motion meets Attention: Video Motion Prompts","summary":"  Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose a modified\nSigmoid function with learnable slope and shift parameters as an attention\nmechanism to modulate motion signals from frame differencing maps. This\napproach generates a sequence of attention maps that enhance the processing of\nmotion-related video content. To ensure temporal continuity and smoothness of\nthe attention maps, we apply pair-wise temporal attention variation\nregularization to remove unwanted motions (e.g., noise) while preserving\nimportant ones. We then perform Hadamard product between each pair of attention\nmaps and the original video frames to highlight the evolving motions of\ninterest over time. These highlighted motions, termed video motion prompts, are\nsubsequently used as inputs to the model instead of the original video frames.\nWe formalize this process as a motion prompt layer and incorporate the\nregularization term into the loss function to learn better motion prompts. This\nlayer serves as an adapter between the model and the video data, bridging the\ngap between traditional 'blind motion extraction' and the extraction of\nrelevant motions of interest. We show that our lightweight, plug-and-play\nmotion prompt layer seamlessly integrates into models like SlowFast, X3D, and\nTimeSformer, enhancing performance on benchmarks such as FineGym and MPII\nCooking 2.\n","authors":["Qixiang Chen","Lei Wang","Piotr Koniusz","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2407.03179v2.pdf","comment":"Accepted at the 16th Asian Conference on Machine Learning (ACML 2024)"},{"id":"http://arxiv.org/abs/2410.01544v1","updated":"2024-10-02T13:30:32Z","published":"2024-10-02T13:30:32Z","title":"Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension","summary":"  This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks.\n","authors":["Zaiquan Yang","Yuhao Liu","Jiaying Lin","Gerhard Hancke","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2410.01544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01540v1","updated":"2024-10-02T13:29:52Z","published":"2024-10-02T13:29:52Z","title":"Edge-preserving noise for diffusion models","summary":"  Classical generative diffusion models learn an isotropic Gaussian denoising\nprocess, treating all spatial regions uniformly, thus neglecting potentially\nvaluable structural information in the data. Inspired by the long-established\nwork on anisotropic diffusion in image processing, we present a novel\nedge-preserving diffusion model that is a generalization of denoising diffusion\nprobablistic models (DDPM). In particular, we introduce an edge-aware noise\nscheduler that varies between edge-preserving and isotropic Gaussian noise. We\nshow that our model's generative process converges faster to results that more\nclosely match the target distribution. We demonstrate its capability to better\nlearn the low-to-mid frequencies within the dataset, which plays a crucial role\nin representing shapes and structural information. Our edge-preserving\ndiffusion process consistently outperforms state-of-the-art baselines in\nunconditional image generation. It is also more robust for generative tasks\nguided by a shape-based prior, such as stroke-to-image generation. We present\nqualitative and quantitative results showing consistent improvements (FID\nscore) of up to 30% for both tasks.\n","authors":["Jente Vandersanden","Sascha Holl","Xingchang Huang","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2410.01540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01539v1","updated":"2024-10-02T13:29:45Z","published":"2024-10-02T13:29:45Z","title":"Multi-Scale Fusion for Object Representation","summary":"  Representing images or videos as object-level feature vectors, rather than\npixel-level feature maps, facilitates advanced visual tasks. Object-Centric\nLearning (OCL) primarily achieves this by reconstructing the input under the\nguidance of Variational Autoencoder (VAE) intermediate representation to drive\nso-called \\textit{slots} to aggregate as much object information as possible.\nHowever, existing VAE guidance does not explicitly address that objects can\nvary in pixel sizes while models typically excel at specific pattern scales. We\npropose \\textit{Multi-Scale Fusion} (MSF) to enhance VAE guidance for OCL\ntraining. To ensure objects of all sizes fall within VAE's comfort zone, we\nadopt the \\textit{image pyramid}, which produces intermediate representations\nat multiple scales; To foster scale-invariance/variance in object super-pixels,\nwe devise \\textit{inter}/\\textit{intra-scale fusion}, which augments\nlow-quality object super-pixels of one scale with corresponding high-quality\nsuper-pixels from another scale. On standard OCL benchmarks, our technique\nimproves mainstream methods, including state-of-the-art diffusion-based ones.\nThe source code is available in the supplemental material.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2410.01539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01535v1","updated":"2024-10-02T13:26:28Z","published":"2024-10-02T13:26:28Z","title":"GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene\n  by Primitives and Gaussians","summary":"  Recently, with the development of Neural Radiance Fields and Gaussian\nSplatting, 3D reconstruction techniques have achieved remarkably high fidelity.\nHowever, the latent representations learnt by these methods are highly\nentangled and lack interpretability. In this paper, we propose a novel\npart-aware compositional reconstruction method, called GaussianBlock, that\nenables semantically coherent and disentangled representations, allowing for\nprecise and physical editing akin to building blocks, while simultaneously\nmaintaining high fidelity. Our GaussianBlock introduces a hybrid representation\nthat leverages the advantages of both primitives, known for their flexible\nactionability and editability, and 3D Gaussians, which excel in reconstruction\nquality. Specifically, we achieve semantically coherent primitives through a\nnovel attention-guided centering loss derived from 2D semantic priors,\ncomplemented by a dynamic splitting and fusion strategy. Furthermore, we\nutilize 3D Gaussians that hybridize with primitives to refine structural\ndetails and enhance fidelity. Additionally, a binding inheritance strategy is\nemployed to strengthen and maintain the connection between the two. Our\nreconstructed scenes are evidenced to be disentangled, compositional, and\ncompact across diverse benchmarks, enabling seamless, direct and precise\nediting while maintaining high quality.\n","authors":["Shuyi Jiang","Qihao Zhao","Hossein Rahmani","De Wen Soh","Jun Liu","Na Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.01535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01534v1","updated":"2024-10-02T13:26:17Z","published":"2024-10-02T13:26:17Z","title":"Toward a Holistic Evaluation of Robustness in CLIP Models","summary":"  Contrastive Language-Image Pre-training (CLIP) models have shown significant\npotential, particularly in zero-shot classification across diverse distribution\nshifts. Building on existing evaluations of overall classification robustness,\nthis work aims to provide a more comprehensive assessment of CLIP by\nintroducing several new perspectives. First, we investigate their robustness to\nvariations in specific visual factors. Second, we assess two critical safety\nobjectives--confidence uncertainty and out-of-distribution detection--beyond\nmere classification accuracy. Third, we evaluate the finesse with which CLIP\nmodels bridge the image and text modalities. Fourth, we extend our examination\nto 3D awareness in CLIP models, moving beyond traditional 2D image\nunderstanding. Finally, we explore the interaction between vision and language\nencoders within modern large multimodal models (LMMs) that utilize CLIP as the\nvisual backbone, focusing on how this interaction impacts classification\nrobustness. In each aspect, we consider the impact of six factors on CLIP\nmodels: model architecture, training distribution, training set size,\nfine-tuning, contrastive loss, and test-time prompts. Our study uncovers\nseveral previously unknown insights into CLIP. For instance, the architecture\nof the visual encoder in CLIP plays a significant role in their robustness\nagainst 3D corruption. CLIP models tend to exhibit a bias towards shape when\nmaking predictions. Moreover, this bias tends to diminish after fine-tuning on\nImageNet. Vision-language models like LLaVA, leveraging the CLIP vision\nencoder, could exhibit benefits in classification performance for challenging\ncategories over CLIP alone. Our findings are poised to offer valuable guidance\nfor enhancing the robustness and reliability of CLIP models.\n","authors":["Weijie Tu","Weijian Deng","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2410.01534v1.pdf","comment":"17 pages, 10 figures, extension of NeurIPS'23 work: A Closer Look at\n  the Robustness of Contrastive Language-Image Pre-Training (CLIP). arXiv admin\n  note: text overlap with arXiv:2402.07410"},{"id":"http://arxiv.org/abs/2410.01529v1","updated":"2024-10-02T13:23:02Z","published":"2024-10-02T13:23:02Z","title":"Robo-MUTUAL: Robotic Multimodal Task Specification via Unimodal Learning","summary":"  Multimodal task specification is essential for enhanced robotic performance,\nwhere \\textit{Cross-modality Alignment} enables the robot to holistically\nunderstand complex task instructions. Directly annotating multimodal\ninstructions for model training proves impractical, due to the sparsity of\npaired multimodal data. In this study, we demonstrate that by leveraging\nunimodal instructions abundant in real data, we can effectively teach robots to\nlearn multimodal task specifications. First, we endow the robot with strong\n\\textit{Cross-modality Alignment} capabilities, by pretraining a robotic\nmultimodal encoder using extensive out-of-domain data. Then, we employ two\nCollapse and Corrupt operations to further bridge the remaining modality gap in\nthe learned multimodal representation. This approach projects different\nmodalities of identical task goal as interchangeable representations, thus\nenabling accurate robotic operations within a well-aligned multimodal latent\nspace. Evaluation across more than 130 tasks and 4000 evaluations on both\nsimulated LIBERO benchmark and real robot platforms showcases the superior\ncapabilities of our proposed framework, demonstrating significant advantage in\novercoming data constraints in robotic learning. Website:\nzh1hao.wang/Robo_MUTUAL\n","authors":["Jianxiong Li","Zhihao Wang","Jinliang Zheng","Xiaoai Zhou","Guanming Wang","Guanglu Song","Yu Liu","Jingjing Liu","Ya-Qin Zhang","Junzhi Yu","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2410.01529v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.01521v1","updated":"2024-10-02T13:10:57Z","published":"2024-10-02T13:10:57Z","title":"MiraGe: Editable 2D Images using Gaussian Splatting","summary":"  Implicit Neural Representations (INRs) approximate discrete data through\ncontinuous functions and are commonly used for encoding 2D images. Traditional\nimage-based INRs employ neural networks to map pixel coordinates to RGB values,\ncapturing shapes, colors, and textures within the network's weights. Recently,\nGaussianImage has been proposed as an alternative, using Gaussian functions\ninstead of neural networks to achieve comparable quality and compression. Such\na solution obtains a quality and compression ratio similar to classical INR\nmodels but does not allow image modification. In contrast, our work introduces\na novel method, MiraGe, which uses mirror reflections to perceive 2D images in\n3D space and employs flat-controlled Gaussians for precise 2D image editing.\nOur approach improves the rendering quality and allows realistic image\nmodifications, including human-inspired perception of photos in the 3D world.\nThanks to modeling images in 3D space, we obtain the illusion of 3D-based\nmodification in 2D images. We also show that our Gaussian representation can be\neasily combined with a physics engine to produce physics-based modification of\n2D images. Consequently, MiraGe allows for better quality than the standard\napproach and natural modification of 2D images.\n","authors":["Joanna Waczyska","Tomasz Szczepanik","Piotr Borycki","Sawomir Tadeja","Thomas Bohn","Przemysaw Spurek"],"pdf_url":"https://arxiv.org/pdf/2410.01521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01517v1","updated":"2024-10-02T13:08:56Z","published":"2024-10-02T13:08:56Z","title":"UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater\n  Scene Reconstruction","summary":"  3D Gaussian splatting (3DGS) offers the capability to achieve real-time high\nquality 3D scene rendering. However, 3DGS assumes that the scene is in a clear\nmedium environment and struggles to generate satisfactory representations in\nunderwater scenes, where light absorption and scattering are prevalent and\nmoving objects are involved. To overcome these, we introduce a novel Gaussian\nSplatting-based method, UW-GS, designed specifically for underwater\napplications. It introduces a color appearance that models distance-dependent\ncolor variation, employs a new physics-based density control strategy to\nenhance clarity for distant objects, and uses a binary motion mask to handle\ndynamic content. Optimized with a well-designed loss function supporting for\nscattering media and strengthened by pseudo-depth maps, UW-GS outperforms\nexisting methods with PSNR gains up to 1.26dB. To fully verify the\neffectiveness of the model, we also developed a new underwater dataset, S-UW,\nwith dynamic object masks.\n","authors":["Haoran Wang","Nantheera Anantrasirichai","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2410.01517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04023v2","updated":"2024-10-02T13:04:04Z","published":"2023-04-08T14:34:07Z","title":"Attack-Augmentation Mixing-Contrastive Skeletal Representation Learning","summary":"  Contrastive learning, relying on effective positive and negative sample\npairs, is beneficial to learn informative skeleton representations in\nunsupervised skeleton-based action recognition. To achieve these positive and\nnegative pairs, existing weak/strong data augmentation methods have to randomly\nchange the appearance of skeletons for indirectly pursuing semantic\nperturbations. However, such approaches have two limitations: i) solely\nperturbing appearance cannot well capture the intrinsic semantic information of\nskeletons, and ii) randomly perturbation may change the original\npositive/negative pairs to soft positive/negative ones. To address the above\ndilemma, we start the first attempt to explore an attack-based augmentation\nscheme that additionally brings in direct semantic perturbation, for\nconstructing hard positive pairs and further assisting in constructing hard\nnegative pairs. In particular, we propose a novel Attack-Augmentation\nMixing-Contrastive skeletal representation learning (A$^2$MC) to contrast hard\npositive features and hard negative features for learning more robust skeleton\nrepresentations. In A$^2$MC, Attack-Augmentation (Att-Aug) is designed to\ncollaboratively perform targeted and untargeted perturbations of skeletons via\nattack and augmentation respectively, for generating high-quality hard positive\nfeatures. Meanwhile, Positive-Negative Mixer (PNM) is presented to mix hard\npositive features and negative features for generating hard negative features,\nwhich are adopted for updating the mixed memory banks. Extensive experiments on\nthree public datasets demonstrate that A$^2$MC is competitive with the\nstate-of-the-art methods. The code will be accessible on A$^2$MC\n(https://github.com/1xbq1/A2MC).\n","authors":["Binqian Xu","Xiangbo Shu","Jiachao Zhang","Rui Yan","Guo-Sen Xie"],"pdf_url":"https://arxiv.org/pdf/2304.04023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13049v2","updated":"2024-10-02T13:04:02Z","published":"2024-09-19T18:55:13Z","title":"DiffSSD: A Diffusion-Based Dataset For Speech Forensics","summary":"  Diffusion-based speech generators are ubiquitous. These methods can generate\nvery high quality synthetic speech and several recent incidents report their\nmalicious use. To counter such misuse, synthetic speech detectors have been\ndeveloped. Many of these detectors are trained on datasets which do not include\ndiffusion-based synthesizers. In this paper, we demonstrate that existing\ndetectors trained on one such dataset, ASVspoof2019, do not perform well in\ndetecting synthetic speech from recent diffusion-based synthesizers. We propose\nthe Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of\nabout 200 hours of labeled speech, including synthetic speech generated by 8\ndiffusion-based open-source and 2 commercial generators. We also examine the\nperformance of existing synthetic speech detectors on DiffSSD in both\nclosed-set and open-set scenarios. The results highlight the importance of this\ndataset in detecting synthetic speech generated from recent open-source and\ncommercial speech generators.\n","authors":["Kratika Bhagtani","Amit Kumar Singh Yadav","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2409.13049v2.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP) 2025"},{"id":"http://arxiv.org/abs/2408.04102v3","updated":"2024-10-02T12:48:44Z","published":"2024-08-07T21:44:29Z","title":"ArtVLM: Attribute Recognition Through Vision-Based Prefix Language\n  Modeling","summary":"  Recognizing and disentangling visual attributes from objects is a foundation\nto many computer vision applications. While large vision language\nrepresentations like CLIP had largely resolved the task of zero-shot object\nrecognition, zero-shot visual attribute recognition remains a challenge because\nCLIP's contrastively-learned vision-language representation cannot effectively\ncapture object-attribute dependencies. In this paper, we target this weakness\nand propose a sentence generation-based retrieval formulation for attribute\nrecognition that is novel in 1) explicitly modeling a to-be-measured and\nretrieved object-attribute relation as a conditional probability graph, which\nconverts the recognition problem into a dependency-sensitive language-modeling\nproblem, and 2) applying a large pretrained Vision-Language Model (VLM) on this\nreformulation and naturally distilling its knowledge of image-object-attribute\nrelations to use towards attribute recognition. Specifically, for each\nattribute to be recognized on an image, we measure the visual-conditioned\nprobability of generating a short sentence encoding the attribute's relation to\nobjects on the image. Unlike contrastive retrieval, which measures likelihood\nby globally aligning elements of the sentence to the image, generative\nretrieval is sensitive to the order and dependency of objects and attributes in\nthe sentence. We demonstrate through experiments that generative retrieval\nconsistently outperforms contrastive retrieval on two visual reasoning\ndatasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual\nGenome Attribute Ranking (VGARank).\n","authors":["William Yicheng Zhu","Keren Ye","Junjie Ke","Jiahui Yu","Leonidas Guibas","Peyman Milanfar","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04102v3.pdf","comment":"Accepted at ECCV 2024. Contact: zhuwilliam[at]google[dot]com. GitHub:\n  https://github.com/google-research/google-research/tree/master/attribute_with_prefixlm"},{"id":"http://arxiv.org/abs/2410.01498v1","updated":"2024-10-02T12:46:18Z","published":"2024-10-02T12:46:18Z","title":"Quo Vadis RankList-based System in Face Recognition?","summary":"  Face recognition in the wild has gained a lot of focus in the last few years,\nand many face recognition models are designed to verify faces in medium-quality\nimages. Especially due to the availability of large training datasets with\nsimilar conditions, deep face recognition models perform exceptionally well in\nsuch tasks. However, in other tasks where substantially less training data is\navailable, such methods struggle, especially when required to compare\nhigh-quality enrollment images with low-quality probes. On the other hand,\ntraditional RankList-based methods have been developed that compare faces\nindirectly by comparing to cohort faces with similar conditions. In this paper,\nwe revisit these RankList methods and extend them to use the logits of the\nstate-of-the-art DaliFace network, instead of an external cohort. We show that\nthrough a reasonable Logit-Cohort Selection (LoCoS) the performance of\nRankList-based functions can be improved drastically. Experiments on two\nchallenging face recognition datasets not only demonstrate the enhanced\nperformance of our proposed method but also set the stage for future\nadvancements in handling diverse image qualities.\n","authors":["Xinyi Zhang","Manuel Gnther"],"pdf_url":"https://arxiv.org/pdf/2410.01498v1.pdf","comment":"Accepted for presentation at IJCB 2024"},{"id":"http://arxiv.org/abs/2402.01879v2","updated":"2024-10-02T12:42:56Z","published":"2024-02-02T20:08:11Z","title":"$$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial\n  Examples","summary":"  Evaluating the adversarial robustness of deep networks to gradient-based\nattacks is challenging. While most attacks consider $\\ell_2$- and\n$\\ell_\\infty$-norm constraints to craft input perturbations, only a few\ninvestigate sparse $\\ell_1$- and $\\ell_0$-norm attacks. In particular,\n$\\ell_0$-norm attacks remain the least studied due to the inherent complexity\nof optimizing over a non-convex and non-differentiable constraint. However,\nevaluating adversarial robustness under these attacks could reveal weaknesses\notherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm\nattacks. In this work, we propose a novel $\\ell_0$-norm attack, called\n$\\sigma$-zero, which leverages a differentiable approximation of the $\\ell_0$\nnorm to facilitate gradient-based optimization, and an adaptive projection\noperator to dynamically adjust the trade-off between loss minimization and\nperturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet\ndatasets, involving robust and non-robust models, show that $\\sigma$-zero finds\nminimum $\\ell_0$-norm adversarial examples without requiring any time-consuming\nhyperparameter tuning, and that it outperforms all competing sparse attacks in\nterms of success rate, perturbation size, and efficiency.\n","authors":["Antonio Emanuele Cin","Francesco Villani","Maura Pintor","Lea Schnherr","Battista Biggio","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2402.01879v2.pdf","comment":"Code available at\n  https://github.com/Cinofix/sigma-zero-adversarial-attack"},{"id":"http://arxiv.org/abs/2409.03553v3","updated":"2024-10-02T12:40:01Z","published":"2024-09-05T14:13:05Z","title":"Organized Grouped Discrete Representation for Object-Centric Learning","summary":"  Object-Centric Learning (OCL) represents dense image or video pixels as\nsparse object features. Representative methods utilize discrete representation\ncomposed of Variational Autoencoder (VAE) template features to suppress\npixel-level information redundancy and guide object-level feature aggregation.\nThe most recent advancement, Grouped Discrete Representation (GDR), further\ndecomposes these template features into attributes. However, its naive channel\ngrouping as decomposition may erroneously group channels belonging to different\nattributes together and discretize them as sub-optimal template attributes,\nwhich losses information and harms expressivity. We propose Organized GDR\n(OGDR) to organize channels belonging to the same attributes together for\ncorrect decomposition from features into attributes. In unsupervised\nsegmentation experiments, OGDR is fully superior to GDR in augmentating\nclassical transformer-based OCL methods; it even improves state-of-the-art\ndiffusion-based ones. Codebook PCA and representation similarity analyses show\nthat compared with GDR, our OGDR eliminates redundancy and preserves\ninformation better for guiding object representation learning. The source code\nis available in the supplementary material.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2409.03553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07762v2","updated":"2024-10-02T12:33:38Z","published":"2024-05-13T14:04:34Z","title":"A method for supervoxel-wise association studies of age and other\n  non-imaging variables from coronary computed tomography angiograms","summary":"  The study of associations between an individual's age and imaging and\nnon-imaging data is an active research area that attempts to aid understanding\nof the effects and patterns of aging. In this work we have conducted a\nsupervoxel-wise association study between both volumetric and tissue density\nfeatures in coronary computed tomography angiograms and the chronological age\nof a subject, to understand the localized changes in morphology and tissue\ndensity with age. To enable a supervoxel-wise study of volume and tissue\ndensity, we developed a novel method based on image segmentation, inter-subject\nimage registration, and robust supervoxel-based correlation analysis, to\nachieve a statistical association study between the images and age. We evaluate\nthe registration methodology in terms of the Dice coefficient for the heart\nchambers and myocardium, and the inverse consistency of the transformations,\nshowing that the method works well in most cases with high overlap and inverse\nconsistency. In a sex-stratified study conducted on a subset of $n=1388$ images\nfrom the SCAPIS study, the supervoxel-wise analysis was able to find localized\nassociations with age outside of the commonly segmented and analyzed\nsub-regions, and several substantial differences between the sexes in the\nassociation of age and volume.\n","authors":["Johan fverstedt","Elin Lundstrm","Gran Bergstrm","Joel Kullberg","Hkan Ahlstrm"],"pdf_url":"https://arxiv.org/pdf/2405.07762v2.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2410.01473v1","updated":"2024-10-02T12:23:49Z","published":"2024-10-02T12:23:49Z","title":"SinkSAM: A Monocular Depth-Guided SAM Framework for Automatic Sinkhole\n  Segmentation","summary":"  Soil sinkholes significantly influence soil degradation, but their irregular\nshapes, along with interference from shadow and vegetation, make it challenging\nto accurately quantify their properties using remotely sensed data. We present\na novel framework for sinkhole segmentation that combines traditional\ntopographic computations of closed depressions with the newly developed\nprompt-based Segment Anything Model (SAM). Within this framework, termed\nSinkSAM, we highlight four key improvements: (1) The integration of topographic\ncomputations with SAM enables pixel-level refinement of sinkhole boundaries\nsegmentation; (2) A coherent mathematical prompting strategy, based on closed\ndepressions, addresses the limitations of purely learning-based models (CNNs)\nin detecting and segmenting undefined sinkhole features, while improving\ngeneralization to new, unseen regions; (3) Using Depth Anything V2 monocular\ndepth for automatic prompts eliminates photogrammetric biases, enabling\nsinkhole mapping without the dependence on LiDAR data; and (4) An established\nsinkhole database facilitates fine-tuning of SAM, improving its zero-shot\nperformance in sinkhole segmentation. These advancements allow the deployment\nof SinkSAM, in an unseen test area, in the highly variable semiarid region,\nachieving an intersection-over-union (IoU) of 40.27\\% and surpassing previous\nresults. This paper also presents the first SAM implementation for sinkhole\nsegmentation and demonstrates the robustness of SinkSAM in extracting sinkhole\nmaps using a single RGB image.\n","authors":["Osher Rafaeli","Tal Svoray","Ariel Nahlieli"],"pdf_url":"https://arxiv.org/pdf/2410.01473v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.15238v3","updated":"2024-10-02T12:09:18Z","published":"2024-03-22T14:32:02Z","title":"WEEP: A method for spatial interpretation of weakly supervised CNN\n  models in computational pathology","summary":"  Deep learning enables the modelling of high-resolution histopathology\nwhole-slide images (WSI). Weakly supervised learning of tile-level data is\ntypically applied for tasks where labels only exist on the patient or WSI level\n(e.g. patient outcomes or histological grading). In this context, there is a\nneed for improved spatial interpretability of predictions from such models. We\npropose a novel method, Wsi rEgion sElection aPproach (WEEP), for model\ninterpretation. It provides a principled yet straightforward way to establish\nthe spatial area of WSI required for assigning a particular prediction label.\nWe demonstrate WEEP on a binary classification task in the area of breast\ncancer computational pathology. WEEP is easy to implement, is directly\nconnected to the model-based decision process, and offers information relevant\nto both research and diagnostic applications.\n","authors":["Abhinav Sharma","Bojing Liu","Mattias Rantalainen"],"pdf_url":"https://arxiv.org/pdf/2403.15238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01726v2","updated":"2024-10-02T11:49:31Z","published":"2024-07-01T19:00:40Z","title":"Grouped Discrete Representation Guides Object-Centric Learning","summary":"  Similar to humans perceiving visual scenes as objects, Object-Centric\nLearning (OCL) can abstract dense images or videos into sparse object-level\nfeatures. Transformer-based OCL handles complex textures well due to the\ndecoding guidance of discrete representation, obtained by discretizing noisy\nfeatures in image or video feature maps using template features from a\ncodebook. However, treating features as minimal units overlooks their composing\nattributes, thus impeding model generalization; indexing features with natural\nnumbers loses attribute-level commonalities and characteristics, thus\ndiminishing heuristics for model convergence. We propose \\textit{Grouped\nDiscrete Representation} (GDR) to address these issues by grouping features\ninto attributes and indexing them with tuple numbers. In extensive experiments\nacross different query initializations, dataset modalities, and model\narchitectures, GDR consistently improves convergence and generalizability.\nVisualizations show that our method effectively captures attribute-level\ninformation in features. The source code will be available upon acceptance.\n","authors":["Rongzhen Zhao","Vivienne Wang","Juho Kannala","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2407.01726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13518v2","updated":"2024-10-02T11:45:38Z","published":"2024-05-22T10:26:44Z","title":"PerSense: Personalized Instance Segmentation in Dense Images","summary":"  Leveraging large-scale pre-training, vision foundational models showcase\nnotable performance benefits. Recent segmentation algorithms for natural scenes\nhave advanced significantly. However, existing models still struggle to\nautomatically segment personalized instances in dense and crowded scenarios,\nwhere severe occlusions, scale variations, and background clutter pose a\nchallenge to accurately delineate densely packed instances of the target\nobject. To address this, we propose PerSense, an end-to-end, training-free, and\nmodel-agnostic one-shot framework for Personalized instance Segmentation in\ndense images. We develop a new baseline capable of automatically generating\ninstance-level point prompts via proposing a novel Instance Detection Module\n(IDM) that leverages density maps, encapsulating spatial distribution of\nobjects in an image. To mitigate false positives within generated point\nprompts, we design Point Prompt Selection Module (PPSM). Both IDM and PPSM\ntransform density maps into personalized precise point prompts for\ninstance-level segmentation and offer a seamless integration in our\nmodel-agnostic framework. We also introduce a feedback mechanism which enables\nPerSense to improve the accuracy of density maps by automating the exemplar\nselection process for density map generation. To promote algorithmic advances\nand effective tools for this relatively underexplored task, we introduce\nPerSense-D, a diverse dataset exclusive to personalized instance segmentation\nin dense images. Our extensive experiments establish PerSense superiority in\ndense scenarios by achieving an mIoU of 71.61% on PerSense-D, outperforming\nrecent SOTA models by significant margins of +47.16%, +42.27%, +8.83%, and\n+5.69%. Additionally, our qualitative findings demonstrate the adaptability of\nour framework to images captured in-the-wild.\n","authors":["Muhammad Ibraheem Siddiqui","Muhammad Umer Sheikh","Hassan Abid","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2405.13518v2.pdf","comment":"Technical report of PerSense"},{"id":"http://arxiv.org/abs/2410.01441v1","updated":"2024-10-02T11:43:58Z","published":"2024-10-02T11:43:58Z","title":"Decorrelation-based Self-Supervised Visual Representation Learning for\n  Writer Identification","summary":"  Self-supervised learning has developed rapidly over the last decade and has\nbeen applied in many areas of computer vision. Decorrelation-based\nself-supervised pretraining has shown great promise among non-contrastive\nalgorithms, yielding performance at par with supervised and contrastive\nself-supervised baselines. In this work, we explore the decorrelation-based\nparadigm of self-supervised learning and apply the same to learning\ndisentangled stroke features for writer identification. Here we propose a\nmodified formulation of the decorrelation-based framework named SWIS which was\nproposed for signature verification by standardizing the features along each\ndimension on top of the existing framework. We show that the proposed framework\noutperforms the contemporary self-supervised learning framework on the writer\nidentification benchmark and also outperforms several supervised methods as\nwell. To the best of our knowledge, this work is the first of its kind to apply\nself-supervised learning for learning representations for writer verification\ntasks.\n","authors":["Arkadip Maitra","Shree Mitra","Siladittya Manna","Saumik Bhattacharya","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2410.01441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15203v2","updated":"2024-10-02T11:41:50Z","published":"2024-03-22T13:46:51Z","title":"DITTO: Demonstration Imitation by Trajectory Transformation","summary":"  Teaching robots new skills quickly and conveniently is crucial for the\nbroader adoption of robotic systems. In this work, we address the problem of\none-shot imitation from a single human demonstration, given by an RGB-D video\nrecording. We propose a two-stage process. In the first stage we extract the\ndemonstration trajectory offline. This entails segmenting manipulated objects\nand determining their relative motion in relation to secondary objects such as\ncontainers. In the online trajectory generation stage, we first re-detect all\nobjects, then warp the demonstration trajectory to the current scene and\nexecute it on the robot. To complete these steps, our method leverages several\nancillary models, including those for segmentation, relative object pose\nestimation, and grasp prediction. We systematically evaluate different\ncombinations of correspondence and re-detection methods to validate our design\ndecision across a diverse range of tasks. Specifically, we collect and\nquantitatively test on demonstrations of ten different tasks including\npick-and-place tasks as well as articulated object manipulation. Finally, we\nperform extensive evaluations on a real robot system to demonstrate the\neffectiveness and utility of our approach in real-world scenarios. We make the\ncode publicly available at http://ditto.cs.uni-freiburg.de.\n","authors":["Nick Heppert","Max Argus","Tim Welschehold","Thomas Brox","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15203v2.pdf","comment":"8 pages, 4 figures, 3 tables, accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2410.01425v1","updated":"2024-10-02T11:23:08Z","published":"2024-10-02T11:23:08Z","title":"EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis\n  under Diverse Camera Settings","summary":"  The feed-forward based 3D Gaussian Splatting method has demonstrated\nexceptional capability in real-time human novel view synthesis. However,\nexisting approaches are restricted to dense viewpoint settings, which limits\ntheir flexibility in free-viewpoint rendering across a wide range of camera\nview angle discrepancies. To address this limitation, we propose a real-time\npipeline named EVA-Gaussian for 3D human novel view synthesis across diverse\ncamera settings. Specifically, we first introduce an Efficient cross-View\nAttention (EVA) module to accurately estimate the position of each 3D Gaussian\nfrom the source images. Then, we integrate the source images with the estimated\nGaussian position map to predict the attributes and feature embeddings of the\n3D Gaussians. Moreover, we employ a recurrent feature refiner to correct\nartifacts caused by geometric errors in position estimation and enhance visual\nfidelity.To further improve synthesis quality, we incorporate a powerful anchor\nloss function for both 3D Gaussian attributes and human face landmarks.\nExperimental results on the THuman2.0 and THumansit datasets showcase the\nsuperiority of our EVA-Gaussian approach in rendering quality across diverse\ncamera settings. Project page:\nhttps://zhenliuzju.github.io/huyingdong/EVA-Gaussian.\n","authors":["Yingdong Hu","Zhening Liu","Jiawei Shao","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04798v3","updated":"2024-10-02T11:06:10Z","published":"2024-02-07T12:38:47Z","title":"Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with\n  Parallel Spike-driven Transformer","summary":"  Artificial neural networks (ANNs) can help camera-based remote\nphotoplethysmography (rPPG) in measuring cardiac activity and physiological\nsignals from facial videos, such as pulse wave, heart rate and respiration rate\nwith better accuracy. However, most existing ANN-based methods require\nsubstantial computing resources, which poses challenges for effective\ndeployment on mobile devices. Spiking neural networks (SNNs), on the other\nhand, hold immense potential for energy-efficient deep learning owing to their\nbinary and event-driven architecture. To the best of our knowledge, we are the\nfirst to introduce SNNs into the realm of rPPG, proposing a hybrid neural\nnetwork (HNN) model, the Spiking-PhysFormer, aimed at reducing power\nconsumption. Specifically, the proposed Spiking-PhyFormer consists of an\nANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based\npredictor head. First, to simplify the transformer block while preserving its\ncapacity to aggregate local and global spatio-temporal features, we design a\nparallel spike transformer block to replace sequential sub-blocks.\nAdditionally, we propose a simplified spiking self-attention mechanism that\nomits the value parameter without compromising the model's performance.\nExperiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD\ndemonstrate that the proposed model achieves a 12.4\\% reduction in power\nconsumption compared to PhysFormer. Additionally, the power consumption of the\ntransformer block is reduced by a factor of 12.2, while maintaining decent\nperformance as PhysFormer and other ANN-based models.\n","authors":["Mingxuan Liu","Jiankai Tang","Yongli Chen","Haoxiang Li","Jiahao Qi","Siwei Li","Kegang Wang","Jie Gan","Yuntao Wang","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04798v3.pdf","comment":"Mingxuan Liu, Jiankai Tang and Yongli Chen are co-first authors of\n  the article"},{"id":"http://arxiv.org/abs/2301.00060v2","updated":"2024-10-02T11:04:09Z","published":"2022-12-30T21:48:32Z","title":"Morphology-based non-rigid registration of coronary computed tomography\n  and intravascular images through virtual catheter path optimization","summary":"  Coronary computed tomography angiography (CCTA) provides 3D information on\nobstructive coronary artery disease, but cannot fully visualize high-resolution\nfeatures within the vessel wall. Intravascular imaging, in contrast, can\nspatially resolve atherosclerotic in cross sectional slices, but is limited in\ncapturing 3D relationships between each slice. Co-registering CCTA and\nintravascular images enables a variety of clinical research applications but is\ntime consuming and user-dependent. This is due to intravascular images\nsuffering from non-rigid distortions arising from irregularities in the imaging\ncatheter path. To address these issues, we present a morphology-based framework\nfor the rigid and non-rigid matching of intravascular images to CCTA images. To\ndo this, we find the optimal virtual catheter path that samples the coronary\nartery in CCTA image space to recapitulate the coronary artery morphology\nobserved in the intravascular image. We validate our framework on a\nmulti-center cohort of 40 patients using bifurcation landmarks as ground truth\nfor longitudinal and rotational registration. Our registration approach\nsignificantly outperforms other approaches for bifurcation alignment. By\nproviding a differentiable framework for multi-modal vascular co-registration,\nour framework reduces the manual effort required to conduct large-scale\nmulti-modal clinical studies and enables the development of machine\nlearning-based co-registration approaches.\n","authors":["Karim Kadry","Abhishek Karmakar","Andreas Schuh","Kersten Peterson","Michiel Schaap","David Marlevi","Charles Taylor","Elazer Edelman","Farhad Nezami"],"pdf_url":"https://arxiv.org/pdf/2301.00060v2.pdf","comment":"Accepted to IEEE Transactions in Medical Imaging"},{"id":"http://arxiv.org/abs/2410.01417v1","updated":"2024-10-02T10:58:54Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01411v1","updated":"2024-10-02T10:46:05Z","published":"2024-10-02T10:46:05Z","title":"CSIM: A Copula-based similarity index sensitive to local changes for\n  Image quality assessment","summary":"  Image similarity metrics play an important role in computer vision\napplications, as they are used in image processing, computer vision and machine\nlearning. Furthermore, those metrics enable tasks such as image retrieval,\nobject recognition and quality assessment, essential in fields like healthcare,\nastronomy and surveillance. Existing metrics, such as PSNR, MSE, SSIM, ISSM and\nFSIM, often face limitations in terms of either speed, complexity or\nsensitivity to small changes in images. To address these challenges, a novel\nimage similarity metric, namely CSIM, that combines real-time while being\nsensitive to subtle image variations is investigated in this paper. The novel\nmetric uses Gaussian Copula from probability theory to transform an image into\nvectors of pixel distribution associated to local image patches. These vectors\ncontain, in addition to intensities and pixel positions, information on the\ndependencies between pixel values, capturing the structural relationships\nwithin the image. By leveraging the properties of Copulas, CSIM effectively\nmodels the joint distribution of pixel intensities, enabling a more nuanced\ncomparison of image patches making it more sensitive to local changes compared\nto other metrics. Experimental results demonstrate that CSIM outperforms\nexisting similarity metrics in various image distortion scenarios, including\nnoise, compression artifacts and blur. The metric's ability to detect subtle\ndifferences makes it suitable for applications requiring high precision, such\nas medical imaging, where the detection of minor anomalies can be of a high\nimportance. The results obtained in this work can be reproduced from this\nGithub repository: https://github.com/safouaneelg/copulasimilarity.\n","authors":["Safouane El Ghazouali","Umberto Michelucci","Yassin El Hillali","Hichem Nouira"],"pdf_url":"https://arxiv.org/pdf/2410.01411v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2405.05007v4","updated":"2024-10-02T10:43:33Z","published":"2024-05-08T12:24:50Z","title":"HC-Mamba: Vision MAMBA with Hybrid Convolutional Techniques for Medical\n  Image Segmentation","summary":"  Automatic medical image segmentation technology has the potential to expedite\npathological diagnoses, thereby enhancing the efficiency of patient care.\nHowever, medical images often have complex textures and structures, and the\nmodels often face the problem of reduced image resolution and information loss\ndue to downsampling. To address this issue, we propose HC-Mamba, a new medical\nimage segmentation model based on the modern state space model Mamba.\nSpecifically, we introduce the technique of dilated convolution in the HC-Mamba\nmodel to capture a more extensive range of contextual information without\nincreasing the computational cost by extending the perceptual field of the\nconvolution kernel. In addition, the HC-Mamba model employs depthwise separable\nconvolutions, significantly reducing the number of parameters and the\ncomputational power of the model. By combining dilated convolution and\ndepthwise separable convolutions, HC-Mamba is able to process large-scale\nmedical image data at a much lower computational cost while maintaining a high\nlevel of performance. We conduct comprehensive experiments on segmentation\ntasks including organ segmentation and skin lesion, and conduct extensive\nexperiments on Synapse, ISIC17 and ISIC18 to demonstrate the potential of the\nHC-Mamba model in medical image segmentation. The experimental results show\nthat HC-Mamba exhibits competitive performance on all these datasets, thereby\nproving its effectiveness and usefulness in medical image segmentation.\n","authors":["Jiashu Xu"],"pdf_url":"https://arxiv.org/pdf/2405.05007v4.pdf","comment":"3figures, 3tabels, fixed data leak"},{"id":"http://arxiv.org/abs/2410.01408v1","updated":"2024-10-02T10:39:31Z","published":"2024-10-02T10:39:31Z","title":"SHAP-CAT: A interpretable multi-modal framework enhancing WSI\n  classification via virtual staining and shapley-value-based multimodal fusion","summary":"  The multimodal model has demonstrated promise in histopathology. However,\nmost multimodal models are based on H\\&E and genomics, adopting increasingly\ncomplex yet black-box designs. In our paper, we propose a novel interpretable\nmultimodal framework named SHAP-CAT, which uses a Shapley-value-based dimension\nreduction technique for effective multimodal fusion. Starting with two paired\nmodalities -- H\\&E and IHC images, we employ virtual staining techniques to\nenhance limited input data by generating a new clinical-related modality.\nLightweight bag-level representations are extracted from image modalities and a\nShapley-value-based mechanism is used for dimension reduction. For each\ndimension of the bag-level representation, attribution values are calculated to\nindicate how changes in the specific dimensions of the input affect the model\noutput. In this way, we select a few top important dimensions of bag-level\nrepresentation for each image modality to late fusion. Our experimental results\ndemonstrate that the proposed SHAP-CAT framework incorporating synthetic\nmodalities significantly enhances model performance, yielding a 5\\% increase in\naccuracy for the BCI, an 8\\% increase for IHC4BC-ER, and an 11\\% increase for\nthe IHC4BC-PR dataset.\n","authors":["Jun Wang","Yu Mao","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2410.01408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07024v2","updated":"2024-10-02T10:37:58Z","published":"2024-07-09T16:44:04Z","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","summary":"  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n","authors":["Jeongseok Hyun","Su Ho Han","Hyolim Kang","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14857v3","updated":"2024-10-02T10:34:09Z","published":"2024-05-23T17:58:03Z","title":"Conditional Diffusion on Web-Scale Image Pairs leads to Diverse Image\n  Variations","summary":"  Generating image variations, where a model produces variations of an input\nimage while preserving the semantic context has gained increasing attention.\nCurrent image variation techniques involve adapting a text-to-image model to\nreconstruct an input image conditioned on the same image. We first demonstrate\nthat a diffusion model trained to reconstruct an input image from frozen\nembeddings, can reconstruct the image with minor variations. Second, inspired\nby how text-to-image models learn from web-scale text-image pairs, we explore a\nnew pretraining strategy to generate image variations using a large collection\nof image pairs. Our diffusion model \\textit{Semantica} receives a random\n(encoded) image from a webpage as conditional input and denoises another noisy\nrandom image from the same webpage. We carefully examine various design choices\nfor the image encoder, given its crucial role in extracting relevant context\nfrom the input image. Once trained, \\textit{Semantica} can adaptively generate\nnew images from a dataset by simply using images from that dataset as input.\nFinally, we identify limitations in standard image consistency metrics for\nevaluating image variations and propose alternative metrics based on few-shot\ngeneration.\n","authors":["Manoj Kumar","Neil Houlsby","Emiel Hoogeboom"],"pdf_url":"https://arxiv.org/pdf/2405.14857v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01407v1","updated":"2024-10-02T10:33:49Z","published":"2024-10-02T10:33:49Z","title":"AgriCLIP: Adapting CLIP for Agriculture and Livestock via\n  Domain-Specialized Cross-Model Alignment","summary":"  Capitalizing on vast amount of image-text data, large-scale vision-language\npre-training has demonstrated remarkable zero-shot capabilities and has been\nutilized in several applications. However, models trained on general everyday\nweb-crawled data often exhibit sub-optimal performance for specialized domains,\nlikely due to domain shift. Recent works have tackled this problem for some\ndomains (e.g., healthcare) by constructing domain-specialized image-text data.\nHowever, constructing a dedicated large-scale image-text dataset for\nsustainable area of agriculture and livestock is still open to research.\nFurther, this domain desires fine-grained feature learning due to the subtle\nnature of the downstream tasks (e.g, nutrient deficiency detection, livestock\nbreed classification). To address this we present AgriCLIP, a vision-language\nfoundational model dedicated to the domain of agriculture and livestock. First,\nwe propose a large-scale dataset, named ALive, that leverages customized prompt\ngeneration strategy to overcome the scarcity of expert annotations. Our ALive\ndataset covers crops, livestock, and fishery, with around 600,000 image-text\npairs. Second, we propose a training pipeline that integrates both contrastive\nand self-supervised learning to learn both global semantic and local\nfine-grained domain-specialized features. Experiments on diverse set of 20\ndownstream tasks demonstrate the effectiveness of AgriCLIP framework, achieving\nan absolute gain of 7.8\\% in terms of average zero-shot classification\naccuracy, over the standard CLIP adaptation via domain-specialized ALive\ndataset. Our ALive dataset and code can be accessible at\n\\href{https://github.com/umair1221/AgriCLIP/tree/main}{Github}.\n","authors":["Umair Nawaz","Muhammad Awais","Hanan Gani","Muzammal Naseer","Fahad Khan","Salman Khan","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2410.01407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01404v1","updated":"2024-10-02T10:31:10Z","published":"2024-10-02T10:31:10Z","title":"Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection","summary":"  Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.\n","authors":["Hongru Yan","Yu Zheng","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2410.01404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11394v2","updated":"2024-10-02T10:28:14Z","published":"2024-07-16T05:26:14Z","title":"DreamCatalyst: Fast and High-Quality 3D Editing via Controlling\n  Editability and Identity Preservation","summary":"  Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks, leveraging diffusion models for 3D consistent\nediting. However, existing SDS-based 3D editing methods suffer from long\ntraining times and produce low-quality results. We identify that the root cause\nof this performance degradation is their conflict with the sampling dynamics of\ndiffusion models. Addressing this conflict allows us to treat SDS as a\ndiffusion reverse process for 3D editing via sampling from data space. In\ncontrast, existing methods naively distill the score function using diffusion\nmodels. From these insights, we propose DreamCatalyst, a novel framework that\nconsiders these sampling dynamics in the SDS framework. Specifically, we devise\nthe optimization process of our DreamCatalyst to approximate the diffusion\nreverse process in editing tasks, thereby aligning with diffusion sampling\ndynamics. As a result, DreamCatalyst successfully reduces training time and\nimproves editing quality. Our method offers two modes: (1) a fast mode that\nedits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than\ncurrent state-of-the-art NeRF editing methods, and (2) a high-quality mode that\nproduces superior results about 8 times faster than these methods. Notably, our\nhigh-quality mode outperforms current state-of-the-art NeRF editing methods in\nterms of both speed and quality. DreamCatalyst also surpasses the\nstate-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing\nitself as an effective and model-agnostic 3D editing solution. See more\nextensive results on our project page: https://dream-catalyst.github.io.\n","authors":["Jiwook Kim","Seonho Lee","Jaeyo Shin","Jiho Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2407.11394v2.pdf","comment":"ProjectPage: https://dream-catalyst.github.io Code:\n  https://github.com/kaist-cvml/DreamCatalyst (Appendix included)"},{"id":"http://arxiv.org/abs/2407.19992v3","updated":"2024-10-02T10:24:45Z","published":"2024-07-29T13:24:55Z","title":"More precise edge detections","summary":"  Image Edge detection (ED) is a base task in computer vision. While the\nperformance of the ED algorithm has been improved greatly by introducing\nCNN-based models, current models still suffer from unsatisfactory precision\nrates especially when only a low error toleration distance is allowed.\nTherefore, model architecture for more precise predictions still needs an\ninvestigation. On the other hand, the unavoidable noise training data provided\nby humans would lead to unsatisfactory model predictions even when inputs are\nedge maps themselves, which also needs a solution. In this paper, more precise\nED models are presented with cascaded skipping density blocks (CSDB). Our\nmodels obtain state-of-the-art(SOTA) predictions in several datasets,\nespecially in average precision rate (AP), over a high-standard benchmark,\nwhich is confirmed by extensive experiments. Also, a novel modification on data\naugmentation for training is employed, which allows noiseless data to be\nemployed in model training for the first time, and thus further improves the\nmodel performance. The relative Python codes can be found on\nhttps://github.com/Hao-B-Shu/SDPED.\n","authors":["Hao Shu"],"pdf_url":"https://arxiv.org/pdf/2407.19992v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.01395v1","updated":"2024-10-02T10:16:42Z","published":"2024-10-02T10:16:42Z","title":"Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical\n  Robots","summary":"  Robot-assisted surgery has profoundly influenced current forms of minimally\ninvasive surgery. However, in transurethral suburethral urological surgical\nrobots, they need to work in a liquid environment. This causes vaporization of\nthe liquid when shearing and heating is performed, resulting in bubble\natomization that affects the visual perception of the robot. This can lead to\nthe need for uninterrupted pauses in the surgical procedure, which makes the\nsurgery take longer. To address the atomization characteristics of liquids\nunder urological surgical robotic vision, we propose an unsupervised zero-shot\ndehaze method (RSF-Dehaze) for urological surgical robotic vision.\nSpecifically, the proposed Region Similarity Filling Module (RSFM) of\nRSF-Dehaze significantly improves the recovery of blurred region tissues. In\naddition, we organize and propose a dehaze dataset for robotic vision in\nurological surgery (USRobot-Dehaze dataset). In particular, this dataset\ncontains the three most common urological surgical robot operation scenarios.\nTo the best of our knowledge, we are the first to organize and propose a\npublicly available dehaze dataset for urological surgical robot vision. The\nproposed RSF-Dehaze proves the effectiveness of our method in three urological\nsurgical robot operation scenarios with extensive comparative experiments with\n20 most classical and advanced dehazing and image recovery algorithms. The\nproposed source code and dataset are available at\nhttps://github.com/wurenkai/RSF-Dehaze .\n","authors":["Renkai Wu","Xianjin Wang","Pengchen Liang","Zhenyu Zhang","Qing Chang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2410.01395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01393v1","updated":"2024-10-02T10:05:43Z","published":"2024-10-02T10:05:43Z","title":"Signal Adversarial Examples Generation for Signal Detection Network via\n  White-Box Attack","summary":"  With the development and application of deep learning in signal detection\ntasks, the vulnerability of neural networks to adversarial attacks has also\nbecome a security threat to signal detection networks. This paper defines a\nsignal adversarial examples generation model for signal detection network from\nthe perspective of adding perturbations to the signal. The model uses the\ninequality relationship of L2-norm between time domain and time-frequency\ndomain to constrain the energy of signal perturbations. Building upon this\nmodel, we propose a method for generating signal adversarial examples utilizing\ngradient-based attacks and Short-Time Fourier Transform. The experimental\nresults show that under the constraint of signal perturbation energy ratio less\nthan 3%, our adversarial attack resulted in a 28.1% reduction in the mean\nAverage Precision (mAP), a 24.7% reduction in recall, and a 30.4% reduction in\nprecision of the signal detection network. Compared to random noise\nperturbation of equivalent intensity, our adversarial attack demonstrates a\nsignificant attack effect.\n","authors":["Dongyang Li","Linyuan Wang","Guangwei Xiong","Bin Yan","Dekui Ma","Jinxian Peng"],"pdf_url":"https://arxiv.org/pdf/2410.01393v1.pdf","comment":"18 pages, 6 figures, submitted to Mobile Networks and Applications"},{"id":"http://arxiv.org/abs/2410.01391v1","updated":"2024-10-02T09:57:45Z","published":"2024-10-02T09:57:45Z","title":"Quantifying Cancer Likeness: A Statistical Approach for Pathological\n  Image Diagnosis","summary":"  In this paper, we present a new statistical approach to automatically\nidentify cancer regions in pathological images. The proposed method is built\nfrom statistical theory in line with evidence-based medicine. The two core\ntechnologies are the classification information of image features, which was\nintroduced based on information theory and which cancer features take positive\nvalues, normal features take negative values, and the calculation technique for\ndetermining their spatial distribution. This method then estimates areas where\nthe classification information content shows a positive value as cancer areas\nin the pathological image. The method achieves AUCs of 0.95 or higher in cancer\nclassification tasks. In addition, the proposed method has the practical\nadvantage of not requiring a precise demarcation line between cancer and\nnormal. This frees pathologists from the monotonous and tedious work of\nbuilding consensus with other pathologists.\n","authors":["Toshiki Kindo"],"pdf_url":"https://arxiv.org/pdf/2410.01391v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.01376v1","updated":"2024-10-02T09:44:54Z","published":"2024-10-02T09:44:54Z","title":"Learning Physics From Video: Unsupervised Physical Parameter Estimation\n  for Continuous Dynamical Systems","summary":"  Extracting physical dynamical system parameters from videos is of great\ninterest to applications in natural science and technology. The\nstate-of-the-art in automatic parameter estimation from video is addressed by\ntraining supervised deep networks on large datasets. Such datasets require\nlabels, which are difficult to acquire. While some unsupervised techniques --\nwhich depend on frame prediction -- exist, they suffer from long training\ntimes, instability under different initializations, and are limited to\nhand-picked motion problems. In this work, we propose a method to estimate the\nphysical parameters of any known, continuous governing equation from single\nvideos; our solution is suitable for different dynamical systems beyond motion\nand is robust to initialization compared to previous approaches. Moreover, we\nremove the need for frame prediction by implementing a KL-divergence-based loss\nfunction in the latent space, which avoids convergence to trivial solutions and\nreduces model size and compute.\n","authors":["Alejandro Castaeda Garcia","Jan van Gemert","Daan Brinks","Nergis Tmen"],"pdf_url":"https://arxiv.org/pdf/2410.01376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20018v2","updated":"2024-10-02T09:34:11Z","published":"2024-09-30T07:25:16Z","title":"Visual Context Window Extension: A New Perspective for Long Video\n  Understanding","summary":"  Large Multimodal Models (LMMs) have demonstrated impressive performance in\nshort video understanding tasks but face great challenges when applied to long\nvideo understanding. In contrast, Large Language Models (LLMs) exhibit\noutstanding capabilities in modeling long texts. Existing work attempts to\naddress this issue by introducing long video-text pairs during training.\nHowever, these approaches require substantial computational and data resources.\nIn this paper, we tackle the challenge of long video understanding from the\nperspective of context windows, aiming to apply LMMs to long video tasks\nwithout retraining on long video datasets. We first conduct an in-depth\nanalysis of why pretrained LMMs struggle to understand lengthy video content,\nidentifying that discrepancies between visual and language modalities lead to\ndifferent context windows for visual and language tokens, making it difficult\nto directly extend the visual tokens to match the language context window.\nBased on this, we propose to adapt LMMs for long video understanding tasks by\nextending the visual context window, eliminating the need for retraining on\nlarge scalelong video datasets. To further mitigate the significant memory\nconsumption caused by long sequences, we introduce a progressive pooling\ninference strategy that selectively adjusts the spatial resolution of frame\nembeddings, reducing the number of visual tokens while retaining important\nspatial information. Across multiple long video understanding benchmarks, our\nmethod consistently improves the performance as the number of video frames\nincreases. On the MLVU benchmark, our method outperforms GPT-4o, even though\nour model size is only 7B. Additionally, in the 256-frame setting, our method\nreduces memory usage by approximately 45% compared to the baseline, without\nintroducing any performance loss.\n","authors":["Hongchen Wei","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.20018v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.01366v1","updated":"2024-10-02T09:28:21Z","published":"2024-10-02T09:28:21Z","title":"Harnessing the Latent Diffusion Model for Training-Free Image Style\n  Transfer","summary":"  Diffusion models have recently shown the ability to generate high-quality\nimages. However, controlling its generation process still poses challenges. The\nimage style transfer task is one of those challenges that transfers the visual\nattributes of a style image to another content image. Typical obstacle of this\ntask is the requirement of additional training of a pre-trained model. We\npropose a training-free style transfer algorithm, Style Tracking Reverse\nDiffusion Process (STRDP) for a pretrained Latent Diffusion Model (LDM). Our\nalgorithm employs Adaptive Instance Normalization (AdaIN) function in a\ndistinct manner during the reverse diffusion process of an LDM while tracking\nthe encoding history of the style image. This algorithm enables style transfer\nin the latent space of LDM for reduced computational cost, and provides\ncompatibility for various LDM models. Through a series of experiments and a\nuser study, we show that our method can quickly transfer the style of an image\nwithout additional training. The speed, compatibility, and training-free aspect\nof our algorithm facilitates agile experiments with combinations of styles and\nLDMs for extensive application.\n","authors":["Kento Masui","Mayu Otani","Masahiro Nomura","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2410.01366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01365v1","updated":"2024-10-02T09:24:57Z","published":"2024-10-02T09:24:57Z","title":"Anti-biofouling Lensless Camera System with Deep Learning based Image\n  Reconstruction","summary":"  In recent years, there has been an increasing demand for underwater cameras\nthat monitor the condition of offshore structures and check the number of\nindividuals in aqua culture environments with long-period observation. One of\nthe significant issues with this observation is that biofouling sticks to the\naperture and lens densely and prevents cameras from capturing clear images.\nThis study examines an underwater camera that applies material technologies\nwith high inherent resistance to biofouling and computer vision technologies\nbased on image reconstruction by deep learning to lens-less cameras. For this\npurpose, our prototype camera uses a coded aperture with 1k rectangular shape\npinholes in a thin metal plate, such as copper, which hinder the growth of\nbiofouling and keep the surface clean. Although images taken by lens-less\ncameras are usually not well formed due to lack of the traditional glass-based\nlens, a deep learning approach using ViT (Vision Transformer) has recently\ndemonstrated reconstructing original photo images well and our study shows that\nusing gated MLP (Multilayer Perceptron) also yields good results. On the other\nhand, a certain degree of thickness for bio-repellence materials is required to\nexhibit their effect the thickness of aperture is necessary to use apertures\nsufficiently thinner than the size of the pinholes to avoid unintentional\nreflection and absorption on the sidewalls. Therefore, we prepared a\nsufficiently thin plate for image reconstruction and now currently we conduct\ntests of the lens-less camera of the bio-repellence aperture with actual\nseawater environments to determine whether it can sufficiently demonstrate the\nbiofouling effect compared with usual camera with only waterproof.\n","authors":["Naoki Ide","Tomohiro Kawahara","Hiroshi Ueno","Daiki Yanagidaira","Susumu Takatsuka"],"pdf_url":"https://arxiv.org/pdf/2410.01365v1.pdf","comment":"9 pages, 8 figures, Ocean Optics 2024"},{"id":"http://arxiv.org/abs/2410.01360v1","updated":"2024-10-02T09:18:43Z","published":"2024-10-02T09:18:43Z","title":"High-quality Animatable Eyelid Shapes from Lightweight Captures","summary":"  High-quality eyelid reconstruction and animation are challenging for the\nsubtle details and complicated deformations. Previous works usually suffer from\nthe trade-off between the capture costs and the quality of details. In this\npaper, we propose a novel method that can achieve detailed eyelid\nreconstruction and animation by only using an RGB video captured by a mobile\nphone. Our method utilizes both static and dynamic information of eyeballs\n(e.g., positions and rotations) to assist the eyelid reconstruction,\ncooperating with an automatic eyeball calibration method to get the required\neyeball parameters. Furthermore, we develop a neural eyelid control module to\nachieve the semantic animation control of eyelids. To the best of our\nknowledge, we present the first method for high-quality eyelid reconstruction\nand animation from lightweight captures. Extensive experiments on both\nsynthetic and real data show that our method can provide more detailed and\nrealistic results compared with previous methods based on the same-level\ncapture setups. The code is available at https://github.com/StoryMY/AniEyelid.\n","authors":["Junfeng Lyu","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.01360v1.pdf","comment":"Accepted by SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2410.01345v1","updated":"2024-10-02T09:02:34Z","published":"2024-10-02T09:02:34Z","title":"Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark\n  and LLM-guided 3D Policy","summary":"  Generalizing language-conditioned robotic policies to new tasks remains a\nsignificant challenge, hampered by the lack of suitable simulation benchmarks.\nIn this paper, we address this gap by introducing GemBench, a novel benchmark\nto assess generalization capabilities of vision-language robotic manipulation\npolicies. GemBench incorporates seven general action primitives and four levels\nof generalization, spanning novel placements, rigid and articulated objects,\nand complex long-horizon tasks. We evaluate state-of-the-art approaches on\nGemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich\n3D information for action prediction conditioned on language. While 3D-LOTUS\nexcels in both efficiency and performance on seen tasks, it struggles with\nnovel tasks. To address this, we present 3D-LOTUS++, a framework that\nintegrates 3D-LOTUS's motion planning capabilities with the task planning\ncapabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++\nachieves state-of-the-art performance on novel tasks of GemBench, setting a new\nstandard for generalization in robotic manipulation. The benchmark, codes and\ntrained models are available at\n\\url{https://www.di.ens.fr/willow/research/gembench/}.\n","authors":["Ricardo Garcia","Shizhe Chen","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2410.01345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12535v2","updated":"2024-10-02T09:00:48Z","published":"2024-03-19T08:19:53Z","title":"High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided\n  Densification and Regularized Optimization","summary":"  We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that\nprovides metrically accurate pose tracking and visually realistic\nreconstruction. To this end, we first propose a Gaussian densification strategy\nbased on the rendering loss to map unobserved areas and refine reobserved\nareas. Second, we introduce extra regularization parameters to alleviate the\nforgetting problem in the continuous mapping problem, where parameters tend to\noverfit the latest frame and result in decreasing rendering quality for\nprevious frames. Both mapping and tracking are performed with Gaussian\nparameters by minimizing re-rendering loss in a differentiable way. Compared to\nrecent neural and concurrently developed gaussian splatting RGBD SLAM\nbaselines, our method achieves state-of-the-art results on the synthetic\ndataset Replica and competitive results on the real-world dataset TUM.\n","authors":["Shuo Sun","Malcolm Mielle","Achim J. Lilienthal","Martin Magnusson"],"pdf_url":"https://arxiv.org/pdf/2403.12535v2.pdf","comment":"Accepted by IROS 2024"},{"id":"http://arxiv.org/abs/2410.01341v1","updated":"2024-10-02T08:58:34Z","published":"2024-10-02T08:58:34Z","title":"Cognition Transferring and Decoupling for Text-supervised Egocentric\n  Semantic Segmentation","summary":"  In this paper, we explore a novel Text-supervised Egocentic Semantic\nSegmentation (TESS) task that aims to assign pixel-level categories to\negocentric images weakly supervised by texts from image-level labels. In this\ntask with prospective potential, the egocentric scenes contain dense\nwearer-object relations and inter-object interference. However, most recent\nthird-view methods leverage the frozen Contrastive Language-Image Pre-training\n(CLIP) model, which is pre-trained on the semantic-oriented third-view data and\nlapses in the egocentric view due to the ``relation insensitive\" problem.\nHence, we propose a Cognition Transferring and Decoupling Network (CTDN) that\nfirst learns the egocentric wearer-object relations via correlating the image\nand text. Besides, a Cognition Transferring Module (CTM) is developed to\ndistill the cognitive knowledge from the large-scale pre-trained model to our\nmodel for recognizing egocentric objects with various semantics. Based on the\ntransferred cognition, the Foreground-background Decoupling Module (FDM)\ndisentangles the visual representations to explicitly discriminate the\nforeground and background regions to mitigate false activation areas caused by\nforeground-background interferential objects during egocentric relation\nlearning. Extensive experiments on four TESS benchmarks demonstrate the\neffectiveness of our approach, which outperforms many recent related methods by\na large margin. Code will be available at https://github.com/ZhaofengSHI/CTDN.\n","authors":["Zhaofeng Shi","Heqian Qiu","Lanxiao Wang","Fanman Meng","Qingbo Wu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.01341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01336v1","updated":"2024-10-02T08:53:20Z","published":"2024-10-02T08:53:20Z","title":"VectorGraphNET: Graph Attention Networks for Accurate Segmentation of\n  Complex Technical Drawings","summary":"  This paper introduces a new approach to extract and analyze vector data from\ntechnical drawings in PDF format. Our method involves converting PDF files into\nSVG format and creating a feature-rich graph representation, which captures the\nrelationships between vector entities using geometrical information. We then\napply a graph attention transformer with hierarchical label definition to\nachieve accurate line-level segmentation. Our approach is evaluated on two\ndatasets, including the public FloorplanCAD dataset, which achieves\nstate-of-the-art results on weighted F1 score, surpassing existing methods. The\nproposed vector-based method offers a more scalable solution for large-scale\ntechnical drawing analysis compared to vision-based approaches, while also\nrequiring significantly less GPU power than current state-of-the-art\nvector-based techniques. Moreover, it demonstrates improved performance in\nterms of the weighted F1 (wF1) score on the semantic segmentation task. Our\nresults demonstrate the effectiveness of our approach in extracting meaningful\ninformation from technical drawings, enabling new applications, and improving\nexisting workflows in the AEC industry. Potential applications of our approach\ninclude automated building information modeling (BIM) and construction\nplanning, which could significantly impact the efficiency and productivity of\nthe industry.\n","authors":["Andrea Carrara","Stavros Nousias","Andr Borrmann"],"pdf_url":"https://arxiv.org/pdf/2410.01336v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2311.06423v2","updated":"2024-10-02T08:50:05Z","published":"2023-11-10T23:10:21Z","title":"Transferability Bound Theory: Exploring Relationship between Adversarial\n  Transferability and Flatness","summary":"  A prevailing belief in attack and defense community is that the higher\nflatness of adversarial examples enables their better cross-model\ntransferability, leading to a growing interest in employing sharpness-aware\nminimization and its variants. However, the theoretical relationship between\nthe transferability of adversarial examples and their flatness has not been\nwell established, making the belief questionable. To bridge this gap, we embark\non a theoretical investigation and, for the first time, derive a theoretical\nbound for the transferability of adversarial examples with few practical\nassumptions. Our analysis challenges this belief by demonstrating that the\nincreased flatness of adversarial examples does not necessarily guarantee\nimproved transferability. Moreover, building upon the theoretical analysis, we\npropose TPA, a Theoretically Provable Attack that optimizes a surrogate of the\nderived bound to craft adversarial examples. Extensive experiments across\nwidely used benchmark datasets and various real-world applications show that\nTPA can craft more transferable adversarial examples compared to\nstate-of-the-art baselines. We hope that these results can recalibrate\npreconceived impressions within the community and facilitate the development of\nstronger adversarial attack and defense mechanisms. The source codes are\navailable in <https://github.com/fmy266/TPA>.\n","authors":["Mingyuan Fan","Xiaodan Li","Cen Chen","Wenmeng Zhou","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2311.06423v2.pdf","comment":"Accepted by NIPS 2024"},{"id":"http://arxiv.org/abs/2409.19734v2","updated":"2024-10-02T08:44:40Z","published":"2024-09-29T15:20:00Z","title":"T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness\n  Recognition","summary":"  To address the risks of encountering inappropriate or harmful content,\nresearchers managed to incorporate several harmful contents datasets with\nmachine learning methods to detect harmful concepts. However, existing harmful\ndatasets are curated by the presence of a narrow range of harmful objects, and\nonly cover real harmful content sources. This hinders the generalizability of\nmethods based on such datasets, potentially leading to misjudgments. Therefore,\nwe propose a comprehensive harmful dataset, Visual Harmful Dataset 11K\n(VHD11K), consisting of 10,000 images and 1,000 videos, crawled from the\nInternet and generated by 4 generative models, across a total of 10 harmful\ncategories covering a full spectrum of harmful concepts with nontrivial\ndefinition. We also propose a novel annotation framework by formulating the\nannotation process as a multi-agent Visual Question Answering (VQA) task,\nhaving 3 different VLMs \"debate\" about whether the given image/video is\nharmful, and incorporating the in-context learning strategy in the debating\nprocess. Therefore, we can ensure that the VLMs consider the context of the\ngiven image/video and both sides of the arguments thoroughly before making\ndecisions, further reducing the likelihood of misjudgments in edge cases.\nEvaluation and experimental results demonstrate that (1) the great alignment\nbetween the annotation from our novel annotation framework and those from\nhuman, ensuring the reliability of VHD11K; (2) our full-spectrum harmful\ndataset successfully identifies the inability of existing harmful content\ndetection methods to detect extensive harmful contents and improves the\nperformance of existing harmfulness recognition methods; (3) VHD11K outperforms\nthe baseline dataset, SMID, as evidenced by the superior improvement in\nharmfulness recognition methods. The complete dataset and code can be found at\nhttps://github.com/nctu-eva-lab/VHD11K.\n","authors":["Chen Yeh","You-Ming Chang","Wei-Chen Chiu","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2409.19734v2.pdf","comment":"Accepted to NeurIPS'24 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.01322v1","updated":"2024-10-02T08:26:37Z","published":"2024-10-02T08:26:37Z","title":"Forte : Finding Outliers with Representation Typicality Estimation","summary":"  Generative models can now produce photorealistic synthetic data which is\nvirtually indistinguishable from the real data used to train it. This is a\nsignificant evolution over previous models which could produce reasonable\nfacsimiles of the training data, but ones which could be visually distinguished\nfrom the training data by human evaluation. Recent work on OOD detection has\nraised doubts that generative model likelihoods are optimal OOD detectors due\nto issues involving likelihood misestimation, entropy in the generative\nprocess, and typicality. We speculate that generative OOD detectors also failed\nbecause their models focused on the pixels rather than the semantic content of\nthe data, leading to failures in near-OOD cases where the pixels may be similar\nbut the information content is significantly different. We hypothesize that\nestimating typical sets using self-supervised learners leads to better OOD\ndetectors. We introduce a novel approach that leverages representation\nlearning, and informative summary statistics based on manifold estimation, to\naddress all of the aforementioned issues. Our method outperforms other\nunsupervised approaches and achieves state-of-the art performance on\nwell-established challenging benchmarks, and new synthetic data detection\ntasks.\n","authors":["Debargha Ganguly","Warren Morningstar","Andrew Yu","Vipin Chaudhary"],"pdf_url":"https://arxiv.org/pdf/2410.01322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10166v2","updated":"2024-10-02T08:24:08Z","published":"2023-07-19T17:50:03Z","title":"Adversarial Latent Autoencoder with Self-Attention for Structural Image\n  Synthesis","summary":"  Generative Engineering Design approaches driven by Deep Generative Models\n(DGM) have been proposed to facilitate industrial engineering processes. In\nsuch processes, designs often come in the form of images, such as blueprints,\nengineering drawings, and CAD models depending on the level of detail. DGMs\nhave been successfully employed for synthesis of natural images, e.g.,\ndisplaying animals, human faces and landscapes. However, industrial design\nimages are fundamentally different from natural scenes in that they contain\nrich structural patterns and long-range dependencies, which are challenging for\nconvolution-based DGMs to generate. Moreover, DGM-driven generation process is\ntypically triggered based on random noisy inputs, which outputs unpredictable\nsamples and thus cannot perform an efficient industrial design exploration. We\ntackle these challenges by proposing a novel model Self-Attention Adversarial\nLatent Autoencoder (SA-ALAE), which allows generating feasible design images of\ncomplex engineering parts. With SA-ALAE, users can not only explore novel\nvariants of an existing design, but also control the generation process by\noperating in latent space. The potential of SA-ALAE is shown by generating\nengineering blueprints in a real automotive design task.\n","authors":["Jiajie Fan","Laure Vuaille","Hao Wang","Thomas Bck"],"pdf_url":"https://arxiv.org/pdf/2307.10166v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.16223v2","updated":"2024-10-02T08:23:07Z","published":"2024-09-24T16:35:16Z","title":"Fine-Tuning is Fine, if Calibrated","summary":"  Fine-tuning is arguably the most straightforward way to tailor a pre-trained\nmodel (e.g., a foundation model) to downstream applications, but it also comes\nwith the risk of losing valuable knowledge the model had learned in\npre-training. For example, fine-tuning a pre-trained classifier capable of\nrecognizing a large number of classes to master a subset of classes at hand is\nshown to drastically degrade the model's accuracy in the other classes it had\npreviously learned. As such, it is hard to further use the fine-tuned model\nwhen it encounters classes beyond the fine-tuning data. In this paper, we\nsystematically dissect the issue, aiming to answer the fundamental question,\n\"What has been damaged in the fine-tuned model?\" To our surprise, we find that\nthe fine-tuned model neither forgets the relationship among the other classes\nnor degrades the features to recognize these classes. Instead, the fine-tuned\nmodel often produces more discriminative features for these other classes, even\nif they were missing during fine-tuning! {What really hurts the accuracy is the\ndiscrepant logit scales between the fine-tuning classes and the other classes},\nimplying that a simple post-processing calibration would bring back the\npre-trained model's capability and at the same time unveil the feature\nimprovement over all classes. We conduct an extensive empirical study to\ndemonstrate the robustness of our findings and provide preliminary explanations\nunderlying them, suggesting new directions for future theoretical analysis. Our\ncode is available at\nhttps://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated.\n","authors":["Zheda Mai","Arpita Chowdhury","Ping Zhang","Cheng-Hao Tu","Hong-You Chen","Vardaan Pahuja","Tanya Berger-Wolf","Song Gao","Charles Stewart","Yu Su","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2409.16223v2.pdf","comment":"The first three authors contribute equally. The paper has been\n  accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.01319v1","updated":"2024-10-02T08:22:42Z","published":"2024-10-02T08:22:42Z","title":"Finetuning Pre-trained Model with Limited Data for LiDAR-based 3D Object\n  Detection by Bridging Domain Gaps","summary":"  LiDAR-based 3D object detectors have been largely utilized in various\napplications, including autonomous vehicles or mobile robots. However,\nLiDAR-based detectors often fail to adapt well to target domains with different\nsensor configurations (e.g., types of sensors, spatial resolution, or FOVs) and\nlocation shifts. Collecting and annotating datasets in a new setup is commonly\nrequired to reduce such gaps, but it is often expensive and time-consuming.\nRecent studies suggest that pre-trained backbones can be learned in a\nself-supervised manner with large-scale unlabeled LiDAR frames. However,\ndespite their expressive representations, they remain challenging to generalize\nwell without substantial amounts of data from the target domain. Thus, we\npropose a novel method, called Domain Adaptive Distill-Tuning (DADT), to adapt\na pre-trained model with limited target data (approximately 100 LiDAR frames),\nretaining its representation power and preventing it from overfitting.\nSpecifically, we use regularizers to align object-level and context-level\nrepresentations between the pre-trained and finetuned models in a\nteacher-student architecture. Our experiments with driving benchmarks, i.e.,\nWaymo Open dataset and KITTI, confirm that our method effectively finetunes a\npre-trained model, achieving significant gains in accuracy.\n","authors":["Jiyun Jang","Mincheol Chang","Jongwon Park","Jinkyu Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01319v1.pdf","comment":"Accepted in IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2311.12421v2","updated":"2024-10-02T08:17:05Z","published":"2023-11-21T08:21:55Z","title":"Two Views Are Better than One: Monocular 3D Pose Estimation with\n  Multiview Consistency","summary":"  Deducing a 3D human pose from a single 2D image or 2D keypoints is inherently\nchallenging, given the fundamental ambiguity wherein multiple 3D poses can\ncorrespond to the same 2D representation. The acquisition of 3D data, while\ninvaluable for resolving pose ambiguity, is expensive and requires an intricate\nsetup, often restricting its applicability to controlled lab environments. We\nimprove performance of monocular human pose estimation models using multiview\ndata for fine-tuning. We propose a novel loss function, multiview consistency,\nto enable adding additional training data with only 2D supervision. This loss\nenforces that the inferred 3D pose from one view aligns with the inferred 3D\npose from another view under similarity transformations. Our consistency loss\nsubstantially improves performance for fine-tuning with no available 3D data.\nOur experiments demonstrate that two views offset by 90 degrees are enough to\nobtain good performance, with only marginal improvements by adding more views.\nThus, we enable the acquisition of domain-specific data by capturing activities\nwith off-the-shelf cameras, eliminating the need for elaborate calibration\nprocedures. This research introduces new possibilities for domain adaptation in\n3D pose estimation, providing a practical and cost-effective solution to\ncustomize models for specific applications. The used dataset, featuring\nadditional views, will be made publicly available.\n","authors":["Christian Keilstrup Ingwersen","Rasmus Tirsgaard","Rasmus Nylander","Janus Nrtoft Jensen","Anders Bjorholm Dahl","Morten Rieger Hannemose"],"pdf_url":"https://arxiv.org/pdf/2311.12421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16434v2","updated":"2024-10-02T08:04:46Z","published":"2024-09-24T19:57:40Z","title":"Lessons Learned from a Unifying Empirical Study of Parameter-Efficient\n  Transfer Learning (PETL) in Visual Recognition","summary":"  Parameter-efficient transfer learning (PETL) has attracted significant\nattention lately, due to the increasing size of pre-trained models and the need\nto fine-tune (FT) them for superior downstream performance. This community-wide\nenthusiasm has sparked a plethora of approaches. Nevertheless, a systematic\nstudy to understand their performance and suitable application scenarios is\nlacking, leaving questions like when to apply PETL and which approach to use\nlargely unanswered. In this paper, we conduct a unifying empirical study of\nrepresentative PETL methods in the context of Vision Transformers. We\nsystematically tune their hyper-parameters to fairly compare their accuracy on\ndownstream tasks. Our study not only offers a valuable user guide but also\nunveils several new insights. First, if tuned carefully, different PETL methods\ncan obtain similar accuracy in the low-shot benchmark VTAB-1K. This includes\nsimple methods like FT the bias terms that were reported inferior. Second,\nthough with similar accuracy, we find that PETL methods make different mistakes\nand high-confidence predictions, likely due to their different inductive\nbiases. Such an inconsistency (or complementariness) opens up the opportunity\nfor ensemble methods, and we make preliminary attempts at this. Third, going\nbeyond the commonly used low-shot tasks, we find that PETL is also useful in\nmany-shot regimes -- it achieves comparable and sometimes better accuracy than\nfull FT, using much fewer learnable parameters. Last but not least, we\ninvestigate PETL's ability to preserve a pre-trained model's robustness to\ndistribution shifts (e.g., a CLIP backbone). Perhaps not surprisingly, PETL\nmethods outperform full FT alone. However, with weight-space ensembles, the\nfully fine-tuned model can better balance target (i.e., downstream)\ndistribution and distribution shift performance, suggesting a future research\ndirection for PETL.\n","authors":["Zheda Mai","Ping Zhang","Cheng-Hao Tu","Hong-You Chen","Li Zhang","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2409.16434v2.pdf","comment":"Code is available at https://github.com/OSU-MLB/PETL_Vision"},{"id":"http://arxiv.org/abs/2409.19111v2","updated":"2024-10-02T07:56:31Z","published":"2024-09-27T19:31:04Z","title":"Fusion is all you need: Face Fusion for Customized Identity-Preserving\n  Image Synthesis","summary":"  Text-to-image (T2I) models have significantly advanced the development of\nartificial intelligence, enabling the generation of high-quality images in\ndiverse contexts based on specific text prompts. However, existing T2I-based\nmethods often struggle to accurately reproduce the appearance of individuals\nfrom a reference image and to create novel representations of those individuals\nin various settings. To address this, we leverage the pre-trained UNet from\nStable Diffusion to incorporate the target face image directly into the\ngeneration process. Our approach diverges from prior methods that depend on\nfixed encoders or static face embeddings, which often fail to bridge encoding\ngaps. Instead, we capitalize on UNet's sophisticated encoding capabilities to\nprocess reference images across multiple scales. By innovatively altering the\ncross-attention layers of the UNet, we effectively fuse individual identities\ninto the generative process. This strategic integration of facial features\nacross various scales not only enhances the robustness and consistency of the\ngenerated images but also facilitates efficient multi-reference and\nmulti-identity generation. Our method sets a new benchmark in\nidentity-preserving image generation, delivering state-of-the-art results in\nsimilarity metrics while maintaining prompt alignment.\n","authors":["Salaheldin Mohamed","Dong Han","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2409.19111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01304v1","updated":"2024-10-02T07:56:15Z","published":"2024-10-02T07:56:15Z","title":"Deep learning for action spotting in association football videos","summary":"  The task of action spotting consists in both identifying actions and\nprecisely localizing them in time with a single timestamp in long, untrimmed\nvideo streams. Automatically extracting those actions is crucial for many\nsports applications, including sports analytics to produce extended statistics\non game actions, coaching to provide support to video analysts, or fan\nengagement to automatically overlay content in the broadcast when specific\nactions occur. However, before 2018, no large-scale datasets for action\nspotting in sports were publicly available, which impeded benchmarking action\nspotting methods. In response, our team built the largest dataset and the most\ncomprehensive benchmarks for sports video understanding, under the umbrella of\nSoccerNet. Particularly, our dataset contains a subset specifically dedicated\nto action spotting, called SoccerNet Action Spotting, containing more than 550\ncomplete broadcast games annotated with almost all types of actions that can\noccur in a football game. This dataset is tailored to develop methods for\nautomatic spotting of actions of interest, including deep learning approaches,\nby providing a large amount of manually annotated actions. To engage with the\nscientific community, the SoccerNet initiative organizes yearly challenges,\nduring which participants from all around the world compete to achieve\nstate-of-the-art performances. Thanks to our dataset and challenges, more than\n60 methods were developed or published over the past five years, improving on\nthe first baselines and making action spotting a viable option for the sports\nindustry. This paper traces the history of action spotting in sports, from the\ncreation of the task back in 2018, to the role it plays today in research and\nthe sports industry.\n","authors":["Silvio Giancola","Anthony Cioppa","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2410.01304v1.pdf","comment":"31 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2310.11811v2","updated":"2024-10-02T07:54:07Z","published":"2023-10-18T09:05:57Z","title":"ShapeGraFormer: GraFormer-Based Network for Hand-Object Reconstruction\n  from a Single Depth Map","summary":"  3D reconstruction of hand-object manipulations is important for emulating\nhuman actions. Most methods dealing with challenging object manipulation\nscenarios, focus on hands reconstruction in isolation, ignoring physical and\nkinematic constraints due to object contact. Some approaches produce more\nrealistic results by jointly reconstructing 3D hand-object interactions.\nHowever, they focus on coarse pose estimation or rely upon known hand and\nobject shapes. We propose the first approach for realistic 3D hand-object shape\nand pose reconstruction from a single depth map. Unlike previous work, our\nvoxel-based reconstruction network regresses the vertex coordinates of a hand\nand an object and reconstructs more realistic interaction. Our pipeline\nadditionally predicts voxelized hand-object shapes, having a one-to-one mapping\nto the input voxelized depth. Thereafter, we exploit the graph nature of the\nhand and object shapes, by utilizing the recent GraFormer network with\npositional embedding to reconstruct shapes from template meshes. In addition,\nwe show the impact of adding another GraFormer component that refines the\nreconstructed shapes based on the hand-object interactions and its ability to\nreconstruct more accurate object shapes. We perform an extensive evaluation on\nthe HO-3D and DexYCB datasets and show that our method outperforms existing\napproaches in hand reconstruction and produces plausible reconstructions for\nthe objects\n","authors":["Ahmed Tawfik Aboukhadra","Jameel Malik","Nadia Robertini","Ahmed Elhayek","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2310.11811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01295v1","updated":"2024-10-02T07:42:20Z","published":"2024-10-02T07:42:20Z","title":"LaGeM: A Large Geometry Model for 3D Representation Learning and\n  Diffusion","summary":"  This paper introduces a novel hierarchical autoencoder that maps 3D models\ninto a highly compressed latent space. The hierarchical autoencoder is\nspecifically designed to tackle the challenges arising from large-scale\ndatasets and generative modeling using diffusion. Different from previous\napproaches that only work on a regular image or volume grid, our hierarchical\nautoencoder operates on unordered sets of vectors. Each level of the\nautoencoder controls different geometric levels of detail. We show that the\nmodel can be used to represent a wide range of 3D models while faithfully\nrepresenting high-resolution geometry details. The training of the new\narchitecture takes 0.70x time and 0.58x memory compared to the baseline. We\nalso explore how the new representation can be used for generative modeling.\nSpecifically, we propose a cascaded diffusion framework where each stage is\nconditioned on the previous stage. Our design extends existing cascaded designs\nfor image and volume grids to vector sets.\n","authors":["Biao Zhang","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2410.01295v1.pdf","comment":"For more information: https://1zb.github.io/LaGeM"},{"id":"http://arxiv.org/abs/2410.01293v1","updated":"2024-10-02T07:40:27Z","published":"2024-10-02T07:40:27Z","title":"SurgeoNet: Realtime 3D Pose Estimation of Articulated Surgical\n  Instruments from Stereo Images using a Synthetically-trained Network","summary":"  Surgery monitoring in Mixed Reality (MR) environments has recently received\nsubstantial focus due to its importance in image-based decisions, skill\nassessment, and robot-assisted surgery. Tracking hands and articulated surgical\ninstruments is crucial for the success of these applications. Due to the lack\nof annotated datasets and the complexity of the task, only a few works have\naddressed this problem. In this work, we present SurgeoNet, a real-time neural\nnetwork pipeline to accurately detect and track surgical instruments from a\nstereo VR view. Our multi-stage approach is inspired by state-of-the-art\nneural-network architectural design, like YOLO and Transformers. We demonstrate\nthe generalization capabilities of SurgeoNet in challenging real-world\nscenarios, achieved solely through training on synthetic data. The approach can\nbe easily extended to any new set of articulated surgical instruments.\nSurgeoNet's code and data are publicly available.\n","authors":["Ahmed Tawfik Aboukhadra","Nadia Robertini","Jameel Malik","Ahmed Elhayek","Gerd Reis","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2410.01293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02567v2","updated":"2024-10-02T07:22:30Z","published":"2024-09-04T09:35:09Z","title":"Evaluation Study on SAM 2 for Class-agnostic Instance-level Segmentation","summary":"  Segment Anything Model (SAM) has demonstrated powerful zero-shot segmentation\nperformance in natural scenes. The recently released Segment Anything Model 2\n(SAM2) has further heightened researchers' expectations towards image\nsegmentation capabilities. To evaluate the performance of SAM2 on\nclass-agnostic instance-level segmentation tasks, we adopt different prompt\nstrategies for SAM2 to cope with instance-level tasks for three relevant\nscenarios: Salient Instance Segmentation (SIS), Camouflaged Instance\nSegmentation (CIS), and Shadow Instance Detection (SID). In addition, to\nfurther explore the effectiveness of SAM2 in segmenting granular object\nstructures, we also conduct detailed tests on the high-resolution Dichotomous\nImage Segmentation (DIS) benchmark to assess the fine-grained segmentation\ncapability. Qualitative and quantitative experimental results indicate that the\nperformance of SAM2 varies significantly across different scenarios. Besides,\nSAM2 is not particularly sensitive to segmenting high-resolution fine details.\nWe hope this technique report can drive the emergence of SAM2-based adapters,\naiming to enhance the performance ceiling of large vision models on\nclass-agnostic instance segmentation tasks.\n","authors":["Jialun Pei","Zhangjun Zhou","Tiantian Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.02567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20113v2","updated":"2024-10-02T07:16:05Z","published":"2024-09-30T09:11:13Z","title":"CBAM-SwinT-BL: Small Rail Surface Defect Detection Method Based on Swin\n  Transformer with Block Level CBAM Enhancement","summary":"  Under high-intensity rail operations, rail tracks endure considerable\nstresses resulting in various defects such as corrugation and spellings.\nFailure to effectively detect defects and provide maintenance in time would\ncompromise service reliability and public safety. While advanced models have\nbeen developed in recent years, efficiently identifying small-scale rail\ndefects has not yet been studied, especially for categories such as Dirt or\nSquat on rail surface. To address this challenge, this study utilizes Swin\nTransformer (SwinT) as baseline and incorporates the Convolutional Block\nAttention Module (CBAM) for enhancement. Our proposed method integrates CBAM\nsuccessively within the swin transformer blocks, resulting in significant\nperformance improvement in rail defect detection, particularly for categories\nwith small instance sizes. The proposed framework is named CBAM-Enhanced Swin\nTransformer in Block Level (CBAM-SwinT-BL). Experiment and ablation study have\nproven the effectiveness of the framework. The proposed framework has a notable\nimprovement in the accuracy of small size defects, such as dirt and dent\ncategories in RIII dataset, with mAP-50 increasing by +23.0% and +38.3%\nrespectively, and the squat category in MUET dataset also reaches +13.2% higher\nthan the original model. Compares to the original SwinT, CBAM-SwinT-BL increase\noverall precision around +5% in the MUET dataset and +7% in the RIII dataset,\nreaching 69.1% and 88.1% respectively. Meanwhile, the additional module CBAM\nmerely extend the model training speed by an average of +0.04s/iteration, which\nis acceptable compared to the significant improvement in system performance.\n","authors":["Jiayi Zhao","Alison Wun-lam Yeung","Ali Muhammad","Songjiang Lai","Vincent To-Yee NG"],"pdf_url":"https://arxiv.org/pdf/2409.20113v2.pdf","comment":"27 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.01273v1","updated":"2024-10-02T06:34:45Z","published":"2024-10-02T06:34:45Z","title":"CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot\n  Interaction","summary":"  Real-life robot navigation involves more than just reaching a destination; it\nrequires optimizing movements while addressing scenario-specific goals. An\nintuitive way for humans to express these goals is through abstract cues like\nverbal commands or rough sketches. Such human guidance may lack details or be\nnoisy. Nonetheless, we expect robots to navigate as intended. For robots to\ninterpret and execute these abstract instructions in line with human\nexpectations, they must share a common understanding of basic navigation\nconcepts with humans. To this end, we introduce CANVAS, a novel framework that\ncombines visual and linguistic instructions for commonsense-aware navigation.\nIts success is driven by imitation learning, enabling the robot to learn from\nhuman navigation behavior. We present COMMAND, a comprehensive dataset with\nhuman-annotated navigation results, spanning over 48 hours and 219 km, designed\nto train commonsense-aware navigation systems in simulated environments. Our\nexperiments show that CANVAS outperforms the strong rule-based system ROS\nNavStack across all environments, demonstrating superior performance with noisy\ninstructions. Notably, in the orchard environment, where ROS NavStack records a\n0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also\nclosely aligns with human demonstrations and commonsense constraints, even in\nunseen environments. Furthermore, real-world deployment of CANVAS showcases\nimpressive Sim2Real transfer with a total success rate of 69%, highlighting the\npotential of learning from human demonstrations in simulated environments for\nreal-world applications.\n","authors":["Suhwan Choi","Yongjun Cho","Minchan Kim","Jaeyoon Jung","Myunchul Joe","Yubeen Park","Minseo Kim","Sungwoong Kim","Sungjae Lee","Hwiseong Park","Jiwan Chung","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01273v1.pdf","comment":"project page https://worv-ai.github.io/canvas"},{"id":"http://arxiv.org/abs/2409.02529v2","updated":"2024-10-02T06:30:19Z","published":"2024-09-04T08:42:42Z","title":"Sample what you cant compress","summary":"  For learned image representations, basic autoencoders often produce blurry\nresults. Reconstruction quality can be improved by incorporating additional\npenalties such as adversarial (GAN) and perceptual losses. Arguably, these\napproaches lack a principled interpretation. Concurrently, in generative\nsettings diffusion has demonstrated a remarkable ability to create crisp, high\nquality results and has solid theoretical underpinnings (from variational\ninference to direct study as the Fisher Divergence). Our work combines\nautoencoder representation learning with diffusion and is, to our knowledge,\nthe first to demonstrate the efficacy of jointly learning a continuous encoder\nand decoder under a diffusion-based loss. We demonstrate that this approach\nyields better reconstruction quality as compared to GAN-based autoencoders\nwhile being easier to tune. We also show that the resulting representation is\neasier to model with a latent diffusion model as compared to the representation\nobtained from a state-of-the-art GAN-based loss. Since our decoder is\nstochastic, it can generate details not encoded in the otherwise deterministic\nlatent representation; we therefore name our approach \"Sample what you can't\ncompress\", or SWYCC for short.\n","authors":["Vighnesh Birodkar","Gabriel Barcik","James Lyon","Sergey Ioffe","David Minnen","Joshua V. Dillon"],"pdf_url":"https://arxiv.org/pdf/2409.02529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01270v1","updated":"2024-10-02T06:28:32Z","published":"2024-10-02T06:28:32Z","title":"Panopticus: Omnidirectional 3D Object Detection on Resource-constrained\n  Edge Devices","summary":"  3D object detection with omnidirectional views enables safety-critical\napplications such as mobile robot navigation. Such applications increasingly\noperate on resource-constrained edge devices, facilitating reliable processing\nwithout privacy concerns or network delays. To enable cost-effective\ndeployment, cameras have been widely adopted as a low-cost alternative to LiDAR\nsensors. However, the compute-intensive workload to achieve high performance of\ncamera-based solutions remains challenging due to the computational limitations\nof edge devices. In this paper, we present Panopticus, a carefully designed\nsystem for omnidirectional and camera-based 3D detection on edge devices.\nPanopticus employs an adaptive multi-branch detection scheme that accounts for\nspatial complexities. To optimize the accuracy within latency limits,\nPanopticus dynamically adjusts the model's architecture and operations based on\navailable edge resources and spatial characteristics. We implemented Panopticus\non three edge devices and conducted experiments across real-world environments\nbased on the public self-driving dataset and our mobile 360{\\deg} camera\ndataset. Experiment results showed that Panopticus improves accuracy by 62% on\naverage given the strict latency objective of 33ms. Also, Panopticus achieves a\n2.1{\\times} latency reduction on average compared to baselines.\n","authors":["Jeho Lee","Chanyoung Jung","Jiwon Kim","Hojung Cha"],"pdf_url":"https://arxiv.org/pdf/2410.01270v1.pdf","comment":"Published at MobiCom 2024"},{"id":"http://arxiv.org/abs/2410.01264v1","updated":"2024-10-02T06:21:00Z","published":"2024-10-02T06:21:00Z","title":"Backdooring Vision-Language Models with Out-Of-Distribution Data","summary":"  The emergence of Vision-Language Models (VLMs) represents a significant\nadvancement in integrating computer vision with Large Language Models (LLMs) to\ngenerate detailed text descriptions from visual inputs. Despite their growing\nimportance, the security of VLMs, particularly against backdoor attacks, is\nunder explored. Moreover, prior works often assume attackers have access to the\noriginal training data, which is often unrealistic. In this paper, we address a\nmore practical and challenging scenario where attackers must rely solely on\nOut-Of-Distribution (OOD) data. We introduce VLOOD (Backdooring Vision-Language\nModels with Out-of-Distribution Data), a novel approach with two key\ncontributions: (1) demonstrating backdoor attacks on VLMs in complex\nimage-to-text tasks while minimizing degradation of the original semantics\nunder poisoned inputs, and (2) proposing innovative techniques for backdoor\ninjection without requiring any access to the original training data. Our\nevaluation on image captioning and visual question answering (VQA) tasks\nconfirms the effectiveness of VLOOD, revealing a critical security\nvulnerability in VLMs and laying the foundation for future research on securing\nmultimodal models against sophisticated threats.\n","authors":["Weimin Lyu","Jiachen Yao","Saumya Gupta","Lu Pang","Tao Sun","Lingjie Yi","Lijie Hu","Haibin Ling","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01262v1","updated":"2024-10-02T06:16:06Z","published":"2024-10-02T06:16:06Z","title":"Aggregation of Multi Diffusion Models for Enhancing Learned\n  Representations","summary":"  Diffusion models have achieved remarkable success in image generation,\nparticularly with the various applications of classifier-free guidance\nconditional diffusion models. While many diffusion models perform well when\ncontrolling for particular aspect among style, character, and interaction, they\nstruggle with fine-grained control due to dataset limitations and intricate\nmodel architecture design. This paper introduces a novel algorithm, Aggregation\nof Multi Diffusion Models (AMDM), which synthesizes features from multiple\ndiffusion models into a specified model, enhancing its learned representations\nto activate specific features for fine-grained control. AMDM consists of two\nkey components: spherical aggregation and manifold optimization. Spherical\naggregation merges intermediate variables from different diffusion models with\nminimal manifold deviation, while manifold optimization refines these variables\nto align with the intermediate data manifold, enhancing sampling quality.\nExperimental results demonstrate that AMDM significantly improves fine-grained\ncontrol without additional training or inference time, proving its\neffectiveness. Additionally, it reveals that diffusion models initially focus\non features such as position, attributes, and style, with later stages\nimproving generation quality and consistency. AMDM offers a new perspective for\ntackling the challenges of fine-grained conditional control generation in\ndiffusion models: We can fully utilize existing conditional diffusion models\nthat control specific aspects, or develop new ones, and then aggregate them\nusing the AMDM algorithm. This eliminates the need for constructing complex\ndatasets, designing intricate model architectures, and incurring high training\ncosts. Code is available at: https://github.com/Hammour-steak/AMDM\n","authors":["Conghan Yue","Zhengwei Peng","Shiyan Du","Zhi Ji","Dongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01261v1","updated":"2024-10-02T06:14:49Z","published":"2024-10-02T06:14:49Z","title":"OCC-MLLM:Empowering Multimodal Large Language Model For the\n  Understanding of Occluded Objects","summary":"  There is a gap in the understanding of occluded objects in existing\nlarge-scale visual language multi-modal models. Current state-of-the-art\nmultimodal models fail to provide satisfactory results in describing occluded\nobjects for visual-language multimodal models through universal visual\nencoders. Another challenge is the limited number of datasets containing\nimage-text pairs with a large number of occluded objects. Therefore, we\nintroduce a novel multimodal model that applies a newly designed visual encoder\nto understand occluded objects in RGB images. We also introduce a large-scale\nvisual-language pair dataset for training large-scale visual-language\nmultimodal models and understanding occluded objects. We start our experiments\ncomparing with the state-of-the-art models.\n","authors":["Wenmo Qiu","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2410.01261v1.pdf","comment":"Accepted by CVPR 2024 T4V Workshop (5 pages, 3 figures, 2 tables)"},{"id":"http://arxiv.org/abs/2410.00700v2","updated":"2024-10-02T06:13:56Z","published":"2024-10-01T13:54:29Z","title":"Mining Your Own Secrets: Diffusion Classifier Scores for Continual\n  Personalization of Text-to-Image Diffusion Models","summary":"  Personalized text-to-image diffusion models have grown popular for their\nability to efficiently acquire a new concept from user-defined text\ndescriptions and a few images. However, in the real world, a user may wish to\npersonalize a model on multiple concepts but one at a time, with no access to\nthe data from previous concepts due to storage/privacy concerns. When faced\nwith this continual learning (CL) setup, most personalization methods fail to\nfind a balance between acquiring new concepts and retaining previous ones -- a\nchallenge that continual personalization (CP) aims to solve. Inspired by the\nsuccessful CL methods that rely on class-specific information for\nregularization, we resort to the inherent class-conditioned density estimates,\nalso known as diffusion classifier (DC) scores, for continual personalization\nof text-to-image diffusion models. Namely, we propose using DC scores for\nregularizing the parameter-space and function-space of text-to-image diffusion\nmodels, to achieve continual personalization. Using several diverse evaluation\nsetups, datasets, and metrics, we show that our proposed regularization-based\nCP methods outperform the state-of-the-art C-LoRA, and other baselines.\nFinally, by operating in the replay-free CL setup and on low-rank adapters, our\nmethod incurs zero storage and parameter overhead, respectively, over the\nstate-of-the-art.\n","authors":["Saurav Jha","Shiqi Yang","Masato Ishii","Mengjie Zhao","Christian Simon","Muhammad Jehanzeb Mirza","Dong Gong","Lina Yao","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.00700v2.pdf","comment":"Work under review, 26 pages of manuscript"},{"id":"http://arxiv.org/abs/2410.01251v1","updated":"2024-10-02T05:51:24Z","published":"2024-10-02T05:51:24Z","title":"Facial Action Unit Detection by Adaptively Constraining Self-Attention\n  and Causally Deconfounding Sample","summary":"  Facial action unit (AU) detection remains a challenging task, due to the\nsubtlety, dynamics, and diversity of AUs. Recently, the prevailing techniques\nof self-attention and causal inference have been introduced to AU detection.\nHowever, most existing methods directly learn self-attention guided by AU\ndetection, or employ common patterns for all AUs during causal intervention.\nThe former often captures irrelevant information in a global range, and the\nlatter ignores the specific causal characteristic of each AU. In this paper, we\npropose a novel AU detection framework called AC2D by adaptively constraining\nself-attention weight distribution and causally deconfounding the sample\nconfounder. Specifically, we explore the mechanism of self-attention weight\ndistribution, in which the self-attention weight distribution of each AU is\nregarded as spatial distribution and is adaptively learned under the constraint\nof location-predefined attention and the guidance of AU detection. Moreover, we\npropose a causal intervention module for each AU, in which the bias caused by\ntraining samples and the interference from irrelevant AUs are both suppressed.\nExtensive experiments show that our method achieves competitive performance\ncompared to state-of-the-art AU detection approaches on challenging benchmarks,\nincluding BP4D, DISFA, GFT, and BP4D+ in constrained scenarios and Aff-Wild2 in\nunconstrained scenarios. The code is available at\nhttps://github.com/ZhiwenShao/AC2D.\n","authors":["Zhiwen Shao","Hancheng Zhu","Yong Zhou","Xiang Xiang","Bing Liu","Rui Yao","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2410.01251v1.pdf","comment":"This paper is accepted by International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2310.05056v4","updated":"2024-10-02T05:32:53Z","published":"2023-10-08T07:42:41Z","title":"Open-Vocabulary Animal Keypoint Detection with Semantic-feature Matching","summary":"  Current image-based keypoint detection methods for animal (including human)\nbodies and faces are generally divided into full-supervised and few-shot\nclass-agnostic approaches. The former typically relies on laborious and\ntime-consuming manual annotations, posing considerable challenges in expanding\nkeypoint detection to a broader range of keypoint categories and animal\nspecies. The latter, though less dependent on extensive manual input, still\nrequires necessary support images with annotation for reference during testing.\nTo realize zero-shot keypoint detection without any prior annotation, we\nintroduce the Open-Vocabulary Keypoint Detection (OVKD) task, which is\ninnovatively designed to use text prompts for identifying arbitrary keypoints\nacross any species. In pursuit of this goal, we have developed a novel\nframework named Open-Vocabulary Keypoint Detection with Semantic-feature\nMatching (KDSM). This framework synergistically combines vision and language\nmodels, creating an interplay between language features and local keypoint\nvisual features. KDSM enhances its capabilities by integrating Domain\nDistribution Matrix Matching (DDMM) and other special modules, such as the\nVision-Keypoint Relational Awareness (VKRA) module, improving the framework's\ngeneralizability and overall performance.Our comprehensive experiments\ndemonstrate that KDSM significantly outperforms the baseline in terms of\nperformance and achieves remarkable success in the OVKD task.Impressively, our\nmethod, operating in a zero-shot fashion, still yields results comparable to\nstate-of-the-art few-shot species class-agnostic keypoint detection methods.We\nwill make the source code publicly accessible.\n","authors":["Hao Zhang","Lumin Xu","Shenqi Lai","Wenqi Shao","Nanning Zheng","Ping Luo","Yu Qiao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05056v4.pdf","comment":"Accepted by International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2405.16915v2","updated":"2024-10-02T05:04:10Z","published":"2024-05-27T08:08:51Z","title":"Multilingual Diversity Improves Vision-Language Representations","summary":"  Massive web-crawled image-text datasets lay the foundation for recent\nprogress in multimodal learning. These datasets are designed with the goal of\ntraining a model to do well on standard computer vision benchmarks, many of\nwhich, however, have been shown to be English-centric (e.g., ImageNet).\nConsequently, existing data curation techniques gravitate towards using\npredominantly English image-text pairs and discard many potentially useful\nnon-English samples. Our work questions this practice. Multilingual data is\ninherently enriching not only because it provides a gateway to learn about\nculturally salient concepts, but also because it depicts common concepts\ndifferently from monolingual data. We thus conduct a systematic study to\nexplore the performance benefits of using more samples of non-English origins\nwith respect to English vision tasks. By translating all multilingual\nimage-text pairs from a raw web crawl to English and re-filtering them, we\nincrease the prevalence of (translated) multilingual data in the resulting\ntraining set. Pre-training on this dataset outperforms using English-only or\nEnglish-dominated datasets on ImageNet, ImageNet distribution shifts,\nimage-English-text retrieval and on average across 38 tasks from the DataComp\nbenchmark. On a geographically diverse task like GeoDE, we also observe\nimprovements across all regions, with the biggest gain coming from Africa. In\naddition, we quantitatively show that English and non-English data are\nsignificantly different in both image and (translated) text space. We hope that\nour findings motivate future work to be more intentional about including\nmulticultural and multilingual data, not just when non-English or\ngeographically diverse tasks are involved, but to enhance model capabilities at\nlarge.\n","authors":["Thao Nguyen","Matthew Wallingford","Sebastin Santy","Wei-Chiu Ma","Sewoong Oh","Ludwig Schmidt","Pang Wei Koh","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2405.16915v2.pdf","comment":"NeurIPS 2024 Spotlight paper"},{"id":"http://arxiv.org/abs/2410.01239v1","updated":"2024-10-02T05:03:54Z","published":"2024-10-02T05:03:54Z","title":"Replacement Learning: Training Vision Tasks with Fewer Learnable\n  Parameters","summary":"  Traditional end-to-end deep learning models often enhance feature\nrepresentation and overall performance by increasing the depth and complexity\nof the network during training. However, this approach inevitably introduces\nissues of parameter redundancy and resource inefficiency, especially in deeper\nnetworks. While existing works attempt to skip certain redundant layers to\nalleviate these problems, challenges related to poor performance, computational\ncomplexity, and inefficient memory usage remain. To address these issues, we\npropose an innovative training approach called Replacement Learning, which\nmitigates these limitations by completely replacing all the parameters of the\nfrozen layers with only two learnable parameters. Specifically, Replacement\nLearning selectively freezes the parameters of certain layers, and the frozen\nlayers utilize parameters from adjacent layers, updating them through a\nparameter integration mechanism controlled by two learnable parameters. This\nmethod leverages information from surrounding structures, reduces computation,\nconserves GPU memory, and maintains a balance between historical context and\nnew inputs, ultimately enhancing overall model performance. We conducted\nexperiments across four benchmark datasets, including CIFAR-10, STL-10, SVHN,\nand ImageNet, utilizing various architectures such as CNNs and ViTs to validate\nthe effectiveness of Replacement Learning. Experimental results demonstrate\nthat our approach reduces the number of parameters, training time, and memory\nconsumption while completely surpassing the performance of end-to-end training.\n","authors":["Yuming Zhang","Peizhe Wang","Shouxin Zhang","Dongzhi Guan","Jiabin Liu","Junhao Su"],"pdf_url":"https://arxiv.org/pdf/2410.01239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01035v2","updated":"2024-10-02T04:20:31Z","published":"2024-09-02T08:10:51Z","title":"Unleashing the Power of Task-Specific Directions in Parameter Efficient\n  Fine-tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet requiring extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs)-critical for transitioning large\nmodels from pretrained states to task-specific enhancements in PEFT. We propose\na framework to clearly define these directions and explore their properties,\nand practical utilization challenges. We then introduce a novel approach,\nLoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning\nprocess, thereby enhancing model performance on targeted tasks. Extensive\nexperiments have conclusively demonstrated the effectiveness of LoRA-Dash, and\nin-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The\ncode is available at https://github.com/Chongjie-Si/Subspace-Tuning.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v2.pdf","comment":"Revisions ongoing. Codes in\n  https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2410.01226v1","updated":"2024-10-02T04:04:10Z","published":"2024-10-02T04:04:10Z","title":"Towards Native Generative Model for 3D Head Avatar","summary":"  Creating 3D head avatars is a significant yet challenging task for many\napplicated scenarios. Previous studies have set out to learn 3D human head\ngenerative models using massive 2D image data. Although these models are highly\ngeneralizable for human appearance, their result models are not\n360$^\\circ$-renderable, and the predicted 3D geometry is unreliable. Therefore,\nsuch results cannot be used in VR, game modeling, and other scenarios that\nrequire 360$^\\circ$-renderable 3D head models. An intuitive idea is that 3D\nhead models with limited amount but high 3D accuracy are more reliable training\ndata for a high-quality 3D generative model. In this vein, we delve into how to\nlearn a native generative model for 360$^\\circ$ full head from a limited 3D\nhead dataset. Specifically, three major problems are studied: 1) how to\neffectively utilize various representations for generating the\n360$^\\circ$-renderable human head; 2) how to disentangle the appearance, shape,\nand motion of human faces to generate a 3D head model that can be edited by\nappearance and driven by motion; 3) and how to extend the generalization\ncapability of the generative model to support downstream tasks. Comprehensive\nexperiments are conducted to verify the effectiveness of the proposed model. We\nhope the proposed models and artist-designed dataset can inspire future\nresearch on learning native generative 3D head models from limited 3D datasets.\n","authors":["Yiyu Zhuang","Yuxiao He","Jiawei Zhang","Yanwen Wang","Jiahe Zhu","Yao Yao","Siyu Zhu","Xun Cao","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.01226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01225v1","updated":"2024-10-02T04:03:07Z","published":"2024-10-02T04:03:07Z","title":"Perceptual Piercing: Human Visual Cue-based Object Detection in Low\n  Visibility Conditions","summary":"  This study proposes a novel deep learning framework inspired by atmospheric\nscattering and human visual cortex mechanisms to enhance object detection under\npoor visibility scenarios such as fog, smoke, and haze. These conditions pose\nsignificant challenges for object recognition, impacting various sectors,\nincluding autonomous driving, aviation management, and security systems. The\nobjective is to enhance the precision and reliability of detection systems\nunder adverse environmental conditions. The research investigates the\nintegration of human-like visual cues, particularly focusing on selective\nattention and environmental adaptability, to ascertain their impact on object\ndetection's computational efficiency and accuracy. This paper proposes a\nmulti-tiered strategy that integrates an initial quick detection process,\nfollowed by targeted region-specific dehazing, and concludes with an in-depth\ndetection phase. The approach is validated using the Foggy Cityscapes,\nRESIDE-beta (OTS and RTTS) datasets and is anticipated to set new performance\nstandards in detection accuracy while significantly optimizing computational\nefficiency. The findings offer a viable solution for enhancing object detection\nin poor visibility and contribute to the broader understanding of integrating\nhuman visual principles into deep learning algorithms for intricate visual\nrecognition challenges.\n","authors":["Ashutosh Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.01225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19811v2","updated":"2024-10-02T04:00:01Z","published":"2024-06-28T10:39:36Z","title":"EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D\n  Gaussian Splatting","summary":"  Human activities are inherently complex, often involving numerous object\ninteractions. To better understand these activities, it is crucial to model\ntheir interactions with the environment captured through dynamic changes. The\nrecent availability of affordable head-mounted cameras and egocentric data\noffers a more accessible and efficient means to understand human-object\ninteractions in 3D environments. However, most existing methods for human\nactivity modeling neglect the dynamic interactions with objects, resulting in\nonly static representations. The few existing solutions often require inputs\nfrom multiple sources, including multi-camera setups, depth-sensing cameras, or\nkinesthetic sensors. To this end, we introduce EgoGaussian, the first method\ncapable of simultaneously reconstructing 3D scenes and dynamically tracking 3D\nobject motion from RGB egocentric input alone. We leverage the uniquely\ndiscrete nature of Gaussian Splatting and segment dynamic interactions from the\nbackground, with both having explicit representations. Our approach employs a\nclip-level online learning pipeline that leverages the dynamic nature of human\nactivities, allowing us to reconstruct the temporal evolution of the scene in\nchronological order and track rigid object motion. EgoGaussian shows\nsignificant improvements in terms of both dynamic object and background\nreconstruction quality compared to the state-of-the-art. We also qualitatively\ndemonstrate the high quality of the reconstructed models.\n","authors":["Daiwei Zhang","Gengyan Li","Jiajie Li","Mickal Bressieux","Otmar Hilliges","Marc Pollefeys","Luc Van Gool","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01216v1","updated":"2024-10-02T03:57:57Z","published":"2024-10-02T03:57:57Z","title":"RS-FME-SwinT: A Novel Feature Map Enhancement Framework Integrating\n  Customized SwinT with Residual and Spatial CNN for Monkeypox Diagnosis","summary":"  Monkeypox (MPox) has emerged as a significant global concern, with cases\nsteadily increasing daily. Conventional detection methods, including polymerase\nchain reaction (PCR) and manual examination, exhibit challenges of low\nsensitivity, high cost, and substantial workload. Therefore, deep learning\noffers an automated solution; however, the datasets include data scarcity,\ntexture, contrast, inter-intra class variability, and similarities with other\nskin infectious diseases. In this regard, a novel hybrid approach is proposed\nthat integrates the learning capacity of Residual Learning and Spatial\nExploitation Convolutional Neural Network (CNN) with a customized Swin\nTransformer (RS-FME-SwinT) to capture multi-scale global and local correlated\nfeatures for MPox diagnosis. The proposed RS-FME-SwinT technique employs a\ntransfer learning-based feature map enhancement (FME) technique, integrating\nthe customized SwinT for global information capture, residual blocks for\ntexture extraction, and spatial blocks for local contrast variations. Moreover,\nincorporating new inverse residual blocks within the proposed SwinT effectively\ncaptures local patterns and mitigates vanishing gradients. The proposed\nRS-FME-SwinT has strong learning potential of diverse features that\nsystematically reduce intra-class MPox variation and enable precise\ndiscrimination from other skin diseases. Finally, the proposed RS-FME-SwinT is\na holdout cross-validated on a diverse MPox dataset and achieved outperformance\non state-of-the-art CNNs and ViTs. The proposed RS-FME-SwinT demonstrates\ncommendable results of an accuracy of 97.80%, sensitivity of 96.82%, precision\nof 98.06%, and an F-score of 97.44% in MPox detection. The RS-FME-SwinT could\nbe a valuable tool for healthcare practitioners, enabling prompt and accurate\nMPox diagnosis and contributing significantly to mitigation efforts.\n","authors":["Saddam Hussain Khan","Rashid Iqbal"],"pdf_url":"https://arxiv.org/pdf/2410.01216v1.pdf","comment":"37 Pages, 5 Tables, 10 Figures"},{"id":"http://arxiv.org/abs/2403.15004v3","updated":"2024-10-02T03:46:17Z","published":"2024-03-22T07:32:21Z","title":"ParFormer: A Vision Transformer with Parallel Mixer and Sparse Channel\n  Attention Patch Embedding","summary":"  Convolutional Neural Networks (CNNs) and Transformers have achieved\nremarkable success in computer vision tasks. However, their deep architectures\noften lead to high computational redundancy, making them less suitable for\nresource-constrained environments, such as edge devices. This paper introduces\nParFormer, a novel vision transformer that addresses this challenge by\nincorporating a Parallel Mixer and a Sparse Channel Attention Patch Embedding\n(SCAPE). By combining convolutional and attention mechanisms, ParFormer\nimproves feature extraction. This makes spatial feature extraction more\nefficient and cuts down on unnecessary computation. The SCAPE module further\nreduces computational redundancy while preserving essential feature information\nduring down-sampling. Experimental results on the ImageNet-1K dataset show that\nParFormer-T achieves 78.9\\% Top-1 accuracy with a high throughput on a GPU that\noutperforms other small models with 2.56$\\times$ higher throughput than\nMobileViT-S, 0.24\\% faster than FasterNet-T2, and 1.79$\\times$ higher than\nEdgeNeXt-S. For edge device deployment, ParFormer-T excels with a throughput of\n278.1 images/sec, which is 1.38 $\\times$ higher than EdgeNeXt-S and\n2.36$\\times$ higher than MobileViT-S, making it highly suitable for real-time\napplications in resource-constrained settings. The larger variant, ParFormer-L,\nreaches 83.5\\% Top-1 accuracy, offering a balanced trade-off between accuracy\nand efficiency, surpassing many state-of-the-art models. In COCO object\ndetection, ParFormer-M achieves 40.7 AP for object detection and 37.6 AP for\ninstance segmentation, surpassing models like ResNet-50, PVT-S and\nPoolFormer-S24 with significantly higher efficiency. These results validate\nParFormer as a highly efficient and scalable model for both high-performance\nand resource-constrained scenarios, making it an ideal solution for edge-based\nAI applications.\n","authors":["Novendra Setyawan","Ghufron Wahyu Kurniawan","Chi-Chia Sun","Jun-Wei Hsieh","Jing-Ming Guo","Wen-Kai Kuo"],"pdf_url":"https://arxiv.org/pdf/2403.15004v3.pdf","comment":"Under Review in IEEE Transactions on Cognitive and Developmental\n  System"},{"id":"http://arxiv.org/abs/2410.01210v1","updated":"2024-10-02T03:34:23Z","published":"2024-10-02T03:34:23Z","title":"Polyp-SES: Automatic Polyp Segmentation with Self-Enriched Semantic\n  Model","summary":"  Automatic polyp segmentation is crucial for effective diagnosis and treatment\nin colonoscopy images. Traditional methods encounter significant challenges in\naccurately delineating polyps due to limitations in feature representation and\nthe handling of variability in polyp appearance. Deep learning techniques,\nincluding CNN and Transformer-based methods, have been explored to improve\npolyp segmentation accuracy. However, existing approaches often neglect\nadditional semantics, restricting their ability to acquire adequate contexts of\npolyps in colonoscopy images. In this paper, we propose an innovative method\nnamed ``Automatic Polyp Segmentation with Self-Enriched Semantic Model'' to\naddress these limitations. First, we extract a sequence of features from an\ninput image and decode high-level features to generate an initial segmentation\nmask. Using the proposed self-enriched semantic module, we query potential\nsemantics and augment deep features with additional semantics, thereby aiding\nthe model in understanding context more effectively. Extensive experiments show\nsuperior segmentation performance of the proposed method against\nstate-of-the-art polyp segmentation baselines across five polyp benchmarks in\nboth superior learning and generalization capabilities.\n","authors":["Quang Vinh Nguyen","Thanh Hoang Son Vo","Sae-Ryung Kang","Soo-Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01210v1.pdf","comment":"Asian Conference on Computer Vision 2024"},{"id":"http://arxiv.org/abs/2410.01202v1","updated":"2024-10-02T03:10:38Z","published":"2024-10-02T03:10:38Z","title":"AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for\n  High-Fidelity 3D Reconstruction","summary":"  Neural radiance fields have recently revolutionized novel-view synthesis and\nachieved high-fidelity renderings. However, these methods sacrifice the\ngeometry for the rendering quality, limiting their further applications\nincluding relighting and deformation. How to synthesize photo-realistic\nrendering while reconstructing accurate geometry remains an unsolved problem.\nIn this work, we present AniSDF, a novel approach that learns fused-granularity\nneural surfaces with physics-based encoding for high-fidelity 3D\nreconstruction. Different from previous neural surfaces, our fused-granularity\ngeometry structure balances the overall structures and fine geometric details,\nproducing accurate geometry reconstruction. To disambiguate geometry from\nreflective appearance, we introduce blended radiance fields to model diffuse\nand specularity following the anisotropic spherical Gaussian encoding, a\nphysics-based rendering pipeline. With these designs, AniSDF can reconstruct\nobjects with complex structures and produce high-quality renderings.\nFurthermore, our method is a unified model that does not require complex\nhyperparameter tuning for specific objects. Extensive experiments demonstrate\nthat our method boosts the quality of SDF-based methods by a great scale in\nboth geometry reconstruction and novel-view synthesis.\n","authors":["Jingnan Gao","Zhuo Chen","Yichao Yan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.01202v1.pdf","comment":"Project Page: https://g-1nonly.github.io/AniSDF_Website/"},{"id":"http://arxiv.org/abs/2410.01189v1","updated":"2024-10-02T02:48:13Z","published":"2024-10-02T02:48:13Z","title":"[Re] Network Deconvolution","summary":"  Our work aims to reproduce the set of findings published in \"Network\nDeconvolution\" by Ye et al. (2020)[1]. That paper proposes an optimization\ntechnique for model training in convolutional neural networks. The proposed\ntechnique \"network deconvolution\" is used in convolutional neural networks to\nremove pixel-wise and channel-wise correlations before data is fed into each\nlayer. In particular, we interrogate the validity of the authors' claim that\nusing network deconvolution instead of batch normalization improves deep\nlearning model performance. Our effort confirms the validity of this claim,\nsuccessfully reproducing the results reported in Tables 1 and 2 of the original\npaper. Our study involved 367 unique experiments across multiple architectures,\ndatasets, and hyper parameter configurations. For Table 1, while there were\nsome minor deviations in accuracy when compared to the original values (within\n10%), the overall trend was consistent with the original study's findings when\ntraining the models with epochs 20 and 100. For Table 2, all 14 reproduced\nvalues were consistent with the original values. Additionally, we document the\ntraining and testing times for each architecture in Table 1 with 1, 20, and 100\nepoch settings for both CIFAR-10 and CIFAR-100 datasets. We document the total\nexecution times for Table 2 architectures with the ImageNet dataset. The data\nand software used for this reproducibility study are publicly available at\nhttps://github.com/lamps-lab/rep-network-deconvolution.\n","authors":["Rochana R. Obadage","Kumushini Thennakoon","Sarah M. Rajtmajer","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2410.01189v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.01185v1","updated":"2024-10-02T02:37:11Z","published":"2024-10-02T02:37:11Z","title":"Formula-Driven Data Augmentation and Partial Retinal Layer Copying for\n  Retinal Layer Segmentation","summary":"  Major retinal layer segmentation methods from OCT images assume that the\nretina is flattened in advance, and thus cannot always deal with retinas that\nhave changes in retinal structure due to ophthalmopathy and/or curvature due to\nmyopia. To eliminate the use of flattening in retinal layer segmentation for\npracticality of such methods, we propose novel data augmentation methods for\nOCT images. Formula-driven data augmentation (FDDA) emulates a variety of\nretinal structures by vertically shifting each column of the OCT images\naccording to a given mathematical formula. We also propose partial retinal\nlayer copying (PRLC) that copies a part of the retinal layers and pastes it\ninto a region outside the retinal layers. Through experiments using the OCT MS\nand Healthy Control dataset and the Duke Cyst DME dataset, we demonstrate that\nthe use of FDDA and PRLC makes it possible to detect the boundaries of retinal\nlayers without flattening even retinal layer segmentation methods that assume\nflattening of the retina.\n","authors":["Tsubasa Konno","Takahiro Ninomiya","Kanta Miura","Koichi Ito","Noriko Himori","Parmanand Sharma","Toru Nakazawa","Takafumi Aoki"],"pdf_url":"https://arxiv.org/pdf/2410.01185v1.pdf","comment":"The 11th OMIA Workshop on MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.01180v1","updated":"2024-10-02T02:33:09Z","published":"2024-10-02T02:33:09Z","title":"UAL-Bench: The First Comprehensive Unusual Activity Localization\n  Benchmark","summary":"  Localizing unusual activities, such as human errors or surveillance\nincidents, in videos holds practical significance. However, current video\nunderstanding models struggle with localizing these unusual events likely\nbecause of their insufficient representation in models' pretraining datasets.\nTo explore foundation models' capability in localizing unusual activity, we\nintroduce UAL-Bench, a comprehensive benchmark for unusual activity\nlocalization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA,\nand an instruction-tune dataset: OOPS-UAG-Instruct, to improve model\ncapabilities. UAL-Bench evaluates three approaches: Video-Language Models\n(Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of\nVision-Language Models and Large Language Models (VLM-LLM). Our results show\nthe VLM-LLM approach excels in localizing short-span unusual events and\npredicting their onset (start time) more accurately than Vid-LLMs. We also\npropose a new metric, R@1, TD <= p, to address limitations in existing\nevaluation methods. Our findings highlight the challenges posed by\nlong-duration videos, particularly in autism diagnosis scenarios, and the need\nfor further advancements in localization techniques. Our work not only provides\na benchmark for unusual activity localization but also outlines the key\nchallenges for existing foundation models, suggesting future research\ndirections on this important task.\n","authors":["Hasnat Md Abdullah","Tian Liu","Kangda Wei","Shu Kong","Ruihong Huang"],"pdf_url":"https://arxiv.org/pdf/2410.01180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09750v2","updated":"2024-10-02T02:10:26Z","published":"2024-06-14T06:35:33Z","title":"ControlVAR: Exploring Controllable Visual Autoregressive Modeling","summary":"  Conditional visual generation has witnessed remarkable progress with the\nadvent of diffusion models (DMs), especially in tasks like control-to-image\ngeneration. However, challenges such as expensive computational cost, high\ninference latency, and difficulties of integration with large language models\n(LLMs) have necessitated exploring alternatives to DMs. This paper introduces\nControlVAR, a novel framework that explores pixel-level controls in visual\nautoregressive (VAR) modeling for flexible and efficient conditional\ngeneration. In contrast to traditional conditional models that learn the\nconditional distribution, ControlVAR jointly models the distribution of image\nand pixel-level conditions during training and imposes conditional controls\nduring testing. To enhance the joint modeling, we adopt the next-scale AR\nprediction paradigm and unify control and image representations. A\nteacher-forcing guidance strategy is proposed to further facilitate\ncontrollable generation with joint modeling. Extensive experiments demonstrate\nthe superior efficacy and flexibility of ControlVAR across various conditional\ngeneration tasks against popular conditional DMs, \\eg, ControlNet and\nT2I-Adaptor. Code: \\url{https://github.com/lxa9867/ControlVAR}.\n","authors":["Xiang Li","Kai Qiu","Hao Chen","Jason Kuen","Zhe Lin","Rita Singh","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2406.09750v2.pdf","comment":"25 pages, 19 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.12963v2","updated":"2024-10-02T01:56:08Z","published":"2024-09-19T17:59:55Z","title":"Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free\n  Manner","summary":"  Advancements in Large Language Models (LLMs) inspire various strategies for\nintegrating video modalities. A key approach is Video-LLMs, which incorporate\nan optimizable interface linking sophisticated video encoders to LLMs. However,\ndue to computation and data limitations, these Video-LLMs are typically\npre-trained to process only short videos, limiting their broader application\nfor understanding longer video content. Additionally, fine-tuning Video-LLMs to\nhandle longer videos is cost-prohibitive. Consequently, it becomes essential to\nexplore the interpolation of Video-LLMs under a completely training-free\nsetting. In this paper, we first identify the primary challenges in\ninterpolating Video-LLMs: (1) the video encoder and modality alignment\nprojector are fixed, preventing the integration of additional frames into\nVideo-LLMs, and (2) the LLM backbone is limited in its content length\ncapabilities, which complicates the processing of an increased number of video\ntokens. To address these challenges, we propose a specific INTerPolation method\nfor Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token\nrearrangement technique that circumvents limitations imposed by the fixed video\nencoder and alignment projector. Furthermore, we introduce a training-free LLM\ncontext window extension method to enable Video-LLMs to understand a\ncorrespondingly increased number of visual tokens.\n","authors":["Yuzhang Shang","Bingxin Xu","Weitai Kang","Mu Cai","Yuheng Li","Zehao Wen","Zhen Dong","Kurt Keutzer","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2409.12963v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15056v2","updated":"2024-10-02T01:49:56Z","published":"2024-05-23T21:09:36Z","title":"ElastoGen: 4D Generative Elastodynamics","summary":"  We present ElastoGen, a knowledge-driven AI model that generates physically\naccurate 4D elastodynamics. Unlike deep models that learn from video- or\nimage-based observations, ElastoGen leverages the principles of physics and\nlearns from established mathematical and optimization procedures. The core idea\nof ElastoGen is converting the differential equation, corresponding to the\nnonlinear force equilibrium, into a series of iterative local convolution-like\noperations, which naturally fit deep architectures. We carefully build our\nnetwork module following this overarching design philosophy. ElastoGen is much\nmore lightweight in terms of both training requirements and network scale than\ndeep generative models. Because of its alignment with actual physical\nprocedures, ElastoGen efficiently generates accurate dynamics for a wide range\nof hyperelastic materials and can be easily integrated with upstream and\ndownstream deep modules to enable end-to-end 4D generation.\n","authors":["Yutao Feng","Yintong Shang","Xiang Feng","Lei Lan","Shandian Zhe","Tianjia Shao","Hongzhi Wu","Kun Zhou","Hao Su","Chenfanfu Jiang","Yin Yang"],"pdf_url":"https://arxiv.org/pdf/2405.15056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01160v1","updated":"2024-10-02T01:29:49Z","published":"2024-10-02T01:29:49Z","title":"GraphRevisedIE: Multimodal Information Extraction with Graph-Revised\n  Network","summary":"  Key information extraction (KIE) from visually rich documents (VRD) has been\na challenging task in document intelligence because of not only the complicated\nand diverse layouts of VRD that make the model hard to generalize but also the\nlack of methods to exploit the multimodal features in VRD. In this paper, we\npropose a light-weight model named GraphRevisedIE that effectively embeds\nmultimodal features such as textual, visual, and layout features from VRD and\nleverages graph revision and graph convolution to enrich the multimodal\nembedding with global context. Extensive experiments on multiple real-world\ndatasets show that GraphRevisedIE generalizes to documents of varied layouts\nand achieves comparable or better performance compared to previous KIE methods.\nWe also publish a business license dataset that contains both real-life and\nsynthesized documents to facilitate research of document KIE.\n","authors":["Panfeng Cao","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2410.01160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01877v3","updated":"2024-10-02T01:13:22Z","published":"2024-08-03T22:55:26Z","title":"Improving Zero-Shot ObjectNav with Generative Communication","summary":"  We propose a new method for improving zero-shot ObjectNav that aims to\nutilize potentially available environmental percepts for navigational\nassistance. Our approach takes into account that the ground agent may have\nlimited and sometimes obstructed view. Our formulation encourages Generative\nCommunication (GC) between an assistive overhead agent with a global view\ncontaining the target object and the ground agent with an obfuscated view; both\nequipped with Vision-Language Models (VLMs) for vision-to-language translation.\nIn this assisted setup, the embodied agents communicate environmental\ninformation before the ground agent executes actions towards a target. Despite\nthe overhead agent having a global view with the target, we note a drop in\nperformance (-13% in OSR and -13% in SPL) of a fully cooperative assistance\nscheme over an unassisted baseline. In contrast, a selective assistance scheme\nwhere the ground agent retains its independent exploratory behaviour shows a\n10% OSR and 7.65% SPL improvement. To explain navigation performance, we\nanalyze the GC for unique traits, quantifying the presence of hallucination and\ncooperation. Specifically, we identify the novel linguistic trait of preemptive\nhallucination in our embodied setting, where the overhead agent assumes that\nthe ground agent has executed an action in the dialogue when it is yet to move,\nand note its strong correlation with navigation performance. We conduct\nreal-world experiments and present some qualitative examples where we mitigate\nhallucinations via prompt finetuning to improve ObjectNav performance.\n","authors":["Vishnu Sashank Dorbala","Vishnu Dutt Sharma","Pratap Tokekar","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2408.01877v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01148v1","updated":"2024-10-02T00:53:48Z","published":"2024-10-02T00:53:48Z","title":"Automatic Image Unfolding and Stitching Framework for Esophageal Lining\n  Video Based on Density-Weighted Feature Matching","summary":"  Endoscopy is a crucial tool for diagnosing the gastrointestinal tract, but\nits effectiveness is often limited by a narrow field of view and the dynamic\nnature of the internal environment, especially in the esophagus, where complex\nand repetitive patterns make image stitching challenging. This paper introduces\na novel automatic image unfolding and stitching framework tailored for\nesophageal videos captured during endoscopy. The method combines feature\nmatching algorithms, including LoFTR, SIFT, and ORB, to create a feature\nfiltering pool and employs a Density-Weighted Homography Optimization (DWHO)\nalgorithm to enhance stitching accuracy. By merging consecutive frames, the\nframework generates a detailed panoramic view of the esophagus, enabling\nthorough and accurate visual analysis. Experimental results show the framework\nachieves low Root Mean Square Error (RMSE) and high Structural Similarity Index\n(SSIM) across extensive video sequences, demonstrating its potential for\nclinical use and improving the quality and continuity of endoscopic visual\ndata.\n","authors":["Muyang Li","Juming Xiong","Ruining Deng","Tianyuan Yao","Regina N Tyree","Girish Hiremath","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2410.01148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07001v3","updated":"2024-10-02T00:46:22Z","published":"2024-05-11T12:33:46Z","title":"ChartInsights: Evaluating Multimodal Large Language Models for Low-Level\n  Chart Question Answering","summary":"  Chart question answering (ChartQA) tasks play a critical role in interpreting\nand extracting insights from visualization charts. While recent advancements in\nmultimodal large language models (MLLMs) like GPT-4o have shown promise in\nhigh-level ChartQA tasks, such as chart captioning, their effectiveness in\nlow-level ChartQA tasks (e.g., identifying correlations) remains underexplored.\nIn this paper, we address this gap by evaluating MLLMs on low-level ChartQA\nusing a newly curated dataset, ChartInsights, which consists of 22,347 (chart,\ntask, query, answer) covering 10 data analysis tasks across 7 chart types. We\nsystematically evaluate 19 advanced MLLMs, including 12 open-source and 7\nclosed-source models. The average accuracy rate across these models is 39.8%,\nwith GPT-4o achieving the highest accuracy at 69.17%. To further explore the\nlimitations of MLLMs in low-level ChartQA, we conduct experiments that alter\nvisual elements of charts (e.g., changing color schemes, adding image noise) to\nassess their impact on the task effectiveness. Furthermore, we propose a new\ntextual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,\nwhich boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,\nincorporating a visual prompt strategy that directs attention to relevant\nvisual elements further improves accuracy to 84.32%.\n","authors":["Yifan Wu","Lutao Yan","Leixian Shen","Yunhai Wang","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2405.07001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01144v1","updated":"2024-10-02T00:46:19Z","published":"2024-10-02T00:46:19Z","title":"Uncertainty-Guided Enhancement on Driving Perception System via\n  Foundation Models","summary":"  Multimodal foundation models offer promising advancements for enhancing\ndriving perception systems, but their high computational and financial costs\npose challenges. We develop a method that leverages foundation models to refine\npredictions from existing driving perception models -- such as enhancing object\nclassification accuracy -- while minimizing the frequency of using these\nresource-intensive models. The method quantitatively characterizes\nuncertainties in the perception model's predictions and engages the foundation\nmodel only when these uncertainties exceed a pre-specified threshold.\nSpecifically, it characterizes uncertainty by calibrating the perception\nmodel's confidence scores into theoretical lower bounds on the probability of\ncorrect predictions using conformal prediction. Then, it sends images to the\nfoundation model and queries for refining the predictions only if the\ntheoretical bound of the perception model's outcome is below the threshold.\nAdditionally, we propose a temporal inference mechanism that enhances\nprediction accuracy by integrating historical predictions, leading to tighter\ntheoretical bounds. The method demonstrates a 10 to 15 percent improvement in\nprediction accuracy and reduces the number of queries to the foundation model\nby 50 percent, based on quantitative evaluations from driving datasets.\n","authors":["Yunhao Yang","Yuxin Hu","Mao Ye","Zaiwei Zhang","Zhichao Lu","Yi Xu","Ufuk Topcu","Ben Snyder"],"pdf_url":"https://arxiv.org/pdf/2410.01144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.12600v2","updated":"2024-10-02T00:36:38Z","published":"2023-04-25T06:17:44Z","title":"Application of Segment Anything Model for Civil Infrastructure Defect\n  Assessment","summary":"  This research assesses the performance of two deep learning models, SAM and\nU-Net, for detecting cracks in concrete structures. The results indicate that\neach model has its own strengths and limitations for detecting different types\nof cracks. Using the SAM's unique crack detection approach, the image is\ndivided into various parts that identify the location of the crack, making it\nmore effective at detecting longitudinal cracks. On the other hand, the U-Net\nmodel can identify positive label pixels to accurately detect the size and\nlocation of spalling cracks. By combining both models, more accurate and\ncomprehensive crack detection results can be achieved. The importance of using\nadvanced technologies for crack detection in ensuring the safety and longevity\nof concrete structures cannot be overstated. This research can have significant\nimplications for civil engineering, as the SAM and U-Net model can be used for\na variety of concrete structures, including bridges, buildings, and roads,\nimproving the accuracy and efficiency of crack detection and saving time and\nresources in maintenance and repair. In conclusion, the SAM and U-Net model\npresented in this study offer promising solutions for detecting cracks in\nconcrete structures and leveraging the strengths of both models that can lead\nto more accurate and comprehensive results.\n","authors":["Mohsen Ahmadi","Ahmad Gholizadeh Lonbar","Hajar Kazemi Naeini","Ali Tarlani Beris","Mohammadsadegh Nouri","Amir Sharifzadeh Javidi","Abbas Sharifi"],"pdf_url":"https://arxiv.org/pdf/2304.12600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14419v3","updated":"2024-10-02T00:07:34Z","published":"2024-05-23T10:39:33Z","title":"A motion-based compression algorithm for resource-constrained video\n  camera traps","summary":"  Field-captured video facilitates detailed studies of spatio-temporal aspects\nof animal locomotion, decision-making and environmental interactions including\npredator-prey relationships and habitat utilisation. But even though data\ncapture is cheap with mass-produced hardware, storage, processing and\ntransmission overheads provide a hurdle to acquisition of high resolution video\nfrom field-situated edge computing devices. Efficient compression algorithms\nare therefore essential if monitoring is to be conducted on single-board\ncomputers in situations where such hurdles must be overcome. Animal motion\ntracking in the field has unique characteristics that necessitate the use of\nnovel video compression techniques, which may be underexplored or unsuitable in\nother contexts. In this article, we therefore introduce a new motion\nanalysis-based video compression algorithm specifically designed for camera\ntraps. We implemented and tested this algorithm using a case study of\ninsect-pollinator motion tracking on three popular edge computing platforms.\nThe algorithm identifies and stores only image regions depicting motion\nrelevant to pollination monitoring, reducing overall data size by an average of\n87% across diverse test datasets. Our experiments demonstrate the algorithm's\ncapability to preserve critical information for insect behaviour analysis\nthrough both manual observation and automatic analysis of the compressed\nfootage. The method presented in this paper enhances the applicability of\nlow-powered computer vision edge devices to remote, in situ animal motion\nmonitoring, and improves the efficiency of playback during behavioural\nanalyses. Our new software, EcoMotionZip, is available Open Access.\n","authors":["Malika Nisal Ratnayake","Lex Gallon","Adel N. Toosi","Alan Dorin"],"pdf_url":"https://arxiv.org/pdf/2405.14419v3.pdf","comment":"17 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.02103v1","updated":"2024-10-02T23:48:31Z","published":"2024-10-02T23:48:31Z","title":"MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis","summary":"  Recent works in volume rendering, \\textit{e.g.} NeRF and 3D Gaussian\nSplatting (3DGS), significantly advance the rendering quality and efficiency\nwith the help of the learned implicit neural radiance field or 3D Gaussians.\nRendering on top of an explicit representation, the vanilla 3DGS and its\nvariants deliver real-time efficiency by optimizing the parametric model with\nsingle-view supervision per iteration during training which is adopted from\nNeRF. Consequently, certain views are overfitted, leading to unsatisfying\nappearance in novel-view synthesis and imprecise 3D geometries. To solve\naforementioned problems, we propose a new 3DGS optimization method embodying\nfour key novel contributions: 1) We transform the conventional single-view\ntraining paradigm into a multi-view training strategy. With our proposed\nmulti-view regulation, 3D Gaussian attributes are further optimized without\noverfitting certain training views. As a general solution, we improve the\noverall accuracy in a variety of scenarios and different Gaussian variants. 2)\nInspired by the benefit introduced by additional views, we further propose a\ncross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure\nconcerning different resolutions. 3) Built on top of our multi-view regulated\ntraining, we further propose a cross-ray densification strategy, densifying\nmore Gaussian kernels in the ray-intersect regions from a selection of views.\n4) By further investigating the densification strategy, we found that the\neffect of densification should be enhanced when certain views are distinct\ndramatically. As a solution, we propose a novel multi-view augmented\ndensification strategy, where 3D Gaussians are encouraged to get densified to a\nsufficient number accordingly, resulting in improved reconstruction accuracy.\n","authors":["Xiaobiao Du","Yida Wang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02103v1.pdf","comment":"Project Page:https://xiaobiaodu.github.io/mvgs-project/"},{"id":"http://arxiv.org/abs/2410.02101v1","updated":"2024-10-02T23:46:45Z","published":"2024-10-02T23:46:45Z","title":"Orient Anything","summary":"  Orientation estimation is a fundamental task in 3D shape analysis which\nconsists of estimating a shape's orientation axes: its side-, up-, and\nfront-axes. Using this data, one can rotate a shape into canonical orientation,\nwhere its orientation axes are aligned with the coordinate axes. Developing an\norientation algorithm that reliably estimates complete orientations of general\nshapes remains an open problem. We introduce a two-stage orientation pipeline\nthat achieves state of the art performance on up-axis estimation and further\ndemonstrate its efficacy on full-orientation estimation, where one seeks all\nthree orientation axes. Unlike previous work, we train and evaluate our method\non all of Shapenet rather than a subset of classes. We motivate our engineering\ncontributions by theory describing fundamental obstacles to orientation\nestimation for rotationally-symmetric shapes, and show how our method avoids\nthese obstacles.\n","authors":["Christopher Scarvelis","David Benhaim","Paul Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02098v1","updated":"2024-10-02T23:39:10Z","published":"2024-10-02T23:39:10Z","title":"EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing","summary":"  Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability.\n","authors":["Haotian Sun","Bowen Zhang","Yanghao Li","Haoshuo Huang","Tao Lei","Ruoming Pang","Bo Dai","Nan Du"],"pdf_url":"https://arxiv.org/pdf/2410.02098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02094v1","updated":"2024-10-02T23:30:05Z","published":"2024-10-02T23:30:05Z","title":"Tracking objects that change in appearance with phase synchrony","summary":"  Objects we encounter often change appearance as we interact with them.\nChanges in illumination (shadows), object pose, or movement of nonrigid objects\ncan drastically alter available image features. How do biological visual\nsystems track objects as they change? It may involve specific attentional\nmechanisms for reasoning about the locations of objects independently of their\nappearances -- a capability that prominent neuroscientific theories have\nassociated with computing through neural synchrony. We computationally test the\nhypothesis that the implementation of visual attention through neural synchrony\nunderlies the ability of biological visual systems to track objects that change\nin appearance over time. We first introduce a novel deep learning circuit that\ncan learn to precisely control attention to features separately from their\nlocation in the world through neural synchrony: the complex-valued recurrent\nneural network (CV-RNN). Next, we compare object tracking in humans, the\nCV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a\nlarge-scale challenge that asks observers to track objects as their locations\nand appearances change in precisely controlled ways. While humans effortlessly\nsolved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN\nbehaved similarly to humans on the challenge, providing a computational\nproof-of-concept for the role of phase synchronization as a neural substrate\nfor tracking appearance-morphing objects as they move about.\n","authors":["Sabine Muzellec","Drew Linsley","Alekh K. Ashok","Ennio Mingolla","Girik Malik","Rufin VanRullen","Thomas Serre"],"pdf_url":"https://arxiv.org/pdf/2410.02094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02086v1","updated":"2024-10-02T23:19:23Z","published":"2024-10-02T23:19:23Z","title":"Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations","summary":"  Multimodal learning plays a crucial role in enabling machine learning models\nto fuse and utilize diverse data sources, such as text, images, and audio, to\nsupport a variety of downstream tasks. A unified representation across various\nmodalities is particularly important for improving efficiency and performance.\nRecent binding methods, such as ImageBind (Girdhar et al., 2023), typically use\na fixed anchor modality to align multimodal data in the anchor modal embedding\nspace. In this paper, we mathematically analyze the fixed anchor binding\nmethods and uncover notable limitations: (1) over-reliance on the choice of the\nanchor modality, (2) failure to capture intra-modal information, and (3)\nfailure to account for inter-modal correlation among non-anchored modalities.\nTo address these limitations, we propose CentroBind, a simple yet powerful\napproach that eliminates the need for a fixed anchor; instead, it employs\ndynamically adjustable centroid-based anchors generated from all available\nmodalities, resulting in a balanced and rich representation space. We\ntheoretically demonstrate that our method captures three crucial properties of\nmultimodal learning: intra-modal learning, inter-modal learning, and multimodal\nalignment, while also constructing a robust unified representation across all\nmodalities. Our experiments on both synthetic and real-world datasets\ndemonstrate the superiority of the proposed method, showing that dynamic anchor\nmethods outperform all fixed anchor binding methods as the former captures more\nnuanced multimodal interactions.\n","authors":["Minoh Jeong","Min Namgung","Zae Myung Kim","Dongyeop Kang","Yao-Yi Chiang","Alfred Hero"],"pdf_url":"https://arxiv.org/pdf/2410.02086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02080v1","updated":"2024-10-02T23:00:31Z","published":"2024-10-02T23:00:31Z","title":"EMMA: Efficient Visual Alignment in Multi-Modal LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have recently exhibited impressive\ngeneral-purpose capabilities by leveraging vision foundation models to encode\nthe core concepts of images into representations. These are then combined with\ninstructions and processed by the language model to generate high-quality\nresponses. Despite significant progress in enhancing the language component,\nchallenges persist in optimally fusing visual encodings within the language\nmodel for task-specific adaptability. Recent research has focused on improving\nthis fusion through modality adaptation modules but at the cost of\nsignificantly increased model complexity and training data needs. In this\npaper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight\ncross-modality module designed to efficiently fuse visual and textual\nencodings, generating instruction-aware visual representations for the language\nmodel. Our key contributions include: (1) an efficient early fusion mechanism\nthat integrates vision and language representations with minimal added\nparameters (less than 0.2% increase in model size), (2) an in-depth\ninterpretability analysis that sheds light on the internal mechanisms of the\nproposed method; (3) comprehensive experiments that demonstrate notable\nimprovements on both specialized and general benchmarks for MLLMs. Empirical\nresults show that EMMA boosts performance across multiple tasks by up to 9.3%\nwhile significantly improving robustness against hallucinations. Our code is\navailable at https://github.com/SaraGhazanfari/EMMA\n","authors":["Sara Ghazanfari","Alexandre Araujo","Prashanth Krishnamurthy","Siddharth Garg","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2410.02080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02078v1","updated":"2024-10-02T22:57:47Z","published":"2024-10-02T22:57:47Z","title":"Posterior sampling via Langevin dynamics based on generative priors","summary":"  Posterior sampling in high-dimensional spaces using generative models holds\nsignificant promise for various applications, including but not limited to\ninverse problems and guided generation tasks. Despite many recent developments,\ngenerating diverse posterior samples remains a challenge, as existing methods\nrequire restarting the entire generative process for each new sample, making\nthe procedure computationally expensive. In this work, we propose efficient\nposterior sampling by simulating Langevin dynamics in the noise space of a\npre-trained generative model. By exploiting the mapping between the noise and\ndata spaces which can be provided by distilled flows or consistency models, our\nmethod enables seamless exploration of the posterior without the need to re-run\nthe full sampling chain, drastically reducing computational overhead.\nTheoretically, we prove a guarantee for the proposed noise-space Langevin\ndynamics to approximate the posterior, assuming that the generative model\nsufficiently approximates the prior distribution. Our framework is\nexperimentally validated on image restoration tasks involving noisy linear and\nnonlinear forward operators applied to LSUN-Bedroom (256 x 256) and ImageNet\n(64 x 64) datasets. The results demonstrate that our approach generates\nhigh-fidelity samples with enhanced semantic diversity even under a limited\nnumber of function evaluations, offering superior efficiency and performance\ncompared to existing diffusion-based posterior sampling techniques.\n","authors":["Vishal Purohit","Matthew Repasky","Jianfeng Lu","Qiang Qiu","Yao Xie","Xiuyuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.02078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02077v1","updated":"2024-10-02T22:56:00Z","published":"2024-10-02T22:56:00Z","title":"Kolmogorov-Arnold Network Autoencoders","summary":"  Deep learning models have revolutionized various domains, with Multi-Layer\nPerceptrons (MLPs) being a cornerstone for tasks like data regression and image\nclassification. However, a recent study has introduced Kolmogorov-Arnold\nNetworks (KANs) as promising alternatives to MLPs, leveraging activation\nfunctions placed on edges rather than nodes. This structural shift aligns KANs\nclosely with the Kolmogorov-Arnold representation theorem, potentially\nenhancing both model accuracy and interpretability. In this study, we explore\nthe efficacy of KANs in the context of data representation via autoencoders,\ncomparing their performance with traditional Convolutional Neural Networks\n(CNNs) on the MNIST, SVHN, and CIFAR-10 datasets. Our results demonstrate that\nKAN-based autoencoders achieve competitive performance in terms of\nreconstruction accuracy, thereby suggesting their viability as effective tools\nin data analysis tasks.\n","authors":["Mohammadamin Moradi","Shirin Panahi","Erik Bollt","Ying-Cheng Lai"],"pdf_url":"https://arxiv.org/pdf/2410.02077v1.pdf","comment":"12 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.02073v1","updated":"2024-10-02T22:42:20Z","published":"2024-10-02T22:42:20Z","title":"Depth Pro: Sharp Monocular Metric Depth in Less Than a Second","summary":"  We present a foundation model for zero-shot metric monocular depth\nestimation. Our model, Depth Pro, synthesizes high-resolution depth maps with\nunparalleled sharpness and high-frequency details. The predictions are metric,\nwith absolute scale, without relying on the availability of metadata such as\ncamera intrinsics. And the model is fast, producing a 2.25-megapixel depth map\nin 0.3 seconds on a standard GPU. These characteristics are enabled by a number\nof technical contributions, including an efficient multi-scale vision\ntransformer for dense prediction, a training protocol that combines real and\nsynthetic datasets to achieve high metric accuracy alongside fine boundary\ntracing, dedicated evaluation metrics for boundary accuracy in estimated depth\nmaps, and state-of-the-art focal length estimation from a single image.\nExtensive experiments analyze specific design choices and demonstrate that\nDepth Pro outperforms prior work along multiple dimensions. We release code and\nweights at https://github.com/apple/ml-depth-pro\n","authors":["Aleksei Bochkovskii","Amal Delaunoy","Hugo Germain","Marcel Santos","Yichao Zhou","Stephan R. Richter","Vladlen Koltun"],"pdf_url":"https://arxiv.org/pdf/2410.02073v1.pdf","comment":"Code and weights available at https://github.com/apple/ml-depth-pro"},{"id":"http://arxiv.org/abs/2410.02072v1","updated":"2024-10-02T22:41:12Z","published":"2024-10-02T22:41:12Z","title":"Learning from the Giants: A Practical Approach to Underwater Depth and\n  Surface Normals Estimation","summary":"  Monocular Depth and Surface Normals Estimation (MDSNE) is crucial for tasks\nsuch as 3D reconstruction, autonomous navigation, and underwater exploration.\nCurrent methods rely either on discriminative models, which struggle with\ntransparent or reflective surfaces, or generative models, which, while\naccurate, are computationally expensive. This paper presents a novel deep\nlearning model for MDSNE, specifically tailored for underwater environments,\nusing a hybrid architecture that integrates Convolutional Neural Networks\n(CNNs) with Transformers, leveraging the strengths of both approaches. Training\neffective MDSNE models is often hampered by noisy real-world datasets and the\nlimited generalization of synthetic datasets. To address this, we generate\npseudo-labeled real data using multiple pre-trained MDSNE models. To ensure the\nquality of this data, we propose the Depth Normal Evaluation and Selection\nAlgorithm (DNESA), which evaluates and selects the most reliable pseudo-labeled\nsamples using domain-specific metrics. A lightweight student model is then\ntrained on this curated dataset. Our model reduces parameters by 90% and\ntraining costs by 80%, allowing real-time 3D perception on resource-constrained\ndevices. Key contributions include: a novel and efficient MDSNE model, the\nDNESA algorithm, a domain-specific data pipeline, and a focus on real-time\nperformance and scalability. Designed for real-world underwater applications,\nour model facilitates low-cost deployments in underwater robots and autonomous\nvehicles, bridging the gap between research and practical implementation.\n","authors":["Alzayat Saleh","Melanie Olsen","Bouchra Senadji","Mostafa Rahimi Azghadi"],"pdf_url":"https://arxiv.org/pdf/2410.02072v1.pdf","comment":"18 pages, 6 figures, 8 tables. Submitted to Elsevier"},{"id":"http://arxiv.org/abs/2410.02069v1","updated":"2024-10-02T22:36:12Z","published":"2024-10-02T22:36:12Z","title":"Semi-Supervised Fine-Tuning of Vision Foundation Models with\n  Content-Style Decomposition","summary":"  In this paper, we present a semi-supervised fine-tuning approach designed to\nimprove the performance of foundation models on downstream tasks with limited\nlabeled data. By leveraging content-style decomposition within an\ninformation-theoretic framework, our method enhances the latent representations\nof pre-trained vision foundation models, aligning them more effectively with\nspecific task objectives and addressing the problem of distribution shift. We\nevaluate our approach on multiple datasets, including MNIST, its augmented\nvariations (with yellow and white stripes), CIFAR-10, SVHN, and GalaxyMNIST.\nThe experiments show improvements over purely supervised baselines,\nparticularly in low-labeled data regimes, across both frozen and trainable\nbackbones for the majority of the tested datasets.\n","authors":["Mariia Drozdova","Vitaliy Kinakh","Yury Belousov","Erica Lastufka","Slava Voloshynovskiy"],"pdf_url":"https://arxiv.org/pdf/2410.02069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11262v2","updated":"2024-10-02T22:33:08Z","published":"2024-06-17T07:06:58Z","title":"Generative Visual Instruction Tuning","summary":"  We propose to use automatically generated instruction-following data to\nimprove the zero-shot capabilities of a large multimodal model with additional\nsupport for generative and image editing tasks. We achieve this by curating a\nnew multimodal instruction-following set using GPT-4V and existing datasets for\nimage generation and editing. Using this instruction set and the existing\nLLaVA-Finetune instruction set for visual understanding tasks, we produce\nGenLLaVA, a Generative Large Language and Visual Assistant. GenLLaVA is built\nthrough a strategy that combines three types of large pretrained models through\ninstruction finetuning: Mistral for language modeling, SigLIP for image-text\nmatching, and StableDiffusion for text-to-image generation. Our model\ndemonstrates visual understanding capabilities superior to LLaVA and\nadditionally demonstrates competitive results with native multimodal models\nsuch as Unified-IO 2, paving the way for building advanced general-purpose\nvisual assistants by effectively re-using existing multimodal models. We\nopen-source our dataset, codebase, and model checkpoints to foster further\nresearch and application in this domain.\n","authors":["Jefferson Hernandez","Ruben Villegas","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.11262v2.pdf","comment":"Add more results using task tokens, expand the introduction and\n  related work FIX: error in LLM-as-judge evaluation that was over-inflating\n  the results"},{"id":"http://arxiv.org/abs/2410.02067v1","updated":"2024-10-02T22:29:14Z","published":"2024-10-02T22:29:14Z","title":"DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized\n  Image Generation","summary":"  In the realm of image generation, creating customized images from visual\nprompt with additional textual instruction emerges as a promising endeavor.\nHowever, existing methods, both tuning-based and tuning-free, struggle with\ninterpreting the subject-essential attributes from the visual prompt. This\nleads to subject-irrelevant attributes infiltrating the generation process,\nultimately compromising the personalization quality in both editability and ID\npreservation. In this paper, we present DisEnvisioner, a novel approach for\neffectively extracting and enriching the subject-essential features while\nfiltering out -irrelevant information, enabling exceptional customization\nperformance, in a tuning-free manner and using only a single image.\nSpecifically, the feature of the subject and other irrelevant components are\neffectively separated into distinctive visual tokens, enabling a much more\naccurate customization. Aiming to further improving the ID consistency, we\nenrich the disentangled features, sculpting them into more granular\nrepresentations. Experiments demonstrate the superiority of our approach over\nexisting methods in instruction response (editability), ID consistency,\ninference speed, and the overall image quality, highlighting the effectiveness\nand efficiency of DisEnvisioner. Project page:\nhttps://disenvisioner.github.io/.\n","authors":["Jing He","Haodong Li","Yongzhe Hu","Guibao Shen","Yingjie Cai","Weichao Qiu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02067v1.pdf","comment":"The first two authors contributed equally. Project page:\n  https://disenvisioner.github.io/"},{"id":"http://arxiv.org/abs/2409.19951v2","updated":"2024-10-02T22:24:44Z","published":"2024-09-30T05:12:01Z","title":"Law of the Weakest Link: Cross Capabilities of Large Language Models","summary":"  The development and evaluation of Large Language Models (LLMs) have largely\nfocused on individual capabilities. However, this overlooks the intersection of\nmultiple abilities across different types of expertise that are often required\nfor real-world tasks, which we term cross capabilities. To systematically\nexplore this concept, we first define seven core individual capabilities and\nthen pair them to form seven common cross capabilities, each supported by a\nmanually constructed taxonomy. Building on these definitions, we introduce\nCrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100\nprompts for each individual and cross capability. To ensure reliable\nevaluation, we involve expert annotators to assess 4,200 model responses,\ngathering 8,400 human ratings with detailed explanations to serve as reference\nexamples. Our findings reveal that, in both static evaluations and attempts to\nenhance specific abilities, current LLMs consistently exhibit the \"Law of the\nWeakest Link,\" where cross-capability performance is significantly constrained\nby the weakest component. Specifically, across 58 cross-capability scores from\n17 models, 38 scores are lower than all individual capabilities, while 20 fall\nbetween strong and weak, but closer to the weaker ability. These results\nhighlight the under-performance of LLMs in cross-capability tasks, making the\nidentification and improvement of the weakest capabilities a critical priority\nfor future research to optimize performance in complex, multi-dimensional\nscenarios.\n","authors":["Ming Zhong","Aston Zhang","Xuewei Wang","Rui Hou","Wenhan Xiong","Chenguang Zhu","Zhengxing Chen","Liang Tan","Chloe Bi","Mike Lewis","Sravya Popuri","Sharan Narang","Melanie Kambadur","Dhruv Mahajan","Sergey Edunov","Jiawei Han","Laurens van der Maaten"],"pdf_url":"https://arxiv.org/pdf/2409.19951v2.pdf","comment":"Data, Code, & Benchmark: www.llm-cross-capabilities.org"},{"id":"http://arxiv.org/abs/2409.18124v2","updated":"2024-10-02T22:17:56Z","published":"2024-09-26T17:58:55Z","title":"Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction","summary":"  Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also enhances efficiency, being\nsignificantly faster than most existing diffusion-based methods. Lotus'\nsuperior quality and efficiency also enable a wide range of practical\napplications, such as joint estimation, single/multi-view 3D reconstruction,\netc. Project page: https://lotus3d.github.io/.\n","authors":["Jing He","Haodong Li","Wei Yin","Yixun Liang","Leheng Li","Kaiqiang Zhou","Hongbo Zhang","Bingbing Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.18124v2.pdf","comment":"The first two authors contributed equally. Project page:\n  https://lotus3d.github.io/"},{"id":"http://arxiv.org/abs/2410.02055v1","updated":"2024-10-02T22:05:30Z","published":"2024-10-02T22:05:30Z","title":"Using Style Ambiguity Loss to Improve Aesthetics of Diffusion Models","summary":"  Teaching text-to-image models to be creative involves using style ambiguity\nloss. In this work, we explore using the style ambiguity training objective,\nused to approximate creativity, on a diffusion model. We then experiment with\nforms of style ambiguity loss that do not require training a classifier or a\nlabeled dataset, and find that the models trained with style ambiguity loss can\ngenerate better images than the baseline diffusion models and GANs. Code is\navailable at https://github.com/jamesBaker361/clipcreate.\n","authors":["James Baker"],"pdf_url":"https://arxiv.org/pdf/2410.02055v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.12009"},{"id":"http://arxiv.org/abs/2303.12001v3","updated":"2024-10-02T21:58:04Z","published":"2023-03-21T16:33:40Z","title":"ViC-MAE: Self-Supervised Representation Learning from Images and Video\n  with Contrastive Masked Autoencoders","summary":"  We propose ViC-MAE, a model that combines both Masked AutoEncoders (MAE) and\ncontrastive learning. ViC-MAE is trained using a global featured obtained by\npooling the local representations learned under an MAE reconstruction loss and\nleveraging this representation under a contrastive objective across images and\nvideo frames. We show that visual representations learned under ViC-MAE\ngeneralize well to both video and image classification tasks. Particularly,\nViC-MAE obtains state-of-the-art transfer learning performance from video to\nimages on Imagenet-1k compared to the recently proposed OmniMAE by achieving a\ntop-1 accuracy of 86% (+1.3% absolute improvement) when trained on the same\ndata and 87.1% (+2.4% absolute improvement) when training on extra data. At the\nsame time ViC-MAE outperforms most other methods on video benchmarks by\nobtaining 75.9% top-1 accuracy on the challenging Something something-v2 video\nbenchmark . When training on videos and images from a diverse combination of\ndatasets, our method maintains a balanced transfer-learning performance between\nvideo and image classification benchmarks, coming only as a close second to the\nbest supervised method.\n","authors":["Jefferson Hernandez","Ruben Villegas","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2303.12001v3.pdf","comment":"Published at ECCV 2024"},{"id":"http://arxiv.org/abs/2409.19291v2","updated":"2024-10-02T21:50:58Z","published":"2024-09-28T09:28:51Z","title":"CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified\n  Multiplet Upcycling","summary":"  In recent years, Contrastive Language-Image Pre-training (CLIP) has become a\ncornerstone in multimodal intelligence. However, recent studies have identified\nthat the information loss in the CLIP encoding process is substantial, and CLIP\ntends to capture only coarse-grained features from the input. This deficiency\nsignificantly limits the ability of a single CLIP model to handle images rich\nin visual detail. In this work, we propose a simple yet effective\nmodel-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU\nefficiently fine-tunes a series of CLIP models that capture different feature\nspaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for\nthe Feed-Forward Network (FFN). These models can then be transformed into a\nCLIP-MoE with a larger model capacity, leading to significantly enhanced\nperformance with minimal computational overhead. To the best of our knowledge,\nDiversified Multiplet Upcycling is the first approach to introduce sparsely\nactivated MoE into CLIP foundation models. Extensive experiments demonstrate\nthe significant performance of CLIP-MoE across various zero-shot retrieval,\nzero-shot image classification tasks, and downstream Multimodal Large Language\nModel (MLLM) benchmarks by serving as a vision encoder. Furthermore,\nDiversified Multiplet Upcycling enables the conversion of any dense CLIP model\ninto CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner\nwithout requiring further adaptation in downstream frameworks. Through\nDiversified Multiplet Upcycling, we aim to provide valuable insights for future\nresearch on developing more efficient and effective multimodal learning\nsystems.\n","authors":["Jihai Zhang","Xiaoye Qu","Tong Zhu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2409.19291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02052v1","updated":"2024-10-02T21:42:35Z","published":"2024-10-02T21:42:35Z","title":"Improving Autonomous AI Agents with Reflective Tree Search and\n  Self-Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon\nplanning tasks. To address these limitations, we introduce Reflective Monte\nCarlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the\nability of AI agents, e.g., powered by GPT-4o, to explore decision space on the\nfly. R-MCTS extends traditional MCTS by 1) incorporating contrastive\nreflection, allowing agents to learn from past interactions and dynamically\nimprove their search efficiency; and 2) using multi-agent debate to provide\nreliable state evaluation. Moreover, we improve the agent's performance by\nfine-tuning GPT-4o through self-learning, using R-MCTS generated tree\ntraversals without any human-provided labels. On the challenging VisualWebArena\nbenchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative\nimprovement across various tasks compared to the previous state-of-the-art.\nAdditionally, we show that the knowledge gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o\nmatches 97% of R-MCTS's performance while reducing compute usage by a factor of\nfour at test time. Furthermore, qualitative results reveal that the fine-tuned\nGPT-4o model demonstrates the ability to explore the environment, evaluate a\nstate, and backtrack to viable ones when it detects that the current state\ncannot lead to success. Moreover, our work demonstrates the compute scaling\nproperties in both training - data collection with R-MCTS - and testing time.\nThese results suggest a promising research direction to enhance VLMs' reasoning\nand planning capabilities for agentic applications via test-time search and\nself-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.16627v2","updated":"2024-10-02T15:57:50Z","published":"2024-09-25T05:12:07Z","title":"Train Once, Deploy Anywhere: Matryoshka Representation Learning for\n  Multimodal Recommendation","summary":"  Despite recent advancements in language and vision modeling, integrating rich\nmultimodal knowledge into recommender systems continues to pose significant\nchallenges. This is primarily due to the need for efficient recommendation,\nwhich requires adaptive and interactive responses. In this study, we focus on\nsequential recommendation and introduce a lightweight framework called\nfull-scale Matryoshka representation learning for multimodal recommendation\n(fMRLRec). Our fMRLRec captures item features at different granularities,\nlearning informative representations for efficient recommendation across\nmultiple dimensions. To integrate item features from diverse modalities,\nfMRLRec employs a simple mapping to project multimodal item features into an\naligned feature space. Additionally, we design an efficient linear\ntransformation that embeds smaller features into larger ones, substantially\nreducing memory requirements for large-scale training on recommendation data.\nCombined with improved state space modeling techniques, fMRLRec scales to\ndifferent dimensions and only requires one-time training to produce multiple\nmodels tailored to various granularities. We demonstrate the effectiveness and\nefficiency of fMRLRec on multiple benchmark datasets, which consistently\nachieves superior performance over state-of-the-art baseline methods. We make\nour code and data publicly available at https://github.com/yueqirex/fMRLRec.\n","authors":["Yueqi Wang","Zhenrui Yue","Huimin Zeng","Dong Wang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2409.16627v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.04701v2","updated":"2024-10-02T15:07:09Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.\n","authors":["Michael Gnther","Isabelle Mohr","Daniel James Williams","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v2.pdf","comment":"11 pages, 3rd draft"},{"id":"http://arxiv.org/abs/2410.01598v1","updated":"2024-10-02T14:36:18Z","published":"2024-10-02T14:36:18Z","title":"Elaborative Subtopic Query Reformulation for Broad and Indirect Queries\n  in Travel Destination Recommendation","summary":"  In Query-driven Travel Recommender Systems (RSs), it is crucial to understand\nthe user intent behind challenging natural language(NL) destination queries\nsuch as the broadly worded \"youth-friendly activities\" or the indirect\ndescription \"a high school graduation trip\". Such queries are challenging due\nto the wide scope and subtlety of potential user intents that confound the\nability of retrieval methods to infer relevant destinations from available\ntextual descriptions such as WikiVoyage. While query reformulation (QR) has\nproven effective in enhancing retrieval by addressing user intent, existing QR\nmethods tend to focus only on expanding the range of potentially matching query\nsubtopics (breadth) or elaborating on the potential meaning of a query (depth),\nbut not both. In this paper, we introduce Elaborative Subtopic Query\nReformulation (EQR), a large language model-based QR method that combines both\nbreadth and depth by generating potential query subtopics with information-rich\nelaborations. We also release TravelDest, a novel dataset for query-driven\ntravel destination RSs. Experiments on TravelDest show that EQR achieves\nsignificant improvements in recall and precision over existing state-of-the-art\nQR methods.\n","authors":["Qianfeng Wen","Yifan Liu","Joshua Zhang","George Saad","Anton Korikov","Yury Sambale","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2410.01598v1.pdf","comment":"9 pages, 7 figures,The 1st Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommender Systems (ROEGEN@RecSys 2024),\n  October 2024, Bari, Italy"},{"id":"http://arxiv.org/abs/2409.13385v2","updated":"2024-10-02T14:30:28Z","published":"2024-09-20T10:36:49Z","title":"Contextual Compression in Retrieval-Augmented Generation for Large\n  Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle\nwith limitations such as hallucinations, outdated knowledge, opacity, and\ninexplicable reasoning. To address these challenges, Retrieval-Augmented\nGeneration (RAG) has proven to be a viable solution, leveraging external\ndatabases to improve the consistency and coherence of generated content,\nespecially valuable for complex, knowledge-rich tasks, and facilitates\ncontinuous improvement by leveraging domain-specific insights. By combining the\nintrinsic knowledge of LLMs with the vast, dynamic repositories of external\ndatabases, RAG achieves a synergistic effect. However, RAG is not without its\nlimitations, including a limited context window, irrelevant information, and\nthe high processing overhead for extensive contextual data. In this\ncomprehensive work, we explore the evolution of Contextual Compression\nparadigms, providing an in-depth examination of the field. Finally, we outline\nthe current challenges and suggest potential research and development\ndirections, paving the way for future advancements in this area.\n","authors":["Sourav Verma"],"pdf_url":"https://arxiv.org/pdf/2409.13385v2.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01470v1","updated":"2024-10-02T12:21:31Z","published":"2024-10-02T12:21:31Z","title":"Peeling Back the Layers: An In-Depth Evaluation of Encoder Architectures\n  in Neural News Recommenders","summary":"  Encoder architectures play a pivotal role in neural news recommenders by\nembedding the semantic and contextual information of news and users. Thus,\nresearch has heavily focused on enhancing the representational capabilities of\nnews and user encoders to improve recommender performance. Despite the\nsignificant impact of encoder architectures on the quality of news and user\nrepresentations, existing analyses of encoder designs focus only on the overall\ndownstream recommendation performance. This offers a one-sided assessment of\nthe encoders' similarity, ignoring more nuanced differences in their behavior,\nand potentially resulting in sub-optimal model selection. In this work, we\nperform a comprehensive analysis of encoder architectures in neural news\nrecommender systems. We systematically evaluate the most prominent news and\nuser encoder architectures, focusing on their (i) representational similarity,\nmeasured with the Central Kernel Alignment, (ii) overlap of generated\nrecommendation lists, quantified with the Jaccard similarity, and (iii) the\noverall recommendation performance. Our analysis reveals that the complexity of\ncertain encoding techniques is often empirically unjustified, highlighting the\npotential for simpler, more efficient architectures. By isolating the effects\nof individual components, we provide valuable insights for researchers and\npractitioners to make better informed decisions about encoder selection and\navoid unnecessary complexity in the design of news recommenders.\n","authors":["Andreea Iana","Goran Glava","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2410.01470v1.pdf","comment":"Accepted at the 12th International Workshop on News Recommendation\n  and Analytics (INRA 2024) in conjunction with ACM RecSys 2024"},{"id":"http://arxiv.org/abs/2410.01448v1","updated":"2024-10-02T11:59:58Z","published":"2024-10-02T11:59:58Z","title":"Analyzing Byte-Pair Encoding on Monophonic and Polyphonic Symbolic\n  Music: A Focus on Musical Phrase Segmentation","summary":"  Byte-Pair Encoding (BPE) is an algorithm commonly used in Natural Language\nProcessing to build a vocabulary of subwords, which has been recently applied\nto symbolic music. Given that symbolic music can differ significantly from\ntext, particularly with polyphony, we investigate how BPE behaves with\ndifferent types of musical content. This study provides a qualitative analysis\nof BPE's behavior across various instrumentations and evaluates its impact on a\nmusical phrase segmentation task for both monophonic and polyphonic music. Our\nfindings show that the BPE training process is highly dependent on the\ninstrumentation and that BPE \"supertokens\" succeed in capturing abstract\nmusical content. In a musical phrase segmentation task, BPE notably improves\nperformance in a polyphonic setting, but enhances performance in monophonic\ntunes only within a specific range of BPE merges.\n","authors":["Dinh-Viet-Toan Le","Louis Bigo","Mikaela Keller"],"pdf_url":"https://arxiv.org/pdf/2410.01448v1.pdf","comment":"Accepted to 3rd Workshop on NLP for Music and Audio (NLP4MusA,\n  co-located with ISMIR 2024)"},{"id":"http://arxiv.org/abs/2410.01396v1","updated":"2024-10-02T10:16:54Z","published":"2024-10-02T10:16:54Z","title":"Can We Delegate Learning to Automation?: A Comparative Study of LLM\n  Chatbots, Search Engines, and Books","summary":"  Learning is a key motivator behind information search behavior. With the\nemergence of LLM-based chatbots, students are increasingly turning to these\ntools as their primary resource for acquiring knowledge. However, the\ntransition from traditional resources like textbooks and web searches raises\nconcerns among educators. They worry that these fully-automated LLMs might lead\nstudents to delegate critical steps of search as learning. In this paper, we\nsystematically uncover three main concerns from educators' perspectives. In\nresponse to these concerns, we conducted a mixed-methods study with 92\nuniversity students to compare three learning sources with different automation\nlevels. Our results show that LLMs support comprehensive understanding of key\nconcepts without promoting passive learning, though their effectiveness in\nknowledge retention was limited. Additionally, we found that academic\nperformance impacted both learning outcomes and search patterns. Notably,\nhigher-competence learners engaged more deeply with content through\nreading-intensive behaviors rather than relying on search activities.\n","authors":["Yeonsun Yang","Ahyeon Shin","Mincheol Kang","Jiheon Kang","Jean Young Song"],"pdf_url":"https://arxiv.org/pdf/2410.01396v1.pdf","comment":"21 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.01383v1","updated":"2024-10-02T09:51:42Z","published":"2024-10-02T09:51:42Z","title":"PairDistill: Pairwise Relevance Distillation for Dense Retrieval","summary":"  Effective information retrieval (IR) from vast datasets relies on advanced\ntechniques to extract relevant information in response to queries. Recent\nadvancements in dense retrieval have showcased remarkable efficacy compared to\ntraditional sparse retrieval methods. To further enhance retrieval performance,\nknowledge distillation techniques, often leveraging robust cross-encoder\nrerankers, have been extensively explored. However, existing approaches\nprimarily distill knowledge from pointwise rerankers, which assign absolute\nrelevance scores to documents, thus facing challenges related to inconsistent\ncomparisons. This paper introduces Pairwise Relevance Distillation\n(PairDistill) to leverage pairwise reranking, offering fine-grained\ndistinctions between similarly relevant documents to enrich the training of\ndense retrieval models. Our experiments demonstrate that PairDistill\noutperforms existing methods, achieving new state-of-the-art results across\nmultiple benchmarks. This highlights the potential of PairDistill in advancing\ndense retrieval techniques effectively. Our source code and trained models are\nreleased at https://github.com/MiuLab/PairDistill\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01383v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.16508v3","updated":"2024-10-02T07:51:47Z","published":"2024-02-26T11:42:29Z","title":"Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision","summary":"  Cross-lingual open domain question answering (CLQA) is a complex problem,\ncomprising cross-lingual retrieval from a multilingual knowledge base, followed\nby answer generation in the query language. Both steps are usually tackled by\nseparate models, requiring substantial annotated datasets, and typically\nauxiliary resources, like machine translation systems to bridge between\nlanguages. In this paper, we show that CLQA can be addressed using a single\nencoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural questions to supervise answer\ngeneration. Together, we show our approach, \\texttt{CLASS}, outperforms\ncomparable methods on both supervised and zero-shot language adaptation\nsettings, including those using machine translation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2402.16508v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2409.13621v2","updated":"2024-10-02T06:14:17Z","published":"2024-09-20T16:32:54Z","title":"Advancing Event Causality Identification via Heuristic Semantic\n  Dependency Inquiry Network","summary":"  Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.\n","authors":["Haoran Li","Qiang Gao","Hongmei Wu","Li Huang"],"pdf_url":"https://arxiv.org/pdf/2409.13621v2.pdf","comment":"EMNLP 2024 camera-ready version. Code is released at\n  https://github.com/hrlics/SemDI"},{"id":"http://arxiv.org/abs/2402.11202v2","updated":"2024-10-02T05:39:46Z","published":"2024-02-17T05:36:13Z","title":"Towards Scalability and Extensibility of Query Reformulation Modeling in\n  E-commerce Search","summary":"  Customer behavioral data significantly impacts e-commerce search systems.\nHowever, in the case of less common queries, the associated behavioral data\ntends to be sparse and noisy, offering inadequate support to the search\nmechanism. To address this challenge, the concept of query reformulation has\nbeen introduced. It suggests that less common queries could utilize the\nbehavior patterns of their popular counterparts with similar meanings. In\nAmazon product search, query reformulation has displayed its effectiveness in\nimproving search relevance and bolstering overall revenue. Nonetheless,\nadapting this method for smaller or emerging businesses operating in regions\nwith lower traffic and complex multilingual settings poses the challenge in\nterms of scalability and extensibility. This study focuses on overcoming this\nchallenge by constructing a query reformulation solution capable of functioning\neffectively, even when faced with limited training data, in terms of quality\nand scale, along with relatively complex linguistic characteristics. In this\npaper we provide an overview of the solution implemented within Amazon product\nsearch infrastructure, which encompasses a range of elements, including\nrefining the data mining process, redefining model training objectives, and\nreshaping training strategies. The effectiveness of the proposed solution is\nvalidated through online A/B testing on search ranking and Ads matching.\nNotably, employing the proposed solution in search ranking resulted in 0.14%\nand 0.29% increase in overall revenue in Japanese and Hindi cases,\nrespectively, and a 0.08% incremental gain in the English case compared to the\nlegacy implementation; while in search Ads matching led to a 0.36% increase in\nAds revenue in the Japanese case.\n","authors":["Ziqi Zhang","Yupin Huang","Quan Deng","Jinghui Xiao","Vivek Mittal","Jingyuan Deng"],"pdf_url":"https://arxiv.org/pdf/2402.11202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05152v2","updated":"2024-10-02T05:02:02Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v2.pdf","comment":"EMNLP 2024 Findings; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2401.05967v3","updated":"2024-10-02T04:17:36Z","published":"2024-01-11T15:13:00Z","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding","summary":"  The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n","authors":["Yihua Zhu","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.05967v3.pdf","comment":"EMNLP2024 findings (Long)"},{"id":"http://arxiv.org/abs/2410.01190v1","updated":"2024-10-02T02:51:02Z","published":"2024-10-02T02:51:02Z","title":"Integrating Visual and Textual Inputs for Searching Large-Scale Map\n  Collections with CLIP","summary":"  Despite the prevalence and historical importance of maps in digital\ncollections, current methods of navigating and exploring map collections are\nlargely restricted to catalog records and structured metadata. In this paper,\nwe explore the potential for interactively searching large-scale map\ncollections using natural language inputs (\"maps with sea monsters\"), visual\ninputs (i.e., reverse image search), and multimodal inputs (an example map +\n\"more grayscale\"). As a case study, we adopt 562,842 images of maps publicly\naccessible via the Library of Congress's API. To accomplish this, we use the\nmulitmodal Contrastive Language-Image Pre-training (CLIP) machine learning\nmodel to generate embeddings for these maps, and we develop code to implement\nexploratory search capabilities with these input strategies. We present results\nfor example searches created in consultation with staff in the Library of\nCongress's Geography and Map Division and describe the strengths, weaknesses,\nand possibilities for these search queries. Moreover, we introduce a\nfine-tuning dataset of 10,504 map-caption pairs, along with an architecture for\nfine-tuning a CLIP model on this dataset. To facilitate re-use, we provide all\nof our code in documented, interactive Jupyter notebooks and place all code\ninto the public domain. Lastly, we discuss the opportunities and challenges for\napplying these approaches across both digitized and born-digital collections\nheld by galleries, libraries, archives, and museums.\n","authors":["Jamie Mahowald","Benjamin Charles Germain Lee"],"pdf_url":"https://arxiv.org/pdf/2410.01190v1.pdf","comment":"18 pages, 7 figures, accepted at the Computational Humanities\n  Research Conference (CHR 2024)"},{"id":"http://arxiv.org/abs/2410.01160v1","updated":"2024-10-02T01:29:49Z","published":"2024-10-02T01:29:49Z","title":"GraphRevisedIE: Multimodal Information Extraction with Graph-Revised\n  Network","summary":"  Key information extraction (KIE) from visually rich documents (VRD) has been\na challenging task in document intelligence because of not only the complicated\nand diverse layouts of VRD that make the model hard to generalize but also the\nlack of methods to exploit the multimodal features in VRD. In this paper, we\npropose a light-weight model named GraphRevisedIE that effectively embeds\nmultimodal features such as textual, visual, and layout features from VRD and\nleverages graph revision and graph convolution to enrich the multimodal\nembedding with global context. Extensive experiments on multiple real-world\ndatasets show that GraphRevisedIE generalizes to documents of varied layouts\nand achieves comparable or better performance compared to previous KIE methods.\nWe also publish a business license dataset that contains both real-life and\nsynthesized documents to facilitate research of document KIE.\n","authors":["Panfeng Cao","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2410.01160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01154v1","updated":"2024-10-02T01:12:54Z","published":"2024-10-02T01:12:54Z","title":"Unleashing the Power of Large Language Models in Zero-shot Relation\n  Extraction via Self-Prompting","summary":"  Recent research in zero-shot Relation Extraction (RE) has focused on using\nLarge Language Models (LLMs) due to their impressive zero-shot capabilities.\nHowever, current methods often perform suboptimally, mainly due to a lack of\ndetailed, context-specific prompts needed for understanding various sentences\nand relations. To address this, we introduce the Self-Prompting framework, a\nnovel method designed to fully harness the embedded RE knowledge within LLMs.\nSpecifically, our framework employs a three-stage diversity approach to prompt\nLLMs, generating multiple synthetic samples that encapsulate specific relations\nfrom scratch. These generated samples act as in-context learning samples,\noffering explicit and context-specific guidance to efficiently prompt LLMs for\nRE. Experimental evaluations on benchmark datasets show our approach\noutperforms existing LLM-based zero-shot RE methods. Additionally, our\nexperiments confirm the effectiveness of our generation pipeline in producing\nhigh-quality synthetic data that enhances performance.\n","authors":["Siyi Liu","Yang Li","Jiang Li","Shan Yang","Yunshi Lan"],"pdf_url":"https://arxiv.org/pdf/2410.01154v1.pdf","comment":"EMNLP 2024 Short"},{"id":"http://arxiv.org/abs/2410.02074v1","updated":"2024-10-02T22:46:51Z","published":"2024-10-02T22:46:51Z","title":"Price-guided user attention in large-scale E-commerce group\n  recommendation","summary":"  Existing group recommender systems utilize attention mechanisms to identify\ncritical users who influence group decisions the most. We analyzed user\nattention scores from a widely-used group recommendation model on a real-world\nE-commerce dataset and found that item price and user interaction history\nsignificantly influence the selection of critical users. When item prices are\nlow, users with extensive interaction histories are more influential in group\ndecision-making. Conversely, their influence diminishes with higher item\nprices. Based on these observations, we propose a novel group recommendation\napproach that incorporates item price as a guiding factor for user aggregation.\nOur model employs an adaptive sigmoid function to adjust output logits based on\nitem prices, enhancing the accuracy of user aggregation. Our model can be\nplugged into any attention-based group recommender system if the price\ninformation is available. We evaluate our model's performance on a public\nbenchmark and a real-world dataset. We compare it with other state-of-the-art\ngroup recommendation methods. Our results demonstrate that our price-guided\nuser attention approach outperforms the state-of-the-art methods in terms of\nhit ratio and mean square error.\n","authors":["Yang Shi","Young-joo Chung"],"pdf_url":"https://arxiv.org/pdf/2410.02074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00026v5","updated":"2024-10-02T20:45:53Z","published":"2024-03-20T21:02:16Z","title":"Ink and Individuality: Crafting a Personalised Narrative in the Age of\n  LLMs","summary":"  Individuality and personalization comprise the distinctive characteristics\nthat make each writer unique and influence their words in order to effectively\nengage readers while conveying authenticity. However, our growing reliance on\nLLM-based writing assistants risks compromising our creativity and\nindividuality over time. We often overlook the negative impacts of this trend\non our creativity and uniqueness, despite the possible consequences. This study\ninvestigates these concerns by performing a brief survey to explore different\nperspectives and concepts, as well as trying to understand people's viewpoints,\nin conjunction with past studies in the area. Addressing these issues is\nessential for improving human-computer interaction systems and enhancing\nwriting assistants for personalization and individuality.\n","authors":["Azmine Toushik Wasi","Raima Islam","Mst Rafia Islam"],"pdf_url":"https://arxiv.org/pdf/2404.00026v5.pdf","comment":"5 Pages, 4 Figures. Accepted in The Third Workshop on Intelligent and\n  Interactive Writing Assistants at CHI 2024"},{"id":"http://arxiv.org/abs/2410.01987v1","updated":"2024-10-02T19:48:17Z","published":"2024-10-02T19:48:17Z","title":"Financial Sentiment Analysis on News and Reports Using Large Language\n  Models and FinBERT","summary":"  Financial sentiment analysis (FSA) is crucial for evaluating market sentiment\nand making well-informed financial decisions. The advent of large language\nmodels (LLMs) such as BERT and its financial variant, FinBERT, has notably\nenhanced sentiment analysis capabilities. This paper investigates the\napplication of LLMs and FinBERT for FSA, comparing their performance on news\narticles, financial reports and company announcements. The study emphasizes the\nadvantages of prompt engineering with zero-shot and few-shot strategy to\nimprove sentiment classification accuracy. Experimental results indicate that\nGPT-4o, with few-shot examples of financial texts, can be as competent as a\nwell fine-tuned FinBERT in this specialized field.\n","authors":["Yanxin Shen","Pulin Kirin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03754v1","updated":"2024-10-02T05:24:49Z","published":"2024-10-02T05:24:49Z","title":"Enhancing Retrieval in QA Systems with Derived Feature Association","summary":"  Retrieval augmented generation (RAG) has become the standard in long context\nquestion answering (QA) systems. However, typical implementations of RAG rely\non a rather naive retrieval mechanism, in which texts whose embeddings are most\nsimilar to that of the query are deemed most relevant. This has consequences in\nsubjective QA tasks, where the most relevant text may not directly contain the\nanswer. In this work, we propose a novel extension to RAG systems, which we\ncall Retrieval from AI Derived Documents (RAIDD). RAIDD leverages the full\npower of the LLM in the retrieval process by deriving inferred features, such\nas summaries and example questions, from the documents at ingest. We\ndemonstrate that this approach significantly improves the performance of RAG\nsystems on long-context QA tasks.\n","authors":["Keyush Shah","Abhishek Goyal","Isaac Wasserman"],"pdf_url":"https://arxiv.org/pdf/2410.03754v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.01802v1","updated":"2024-10-02T17:57:38Z","published":"2024-10-02T17:57:38Z","title":"PROXI: Challenging the GNNs for Link Prediction","summary":"  Over the past decade, Graph Neural Networks (GNNs) have transformed graph\nrepresentation learning. In the widely adopted message-passing GNN framework,\nnodes refine their representations by aggregating information from neighboring\nnodes iteratively. While GNNs excel in various domains, recent theoretical\nstudies have raised concerns about their capabilities. GNNs aim to address\nvarious graph-related tasks by utilizing such node representations, however,\nthis one-size-fits-all approach proves suboptimal for diverse tasks.\n  Motivated by these observations, we conduct empirical tests to compare the\nperformance of current GNN models with more conventional and direct methods in\nlink prediction tasks. Introducing our model, PROXI, which leverages proximity\ninformation of node pairs in both graph and attribute spaces, we find that\nstandard machine learning (ML) models perform competitively, even outperforming\ncutting-edge GNN models when applied to these proximity metrics derived from\nnode neighborhoods and attributes. This holds true across both homophilic and\nheterophilic networks, as well as small and large benchmark datasets, including\nthose from the Open Graph Benchmark (OGB). Moreover, we show that augmenting\ntraditional GNNs with PROXI significantly boosts their link prediction\nperformance. Our empirical findings corroborate the previously mentioned\ntheoretical observations and imply that there exists ample room for enhancement\nin current GNN models to reach their potential.\n","authors":["Astrit Tola","Jack Myrick","Baris Coskunuzer"],"pdf_url":"https://arxiv.org/pdf/2410.01802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01803v1","updated":"2024-10-02T17:57:38Z","published":"2024-10-02T17:57:38Z","title":"On the expressiveness and spectral bias of KANs","summary":"  Kolmogorov-Arnold Networks (KAN) \\cite{liu2024kan} were very recently\nproposed as a potential alternative to the prevalent architectural backbone of\nmany deep learning models, the multi-layer perceptron (MLP). KANs have seen\nsuccess in various tasks of AI for science, with their empirical efficiency and\naccuracy demostrated in function regression, PDE solving, and many more\nscientific problems.\n  In this article, we revisit the comparison of KANs and MLPs, with emphasis on\na theoretical perspective. On the one hand, we compare the representation and\napproximation capabilities of KANs and MLPs. We establish that MLPs can be\nrepresented using KANs of a comparable size. This shows that the approximation\nand representation capabilities of KANs are at least as good as MLPs.\nConversely, we show that KANs can be represented using MLPs, but that in this\nrepresentation the number of parameters increases by a factor of the KAN grid\nsize. This suggests that KANs with a large grid size may be more efficient than\nMLPs at approximating certain functions. On the other hand, from the\nperspective of learning and optimization, we study the spectral bias of KANs\ncompared with MLPs. We demonstrate that KANs are less biased toward low\nfrequencies than MLPs. We highlight that the multi-level learning feature\nspecific to KANs, i.e. grid extension of splines, improves the learning process\nfor high-frequency components. Detailed comparisons with different choices of\ndepth, width, and grid sizes of KANs are made, shedding some light on how to\nchoose the hyperparameters in practice.\n","authors":["Yixuan Wang","Jonathan W. Siegel","Ziming Liu","Thomas Y. Hou"],"pdf_url":"https://arxiv.org/pdf/2410.01803v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.01799v1","updated":"2024-10-02T17:56:32Z","published":"2024-10-02T17:56:32Z","title":"Efficient $1$-bit tensor approximations","summary":"  We present a spatially efficient decomposition of matrices and\narbitrary-order tensors as linear combinations of tensor products of $\\{-1,\n1\\}$-valued vectors. For any matrix $A \\in \\mathbb{R}^{m \\times n}$, $$A - R_w\n= S_w C_w T_w^\\top = \\sum_{j=1}^w c_j \\cdot \\mathbf{s}_j \\mathbf{t}_j^\\top$$ is\na {\\it $w$-width signed cut decomposition of $A$}. Here $C_w =\n\"diag\"(\\mathbf{c}_w)$ for some $\\mathbf{c}_w \\in \\mathbb{R}^w,$ and $S_w, T_w$,\nand the vectors $\\mathbf{s}_j, \\mathbf{t}_j$ are $\\{-1, 1\\}$-valued. To store\n$(S_w, T_w, C_w)$, we may pack $w \\cdot (m + n)$ bits, and require only $w$\nfloating point numbers. As a function of $w$, $\\|R_w\\|_F$ exhibits exponential\ndecay when applied to #f32 matrices with i.i.d. $\\mathcal N (0, 1)$ entries.\nChoosing $w$ so that $(S_w, T_w, C_w)$ has the same memory footprint as a\n\\textit{f16} or \\textit{bf16} matrix, the relative error is comparable. Our\nalgorithm yields efficient signed cut decompositions in $20$ lines of\npseudocode. It reflects a simple modification from a celebrated 1999 paper [1]\nof Frieze and Kannan. As a first application, we approximate the weight\nmatrices in the open \\textit{Mistral-7B-v0.1} Large Language Model to a $50\\%$\nspatial compression. Remarkably, all $226$ remainder matrices have a relative\nerror $<6\\%$ and the expanded model closely matches \\textit{Mistral-7B-v0.1} on\nthe {\\it huggingface} leaderboard [2]. Benchmark performance degrades slowly as\nwe reduce the spatial compression from $50\\%$ to $25\\%$. We optimize our open\nsource \\textit{rust} implementation [3] with \\textit{simd} instructions on\n\\textit{avx2} and \\textit{avx512} architectures. We also extend our algorithm\nfrom matrices to tensors of arbitrary order and use it to compress a picture of\nthe first author's cat Angus.\n","authors":["Alex W. Neal Riasanovsky","Sarah El Kazdadi"],"pdf_url":"https://arxiv.org/pdf/2410.01799v1.pdf","comment":"16 pages, one cat picture reused a lot"},{"id":"http://arxiv.org/abs/2410.01796v1","updated":"2024-10-02T17:53:23Z","published":"2024-10-02T17:53:23Z","title":"Bellman Diffusion: Generative Modeling as Learning a Linear Operator in\n  the Distribution Space","summary":"  Deep Generative Models (DGMs), including Energy-Based Models (EBMs) and\nScore-based Generative Models (SGMs), have advanced high-fidelity data\ngeneration and complex continuous distribution approximation. However, their\napplication in Markov Decision Processes (MDPs), particularly in distributional\nReinforcement Learning (RL), remains underexplored, with conventional\nhistogram-based methods dominating the field. This paper rigorously highlights\nthat this application gap is caused by the nonlinearity of modern DGMs, which\nconflicts with the linearity required by the Bellman equation in MDPs. For\ninstance, EBMs involve nonlinear operations such as exponentiating energy\nfunctions and normalizing constants. To address this, we introduce Bellman\nDiffusion, a novel DGM framework that maintains linearity in MDPs through\ngradient and scalar field modeling. With divergence-based training techniques\nto optimize neural network proxies and a new type of stochastic differential\nequation (SDE) for sampling, Bellman Diffusion is guaranteed to converge to the\ntarget distribution. Our empirical results show that Bellman Diffusion achieves\naccurate field estimations and is a capable image generator, converging 1.5x\nfaster than the traditional histogram-based baseline in distributional RL\ntasks. This work enables the effective integration of DGMs into MDP\napplications, unlocking new avenues for advanced decision-making frameworks.\n","authors":["Yangming Li","Chieh-Hsin Lai","Carola-Bibiane Schnlieb","Yuki Mitsufuji","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.01796v1.pdf","comment":"Paper under review"},{"id":"http://arxiv.org/abs/2410.01795v1","updated":"2024-10-02T17:53:08Z","published":"2024-10-02T17:53:08Z","title":"Knowledge-Driven Feature Selection and Engineering for Genotype Data\n  with Large Language Models","summary":"  Predicting phenotypes with complex genetic bases based on a small,\ninterpretable set of variant features remains a challenging task.\nConventionally, data-driven approaches are utilized for this task, yet the high\ndimensional nature of genotype data makes the analysis and prediction\ndifficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and\ntheir success in processing complex biomedical concepts, we set to examine the\nability of LLMs in feature selection and engineering for tabular genotype data,\nwith a novel knowledge-driven framework. We develop FREEFORM, Free-flow\nReasoning and Ensembling for Enhanced Feature Output and Robust Modeling,\ndesigned with chain-of-thought and ensembling principles, to select and\nengineer features with the intrinsic knowledge of LLMs. Evaluated on two\ndistinct genotype-phenotype datasets, genetic ancestry and hereditary hearing\nloss, we find this framework outperforms several data-driven methods,\nparticularly on low-shot regimes. FREEFORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\n","authors":["Joseph Lee","Shu Yang","Jae Young Baik","Xiaoxi Liu","Zhen Tan","Dawei Li","Zixuan Wen","Bojian Hou","Duy Duong-Tran","Tianlong Chen","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.01795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01793v1","updated":"2024-10-02T17:51:58Z","published":"2024-10-02T17:51:58Z","title":"Thermodynamic Bayesian Inference","summary":"  A fully Bayesian treatment of complicated predictive models (such as deep\nneural networks) would enable rigorous uncertainty quantification and the\nautomation of higher-level tasks including model selection. However, the\nintractability of sampling Bayesian posteriors over many parameters inhibits\nthe use of Bayesian methods where they are most needed. Thermodynamic computing\nhas emerged as a paradigm for accelerating operations used in machine learning,\nsuch as matrix inversion, and is based on the mapping of Langevin equations to\nthe dynamics of noisy physical systems. Hence, it is natural to consider the\nimplementation of Langevin sampling algorithms on thermodynamic devices. In\nthis work we propose electronic analog devices that sample from Bayesian\nposteriors by realizing Langevin dynamics physically. Circuit designs are given\nfor sampling the posterior of a Gaussian-Gaussian model and for Bayesian\nlogistic regression, and are validated by simulations. It is shown, under\nreasonable assumptions, that the Bayesian posteriors for these models can be\nsampled in time scaling with $\\ln(d)$, where $d$ is dimension. For the\nGaussian-Gaussian model, the energy cost is shown to scale with $ d \\ln(d)$.\nThese results highlight the potential for fast, energy-efficient Bayesian\ninference using thermodynamic computing.\n","authors":["Maxwell Aifer","Samuel Duffield","Kaelan Donatella","Denis Melanson","Phoebe Klett","Zach Belateche","Gavin Crooks","Antonio J. Martinez","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2410.01793v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.01789v1","updated":"2024-10-02T17:46:22Z","published":"2024-10-02T17:46:22Z","title":"Investigating on RLHF methodology","summary":"  In this article, we investigate the alignment of Large Language Models\naccording to human preferences. We discuss the features of training a\nPreference Model, which simulates human preferences, and the methods and\ndetails we found essential for achieving the best results. We also discuss\nusing Reinforcement Learning to fine-tune Large Language Models and describe\nthe challenges we faced and the ways to overcome them. Additionally, we present\nour experience with the Direct Preference Optimization method, which enables us\nto align a Large Language Model with human preferences without creating a\nseparate Preference Model. As our contribution, we introduce the approach for\ncollecting a preference dataset through perplexity filtering, which makes the\nprocess of creating such a dataset for a specific Language Model much easier\nand more cost-effective.\n","authors":["Alexey Kutalev","Sergei Markoff"],"pdf_url":"https://arxiv.org/pdf/2410.01789v1.pdf","comment":"23 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.00314v3","updated":"2024-10-02T17:44:46Z","published":"2024-06-01T06:17:32Z","title":"CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models","summary":"  The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.\n","authors":["Sarthak Harne","Monjoy Narayan Choudhury","Madhav Rao","TK Srikanth","Seema Mehrotra","Apoorva Vashisht","Aarushi Basu","Manjit Sodhi"],"pdf_url":"https://arxiv.org/pdf/2406.00314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01786v1","updated":"2024-10-02T17:42:16Z","published":"2024-10-02T17:42:16Z","title":"Learning To Solve Differential Equation Constrained Optimization\n  Problems","summary":"  Differential equations (DE) constrained optimization plays a critical role in\nnumerous scientific and engineering fields, including energy systems, aerospace\nengineering, ecology, and finance, where optimal configurations or control\nstrategies must be determined for systems governed by ordinary or stochastic\ndifferential equations. Despite its significance, the computational challenges\nassociated with these problems have limited their practical use. To address\nthese limitations, this paper introduces a learning-based approach to\nDE-constrained optimization that combines techniques from proxy optimization\nand neural differential equations. The proposed approach uses a dual-network\narchitecture, with one approximating the control strategies, focusing on\nsteady-state constraints, and another solving the associated DEs. This\ncombination enables the approximation of optimal strategies while accounting\nfor dynamic constraints in near real-time. Experiments across problems in\nenergy optimization and finance modeling show that this method provides full\ncompliance with dynamic constraints and it produces results up to 25 times more\nprecise than other methods which do not explicitly model the system's dynamic\nequations.\n","authors":["Vincenzo Di Vito","Mostafa Mohammadian","Kyri Baker","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2410.01786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01782v1","updated":"2024-10-02T17:37:18Z","published":"2024-10-02T17:37:18Z","title":"Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) has been shown to enhance the factual\naccuracy of Large Language Models (LLMs), but existing methods often suffer\nfrom limited reasoning capabilities in effectively using the retrieved\nevidence, particularly when using open-source LLMs. To mitigate this gap, we\nintroduce a novel framework, Open-RAG, designed to enhance reasoning\ncapabilities in RAG with open-source LLMs. Our framework transforms an\narbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)\nmodel capable of handling complex reasoning tasks, including both single- and\nmulti-hop queries. Open-RAG uniquely trains the model to navigate challenging\ndistractors that appear relevant but are misleading. As a result, Open-RAG\nleverages latent learning, dynamically selecting relevant experts and\nintegrating external knowledge effectively for more accurate and contextually\nrelevant responses. In addition, we propose a hybrid adaptive retrieval method\nto determine retrieval necessity and balance the trade-off between performance\ngain and inference speed. Experimental results show that the Llama2-7B-based\nOpen-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,\nSelf-RAG, and Command R+ in various knowledge-intensive tasks. We open-source\nour code and models at https://openragmoe.github.io/\n","authors":["Shayekh Bin Islam","Md Asib Rahman","K S M Tozammel Hossain","Enamul Hoque","Shafiq Joty","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2410.01782v1.pdf","comment":"Accepted to EMNLP 2024 Findings. Website:\n  https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.01445v3","updated":"2024-10-02T17:34:06Z","published":"2024-07-01T16:37:18Z","title":"FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training\n  with Limited Resources","summary":"  Existing studies of training state-of-the-art Contrastive Language-Image\nPretraining (CLIP) models on large-scale data involve hundreds of or even\nthousands of GPUs due to the requirement of a large batch size. However, such a\nlarge amount of resources is not accessible to most people. While advanced\ncompositional optimization techniques for optimizing global contrastive losses\nhave been demonstrated effective for removing the requirement of large batch\nsize, their performance on large-scale data remains underexplored and not\noptimized. To bridge the gap, this paper explores several aspects of CLIP\ntraining with limited resources (e.g., up to tens of GPUs). First, we introduce\nFastCLIP, a general CLIP training framework built on advanced compositional\noptimization techniques while designed and optimized for the distributed\nsetting. Our framework is equipped with an efficient gradient reduction\nstrategy to reduce communication overhead. Second, to further boost training\nefficiency, we investigate three components of the framework from an\noptimization perspective: the schedule of the inner learning rate, the update\nrules of the temperature parameter and the model parameters, respectively.\nExperiments on different strategies for each component shed light on how to\nconduct CLIP training more efficiently. Finally, we benchmark the performance\nof FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different\ncompute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7\nmillion, 9.1 million to 315 million image-text pairs to demonstrate the\nsignificant improvement of FastCLIP in the resource-limited setting. We release\nthe code of FastCLIP at https://github.com/Optimization-AI/fast_clip .\n","authors":["Xiyuan Wei","Fanjiang Ye","Ori Yonay","Xingyu Chen","Baixi Sun","Dingwen Tao","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01445v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.01779v1","updated":"2024-10-02T17:33:26Z","published":"2024-10-02T17:33:26Z","title":"Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in\n  Neural Nets","summary":"  We prove rich algebraic structures of the solution space for 2-layer neural\nnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasks\nin Abelian group (e.g., modular addition). Such a rich structure enables\nanalytical construction of global optimal solutions from partial solutions that\nonly satisfy part of the loss, despite its high nonlinearity. We coin the\nframework as CoGO (Composing Global Optimizers). Specifically, we show that the\nweight space over different numbers of hidden nodes of the 2-layer network is\nequipped with a semi-ring algebraic structure, and the loss function to be\noptimized consists of monomial potentials, which are ring homomorphism,\nallowing partial solutions to be composed into global ones by ring addition and\nmultiplication. Our experiments show that around $95\\%$ of the solutions\nobtained by gradient descent match exactly our theoretical constructions.\nAlthough the global optimizers constructed only required a small number of\nhidden nodes, our analysis on gradient dynamics shows that\nover-parameterization asymptotically decouples training dynamics and is\nbeneficial. We further show that training dynamics favors simpler solutions\nunder weight decay, and thus high-order global optimizers such as perfect\nmemorization are unfavorable.\n","authors":["Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01776v1","updated":"2024-10-02T17:31:01Z","published":"2024-10-02T17:31:01Z","title":"Dynamical-generative downscaling of climate model ensembles","summary":"  Regional high-resolution climate projections are crucial for many\napplications, such as agriculture, hydrology, and natural hazard risk\nassessment. Dynamical downscaling, the state-of-the-art method to produce\nlocalized future climate information, involves running a regional climate model\n(RCM) driven by an Earth System Model (ESM), but it is too computationally\nexpensive to apply to large climate projection ensembles. We propose a novel\napproach combining dynamical downscaling with generative artificial\nintelligence to reduce the cost and improve the uncertainty estimates of\ndownscaled climate projections. In our framework, an RCM dynamically downscales\nESM output to an intermediate resolution, followed by a generative diffusion\nmodel that further refines the resolution to the target scale. This approach\nleverages the generalizability of physics-based models and the sampling\nefficiency of diffusion models, enabling the downscaling of large multi-model\nensembles. We evaluate our method against dynamically-downscaled climate\nprojections from the CMIP6 ensemble. Our results demonstrate its ability to\nprovide more accurate uncertainty bounds on future regional climate than\nalternatives such as dynamical downscaling of smaller ensembles, or traditional\nempirical statistical downscaling methods. We also show that\ndynamical-generative downscaling results in significantly lower errors than\nbias correction and spatial disaggregation (BCSD), and captures more accurately\nthe spectra and multivariate correlations of meteorological fields. These\ncharacteristics make the dynamical-generative framework a flexible, accurate,\nand efficient way to downscale large ensembles of climate projections,\ncurrently out of reach for pure dynamical downscaling.\n","authors":["Ignacio Lopez-Gomez","Zhong Yi Wan","Leonardo Zepeda-Nez","Tapio Schneider","John Anderson","Fei Sha"],"pdf_url":"https://arxiv.org/pdf/2410.01776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01774v1","updated":"2024-10-02T17:30:21Z","published":"2024-10-02T17:30:21Z","title":"Trained Transformer Classifiers Generalize and Exhibit Benign\n  Overfitting In-Context","summary":"  Transformers have the capacity to act as supervised learning algorithms: by\nproperly encoding a set of labeled training (\"in-context\") examples and an\nunlabeled test example into an input sequence of vectors of the same dimension,\nthe forward pass of the transformer can produce predictions for that unlabeled\ntest example. A line of recent work has shown that when linear transformers are\npre-trained on random instances for linear regression tasks, these trained\ntransformers make predictions using an algorithm similar to that of ordinary\nleast squares. In this work, we investigate the behavior of linear transformers\ntrained on random linear classification tasks. Via an analysis of the implicit\nregularization of gradient descent, we characterize how many pre-training tasks\nand in-context examples are needed for the trained transformer to generalize\nwell at test-time. We further show that in some settings, these trained\ntransformers can exhibit \"benign overfitting in-context\": when in-context\nexamples are corrupted by label flipping noise, the transformer memorizes all\nof its in-context examples (including those with noisy labels) yet still\ngeneralizes near-optimally for clean test examples.\n","authors":["Spencer Frei","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2410.01774v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2407.12492v2","updated":"2024-10-02T17:29:54Z","published":"2024-07-17T11:18:49Z","title":"Temporal Test-Time Adaptation with State-Space Models","summary":"  Distribution shifts between training and test data are inevitable over the\nlifecycle of a deployed model, leading to performance decay. Adapting a model\non test samples can help mitigate this drop in performance. However, most\ntest-time adaptation methods have focused on synthetic corruption shifts,\nleaving a variety of distribution shifts underexplored. In this paper, we focus\non distribution shifts that evolve gradually over time, which are common in the\nwild but challenging for existing methods, as we show. To address this, we\npropose STAD, a probabilistic state-space model that adapts a deployed model to\ntemporal distribution shifts by learning the time-varying dynamics in the last\nset of hidden features. Without requiring labels, our model infers\ntime-evolving class prototypes that act as a dynamic classification head.\nThrough experiments on real-world temporal distribution shifts, we show that\nour method excels in handling small batch sizes and label shift.\n","authors":["Mona Schirmer","Dan Zhang","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2407.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01771v1","updated":"2024-10-02T17:28:22Z","published":"2024-10-02T17:28:22Z","title":"Bayesian Binary Search","summary":"  We present Bayesian Binary Search (BBS), a novel probabilistic variant of the\nclassical binary search/bisection algorithm. BBS leverages machine\nlearning/statistical techniques to estimate the probability density of the\nsearch space and modifies the bisection step to split based on probability\ndensity rather than the traditional midpoint, allowing for the learned\ndistribution of the search space to guide the search algorithm. Search space\ndensity estimation can flexibly be performed using supervised probabilistic\nmachine learning techniques (e.g., Gaussian process regression, Bayesian neural\nnetworks, quantile regression) or unsupervised learning algorithms (e.g.,\nGaussian mixture models, kernel density estimation (KDE), maximum likelihood\nestimation (MLE)). We demonstrate significant efficiency gains of using BBS on\nboth simulated data across a variety of distributions and in a real-world\nbinary search use case of probing channel balances in the Bitcoin Lightning\nNetwork, for which we have deployed the BBS algorithm in a production setting.\n","authors":["Vikash Singh","Matthew Khanzadeh","Vincent Davis","Harrison Rush","Emanuele Rossi","Jesse Shrader","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2410.01771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01770v1","updated":"2024-10-02T17:27:13Z","published":"2024-10-02T17:27:13Z","title":"Explainable Earth Surface Forecasting under Extreme Events","summary":"  With climate change-related extreme events on the rise, high dimensional\nEarth observation data presents a unique opportunity for forecasting and\nunderstanding impacts on ecosystems. This is, however, impeded by the\ncomplexity of processing, visualizing, modeling, and explaining this data. To\nshowcase how this challenge can be met, here we train a convolutional long\nshort-term memory-based architecture on the novel DeepExtremeCubes dataset.\nDeepExtremeCubes includes around 40,000 long-term Sentinel-2 minicubes (January\n2016-October 2022) worldwide, along with labeled extreme events, meteorological\ndata, vegetation land cover, and topography map, sampled from locations\naffected by extreme climate events and surrounding areas. When predicting\nfuture reflectances and vegetation impacts through kernel normalized difference\nvegetation index, the model achieved an R$^2$ score of 0.9055 in the test set.\nExplainable artificial intelligence was used to analyze the model's predictions\nduring the October 2020 Central South America compound heatwave and drought\nevent. We chose the same area exactly one year before the event as\ncounterfactual, finding that the average temperature and surface pressure are\ngenerally the best predictors under normal conditions. In contrast, minimum\nanomalies of evaporation and surface latent heat flux take the lead during the\nevent. A change of regime is also observed in the attributions before the\nevent, which might help assess how long the event was brewing before happening.\nThe code to replicate all experiments and figures in this paper is publicly\navailable at https://github.com/DeepExtremes/txyXAI\n","authors":["Oscar J. Pellicer-Valero","Miguel-ngel Fernndez-Torres","Chaonan Ji","Miguel D. Mahecha","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2410.01770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01767v1","updated":"2024-10-02T17:22:09Z","published":"2024-10-02T17:22:09Z","title":"Decision-Focused Uncertainty Quantification","summary":"  There is increasing interest in ''decision-focused'' machine learning methods\nwhich train models to account for how their predictions are used in downstream\noptimization problems. Doing so can often improve performance on subsequent\ndecision problems. However, current methods for uncertainty quantification do\nnot incorporate any information at all about downstream decisions. We develop a\nframework based on conformal prediction to produce prediction sets that account\nfor a downstream decision loss function, making them more appropriate to inform\nhigh-stakes decision-making. Our approach harnesses the strengths of conformal\nmethods--modularity, model-agnosticism, and statistical coverage\nguarantees--while incorporating downstream decisions and user-specified utility\nfunctions. We prove that our methods retain standard coverage guarantees.\nEmpirical evaluation across a range of datasets and utility metrics\ndemonstrates that our methods achieve significantly lower decision loss\ncompared to standard conformal methods. Additionally, we present a real-world\nuse case in healthcare diagnosis, where our method effectively incorporates the\nhierarchical structure of dermatological diseases. It successfully generates\nsets with coherent diagnostic meaning, aiding the triage process during\ndermatology diagnosis and illustrating how our method can ground high-stakes\ndecision-making on external domain knowledge.\n","authors":["Santiago Cortes-Gomez","Carlos Patio","Yewon Byun","Steven Wu","Eric Horvitz","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2410.01767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17253v2","updated":"2024-10-02T17:21:47Z","published":"2024-08-30T12:51:55Z","title":"VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time\n  Series Forecasters","summary":"  Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either repurpose large language models\n(LLMs) or build large-scale time series datasets to develop TSF foundation\nmodels for universal forecasting. However, these methods face challenges due to\nthe severe cross-domain gap or in-domain heterogeneity. This paper explores a\nnew road to building a TSF foundation model from rich, high-quality natural\nimages. Our key insight is that a visual masked autoencoder, pre-trained on the\nImageNet dataset, can naturally be a numeric series forecaster. By\nreformulating TSF as an image reconstruction task, we bridge the gap between\nimage pre-training and TSF downstream tasks. Surprisingly, without further\nadaptation in the time-series domain, the proposed VisionTS could achieve\nsuperior zero-shot forecasting performance compared to existing TSF foundation\nmodels. With fine-tuning for one epoch, VisionTS could further improve the\nforecasting and achieve state-of-the-art performance in most cases. Extensive\nexperiments reveal intrinsic similarities between images and real-world time\nseries, suggesting visual models may offer a ``free lunch'' for TSF and\nhighlight the potential for future cross-modality research. Our code is\npublicly available at https://github.com/Keytoyze/VisionTS.\n","authors":["Mouxiang Chen","Lefei Shen","Zhuo Li","Xiaoyun Joy Wang","Jianling Sun","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17253v2.pdf","comment":"v2: add more experiments"},{"id":"http://arxiv.org/abs/2410.01766v1","updated":"2024-10-02T17:21:43Z","published":"2024-10-02T17:21:43Z","title":"SegHeD: Segmentation of Heterogeneous Data for Multiple Sclerosis\n  Lesions with Anatomical Constraints","summary":"  Assessment of lesions and their longitudinal progression from brain magnetic\nresonance (MR) images plays a crucial role in diagnosing and monitoring\nmultiple sclerosis (MS). Machine learning models have demonstrated a great\npotential for automated MS lesion segmentation. Training such models typically\nrequires large-scale high-quality datasets that are consistently annotated.\nHowever, MS imaging datasets are often small, segregated across multiple sites,\nwith different formats (cross-sectional or longitudinal), and diverse\nannotation styles. This poses a significant challenge to train a unified MS\nlesion segmentation model. To tackle this challenge, we present SegHeD, a novel\nmulti-dataset multi-task segmentation model that can incorporate heterogeneous\ndata as input and perform all-lesion, new-lesion, as well as vanishing-lesion\nsegmentation. Furthermore, we account for domain knowledge about MS lesions,\nincorporating longitudinal, spatial, and volumetric constraints into the\nsegmentation model. SegHeD is assessed on five MS datasets and achieves a high\nperformance in all, new, and vanishing-lesion segmentation, outperforming\nseveral state-of-the-art methods in this field.\n","authors":["Berke Doga Basaran","Xinru Zhang","Paul M. Matthews","Wenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2410.01766v1.pdf","comment":"13 pages, 4 figures, MICCAI, LDTM Workshop"},{"id":"http://arxiv.org/abs/2406.10995v2","updated":"2024-10-02T17:20:28Z","published":"2024-06-16T16:15:20Z","title":"Concept-skill Transferability-based Data Selection for Large\n  Vision-Language Models","summary":"  Instruction tuning, or supervised finetuning on extensive task-specific data,\nis necessary for Large Vision-Language Models (LVLMs) to generalize well across\na broad range of vision-language (VL) tasks. However, training on large VL\ndatasets can become prohibitively expensive. In this work, we introduce\nCOINCIDE, an effective and scalable data selection technique that uses a small\nmodel as a reference model to select visual instruction tuning data for\nefficient finetuning of a target LVLM, focusing on diversity and\ntransferability. Specifically, we cluster the training data using internal\nactivations from a small model, which identifies VL concept-skill compositions\nneeded by a target LVLM. We then sample data from these diverse clusters by\nconsidering their density and transferability, or the ability to transfer well\nto other concept-skill compositions. This approach ensures the diversity of\nthese compositions, which is vital for LVLM generalization. Extensive\nexperiments demonstrate that COINCIDE achieves superior performance and data\nselection efficiency against 8 strong baselines on two distinct datasets:\nLLaVA-1.5 and Vision-Flan. Using only 20% of the LLaVA-1.5 dataset, COINCIDE\nachieves performance comparable to the LVLM finetuned on the whole dataset,\nwith 70% reduction of the wall-clock running time. On the Vision-Flan dataset,\nour method achieves superior results with only 16.7% of the training data.\n","authors":["Jaewoo Lee","Boyang Li","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.10995v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.01100v2","updated":"2024-10-02T17:09:53Z","published":"2024-07-01T09:06:57Z","title":"Eliminating Position Bias of Language Models: A Mechanistic Approach","summary":"  Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.\n","authors":["Ziqi Wang","Hanlin Zhang","Xiner Li","Kuan-Hao Huang","Chi Han","Shuiwang Ji","Sham M. Kakade","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.01100v2.pdf","comment":"26 pages, 6 figures, 15 tables"},{"id":"http://arxiv.org/abs/2410.01755v1","updated":"2024-10-02T17:05:48Z","published":"2024-10-02T17:05:48Z","title":"Integrating Protein Sequence and Expression Level to Analysis Molecular\n  Characterization of Breast Cancer Subtypes","summary":"  Breast cancer's complexity and variability pose significant challenges in\nunderstanding its progression and guiding effective treatment. This study aims\nto integrate protein sequence data with expression levels to improve the\nmolecular characterization of breast cancer subtypes and predict clinical\noutcomes. Using ProtGPT2, a language model designed for protein sequences, we\ngenerated embeddings that capture the functional and structural properties of\nproteins sequence. These embeddings were integrated with protein expression\nlevel to form enriched biological representations, which were analyzed using\nmachine learning methods like ensemble K-means for clustering and XGBoost for\nclassification. Our approach enabled successful clustering of patients into\nbiologically distinct groups and accurately predicted clinical outcomes such as\nsurvival and biomarkers status, achieving high performance metrics, notably an\nF1 score of 0.88 for survival and 0.87 for biomarkers status prediction.\nAnalysis of feature importance highlighted key proteins like KMT2C, GCN1, and\nCLASP2, linked to hormone receptor and Human Epidermal Growth Factor Receptor 2\n(HER2) expression, which play a role in tumor progression and patient outcomes,\nrespectively. Furthermore, protein-protein interaction networks and correlation\nanalyses revealed the interdependence of proteins that may influence breast\ncancer subtype behaviors. These findings suggest that integrating protein\nsequence and expression data provides valuable insights into tumor biology and\nhas significant potential to enhance personalized treatment strategies in\nbreast cancer care.\n","authors":["Hossein Sholehrasa"],"pdf_url":"https://arxiv.org/pdf/2410.01755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19913v2","updated":"2024-10-02T17:03:25Z","published":"2024-09-30T03:32:02Z","title":"Scaling Optimal LR Across Token Horizons","summary":"  State-of-the-art LLMs are powered by scaling -- scaling model size, dataset\nsize and cluster size. It is economically infeasible to extensively tune\nhyperparameter for the largest runs. Instead, approximately optimal\nhyperparameters must be inferred or \\textit{transferred} from smaller\nexperiments. Hyperparameter transfer across model sizes has been studied in\nYang et al. However, hyperparameter transfer across dataset size -- or token\nhorizon -- has not been studied yet. To remedy this we conduct a large scale\nempirical study on how optimal learning rate (LR) depends on token horizon in\nLLM training. We first demonstrate that the optimal LR changes significantly\nwith token horizon -- longer training necessitates smaller LR. Secondly we\ndemonstrate the the optimal LR follows a scaling law, and that the optimal LR\nfor longer horizons can be accurately estimated from shorter horizons via such\nscaling laws. We also provide a rule-of-thumb for transferring LR across token\nhorizons with zero overhead over current practices. Lastly we provide evidence\nthat LLama-1 used too high LR, and estimate the performance hit from this. We\nthus argue that hyperparameter transfer across data size is an important and\noverlooked component of LLM training.\n","authors":["Johan Bjorck","Alon Benhaim","Vishrav Chaudhary","Furu Wei","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2409.19913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01752v1","updated":"2024-10-02T17:02:17Z","published":"2024-10-02T17:02:17Z","title":"TorchSISSO: A PyTorch-Based Implementation of the Sure Independence\n  Screening and Sparsifying Operator for Efficient and Interpretable Model\n  Discovery","summary":"  Symbolic regression (SR) is a powerful machine learning approach that\nsearches for both the structure and parameters of algebraic models, offering\ninterpretable and compact representations of complex data. Unlike traditional\nregression methods, SR explores progressively complex feature spaces, which can\nuncover simple models that generalize well, even from small datasets. Among SR\nalgorithms, the Sure Independence Screening and Sparsifying Operator (SISSO)\nhas proven particularly effective in the natural sciences, helping to\nrediscover fundamental physical laws as well as discover new interpretable\nequations for materials property modeling. However, its widespread adoption has\nbeen limited by performance inefficiencies and the challenges posed by its\nFORTRAN-based implementation, especially in modern computing environments. In\nthis work, we introduce TorchSISSO, a native Python implementation built in the\nPyTorch framework. TorchSISSO leverages GPU acceleration, easy integration, and\nextensibility, offering a significant speed-up and improved accuracy over the\noriginal. We demonstrate that TorchSISSO matches or exceeds the performance of\nthe original SISSO across a range of tasks, while dramatically reducing\ncomputational time and improving accessibility for broader scientific\napplications.\n","authors":["Madhav Muthyala","Farshud Sorourifar","Joel A. Paulson"],"pdf_url":"https://arxiv.org/pdf/2410.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04194v2","updated":"2024-10-02T17:01:58Z","published":"2024-09-06T11:24:25Z","title":"Towards Privacy-Preserving Relational Data Synthesis via Probabilistic\n  Relational Models","summary":"  Probabilistic relational models provide a well-established formalism to\ncombine first-order logic and probabilistic models, thereby allowing to\nrepresent relationships between objects in a relational domain. At the same\ntime, the field of artificial intelligence requires increasingly large amounts\nof relational training data for various machine learning tasks. Collecting\nreal-world data, however, is often challenging due to privacy concerns, data\nprotection regulations, high costs, and so on. To mitigate these challenges,\nthe generation of synthetic data is a promising approach. In this paper, we\nsolve the problem of generating synthetic relational data via probabilistic\nrelational models. In particular, we propose a fully-fledged pipeline to go\nfrom relational database to probabilistic relational model, which can then be\nused to sample new synthetic relational data points from its underlying\nprobability distribution. As part of our proposed pipeline, we introduce a\nlearning algorithm to construct a probabilistic relational model from a given\nrelational database.\n","authors":["Malte Luttermann","Ralf Mller","Mattis Hartwig"],"pdf_url":"https://arxiv.org/pdf/2409.04194v2.pdf","comment":"Accepted to the Proceedings of the 47th German Conference on\n  Artificial Intelligence (KI 2024)"},{"id":"http://arxiv.org/abs/2410.01748v1","updated":"2024-10-02T17:01:10Z","published":"2024-10-02T17:01:10Z","title":"Not All LLM Reasoners Are Created Equal","summary":"  We study the depth of grade-school math (GSM) problem-solving capabilities of\nLLMs. To this end, we evaluate their performance on pairs of existing math word\nproblems together so that the answer to the second problem depends on correctly\nanswering the first problem. Our findings reveal a significant reasoning gap in\nmost LLMs, that is performance difference between solving the compositional\npairs and solving each question independently. This gap is more pronounced in\nsmaller, more cost-efficient, and math-specialized models. Moreover,\ninstruction-tuning recipes and code generation have varying effects across LLM\nsizes, while finetuning on GSM can lead to task overfitting. Our analysis\nindicates that large reasoning gaps are not because of test-set leakage, but\ndue to distraction from additional context and poor second-hop reasoning.\nOverall, LLMs exhibit systematic differences in their reasoning abilities,\ndespite what their performance on standard benchmarks indicates.\n","authors":["Arian Hosseini","Alessandro Sordoni","Daniel Toyama","Aaron Courville","Rishabh Agarwal"],"pdf_url":"https://arxiv.org/pdf/2410.01748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01746v1","updated":"2024-10-02T17:01:01Z","published":"2024-10-02T17:01:01Z","title":"Leray-Schauder Mappings for Operator Learning","summary":"  We present an algorithm for learning operators between Banach spaces, based\non the use of Leray-Schauder mappings to learn a finite-dimensional\napproximation of compact subspaces. We show that the resulting method is a\nuniversal approximator of (possibly nonlinear) operators. We demonstrate the\nefficiency of the approach on two benchmark datasets showing it achieves\nresults comparable to state of the art models.\n","authors":["Emanuele Zappala"],"pdf_url":"https://arxiv.org/pdf/2410.01746v1.pdf","comment":"6 pages, 2 figures, 1 table. Comments are welcome!"},{"id":"http://arxiv.org/abs/2408.12186v2","updated":"2024-10-02T16:58:37Z","published":"2024-08-22T08:02:10Z","title":"Transformers are Minimax Optimal Nonparametric In-Context Learners","summary":"  In-context learning (ICL) of large language models has proven to be a\nsurprisingly effective method of learning a new task from only a few\ndemonstrative examples. In this paper, we study the efficacy of ICL from the\nviewpoint of statistical learning theory. We develop approximation and\ngeneralization error bounds for a transformer composed of a deep neural network\nand one linear attention layer, pretrained on nonparametric regression tasks\nsampled from general function spaces including the Besov space and piecewise\n$\\gamma$-smooth class. We show that sufficiently trained transformers can\nachieve -- and even improve upon -- the minimax optimal estimation risk in\ncontext by encoding the most relevant basis representations during pretraining.\nOur analysis extends to high-dimensional or sequential data and distinguishes\nthe \\emph{pretraining} and \\emph{in-context} generalization gaps. Furthermore,\nwe establish information-theoretic lower bounds for meta-learners w.r.t. both\nthe number of tasks and in-context examples. These findings shed light on the\nroles of task diversity and representation learning for ICL.\n","authors":["Juno Kim","Tai Nakamaki","Taiji Suzuki"],"pdf_url":"https://arxiv.org/pdf/2408.12186v2.pdf","comment":"NeurIPS 2024; 40 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.13975v2","updated":"2024-10-02T16:56:09Z","published":"2024-05-22T20:20:14Z","title":"HOPE for a Robust Parameterization of Long-memory State Space Models","summary":"  State-space models (SSMs) that utilize linear, time-invariant (LTI) systems\nare known for their effectiveness in learning long sequences. To achieve\nstate-of-the-art performance, an SSM often needs a specifically designed\ninitialization, and the training of state matrices is on a logarithmic scale\nwith a very small learning rate. To understand these choices from a unified\nperspective, we view SSMs through the lens of Hankel operator theory. Building\nupon it, we develop a new parameterization scheme, called HOPE, for LTI systems\nthat utilizes Markov parameters within Hankel operators. Our approach helps\nimprove the initialization and training stability, leading to a more robust\nparameterization. We efficiently implement these innovations by nonuniformly\nsampling the transfer functions of LTI systems, and they require fewer\nparameters compared to canonical SSMs. When benchmarked against\nHiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel\noperators demonstrates improved performance on Long-Range Arena (LRA) tasks.\nMoreover, our new parameterization endows the SSM with non-decaying memory\nwithin a fixed time window, which is empirically corroborated by a sequential\nCIFAR-10 task with padded noise.\n","authors":["Annan Yu","Michael W. Mahoney","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2405.13975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01745v1","updated":"2024-10-02T16:56:03Z","published":"2024-10-02T16:56:03Z","title":"PreND: Enhancing Intrinsic Motivation in Reinforcement Learning through\n  Pre-trained Network Distillation","summary":"  Intrinsic motivation, inspired by the psychology of developmental learning in\ninfants, stimulates exploration in agents without relying solely on sparse\nexternal rewards. Existing methods in reinforcement learning like Random\nNetwork Distillation (RND) face significant limitations, including (1) relying\non raw visual inputs, leading to a lack of meaningful representations, (2) the\ninability to build a robust latent space, (3) poor target network\ninitialization and (4) rapid degradation of intrinsic rewards. In this paper,\nwe introduce Pre-trained Network Distillation (PreND), a novel approach to\nenhance intrinsic motivation in reinforcement learning (RL) by improving upon\nthe widely used prediction-based method, RND. PreND addresses these challenges\nby incorporating pre-trained representation models into both the target and\npredictor networks, resulting in more meaningful and stable intrinsic rewards,\nwhile enhancing the representation learned by the model. We also tried simple\nbut effective variants of the predictor network optimization by controlling the\nlearning rate. Through experiments on the Atari domain, we demonstrate that\nPreND significantly outperforms RND, offering a more robust intrinsic\nmotivation signal that leads to better exploration, improving overall\nperformance and sample efficiency. This research highlights the importance of\ntarget and predictor networks representation in prediction-based intrinsic\nmotivation, setting a new direction for improving RL agents' learning\nefficiency in sparse reward environments.\n","authors":["Mohammadamin Davoodabadi","Negin Hashemi Dijujin","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2410.01745v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.01739v1","updated":"2024-10-02T16:50:29Z","published":"2024-10-02T16:50:29Z","title":"Mimicking Human Intuition: Cognitive Belief-Driven Q-Learning","summary":"  Reinforcement learning encounters challenges in various environments related\nto robustness and explainability. Traditional Q-learning algorithms cannot\neffectively make decisions and utilize the historical learning experience. To\novercome these limitations, we propose Cognitive Belief-Driven Q-Learning\n(CBDQ), which integrates subjective belief modeling into the Q-learning\nframework, enhancing decision-making accuracy by endowing agents with\nhuman-like learning and reasoning capabilities. Drawing inspiration from\ncognitive science, our method maintains a subjective belief distribution over\nthe expectation of actions, leveraging a cluster-based subjective belief model\nthat enables agents to reason about the potential probability associated with\neach decision. CBDQ effectively mitigates overestimated phenomena and optimizes\ndecision-making policies by integrating historical experiences with current\ncontextual information, mimicking the dynamics of human decision-making. We\nevaluate the proposed method on discrete control benchmark tasks in various\ncomplicate environments. The results demonstrate that CBDQ exhibits stronger\nadaptability, robustness, and human-like characteristics in handling these\nenvironments, outperforming other baselines. We hope this work will give\nresearchers a fresh perspective on understanding and explaining Q-learning.\n","authors":["Xingrui Gu","Guanren Qiao","Chuyi Jiang","Tianqing Xia","Hangyu Mao"],"pdf_url":"https://arxiv.org/pdf/2410.01739v1.pdf","comment":"Under review by ICLR 25"},{"id":"http://arxiv.org/abs/2410.01736v1","updated":"2024-10-02T16:47:35Z","published":"2024-10-02T16:47:35Z","title":"Recursive Abstractive Processing for Retrieval in Dynamic Datasets","summary":"  Recent retrieval-augmented models enhance basic methods by building a\nhierarchical structure over retrieved text chunks through recursive embedding,\nclustering, and summarization. The most relevant information is then retrieved\nfrom both the original text and generated summaries. However, such approaches\nface limitations with dynamic datasets, where adding or removing documents over\ntime complicates the updating of hierarchical representations formed through\nclustering. We propose a new algorithm to efficiently maintain the\nrecursive-abstractive tree structure in dynamic datasets, without compromising\nperformance. Additionally, we introduce a novel post-retrieval method that\napplies query-focused recursive abstractive processing to substantially improve\ncontext quality. Our method overcomes the limitations of other approaches by\nfunctioning as a black-box post-retrieval layer compatible with any retrieval\nalgorithm. Both algorithms are validated through extensive experiments on\nreal-world datasets, demonstrating their effectiveness in handling dynamic data\nand improving retrieval performance.\n","authors":["Charbel Chucri","Rami Azouz","Joachim Ott"],"pdf_url":"https://arxiv.org/pdf/2410.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10490v2","updated":"2024-10-02T16:47:30Z","published":"2024-07-15T07:30:28Z","title":"Learning Dynamics of LLM Finetuning","summary":"  Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2407.10490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01735v1","updated":"2024-10-02T16:46:38Z","published":"2024-10-02T16:46:38Z","title":"LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits","summary":"  Reward Models (RMs) play a crucial role in aligning LLMs with human\npreferences, enhancing their performance by ranking outputs during inference or\niterative training. However, the degree to which an RM generalizes to new tasks\nis often not known a priori (e.g. some RMs may excel at scoring creative\nwriting vs. math reasoning). Therefore, using only one fixed RM while training\nLLMs can be suboptimal. Moreover, optimizing LLMs with multiple RMs\nsimultaneously can be prohibitively computationally-intensive and challenging\ndue to conflicting signals from different RMs, potentially degrading\nperformance. To address these challenges, we introduce LASeR (Learning to\nAdaptively Select Rewards), which iteratively trains LLMs using multiple RMs,\nselecting and utilizing the most well-suited RM for each instance to rank\noutputs and generate preference data, framed as a multi-armed bandit problem.\nOur results on commonsense and math reasoning tasks demonstrate that LASeR can\nboost iterative LLM optimization by optimizing for multiple RMs, improving the\nabsolute average accuracy of Llama-3-8B over three datasets by 2.67% over\ntraining with ensemble RM scores while also showing superior training\nefficiency (e.g., a 2x speedup). Moreover, on WildChat, a benchmark of\ninstruction-following prompts, we find that using Llama-3-8B LASeR leads to a\n71.45% AlpacaEval win rate over sequentially optimizing multiple RMs. Extending\nto long-context generation tasks, we find that on Llama-3-8B, LASeR achieves an\naverage improvement of 2.64 F1 and 2.42 F1 on single- and multi-document QA\nover random RM selection when used with best-of-n sampling. LASeR is robust to\nnoisy rewards and generalizes to multiple settings. Finally, LASeR's RM\nselection changes depending on the underlying task or instance and we verify\nthe presence of conflicting preferences from multiple RMs that can be mitigated\nusing LASeR.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.01735v1.pdf","comment":"20 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB"},{"id":"http://arxiv.org/abs/2409.09828v2","updated":"2024-10-02T16:42:46Z","published":"2024-09-15T19:04:50Z","title":"Latent Diffusion Models for Controllable RNA Sequence Generation","summary":"  This work presents RNAdiffusion, a latent diffusion model for generating and\noptimizing discrete RNA sequences of variable lengths. RNA is a key\nintermediary between DNA and protein, exhibiting high sequence diversity and\ncomplex three-dimensional structures to support a wide range of functions. We\nutilize pretrained BERT-type models to encode raw RNA sequences into\ntoken-level, biologically meaningful representations. A Query Transformer is\nemployed to compress such representations into a set of fixed-length latent\nvectors, with an autoregressive decoder trained to reconstruct RNA sequences\nfrom these latent variables. We then develop a continuous diffusion model\nwithin this latent space. To enable optimization, we integrate the gradients of\nreward models--surrogates for RNA functional properties--into the backward\ndiffusion process, thereby generating RNAs with high reward scores. Empirical\nresults confirm that RNAdiffusion generates non-coding RNAs that align with\nnatural distributions across various biological metrics. Further, we fine-tune\nthe diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize\nsequences for high translation efficiencies. Our guided diffusion model\neffectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and\nTranslation Efficiency (TE), outperforming baselines in balancing rewards and\nstructural stability trade-off. Our findings hold potential for advancing RNA\nsequence-function research and therapeutic RNA design.\n","authors":["Kaixuan Huang","Yukang Yang","Kaidi Fu","Yanyi Chu","Le Cong","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15939v2","updated":"2024-10-02T16:42:35Z","published":"2024-02-24T23:56:15Z","title":"Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI","summary":"  Dynamic magnetic resonance imaging (MRI) plays an indispensable role in\ncardiac diagnosis. To enable fast imaging, the k-space data can be undersampled\nbut the image reconstruction poses a great challenge of high-dimensional\nprocessing. This challenge necessitates extensive training data in deep\nlearning reconstruction methods. In this work, we propose a novel and efficient\napproach, leveraging a dimension-reduced separable learning scheme that can\nperform exceptionally well even with highly limited training data. We design\nthis new approach by incorporating spatiotemporal priors into the development\nof a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an\niteration process of a 2D spatiotemporal reconstruction model with both\ntemporal low-rankness and spatial sparsity. Intermediate outputs can also be\nvisualized to provide insights into the network behavior and enhance\ninterpretability. Extensive results on cardiac cine datasets demonstrate that\nthe proposed DeepSSL surpasses state-of-the-art methods both visually and\nquantitatively, while reducing the demand for training cases by up to 75%.\nAdditionally, its preliminary adaptability to unseen cardiac patients has been\nverified through a blind reader study conducted by experienced radiologists and\ncardiologists. Furthermore, DeepSSL enhances the accuracy of the downstream\ntask of cardiac segmentation and exhibits robustness in prospectively\nundersampled real-time cardiac MRI.\n","authors":["Zi Wang","Min Xiao","Yirong Zhou","Chengyan Wang","Naiming Wu","Yi Li","Yiwen Gong","Shufu Chang","Yinyin Chen","Liuhong Zhu","Jianjun Zhou","Congbo Cai","He Wang","Di Guo","Guang Yang","Xiaobo Qu"],"pdf_url":"https://arxiv.org/pdf/2402.15939v2.pdf","comment":"12 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.14012v2","updated":"2024-10-02T16:40:10Z","published":"2024-09-21T04:40:08Z","title":"Test Time Learning for Time Series Forecasting","summary":"  Time-series forecasting has seen significant advancements with the\nintroduction of token prediction mechanisms such as multi-head attention.\nHowever, these methods often struggle to achieve the same performance as in\nlanguage modeling, primarily due to the quadratic computational cost and the\ncomplexity of capturing long-range dependencies in time-series data.\nState-space models (SSMs), such as Mamba, have shown promise in addressing\nthese challenges by offering efficient solutions with linear RNNs capable of\nmodeling long sequences with larger context windows. However, there remains\nroom for improvement in accuracy and scalability.\n  We propose the use of Test-Time Training (TTT) modules in a parallel\narchitecture to enhance performance in long-term time series forecasting.\nThrough extensive experiments on standard benchmark datasets, we demonstrate\nthat TTT modules consistently outperform state-of-the-art models, including the\nMamba-based TimeMachine, particularly in scenarios involving extended sequence\nand prediction lengths. Our results show significant improvements in Mean\nSquared Error (MSE) and Mean Absolute Error (MAE), especially on larger\ndatasets such as Electricity, Traffic, and Weather, underscoring the\neffectiveness of TTT in capturing long-range dependencies. Additionally, we\nexplore various convolutional architectures within the TTT framework, showing\nthat even simple configurations like 1D convolution with small filters can\nachieve competitive results. This work sets a new benchmark for time-series\nforecasting and lays the groundwork for future research in scalable,\nhigh-performance forecasting models.\n","authors":["Panayiotis Christou","Shichu Chen","Xupeng Chen","Parijat Dube"],"pdf_url":"https://arxiv.org/pdf/2409.14012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01729v1","updated":"2024-10-02T16:39:58Z","published":"2024-10-02T16:39:58Z","title":"Evaluating Robustness of Reward Models for Mathematical Reasoning","summary":"  Reward models are key in reinforcement learning from human feedback (RLHF)\nsystems, aligning the model behavior with human preferences. Particularly in\nthe math domain, there have been plenty of studies using reward models to align\npolicies for improving reasoning capabilities. Recently, as the importance of\nreward models has been emphasized, RewardBench is proposed to understand their\nbehavior. However, we figure out that the math subset of RewardBench has\ndifferent representations between chosen and rejected completions, and relies\non a single comparison, which may lead to unreliable results as it only see an\nisolated case. Therefore, it fails to accurately present the robustness of\nreward models, leading to a misunderstanding of its performance and potentially\nresulting in reward hacking. In this work, we introduce a new design for\nreliable evaluation of reward models, and to validate this, we construct\nRewardMATH, a benchmark that effectively represents the robustness of reward\nmodels in mathematical reasoning tasks. We demonstrate that the scores on\nRewardMATH strongly correlate with the results of optimized policy and\neffectively estimate reward overoptimization, whereas the existing benchmark\nshows almost no correlation. The results underscore the potential of our design\nto enhance the reliability of evaluation, and represent the robustness of\nreward model. We make our code and data publicly available.\n","authors":["Sunghwan Kim","Dongjin Kang","Taeyoon Kwon","Hyungjoo Chae","Jungsoo Won","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2410.01729v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01727v1","updated":"2024-10-02T16:37:19Z","published":"2024-10-02T16:37:19Z","title":"Automated Knowledge Concept Annotation and Question Representation\n  Learning for Knowledge Tracing","summary":"  Knowledge tracing (KT) is a popular approach for modeling students' learning\nprogress over time, which can enable more personalized and adaptive learning.\nHowever, existing KT approaches face two major limitations: (1) they rely\nheavily on expert-defined knowledge concepts (KCs) in questions, which is\ntime-consuming and prone to errors; and (2) KT methods tend to overlook the\nsemantics of both questions and the given KCs. In this work, we address these\nchallenges and present KCQRL, a framework for automated knowledge concept\nannotation and question representation learning that can improve the\neffectiveness of any existing KT model. First, we propose an automated KC\nannotation process using large language models (LLMs), which generates question\nsolutions and then annotates KCs in each solution step of the questions.\nSecond, we introduce a contrastive learning approach to generate semantically\nrich embeddings for questions and solution steps, aligning them with their\nassociated KCs via a tailored false negative elimination approach. These\nembeddings can be readily integrated into existing KT models, replacing their\nrandomly initialized embeddings. We demonstrate the effectiveness of KCQRL\nacross 15 KT algorithms on two large real-world Math learning datasets, where\nwe achieve consistent performance improvements.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2410.01727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08473v2","updated":"2024-10-02T16:37:16Z","published":"2024-06-12T17:56:46Z","title":"Strategies for Pretraining Neural Operators","summary":"  Pretraining for partial differential equation (PDE) modeling has recently\nshown promise in scaling neural operators across datasets to improve\ngeneralizability and performance. Despite these advances, our understanding of\nhow pretraining affects neural operators is still limited; studies generally\npropose tailored architectures and datasets that make it challenging to compare\nor examine different pretraining frameworks. To address this, we compare\nvarious pretraining methods without optimizing architecture choices to\ncharacterize pretraining dynamics on different models and datasets as well as\nto understand its scaling and generalization behavior. We find that pretraining\nis highly dependent on model and dataset choices, but in general transfer\nlearning or physics-based pretraining strategies work best. In addition,\npretraining performance can be further improved by using data augmentations.\nLastly, pretraining can be additionally beneficial when fine-tuning in scarce\ndata regimes or when generalizing to downstream data similar to the pretraining\ndistribution. Through providing insights into pretraining neural operators for\nphysics prediction, we hope to motivate future work in developing and\nevaluating pretraining methods for PDEs.\n","authors":["Anthony Zhou","Cooper Lorsung","AmirPouya Hemmasian","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2406.08473v2.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.01720v1","updated":"2024-10-02T16:32:05Z","published":"2024-10-02T16:32:05Z","title":"Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective","summary":"  Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open source\nour code through an anonymous GitHub repository at\nhttps://anonymous.4open.science/r/Understanding-Synthetic.\n","authors":["Zeyu Gan","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19653v2","updated":"2024-10-02T16:23:12Z","published":"2024-05-30T03:12:04Z","title":"SysCaps: Language Interfaces for Simulation Surrogates of Complex\n  Systems","summary":"  Surrogate models are used to predict the behavior of complex energy systems\nthat are too expensive to simulate with traditional numerical methods. Our work\nintroduces the use of language descriptions, which we call \"system captions\" or\nSysCaps, to interface with such surrogates. We argue that interacting with\nsurrogates through text, particularly natural language, makes these models more\naccessible for both experts and non-experts. We introduce a lightweight\nmultimodal text and timeseries regression model and a training pipeline that\nuses large language models (LLMs) to synthesize high-quality captions from\nsimulation metadata. Our experiments on two real-world simulators of buildings\nand wind farms show that our SysCaps-augmented surrogates have better accuracy\non held-out systems than traditional methods while enjoying new generalization\nabilities, such as handling semantically related descriptions of the same test\nsystem. Additional experiments also highlight the potential of SysCaps to\nunlock language-driven design space exploration and to regularize training\nthrough prompt augmentation.\n","authors":["Patrick Emami","Zhaonan Li","Saumya Sinha","Truc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2405.19653v2.pdf","comment":"21 pages. Under review"},{"id":"http://arxiv.org/abs/2410.01709v1","updated":"2024-10-02T16:16:05Z","published":"2024-10-02T16:16:05Z","title":"Meta-TTT: A Meta-learning Minimax Framework For Test-Time Training","summary":"  Test-time domain adaptation is a challenging task that aims to adapt a\npre-trained model to limited, unlabeled target data during inference. Current\nmethods that rely on self-supervision and entropy minimization underperform\nwhen the self-supervised learning (SSL) task does not align well with the\nprimary objective. Additionally, minimizing entropy can lead to suboptimal\nsolutions when there is limited diversity within minibatches. This paper\nintroduces a meta-learning minimax framework for test-time training on batch\nnormalization (BN) layers, ensuring that the SSL task aligns with the primary\ntask while addressing minibatch overfitting. We adopt a mixed-BN approach that\ninterpolates current test batch statistics with the statistics from source\ndomains and propose a stochastic domain synthesizing method to improve model\ngeneralization and robustness to domain shifts. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art techniques across\nvarious domain adaptation and generalization benchmarks, significantly\nenhancing the pre-trained model's robustness on unseen domains.\n","authors":["Chen Tao","Li Shen","Soumik Mondal"],"pdf_url":"https://arxiv.org/pdf/2410.01709v1.pdf","comment":"10 pages, 7 tables, 1 figure"},{"id":"http://arxiv.org/abs/2410.01706v1","updated":"2024-10-02T16:15:26Z","published":"2024-10-02T16:15:26Z","title":"Performant, Memory Efficient and Scalable Multi-Agent Reinforcement\n  Learning","summary":"  As the field of multi-agent reinforcement learning (MARL) progresses towards\nlarger and more complex environments, achieving strong performance while\nmaintaining memory efficiency and scalability to many agents becomes\nincreasingly important. Although recent research has led to several advanced\nalgorithms, to date, none fully address all of these key properties\nsimultaneously. In this work, we introduce Sable, a novel and theoretically\nsound algorithm that adapts the retention mechanism from Retentive Networks to\nMARL. Sable's retention-based sequence modelling architecture allows for\ncomputationally efficient scaling to a large number of agents, as well as\nmaintaining a long temporal context, making it well-suited for large-scale\npartially observable environments. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in the majority of tasks (34 out\nof 45, roughly 75\\%). Furthermore, Sable demonstrates stable performance as we\nscale the number of agents, handling environments with more than a thousand\nagents while exhibiting a linear increase in memory usage. Finally, we conduct\nablation studies to isolate the source of Sable's performance gains and confirm\nits efficient computational memory usage. Our results highlight Sable's\nperformance and efficiency, positioning it as a leading approach to MARL at\nscale.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09722v2","updated":"2024-10-02T16:14:09Z","published":"2024-07-12T23:29:54Z","title":"Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM\n  Inference","summary":"  Large language models (LLMs) have achieved remarkable success across diverse\ntasks, yet their inference processes are hindered by substantial time and\nenergy demands due to single-token generation at each decoding step. While\nprevious methods such as speculative decoding mitigate these inefficiencies by\nproducing multiple tokens per step, each token is still generated by its\nsingle-token distribution, thereby enhancing speed without improving\neffectiveness. In contrast, our work simultaneously enhances inference speed\nand improves the output effectiveness. We consider multi-token joint decoding\n(MTJD), which generates multiple tokens from their joint distribution at each\niteration, theoretically reducing perplexity and enhancing task performance.\nHowever, MTJD suffers from the high cost of sampling from the joint\ndistribution of multiple tokens. Inspired by speculative decoding, we introduce\nmulti-token assisted decoding (MTAD), a novel framework designed to accelerate\nMTJD. MTAD leverages a smaller auxiliary model to approximate the joint\ndistribution of a larger model, incorporating a verification mechanism that not\nonly ensures the accuracy of this approximation, but also improves the decoding\nefficiency over conventional speculative decoding. Theoretically, we\ndemonstrate that MTAD closely approximates exact MTJD with bounded error.\nEmpirical evaluations using Llama-2 and OPT models ranging from 13B to 70B\nparameters across various tasks reveal that MTAD reduces perplexity by 21.2%\nand improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42x speed-up and consumes 1.54x less energy than\nconventional speculative decoding methods. These results highlight MTAD's\nability to make multi-token joint decoding both effective and efficient,\npromoting more sustainable and high-performance deployment of LLMs.\n","authors":["Zongyue Qin","Ziniu Hu","Zifan He","Neha Prakriya","Jason Cong","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2407.09722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v3","updated":"2024-10-02T16:00:29Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel","Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:020. Expansion of\n  \"Dimensionality Reduction for Improving Out-of-Distribution Detection in\n  Medical Image Segmentation\" arXiv:2308.03723. Code available at\n  https://github.com/mckellwoodland/dimen_reduce_mahal\n  (https://zenodo.org/records/13881989)"},{"id":"http://arxiv.org/abs/2409.19546v3","updated":"2024-10-02T15:57:57Z","published":"2024-09-29T04:16:24Z","title":"Almost Sure Convergence of Average Reward Temporal Difference Learning","summary":"  Tabular average reward Temporal Difference (TD) learning is perhaps the\nsimplest and the most fundamental policy evaluation algorithm in average reward\nreinforcement learning. After at least 25 years since its discovery, we are\nfinally able to provide a long-awaited almost sure convergence analysis.\nNamely, we are the first to prove that, under very mild conditions, tabular\naverage reward TD converges almost surely to a sample path dependent fixed\npoint. Key to this success is a new general stochastic approximation result\nconcerning nonexpansive mappings with Markovian and additive noise, built on\nrecent advances in stochastic Krasnoselskii-Mann iterations.\n","authors":["Ethan Blaser","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19546v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01686v1","updated":"2024-10-02T15:55:08Z","published":"2024-10-02T15:55:08Z","title":"Positional Attention: Out-of-Distribution Generalization and\n  Expressivity for Neural Algorithmic Reasoning","summary":"  There has been a growing interest in the ability of neural networks to solve\nalgorithmic tasks, such as arithmetic, summary statistics, and sorting. While\nstate-of-the-art models like Transformers have demonstrated good generalization\nperformance on in-distribution tasks, their out-of-distribution (OOD)\nperformance is poor when trained end-to-end. In this paper, we focus on value\ngeneralization, a common instance of OOD generalization where the test\ndistribution has the same input sequence length as the training distribution,\nbut the value ranges in the training and test distributions do not necessarily\noverlap. To address this issue, we propose that using fixed positional\nencodings to determine attention weights-referred to as positional\nattention-enhances empirical OOD performance while maintaining expressivity. We\nsupport our claim about expressivity by proving that Transformers with\npositional attention can effectively simulate parallel algorithms.\n","authors":["Artur Back de Luca","George Giapitzakis","Shenghao Yang","Petar Velikovi","Kimon Fountoulakis"],"pdf_url":"https://arxiv.org/pdf/2410.01686v1.pdf","comment":"37 pages, 22 figures"},{"id":"http://arxiv.org/abs/2410.01680v1","updated":"2024-10-02T15:50:35Z","published":"2024-10-02T15:50:35Z","title":"PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation","summary":"  Various visual foundation models have distinct strengths and weaknesses, both\nof which can be improved through heterogeneous multi-teacher knowledge\ndistillation without labels, termed \"agglomerative models.\" We build upon this\nbody of work by studying the effect of the teachers' activation statistics,\nparticularly the impact of the loss function on the resulting student model\nquality. We explore a standard toolkit of statistical normalization techniques\nto better align the different distributions and assess their effects. Further,\nwe examine the impact on downstream teacher-matching metrics, which motivates\nthe use of Hadamard matrices. With these matrices, we demonstrate useful\nproperties, showing how they can be used for isotropic standardization, where\neach dimension of a multivariate distribution is standardized using the same\nscale. We call this technique \"PHI Standardization\" (PHI-S) and empirically\ndemonstrate that it produces the best student model across the suite of methods\nstudied.\n","authors":["Mike Ranzinger","Jon Barker","Greg Heinrich","Pavlo Molchanov","Bryan Catanzaro","Andrew Tao"],"pdf_url":"https://arxiv.org/pdf/2410.01680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01679v1","updated":"2024-10-02T15:49:30Z","published":"2024-10-02T15:49:30Z","title":"VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment","summary":"  Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.\n","authors":["Amirhossein Kazemnejad","Milad Aghajohari","Eva Portelance","Alessandro Sordoni","Siva Reddy","Aaron Courville","Nicolas Le Roux"],"pdf_url":"https://arxiv.org/pdf/2410.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06140v3","updated":"2024-10-02T15:43:43Z","published":"2022-10-12T12:48:25Z","title":"Differentially Private Bootstrap: New Privacy Analysis and Inference\n  Strategies","summary":"  Differentially private (DP) mechanisms protect individual-level information\nby introducing randomness into the statistical analysis procedure. Despite the\navailability of numerous DP tools, there remains a lack of general techniques\nfor conducting statistical inference under DP. We examine a DP bootstrap\nprocedure that releases multiple private bootstrap estimates to infer the\nsampling distribution and construct confidence intervals (CIs). Our privacy\nanalysis presents new results on the privacy cost of a single DP bootstrap\nestimate, applicable to any DP mechanism, and identifies some misapplications\nof the bootstrap in the existing literature. For the composition of the DP\nbootstrap, we present a numerical method to compute the exact privacy cost of\nreleasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP)\nframework (Dong et al., 2022), we show that the release of $B$ DP bootstrap\nestimates from mechanisms satisfying $(\\mu/\\sqrt{(2-2/\\mathrm{e})B})$-GDP\nasymptotically satisfies $\\mu$-GDP as $B$ goes to infinity. Then, we perform\nprivate statistical inference by post-processing the DP bootstrap estimates. We\nprove that our point estimates are consistent, our standard CIs are\nasymptotically valid, and both enjoy optimal convergence rates. To further\nimprove the finite performance, we use deconvolution with DP bootstrap\nestimates to accurately infer the sampling distribution. We derive CIs for\ntasks such as population mean estimation, logistic regression, and quantile\nregression, and we compare them to existing methods using simulations and\nreal-world experiments on 2016 Canada Census data. Our private CIs achieve the\nnominal coverage level and offer the first approach to private inference for\nquantile regression.\n","authors":["Zhanyu Wang","Guang Cheng","Jordan Awan"],"pdf_url":"https://arxiv.org/pdf/2210.06140v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01669v1","updated":"2024-10-02T15:37:12Z","published":"2024-10-02T15:37:12Z","title":"Sparse Covariance Neural Networks","summary":"  Covariance Neural Networks (VNNs) perform graph convolutions on the\ncovariance matrix of tabular data and achieve success in a variety of\napplications. However, the empirical covariance matrix on which the VNNs\noperate may contain many spurious correlations, making VNNs' performance\ninconsistent due to these noisy estimates and decreasing their computational\nefficiency. To tackle this issue, we put forth Sparse coVariance Neural\nNetworks (S-VNNs), a framework that applies sparsification techniques on the\nsample covariance matrix before convolution. When the true covariance matrix is\nsparse, we propose hard and soft thresholding to improve covariance estimation\nand reduce computational cost. Instead, when the true covariance is dense, we\npropose stochastic sparsification where data correlations are dropped in\nprobability according to principled strategies. We show that S-VNNs are more\nstable than nominal VNNs as well as sparse principal component analysis. By\nanalyzing the impact of sparsification on their behavior, we provide novel\nconnections between S-VNN stability and data distribution. We support our\ntheoretical findings with experimental results on various application\nscenarios, ranging from brain data to human action recognition, and show an\nimproved task performance, stability, and computational efficiency of S-VNNs\ncompared with nominal VNNs.\n","authors":["Andrea Cavallo","Zhan Gao","Elvin Isufi"],"pdf_url":"https://arxiv.org/pdf/2410.01669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01660v1","updated":"2024-10-02T15:26:52Z","published":"2024-10-02T15:26:52Z","title":"Conformal Generative Modeling with Improved Sample Efficiency through\n  Sequential Greedy Filtering","summary":"  Generative models lack rigorous statistical guarantees for their outputs and\nare therefore unreliable in safety-critical applications. In this work, we\npropose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a\nsequential conformal prediction method producing prediction sets that satisfy a\nrigorous statistical guarantee called conformal admissibility control. This\nguarantee states that with high probability, the prediction sets contain at\nleast one admissible (or valid) example. To this end, our method first samples\nan initial set of i.i.d. examples from a black box generative model. Then, this\nset is iteratively pruned via so-called greedy filters. As a consequence of the\niterative generation procedure, admissibility of the final prediction set\nfactorizes as a Markov chain. This factorization is crucial, because it allows\nto control each factor separately, using conformal prediction. In comparison to\nprior work, our method demonstrates a large reduction in the number of\nadmissibility evaluations during calibration. This reduction is important in\nsafety-critical applications, where these evaluations must be conducted\nmanually by domain experts and are therefore costly and time consuming. We\nhighlight the advantages of our method in terms of admissibility evaluations\nand cardinality of the prediction sets through experiments in natural language\ngeneration and molecular graph extension tasks.\n","authors":["Klaus-Rudolf Kladny","Bernhard Schlkopf","Michael Muehlebach"],"pdf_url":"https://arxiv.org/pdf/2410.01660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01658v1","updated":"2024-10-02T15:25:26Z","published":"2024-10-02T15:25:26Z","title":"Smaller Confidence Intervals From IPW Estimators via Data-Dependent\n  Coarsening","summary":"  Inverse propensity-score weighted (IPW) estimators are prevalent in causal\ninference for estimating average treatment effects in observational studies.\nUnder unconfoundedness, given accurate propensity scores and $n$ samples, the\nsize of confidence intervals of IPW estimators scales down with $n$, and,\nseveral of their variants improve the rate of scaling. However, neither IPW\nestimators nor their variants are robust to inaccuracies: even if a single\ncovariate has an $\\varepsilon>0$ additive error in the propensity score, the\nsize of confidence intervals of these estimators can increase arbitrarily.\nMoreover, even without errors, the rate with which the confidence intervals of\nthese estimators go to zero with $n$ can be arbitrarily slow in the presence of\nextreme propensity scores (those close to 0 or 1).\n  We introduce a family of Coarse IPW (CIPW) estimators that captures existing\nIPW estimators and their variants. Each CIPW estimator is an IPW estimator on a\ncoarsened covariate space, where certain covariates are merged. Under mild\nassumptions, e.g., Lipschitzness in expected outcomes and sparsity of extreme\npropensity scores, we give an efficient algorithm to find a robust estimator:\ngiven $\\varepsilon$-inaccurate propensity scores and $n$ samples, its\nconfidence interval size scales with $\\varepsilon+1/\\sqrt{n}$. In contrast,\nunder the same assumptions, existing estimators' confidence interval sizes are\n$\\Omega(1)$ irrespective of $\\varepsilon$ and $n$. Crucially, our estimator is\ndata-dependent and we show that no data-independent CIPW estimator can be\nrobust to inaccuracies.\n","authors":["Alkis Kalavasis","Anay Mehrotra","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2410.01658v1.pdf","comment":"Accepted for presentation at the 37th Conference on Learning Theory\n  (COLT) 2024"},{"id":"http://arxiv.org/abs/2410.01657v1","updated":"2024-10-02T15:22:27Z","published":"2024-10-02T15:22:27Z","title":"Scalable and Consistent Graph Neural Networks for Distributed Mesh-based\n  Data-driven Modeling","summary":"  This work develops a distributed graph neural network (GNN) methodology for\nmesh-based modeling applications using a consistent neural message passing\nlayer. As the name implies, the focus is on enabling scalable operations that\nsatisfy physical consistency via halo nodes at sub-graph boundaries. Here,\nconsistency refers to the fact that a GNN trained and evaluated on one rank\n(one large graph) is arithmetically equivalent to evaluations on multiple ranks\n(a partitioned graph). This concept is demonstrated by interfacing GNNs with\nNekRS, a GPU-capable exascale CFD solver developed at Argonne National\nLaboratory. It is shown how the NekRS mesh partitioning can be linked to the\ndistributed GNN training and inference routines, resulting in a scalable\nmesh-based data-driven modeling workflow. We study the impact of consistency on\nthe scalability of mesh-based GNNs, demonstrating efficient scaling in\nconsistent GNNs for up to O(1B) graph nodes on the Frontier exascale\nsupercomputer.\n","authors":["Shivam Barwey","Riccardo Balin","Bethany Lusch","Saumil Patel","Ramesh Balakrishnan","Pinaki Pal","Romit Maulik","Venkatram Vishwanath"],"pdf_url":"https://arxiv.org/pdf/2410.01657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01656v1","updated":"2024-10-02T15:21:07Z","published":"2024-10-02T15:21:07Z","title":"Efficient Statistics With Unknown Truncation, Polynomial Time\n  Algorithms, Beyond Gaussians","summary":"  We study the estimation of distributional parameters when samples are shown\nonly if they fall in some unknown set $S \\subseteq \\mathbb{R}^d$. Kontonis,\nTzamos, and Zampetakis (FOCS'19) gave a $d^{\\mathrm{poly}(1/\\varepsilon)}$ time\nalgorithm for finding $\\varepsilon$-accurate parameters for the special case of\nGaussian distributions with diagonal covariance matrix. Recently, Diakonikolas,\nKane, Pittas, and Zarifis (COLT'24) showed that this exponential dependence on\n$1/\\varepsilon$ is necessary even when $S$ belongs to some well-behaved\nclasses. These works leave the following open problems which we address in this\nwork: Can we estimate the parameters of any Gaussian or even extend beyond\nGaussians? Can we design $\\mathrm{poly}(d/\\varepsilon)$ time algorithms when\n$S$ is a simple set such as a halfspace?\n  We make progress on both of these questions by providing the following\nresults:\n  1. Toward the first question, we give a $d^{\\mathrm{poly}(\\ell/\\varepsilon)}$\ntime algorithm for any exponential family that satisfies some structural\nassumptions and any unknown set $S$ that is $\\varepsilon$-approximable by\ndegree-$\\ell$ polynomials. This result has two important applications:\n  1a) The first algorithm for estimating arbitrary Gaussian distributions from\nsamples truncated to an unknown $S$; and\n  1b) The first algorithm for linear regression with unknown truncation and\nGaussian features.\n  2. To address the second question, we provide an algorithm with runtime\n$\\mathrm{poly}(d/\\varepsilon)$ that works for a set of exponential families\n(containing all Gaussians) when $S$ is a halfspace or an axis-aligned\nrectangle.\n  Along the way, we develop tools that may be of independent interest,\nincluding, a reduction from PAC learning with positive and unlabeled samples to\nPAC learning with positive and negative samples that is robust to certain\ncovariate shifts.\n","authors":["Jane H. Lee","Anay Mehrotra","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2410.01656v1.pdf","comment":"Accepted for presentation at the 65th IEEE Symposium on Foundations\n  of Computer Science (FOCS), 2024; abstract shortened for arXiv"},{"id":"http://arxiv.org/abs/2410.01655v1","updated":"2024-10-02T15:19:35Z","published":"2024-10-02T15:19:35Z","title":"Extending Contextual Self-Modulation: Meta-Learning Across Modalities,\n  Task Dimensionalities, and Data Regimes","summary":"  Contextual Self-Modulation (CSM) is a potent regularization mechanism for the\nNeural Context Flow (NCF) framework which demonstrates powerful meta-learning\nof physical systems. However, CSM has limitations in its applicability across\ndifferent modalities and in high-data regimes. In this work, we introduce two\nextensions: $i$CSM, which expands CSM to infinite-dimensional tasks, and\nStochasticNCF, which improves scalability. These extensions are demonstrated\nthrough comprehensive experimentation on a range of tasks, including dynamical\nsystems with parameter variations, computer vision challenges, and curve\nfitting problems. $i$CSM embeds the contexts into an infinite-dimensional\nfunction space, as opposed to CSM which uses finite-dimensional context\nvectors. StochasticNCF enables the application of both CSM and $i$CSM to\nhigh-data scenarios by providing an unbiased approximation of meta-gradient\nupdates through a sampled set of nearest environments. Additionally, we\nincorporate higher-order Taylor expansions via Taylor-Mode automatic\ndifferentiation, revealing that higher-order approximations do not necessarily\nenhance generalization. Finally, we demonstrate how CSM can be integrated into\nother meta-learning frameworks with FlashCAVIA, a computationally efficient\nextension of the CAVIA meta-learning framework (Zintgraf et al. 2019).\nFlashCAVIA outperforms its predecessor across various benchmarks and reinforces\nthe utility of bi-level optimization techniques. Together, these contributions\nestablish a robust framework for tackling an expanded spectrum of meta-learning\ntasks, offering practical insights for out-of-distribution generalization. Our\nopen-sourced library, designed for flexible integration of self-modulation into\ncontextual meta-learning workflows, is available at\n\\url{github.com/ddrous/self-mod}.\n","authors":["Roussel Desmond Nzoyem","David A. W. Barton","Tom Deakin"],"pdf_url":"https://arxiv.org/pdf/2410.01655v1.pdf","comment":"23 pages, 11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2405.02154v3","updated":"2024-10-02T15:18:44Z","published":"2024-05-03T15:02:21Z","title":"Neural Context Flows for Meta-Learning of Dynamical Systems","summary":"  Neural Ordinary Differential Equations (NODEs) often struggle to adapt to new\ndynamic behaviors caused by parameter changes in the underlying system, even\nwhen these dynamics are similar to previously observed behaviors. This problem\nbecomes more challenging when the changing parameters are unobserved, meaning\ntheir value or influence cannot be directly measured when collecting data. To\naddress this issue, we introduce Neural Context Flow (NCF), a robust and\ninterpretable Meta-Learning framework that includes uncertainty estimation. NCF\nuses higher-order Taylor expansion to enable contextual self-modulation,\nallowing context vectors to influence dynamics from other domains while also\nmodulating themselves. After establishing convergence guarantees, we\nempirically test NCF and compare it to related adaptation methods. Our results\nshow that NCF achieves state-of-the-art Out-of-Distribution performance on 5\nout of 6 linear and non-linear benchmark problems. Through extensive\nexperiments, we explore the flexible model architecture of NCF and the encoded\nrepresentations within the learned context vectors. Our findings highlight the\npotential implications of NCF for foundational models in the physical sciences,\noffering a promising approach to improving the adaptability and generalization\nof NODEs in various scientific applications. Our code is openly available at\n\\url{https://github.com/ddrous/ncflow}.\n","authors":["Roussel Desmond Nzoyem","David A. W. Barton","Tom Deakin"],"pdf_url":"https://arxiv.org/pdf/2405.02154v3.pdf","comment":"31 pages, 19 figures, 8 tables"},{"id":"http://arxiv.org/abs/2112.00600v3","updated":"2024-10-02T15:17:56Z","published":"2021-12-01T16:14:49Z","title":"Towards Futuristic Autonomous Experimentation--A Surprise-Reacting\n  Sequential Experiment Policy","summary":"  An autonomous experimentation platform in manufacturing is supposedly capable\nof conducting a sequential search for finding suitable manufacturing conditions\nby itself or even for discovering new materials with minimal human\nintervention. The core of the intelligent control of such platforms is a policy\nto decide where to conduct the next experiment based on what has been done thus\nfar. Such policy inevitably trades off between exploitation and exploration.\nCurrently, the prevailing approach is to use various acquisition functions in\nthe Bayesian optimization framework. We discuss whether it is beneficial to\ntrade off exploitation versus exploration by measuring the element and degree\nof surprise associated with the immediate past observation. We devise a\nsurprise-reacting policy using two existing surprise metrics, known as the\nShannon surprise and Bayesian surprise. Our analysis shows that the\nsurprise-reacting policy appears to be better suited for quickly characterizing\nthe overall landscape of a response surface under resource constraints. We do\nnot claim that we have a fully autonomous experimentation system but believe\nthat the surprise-reacting capability benefits the automation of sequential\ndecisions in autonomous experimentation.\n","authors":["Imtiaz Ahmed","Satish Bukkapatnam","Bhaskar Botcha","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2112.00600v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01649v1","updated":"2024-10-02T15:16:53Z","published":"2024-10-02T15:16:53Z","title":"shapiq: Shapley Interactions for Machine Learning","summary":"  Originally rooted in game theory, the Shapley Value (SV) has recently become\nan important tool in machine learning research. Perhaps most notably, it is\nused for feature attribution and data valuation in explainable artificial\nintelligence. Shapley Interactions (SIs) naturally extend the SV and address\nits limitations by assigning joint contributions to groups of entities, which\nenhance understanding of black box machine learning models. Due to the\nexponential complexity of computing SVs and SIs, various methods have been\nproposed that exploit structural assumptions or yield probabilistic estimates\ngiven limited resources. In this work, we introduce shapiq, an open-source\nPython package that unifies state-of-the-art algorithms to efficiently compute\nSVs and any-order SIs in an application-agnostic framework. Moreover, it\nincludes a benchmarking suite containing 11 machine learning applications of\nSIs with pre-computed games and ground-truth values to systematically assess\ncomputational performance across domains. For practitioners, shapiq is able to\nexplain and visualize any-order feature interactions in predictions of models,\nincluding vision transformers, language models, as well as XGBoost and LightGBM\nwith TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and\nconsolidate the application of SVs and SIs in machine learning that facilitates\nfuture research. The source code and documentation are available at\nhttps://github.com/mmschlk/shapiq.\n","authors":["Maximilian Muschalik","Hubert Baniecki","Fabian Fumagalli","Patrick Kolpaczki","Barbara Hammer","Eyke Hllermeier"],"pdf_url":"https://arxiv.org/pdf/2410.01649v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2307.01181v2","updated":"2024-10-02T15:13:40Z","published":"2023-07-03T17:46:23Z","title":"Fitting an ellipsoid to a quadratic number of random points","summary":"  We consider the problem $(\\mathrm{P})$ of fitting $n$ standard Gaussian\nrandom vectors in $\\mathbb{R}^d$ to the boundary of a centered ellipsoid, as\n$n, d \\to \\infty$. This problem is conjectured to have a sharp feasibility\ntransition: for any $\\varepsilon > 0$, if $n \\leq (1 - \\varepsilon) d^2 / 4$\nthen $(\\mathrm{P})$ has a solution with high probability, while $(\\mathrm{P})$\nhas no solutions with high probability if $n \\geq (1 + \\varepsilon) d^2 /4$. So\nfar, only a trivial bound $n \\geq d^2 / 2$ is known on the negative side, while\nthe best results on the positive side assume $n \\leq d^2 /\n\\mathrm{polylog}(d)$. In this work, we improve over previous approaches using a\nkey result of Bartl & Mendelson (2022) on the concentration of Gram matrices of\nrandom vectors under mild assumptions on their tail behavior. This allows us to\ngive a simple proof that $(\\mathrm{P})$ is feasible with high probability when\n$n \\leq d^2 / C$, for a (possibly large) constant $C > 0$.\n","authors":["Afonso S. Bandeira","Antoine Maillard","Shahar Mendelson","Elliot Paquette"],"pdf_url":"https://arxiv.org/pdf/2307.01181v2.pdf","comment":"17 pages; Update (v2) to match the published version"},{"id":"http://arxiv.org/abs/2410.01644v1","updated":"2024-10-02T15:13:26Z","published":"2024-10-02T15:13:26Z","title":"A Novel Framework of Horizontal-Vertical Hybrid Federated Learning for\n  EdgeIoT","summary":"  This letter puts forth a new hybrid horizontal-vertical federated learning\n(HoVeFL) for mobile edge computing-enabled Internet of Things (EdgeIoT). In\nthis framework, certain EdgeIoT devices train local models using the same data\nsamples but analyze disparate data features, while the others focus on the same\nfeatures using non-independent and identically distributed (non-IID) data\nsamples. Thus, even though the data features are consistent, the data samples\nvary across devices. The proposed HoVeFL formulates the training of local and\nglobal models to minimize the global loss function. Performance evaluations on\nCIFAR-10 and SVHN datasets reveal that the testing loss of HoVeFL with 12\nhorizontal FL devices and six vertical FL devices is 5.5% and 25.2% higher,\nrespectively, compared to a setup with six horizontal FL devices and 12\nvertical FL devices.\n","authors":["Kai Li","Yilei Liang","Xin Yuan","Wei Ni","Jon Crowcroft","Chau Yuen","Ozgur B. Akan"],"pdf_url":"https://arxiv.org/pdf/2410.01644v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.01643v1","updated":"2024-10-02T15:13:25Z","published":"2024-10-02T15:13:25Z","title":"Stable Offline Value Function Learning with Bisimulation-based\n  Representations","summary":"  In reinforcement learning, offline value function learning is the procedure\nof using an offline dataset to estimate the expected discounted return from\neach state when taking actions according to a fixed target policy. The\nstability of this procedure, i.e., whether it converges to its fixed-point,\ncritically depends on the representations of the state-action pairs. Poorly\nlearned representations can make value function learning unstable, or even\ndivergent. Therefore, it is critical to stabilize value function learning by\nexplicitly shaping the state-action representations. Recently, the class of\nbisimulation-based algorithms have shown promise in shaping representations for\ncontrol. However, it is still unclear if this class of methods can stabilize\nvalue function learning. In this work, we investigate this question and answer\nit affirmatively. We introduce a bisimulation-based algorithm called kernel\nrepresentations for offline policy evaluation (KROPE). KROPE uses a kernel to\nshape state-action representations such that state-action pairs that have\nsimilar immediate rewards and lead to similar next state-action pairs under the\ntarget policy also have similar representations. We show that KROPE: 1) learns\nstable representations and 2) leads to lower value error than baselines. Our\nanalysis provides new theoretical insight into the stability properties of\nbisimulation-based methods and suggests that practitioners can use these\nmethods for stable and accurate evaluation of offline reinforcement learning\nagents.\n","authors":["Brahma S. Pavse","Yudong Chen","Qiaomin Xie","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2410.01643v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.01639v1","updated":"2024-10-02T15:09:36Z","published":"2024-10-02T15:09:36Z","title":"Moral Alignment for LLM Agents","summary":"  Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2410.01639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01635v1","updated":"2024-10-02T15:07:13Z","published":"2024-10-02T15:07:13Z","title":"Does Graph Prompt Work? A Data Operation Perspective with Theoretical\n  Analysis","summary":"  In recent years, graph prompting has emerged as a promising research\ndirection, enabling the learning of additional tokens or subgraphs appended to\nthe original graphs without requiring retraining of pre-trained graph models\nacross various applications. This novel paradigm, shifting from the traditional\npretraining and finetuning to pretraining and prompting has shown significant\nempirical success in simulating graph data operations, with applications\nranging from recommendation systems to biological networks and graph\ntransferring. However, despite its potential, the theoretical underpinnings of\ngraph prompting remain underexplored, raising critical questions about its\nfundamental effectiveness. The lack of rigorous theoretical proof of why and\nhow much it works is more like a dark cloud over the graph prompt area to go\nfurther. To fill this gap, this paper introduces a theoretical framework that\nrigorously analyzes graph prompting from a data operation perspective. Our\ncontributions are threefold: First, we provide a formal guarantee theorem,\ndemonstrating graph prompts capacity to approximate graph transformation\noperators, effectively linking upstream and downstream tasks. Second, we derive\nupper bounds on the error of these data operations by graph prompts for a\nsingle graph and extend this discussion to batches of graphs, which are common\nin graph model training. Third, we analyze the distribution of data operation\nerrors, extending our theoretical findings from linear graph models (e.g., GCN)\nto non-linear graph models (e.g., GAT). Extensive experiments support our\ntheoretical results and confirm the practical implications of these guarantees.\n","authors":["Qunzhong Wang","Xiangguo Sun","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07480v2","updated":"2024-10-02T15:02:13Z","published":"2024-09-02T10:03:03Z","title":"EEG-Language Modeling for Pathology Detection","summary":"  Multimodal language modeling constitutes a recent breakthrough which\nleverages advances in large language models to pretrain capable multimodal\nmodels. The integration of natural language during pretraining has been shown\nto significantly improve learned representations, particularly in computer\nvision. However, the efficacy of multimodal language modeling in the realm of\nfunctional brain data, specifically for advancing pathology detection, remains\nunexplored. This study pioneers EEG-language models trained on clinical reports\nand 15000 EEGs. We extend methods for multimodal alignment to this novel domain\nand investigate which textual information in reports is useful for training\nEEG-language models. Our results indicate that models learn richer\nrepresentations from being exposed to a variety of report segments, including\nthe patient's clinical history, description of the EEG, and the physician's\ninterpretation. Compared to models exposed to narrower clinical text\ninformation, we find such models to retrieve EEGs based on clinical reports\n(and vice versa) with substantially higher accuracy. Yet, this is only observed\nwhen using a contrastive learning approach. Particularly in regimes with few\nannotations, we observe that representations of EEG-language models can\nsignificantly improve pathology detection compared to those of EEG-only models,\nas demonstrated by both zero-shot classification and linear probes. In sum,\nthese results highlight the potential of integrating brain activity data with\nclinical text, suggesting that EEG-language models represent significant\nprogress for clinical applications.\n","authors":["Sam Gijsen","Kerstin Ritter"],"pdf_url":"https://arxiv.org/pdf/2409.07480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01623v1","updated":"2024-10-02T14:58:27Z","published":"2024-10-02T14:58:27Z","title":"Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank\n  Constraint?","summary":"  Low-rank training has emerged as a promising approach for reducing memory\nusage in training Large Language Models (LLMs). Previous methods either rely on\ndecomposing weight matrices (e.g., LoRA), or seek to decompose gradient\nmatrices (e.g., GaLore) to ensure reduced memory consumption. However, both of\nthem constrain the training in a low-rank subspace, thus inevitably leading to\nsub-optimal performance. This raises a question: whether it is possible to\nconsistently preserve the low-rank constraint for memory efficiency, while\nachieving full-rank training (i.e., training with full-rank gradients of\nfull-rank weights) to avoid inferior outcomes? In this paper, we propose a new\nplug-and-play training framework for LLMs called Fira, as the first attempt to\nachieve this goal. First, we observe an interesting phenomenon during LLM\ntraining: the scaling impact of adaptive optimizers (e.g., Adam) on the\ngradient norm remains similar from low-rank to full-rank training. Based on\nthis observation, we propose a norm-based scaling method, which utilizes the\nscaling impact of low-rank optimizers as substitutes for that of original\nfull-rank optimizers to enable full-rank training. In this way, we can preserve\nthe low-rank constraint in the optimizer while achieving full-rank training for\nbetter performance. Moreover, we find that there are sudden gradient rises\nduring the optimization process, potentially causing loss spikes. To address\nthis, we further put forward a norm-growth limiter to smooth the gradient via\nregulating the relative increase of gradient norms. Extensive experiments on\nthe pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA\nand GaLore, achieving performance that is comparable to or even better than\nfull-rank training.\n","authors":["Xi Chen","Kaituo Feng","Changsheng Li","Xunhao Lai","Xiangyu Yue","Ye Yuan","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2410.01623v1.pdf","comment":"Code is available at: https://github.com/xichen-fy/Fira"},{"id":"http://arxiv.org/abs/2405.14953v3","updated":"2024-10-02T14:56:33Z","published":"2024-05-23T18:01:11Z","title":"MallowsPO: Fine-Tune Your LLM with Preference Dispersions","summary":"  Direct Preference Optimization (DPO) has recently emerged as a popular\napproach to improve reinforcement learning with human feedback (RLHF), leading\nto better techniques to fine-tune large language models (LLM). A weakness of\nDPO, however, lies in its lack of capability to characterize the diversity of\nhuman preferences. Inspired by Mallows' theory of preference ranking, we\ndevelop in this paper a new approach, the MallowsPO. A distinct feature of this\napproach is a dispersion index, which reflects the dispersion of human\npreference to prompts. We show that existing DPO models can be reduced to\nspecial cases of this dispersion index, thus unified with MallowsPO. More\nimportantly, we demonstrate (empirically) how to use this dispersion index to\nenhance the performance of DPO in a broad array of benchmark tasks, from\nsynthetic bandit selection to controllable generations and dialogues, while\nmaintaining great generalization capabilities. MallowsPO is also compatible\nwith other SOTA offline preference optimization methods, boosting nearly 2\\%\nextra LC win rate when used as a plugin for fine-tuning Llama3-Instruct.\n","authors":["Haoxian Chen","Hanyang Zhao","Henry Lam","David Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2405.14953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01617v1","updated":"2024-10-02T14:56:21Z","published":"2024-10-02T14:56:21Z","title":"On Using Certified Training towards Empirical Robustness","summary":"  Adversarial training is arguably the most popular way to provide empirical\nrobustness against specific adversarial examples. While variants based on\nmulti-step attacks incur significant computational overhead, single-step\nvariants are vulnerable to a failure mode known as catastrophic overfitting,\nwhich hinders their practical utility for large perturbations. A parallel line\nof work, certified training, has focused on producing networks amenable to\nformal guarantees of robustness against any possible attack. However, the wide\ngap between the best-performing empirical and certified defenses has severely\nlimited the applicability of the latter. Inspired by recent developments in\ncertified training, which rely on a combination of adversarial attacks with\nnetwork over-approximations, and by the connections between local linearity and\ncatastrophic overfitting, we present experimental evidence on the practical\nutility and limitations of using certified training towards empirical\nrobustness. We show that, when tuned for the purpose, a recent certified\ntraining algorithm can prevent catastrophic overfitting on single-step attacks,\nand that it can bridge the gap to multi-step baselines under appropriate\nexperimental settings. Finally, we present a novel regularizer for network\nover-approximations that can achieve similar effects while markedly reducing\nruntime.\n","authors":["Alessandro De Palma","Serge Durand","Zakaria Chihani","Franois Terrier","Caterina Urban"],"pdf_url":"https://arxiv.org/pdf/2410.01617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03869v2","updated":"2024-10-02T14:52:13Z","published":"2024-04-05T03:02:57Z","title":"Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable\n  Collaboration","summary":"  The emergence of multi-agent reinforcement learning (MARL) is significantly\ntransforming various fields like autonomous vehicle networks. However,\nreal-world multi-agent systems typically contain multiple roles, and the scale\nof these systems dynamically fluctuates. Consequently, in order to achieve\nzero-shot scalable collaboration, it is essential that strategies for different\nroles can be updated flexibly according to the scales, which is still a\nchallenge for current MARL frameworks. To address this, we propose a novel MARL\nframework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. We first leverage a latent network to learn strategy patterns for\neach agent adaptively. Second, we introduce a heterogeneous layer to be\ninserted into decision-making networks, whose parameters are specifically\ngenerated by the learned latent variables. Our approach is scalable as all the\nparameters are shared except for the heterogeneous layer, and gains both\ninter-individual and temporal heterogeneity, allowing SHPPO to adapt\neffectively to varying scales. SHPPO exhibits superior performance in classic\nMARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google\nResearch Football (GRF), showcasing enhanced zero-shot scalability, and\noffering insights into the learned latent variables' impact on team performance\nby visualization.\n","authors":["Xudong Guo","Daming Shi","Junjie Yu","Wenhui Fan"],"pdf_url":"https://arxiv.org/pdf/2404.03869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01611v1","updated":"2024-10-02T14:49:05Z","published":"2024-10-02T14:49:05Z","title":"DRUPI: Dataset Reduction Using Privileged Information","summary":"  Dataset reduction (DR) seeks to select or distill samples from large datasets\ninto smaller subsets while preserving performance on target tasks. Existing\nmethods primarily focus on pruning or synthesizing data in the same format as\nthe original dataset, typically the input data and corresponding labels.\nHowever, in DR settings, we find it is possible to synthesize more information\nbeyond the data-label pair as an additional learning target to facilitate model\ntraining. In this paper, we introduce Dataset Reduction Using Privileged\nInformation (DRUPI), which enriches DR by synthesizing privileged information\nalongside the reduced dataset. This privileged information can take the form of\nfeature labels or attention labels, providing auxiliary supervision to improve\nmodel learning. Our findings reveal that effective feature labels must balance\nbetween being overly discriminative and excessively diverse, with a moderate\nlevel proving optimal for improving the reduced dataset's efficacy. Extensive\nexperiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI\nintegrates seamlessly with existing dataset reduction methods, offering\nsignificant performance gains.\n","authors":["Shaobo Wang","Yantai Yang","Shuaiyu Zhang","Chenghao Sun","Weiya Li","Xuming Hu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01606v1","updated":"2024-10-02T14:47:05Z","published":"2024-10-02T14:47:05Z","title":"Automated Red Teaming with GOAT: the Generative Offensive Agent Tester","summary":"  Red teaming assesses how large language models (LLMs) can produce content\nthat violates norms, policies, and rules set during their safety training.\nHowever, most existing automated methods in the literature are not\nrepresentative of the way humans tend to interact with AI models. Common users\nof AI models may not have advanced knowledge of adversarial machine learning\nmethods or access to model internals, and they do not spend a lot of time\ncrafting a single highly effective adversarial prompt. Instead, they are likely\nto make use of techniques commonly shared online and exploit the multiturn\nconversational nature of LLMs. While manual testing addresses this gap, it is\nan inefficient and often expensive process. To address these limitations, we\nintroduce the Generative Offensive Agent Tester (GOAT), an automated agentic\nred teaming system that simulates plain language adversarial conversations\nwhile leveraging multiple adversarial prompting techniques to identify\nvulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by\nprompting a general-purpose model in a way that encourages reasoning through\nthe choices of methods available, the current target model's response, and the\nnext steps. Our approach is designed to be extensible and efficient, allowing\nhuman testers to focus on exploring new areas of risk while automation covers\nthe scaled adversarial stress-testing of known risk territory. We present the\ndesign and evaluation of GOAT, demonstrating its effectiveness in identifying\nvulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama\n3.1 and 88% against GPT-4 on the JailbreakBench dataset.\n","authors":["Maya Pavlova","Erik Brinkman","Krithika Iyer","Vitor Albiero","Joanna Bitton","Hailey Nguyen","Joe Li","Cristian Canton Ferrer","Ivan Evtimov","Aaron Grattafiori"],"pdf_url":"https://arxiv.org/pdf/2410.01606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01600v1","updated":"2024-10-02T14:39:13Z","published":"2024-10-02T14:39:13Z","title":"ENTP: Encoder-only Next Token Prediction","summary":"  Next-token prediction models have predominantly relied on decoder-only\nTransformers with causal attention, driven by the common belief that causal\nattention is essential to prevent \"cheating\" by masking future tokens. We\nchallenge this widely accepted notion and argue that this design choice is\nabout efficiency rather than necessity. While decoder-only Transformers are\nstill a good choice for practical reasons, they are not the only viable option.\nIn this work, we introduce Encoder-only Next Token Prediction (ENTP). We\nexplore the differences between ENTP and decoder-only Transformers in\nexpressive power and complexity, highlighting potential advantages of ENTP. We\nintroduce the Triplet-Counting task and show, both theoretically and\nexperimentally, that while ENTP can perform this task easily, a decoder-only\nTransformer cannot. Finally, we empirically demonstrate ENTP's superior\nperformance across various realistic tasks, such as length generalization and\nin-context learning.\n","authors":["Ethan Ewer","Daewon Chae","Thomas Zeng","Jinkyu Kim","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2410.01600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01599v1","updated":"2024-10-02T14:38:37Z","published":"2024-10-02T14:38:37Z","title":"Towards Model Discovery Using Domain Decomposition and PINNs","summary":"  We enhance machine learning algorithms for learning model parameters in\ncomplex systems represented by ordinary differential equations (ODEs) with\ndomain decomposition methods. The study evaluates the performance of two\napproaches, namely (vanilla) Physics-Informed Neural Networks (PINNs) and\nFinite Basis Physics-Informed Neural Networks (FBPINNs), in learning the\ndynamics of test models with a quasi-stationary longtime behavior. We test the\napproaches for data sets in different dynamical regions and with varying noise\nlevel. As results, we find a better performance for the FBPINN approach\ncompared to the vanilla PINN approach, even in cases with data from only a\nquasi-stationary time domain with few dynamics.\n","authors":["Tirtho S. Saha","Alexander Heinlein","Cordula Reisch"],"pdf_url":"https://arxiv.org/pdf/2410.01599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17943v2","updated":"2024-10-02T14:37:12Z","published":"2024-02-27T23:52:58Z","title":"Sequential transport maps using SoS density estimation and\n  $$-divergences","summary":"  Transport-based density estimation methods are receiving growing interest\nbecause of their ability to efficiently generate samples from the approximated\ndensity. We further invertigate the sequential transport maps framework\nproposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of\ncomposed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first\nestimating an intermediate density of moderate complexity, and then by\ncomputing the exact KR map from a reference density to the precomputed\napproximate density. In our work, we explore the use of Sum-of-Squares (SoS)\ndensities and $\\alpha$-divergences for approximating the intermediate\ndensities. Combining SoS densities with $\\alpha$-divergence interestingly\nyields convex optimization problems which can be efficiently solved using\nsemidefinite programming. The main advantage of $\\alpha$-divergences is to\nenable working with unnormalized densities, which provides benefits both\nnumerically and theoretically. In particular, we provide a new convergence\nanalyses of the sequential transport maps based on information geometric\nproperties of $\\alpha$-divergences. The choice of intermediate densities is\nalso crucial for the efficiency of the method. While tempered (or annealed)\ndensities are the state-of-the-art, we introduce diffusion-based intermediate\ndensities which permits to approximate densities known from samples only. Such\nintermediate densities are well-established in machine learning for generative\nmodeling. Finally we propose low-dimensional maps (or lazy maps) for dealing\nwith high-dimensional problems and numerically demonstrate our methods on\nBayesian inference problems and unsupervised learning tasks.\n","authors":["Benjamin Zanger","Olivier Zahm","Tiangang Cui","Martin Schreiber"],"pdf_url":"https://arxiv.org/pdf/2402.17943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01597v1","updated":"2024-10-02T14:34:45Z","published":"2024-10-02T14:34:45Z","title":"SAFE: Semantic Adaptive Feature Extraction with Rate Control for 6G\n  Wireless Communications","summary":"  Most current Deep Learning-based Semantic Communication (DeepSC) systems are\ndesigned and trained exclusively for particular single-channel conditions,\nwhich restricts their adaptability and overall bandwidth utilization. To\naddress this, we propose an innovative Semantic Adaptive Feature Extraction\n(SAFE) framework, which significantly improves bandwidth efficiency by allowing\nusers to select different sub-semantic combinations based on their channel\nconditions. This paper also introduces three advanced learning algorithms to\noptimize the performance of SAFE framework as a whole. Through a series of\nsimulation experiments, we demonstrate that the SAFE framework can effectively\nand adaptively extract and transmit semantics under different channel bandwidth\nconditions, of which effectiveness is verified through objective and subjective\nquality evaluations.\n","authors":["Yuna Yan","Lixin Li","Xin Zhang","Wensheng Lin","Wenchi Cheng","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2410.01597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15317v3","updated":"2024-10-02T14:34:08Z","published":"2024-05-24T07:59:02Z","title":"NuwaTS: a Foundation Model Mending Every Incomplete Time Series","summary":"  Time series imputation is critical for many real-world applications and has\nbeen widely studied. However, existing models often require specialized designs\ntailored to specific missing patterns, variables, or domains which limits their\ngeneralizability. In addition, current evaluation frameworks primarily focus on\ndomain-specific tasks and often rely on time-wise train/validation/test data\nsplits, which fail to rigorously assess a model's ability to generalize across\nunseen variables or domains. In this paper, we present \\textbf{NuwaTS}, a novel\nframework that repurposes Pre-trained Language Models (PLMs) for general time\nseries imputation. Once trained, NuwaTS can be applied to impute missing data\nacross any domain. We introduce specialized embeddings for each sub-series\npatch, capturing information about the patch, its missing data patterns, and\nits statistical characteristics. By combining contrastive learning with the\nimputation task, we train PLMs to create a versatile, one-for-all imputation\nmodel. Additionally, we employ a plug-and-play fine-tuning approach, enabling\nefficient adaptation to domain-specific tasks with minimal adjustments. To\nevaluate cross-variable and cross-domain generalization, we propose a new\nbenchmarking protocol that partitions the datasets along the variable\ndimension. Experimental results on over seventeen million time series samples\nfrom diverse domains demonstrate that NuwaTS outperforms state-of-the-art\ndomain-specific models across various datasets under the proposed benchmarking\nprotocol. Furthermore, we show that NuwaTS generalizes to other time series\ntasks, such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.\n","authors":["Jinguo Cheng","Chunwei Yang","Wanlin Cai","Yuxuan Liang","Qingsong Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2405.15317v3.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2407.14207v5","updated":"2024-10-02T14:32:59Z","published":"2024-07-19T11:12:08Z","title":"Longhorn: State Space Models are Amortized Online Learners","summary":"  Modern large language models are built on sequence modeling via next-token\nprediction. While the Transformer remains the dominant architecture for\nsequence modeling, its quadratic decoding complexity in sequence length poses a\nmajor limitation. State-space models (SSMs) present a competitive alternative,\noffering linear decoding efficiency while maintaining parallelism during\ntraining. However, most existing SSMs rely on linear recurrence designs that\nappear somewhat ad hoc. In this work, we explore SSM design through the lens of\nonline learning, conceptualizing SSMs as meta-modules for specific online\nlearning problems. This approach links SSM design to formulating precise online\nlearning objectives, with state transition rules derived from solving these\nobjectives. Based on this insight, we introduce a novel deep SSM architecture,\nLonghorn, whose update resembles the closed-form solution for solving the\nonline associative recall problem. Our experimental results show that Longhorn\noutperforms state-of-the-art SSMs, including the Mamba model, on standard\nsequence modeling benchmarks, language modeling, and vision tasks.\nSpecifically, Longhorn achieves a 1.8x improvement in sample efficiency\ncompared to Mamba, and can extrapolate over contexts that are up to 16x longer\nduring inference.\n","authors":["Bo Liu","Rui Wang","Lemeng Wu","Yihao Feng","Peter Stone","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14207v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03541v2","updated":"2024-10-02T14:30:15Z","published":"2024-02-05T21:55:24Z","title":"HAMLET: Graph Transformer Neural Operator for Partial Differential\n  Equations","summary":"  We present a novel graph transformer framework, HAMLET, designed to address\nthe challenges in solving partial differential equations (PDEs) using neural\nnetworks. The framework uses graph transformers with modular input encoders to\ndirectly incorporate differential equation information into the solution\nprocess. This modularity enhances parameter correspondence control, making\nHAMLET adaptable to PDEs of arbitrary geometries and varied input formats.\nNotably, HAMLET scales effectively with increasing data complexity and noise,\nshowcasing its robustness. HAMLET is not just tailored to a single type of\nphysical simulation, but can be applied across various domains. Moreover, it\nboosts model resilience and performance, especially in scenarios with limited\ndata. We demonstrate, through extensive experiments, that our framework is\ncapable of outperforming current techniques for PDEs.\n","authors":["Andrey Bryutkin","Jiahao Huang","Zhongying Deng","Guang Yang","Carola-Bibiane Schnlieb","Angelica Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2402.03541v2.pdf","comment":"18 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.09031v3","updated":"2024-10-02T14:24:50Z","published":"2024-06-13T12:04:40Z","title":"A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and\n  Generalizability","summary":"  Graph pooling has gained attention for its ability to obtain effective node\nand graph representations for various downstream tasks. Despite the recent\nsurge in graph pooling approaches, there is a lack of standardized experimental\nsettings and fair benchmarks to evaluate their performance. To address this\nissue, we have constructed a comprehensive benchmark that includes 17 graph\npooling methods and 28 different graph datasets. This benchmark systematically\nassesses the performance of graph pooling methods in three dimensions, i.e.,\neffectiveness, robustness, and generalizability. We first evaluate the\nperformance of these graph pooling approaches across different tasks including\ngraph classification, graph regression and node classification. Then, we\ninvestigate their performance under potential noise attacks and\nout-of-distribution shifts in real-world scenarios. We also involve detailed\nefficiency analysis, backbone analysis, parameter analysis and visualization to\nprovide more evidence. Extensive experiments validate the strong capability and\napplicability of graph pooling approaches in various scenarios, which can\nprovide valuable insights and guidance for deep geometric learning research.\nThe source code of our benchmark is available at\nhttps://github.com/goose315/Graph_Pooling_Benchmark.\n","authors":["Pengyun Wang","Junyu Luo","Yanxin Shen","Ming Zhang","Siyu Heng","Xiao Luo"],"pdf_url":"https://arxiv.org/pdf/2406.09031v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01588v1","updated":"2024-10-02T14:20:30Z","published":"2024-10-02T14:20:30Z","title":"DynFrs: An Efficient Framework for Machine Unlearning in Random Forest","summary":"  Random Forests are widely recognized for establishing efficacy in\nclassification and regression tasks, standing out in various domains such as\nmedical diagnosis, finance, and personalized recommendations. These domains,\nhowever, are inherently sensitive to privacy concerns, as personal and\nconfidential data are involved. With increasing demand for the right to be\nforgotten, particularly under regulations such as GDPR and CCPA, the ability to\nperform machine unlearning has become crucial for Random Forests. However,\ninsufficient attention was paid to this topic, and existing approaches face\ndifficulties in being applied to real-world scenarios. Addressing this gap, we\npropose the DynFrs framework designed to enable efficient machine unlearning in\nRandom Forests while preserving predictive accuracy. Dynfrs leverages\nsubsampling method Occ(q) and a lazy tag strategy Lzy, and is still adaptable\nto any Random Forest variant. In essence, Occ(q) ensures that each sample in\nthe training set occurs only in a proportion of trees so that the impact of\ndeleting samples is limited, and Lzy delays the reconstruction of a tree node\nuntil necessary, thereby avoiding unnecessary modifications on tree structures.\nIn experiments, applying Dynfrs on Extremely Randomized Trees yields\nsubstantial improvements, achieving orders of magnitude faster unlearning\nperformance and better predictive accuracy than existing machine unlearning\nmethods for Random Forests.\n","authors":["Shurong Wang","Zhuoyang Shen","Xinbao Qiao","Tongning Zhang","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01580v1","updated":"2024-10-02T14:15:32Z","published":"2024-10-02T14:15:32Z","title":"Learning-Augmented Robust Algorithmic Recourse","summary":"  The widespread use of machine learning models in high-stakes domains can have\na major negative impact, especially on individuals who receive undesirable\noutcomes. Algorithmic recourse provides such individuals with suggestions of\nminimum-cost improvements they can make to achieve a desirable outcome in the\nfuture. However, machine learning models often get updated over time and this\ncan cause a recourse to become invalid (i.e., not lead to the desirable\noutcome). The robust recourse literature aims to choose recourses that are less\nsensitive, even against adversarial model changes, but this comes at a higher\ncost. To overcome this obstacle, we initiate the study of algorithmic recourse\nthrough the learning-augmented framework and evaluate the extent to which a\ndesigner equipped with a prediction regarding future model changes can reduce\nthe cost of recourse when the prediction is accurate (consistency) while also\nlimiting the cost even when the prediction is inaccurate (robustness). We\npropose a novel algorithm for this problem, study the robustness-consistency\ntrade-off, and analyze how prediction accuracy affects performance.\n","authors":["Kshitij Kayastha","Vasilis Gkatzelis","Shahin Jabbari"],"pdf_url":"https://arxiv.org/pdf/2410.01580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01577v1","updated":"2024-10-02T14:13:06Z","published":"2024-10-02T14:13:06Z","title":"Coordinate-Based Neural Representation Enabling Zero-Shot Learning for\n  3D Multiparametric Quantitative MRI","summary":"  Quantitative magnetic resonance imaging (qMRI) offers tissue-specific\nphysical parameters with significant potential for neuroscience research and\nclinical practice. However, lengthy scan times for 3D multiparametric qMRI\nacquisition limit its clinical utility. Here, we propose SUMMIT, an innovative\nimaging methodology that includes data acquisition and an unsupervised\nreconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes\nmultiple important quantitative properties into highly undersampled k-space. It\nfurther leverages implicit neural representation incorporated with a dedicated\nphysics model to reconstruct the desired multiparametric maps without needing\nexternal training datasets. SUMMIT delivers co-registered T1, T2, T2*, and\nquantitative susceptibility mapping. Extensive simulations and phantom imaging\ndemonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised\napproach for qMRI reconstruction also introduces a novel zero-shot learning\nparadigm for multiparametric imaging applicable to various medical imaging\nmodalities.\n","authors":["Guoyan Lao","Ruimin Feng","Haikun Qi","Zhenfeng Lv","Qiangqiang Liu","Chunlei Liu","Yuyao Zhang","Hongjiang Wei"],"pdf_url":"https://arxiv.org/pdf/2410.01577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01570v1","updated":"2024-10-02T14:09:51Z","published":"2024-10-02T14:09:51Z","title":"Truncated Kernel Stochastic Gradient Descent on Spheres","summary":"  Inspired by the structure of spherical harmonics, we propose the truncated\nkernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square\nloss function for spherical data fitting. T-kernel SGD employs a \"truncation\"\noperation, enabling the application of a series-based kernel function in\nstochastic gradient descent, thereby avoiding the difficulties of finding\nsuitable closed-form kernel functions in high-dimensional spaces. In contrast\nto traditional kernel SGD, T-kernel SGD is more effective in balancing bias and\nvariance by dynamically adjusting the hypothesis space during iterations. The\nmost significant advantage of the proposed algorithm is that it can achieve\ntheoretically optimal convergence rates using a constant step size (independent\nof the sample size) while overcoming the inherent saturation problem of kernel\nSGD. Additionally, we leverage the structure of spherical polynomials to derive\nan equivalent T-kernel SGD, significantly reducing storage and computational\ncosts compared to kernel SGD. Typically, T-kernel SGD requires only\n$\\mathcal{O}(n^{1+\\frac{d}{d-1}\\epsilon})$ computational complexity and\n$\\mathcal{O}(n^{\\frac{d}{d-1}\\epsilon})$ storage to achieve optimal rates for\nthe d-dimensional sphere, where $0<\\epsilon<\\frac{1}{2}$ can be arbitrarily\nsmall if the optimal fitting or the underlying space possesses sufficient\nregularity. This regularity is determined by the smoothness parameter of the\nobjective function and the decaying rate of the eigenvalues of the integral\noperator associated with the kernel function, both of which reflect the\ndifficulty of the estimation problem. Our main results quantitatively\ncharacterize how this prior information influences the convergence of T-kernel\nSGD. The numerical experiments further validate the theoretical findings\npresented in this paper.\n","authors":["JinHui Bai","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.01570v1.pdf","comment":"57 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.13977v2","updated":"2024-10-02T14:01:49Z","published":"2024-05-22T20:24:41Z","title":"Improving Fairness and Mitigating MADness in Generative Models","summary":"  Generative models unfairly penalize data belonging to minority classes,\nsuffer from model autophagy disorder (MADness), and learn biased estimates of\nthe underlying distribution parameters. Our theoretical and empirical results\nshow that training generative models with intentionally designed hypernetworks\nleads to models that 1) are more fair when generating datapoints belonging to\nminority classes 2) are more stable in a self-consumed (i.e., MAD) setting, and\n3) learn parameters that are less statistically biased. To further mitigate\nunfairness, MADness, and bias, we introduce a regularization term that\npenalizes discrepancies between a generative model's estimated weights when\ntrained on real data versus its own synthetic data. To facilitate training\nexisting deep generative models within our framework, we offer a scalable\nimplementation of hypernetworks that automatically generates a hypernetwork\narchitecture for any given generative model.\n","authors":["Paul Mayer","Lorenzo Luzi","Ali Siahkoohi","Don H. Johnson","Richard G. Baraniuk"],"pdf_url":"https://arxiv.org/pdf/2405.13977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01565v1","updated":"2024-10-02T14:01:34Z","published":"2024-10-02T14:01:34Z","title":"Bayes' Power for Explaining In-Context Learning Generalizations","summary":"  Traditionally, neural network training has been primarily viewed as an\napproximation of maximum likelihood estimation (MLE). This interpretation\noriginated in a time when training for multiple epochs on small datasets was\ncommon and performance was data bound; but it falls short in the era of\nlarge-scale single-epoch trainings ushered in by large self-supervised setups,\nlike language models. In this new setup, performance is compute-bound, but data\nis readily available. As models became more powerful, in-context learning\n(ICL), i.e., learning in a single forward-pass based on the context, emerged as\none of the dominant paradigms. In this paper, we argue that a more useful\ninterpretation of neural network behavior in this era is as an approximation of\nthe true posterior, as defined by the data-generating process. We demonstrate\nthis interpretations' power for ICL and its usefulness to predict\ngeneralizations to previously unseen tasks. We show how models become robust\nin-context learners by effectively composing knowledge from their training\ndata. We illustrate this with experiments that reveal surprising\ngeneralizations, all explicable through the exact posterior. Finally, we show\nthe inherent constraints of the generalization capabilities of posteriors and\nthe limitations of neural networks in approximating these posteriors.\n","authors":["Samuel Mller","Noah Hollmann","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2410.01565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01562v1","updated":"2024-10-02T14:00:41Z","published":"2024-10-02T14:00:41Z","title":"HRTF Estimation using a Score-based Prior","summary":"  We present a head-related transfer function (HRTF) estimation method which\nrelies on a data-driven prior given by a score-based diffusion model. The HRTF\nis estimated in reverberant environments using natural excitation signals, e.g.\nhuman speech. The impulse response of the room is estimated along with the HRTF\nby optimizing a parametric model of reverberation based on the statistical\nbehaviour of room acoustics. The posterior distribution of HRTF given the\nreverberant measurement and excitation signal is modelled using the score-based\nHRTF prior and a log-likelihood approximation. We show that the resulting\nmethod outperforms several baselines, including an oracle recommender system\nthat assigns the optimal HRTF in our training set based on the smallest\ndistance to the true HRTF at the given direction of arrival. In particular, we\nshow that the diffusion prior can account for the large variability of\nhigh-frequency content in HRTFs.\n","authors":["Etienne Thuillier","Jean-Marie Lemercier","Eloi Moliner","Timo Gerkmann","Vesa Vlimki"],"pdf_url":"https://arxiv.org/pdf/2410.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01560v1","updated":"2024-10-02T14:00:09Z","published":"2024-10-02T14:00:09Z","title":"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source\n  Instruction Data","summary":"  Mathematical reasoning continues to be a critical challenge in large language\nmodel (LLM) development with significant interest. However, most of the\ncutting-edge progress in mathematical reasoning with LLMs has become\n\\emph{closed-source} due to lack of access to training data. This lack of data\naccess limits researchers from understanding the impact of different choices\nfor synthesizing and utilizing the data. With the goal of creating a\nhigh-quality finetuning (SFT) dataset for math reasoning, we conduct careful\nablation experiments on data synthesis using the recently released\n\\texttt{Llama3.1} family of models. Our experiments show that: (a) solution\nformat matters, with excessively verbose solutions proving detrimental to SFT\nperformance, (b) data generated by a strong teacher outperforms\n\\emph{on-policy} data generated by a weak student model, (c) SFT is robust to\nlow-quality solutions, allowing for imprecise data filtering, and (d) question\ndiversity is crucial for achieving data scaling gains. Based on these insights,\nwe create the OpenMathInstruct-2 dataset, which consists of 14M\nquestion-solution pairs ($\\approx$ 600K unique questions), making it nearly\neight times larger than the previous largest open-source math reasoning\ndataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2\noutperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\%\n$\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we\nrelease the code, the finetuned models, and the OpenMathInstruct-2 dataset\nunder a commercially permissive license.\n","authors":["Shubham Toshniwal","Wei Du","Ivan Moshkov","Branislav Kisacanin","Alexan Ayrapetyan","Igor Gitman"],"pdf_url":"https://arxiv.org/pdf/2410.01560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07191v2","updated":"2024-10-02T13:45:53Z","published":"2024-08-13T20:16:11Z","title":"Joint Graph Rewiring and Feature Denoising via Spectral Resonance","summary":"  In graph learning the graph and the node features both contain noisy\ninformation about the node labels. In this paper we propose joint denoising and\nrewiring (JDR)--an algorithm to jointly rewire the graph and denoise the\nfeatures, which improves the performance of downstream node classification\ngraph neural nets (GNNs). JDR improves the alignment between the leading\neigenspaces of graph and feature matrices. To approximately solve the\nassociated non-convex optimization problem we propose a heuristic that\nefficiently handles real-world graph datasets with multiple classes and\ndifferent levels of homophily or heterophily. We theoretically justify JDR in a\nstylized setting and verify the effectiveness of our approach through extensive\nexperiments on synthetic and real-world graph datasets. The results show that\nJDR consistently outperforms existing rewiring methods on node classification\nusing GNNs as downstream models.\n","authors":["Jonas Linkerhgner","Cheng Shi","Ivan Dokmani"],"pdf_url":"https://arxiv.org/pdf/2408.07191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03124v2","updated":"2024-10-02T13:45:11Z","published":"2024-07-31T14:54:29Z","title":"Closed-loop Diffusion Control of Complex Physical Systems","summary":"  The control problems of complex physical systems have broad applications in\nscience and engineering. Previous studies have shown that generative control\nmethods based on diffusion models offer significant advantages for solving\nthese problems. However, existing generative control approaches face challenges\nin both performance and efficiency when extended to the closed-loop setting,\nwhich is essential for effective control. In this paper, we propose an\nefficient Closed-Loop Diffusion method for Physical systems Control\n(CL-DiffPhyCon). By employing an asynchronous denoising framework for different\nphysical time steps, CL-DiffPhyCon generates control signals conditioned on\nreal-time feedback from the environment with significantly reduced\ncomputational cost during sampling. Additionally, the control process could be\nfurther accelerated by incorporating fast sampling techniques, such as DDIM. We\nevaluate CL-DiffPhyCon on two tasks: 1D Burgers' equation control and 2D\nincompressible fluid control. The results demonstrate that CL-DiffPhyCon\nachieves superior control performance with significant improvements in sampling\nefficiency.\n","authors":["Long Wei","Haodong Feng","Yuchen Yang","Ruiqi Feng","Peiyan Hu","Xiang Zheng","Tao Zhang","Dixia Fan","Tailin Wu"],"pdf_url":"https://arxiv.org/pdf/2408.03124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11062v2","updated":"2024-10-02T13:44:30Z","published":"2024-07-10T17:53:30Z","title":"EfficientQAT: Efficient Quantization-Aware Training for Large Language\n  Models","summary":"  Large language models (LLMs) are crucial in modern natural language\nprocessing and artificial intelligence. However, they face challenges in\nmanaging their significant memory requirements. Although quantization-aware\ntraining (QAT) offers a solution by reducing memory consumption through low-bit\nrepresentations with minimal accuracy loss, it is impractical due to\nsubstantial training resources. To address this, we propose Efficient\nQuantization-Aware Training (EfficientQAT), a more feasible QAT algorithm.\nEfficientQAT involves two consecutive phases: Block-wise training of all\nparameters (Block-AP) and end-to-end training of quantization parameters\n(E2E-QP). To the best of our knowledge, Block-AP is the first method to enable\ndirect training of all parameters in a block-wise manner, reducing accuracy\nloss in low-bit scenarios by enhancing the solution space during optimization.\nE2E-QP then trains only the quantization parameters (step sizes) end-to-end,\nfurther improving the performance of quantized models by considering\ninteractions among all sub-modules. Extensive experiments demonstrate that\nEfficientQAT outperforms previous quantization methods across a range of\nmodels, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with\nscales from 7B to 70B parameters at various quantization bits. For instance,\nEfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41\nhours, with less than 3 points accuracy degradation compared to the full\nprecision (69.48 vs. 72.41). Code is available at\nhttps://github.com/OpenGVLab/EfficientQAT.\n","authors":["Mengzhao Chen","Wenqi Shao","Peng Xu","Jiahao Wang","Peng Gao","Kaipeng Zhang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11062v2.pdf","comment":"An efficient and effective quantization technical to improve the\n  performance of low-bits LMMs and LVLMs"},{"id":"http://arxiv.org/abs/2312.09610v2","updated":"2024-10-02T13:42:53Z","published":"2023-12-15T08:53:45Z","title":"A Synthesis of Green Architectural Tactics for ML-Enabled Systems","summary":"  The rapid adoption of artificial intelligence (AI) and machine learning (ML)\nhas generated growing interest in understanding their environmental impact and\nthe challenges associated with designing environmentally friendly ML-enabled\nsystems. While Green AI research, i.e., research that tries to minimize the\nenergy footprint of AI, is receiving increasing attention, very few concrete\nguidelines are available on how ML-enabled systems can be designed to be more\nenvironmentally sustainable. In this paper, we provide a catalog of 30 green\narchitectural tactics for ML-enabled systems to fill this gap. An architectural\ntactic is a high-level design technique to improve software quality, in our\ncase environmental sustainability. We derived the tactics from the analysis of\n51 peer-reviewed publications that primarily explore Green AI, and validated\nthem using a focus group approach with three experts. The 30 tactics we\nidentified are aimed to serve as an initial reference guide for further\nexploration into Green AI from a software engineering perspective, and assist\nin designing sustainable ML-enabled systems. To enhance transparency and\nfacilitate their widespread use and extension, we make the tactics available\nonline in easily consumable formats. Wide-spread adoption of these tactics has\nthe potential to substantially reduce the societal impact of ML-enabled systems\nregarding their energy and carbon footprint.\n","authors":["Heli Jrvenp","Patricia Lago","Justus Bogner","Grace Lewis","Henry Muccini","Ipek Ozkaya"],"pdf_url":"https://arxiv.org/pdf/2312.09610v2.pdf","comment":"Accepted for publication at the 2024 International Conference on\n  Software Engineering - Software Engineering in Society (ICSE-SEIS'2024)"},{"id":"http://arxiv.org/abs/2407.14129v2","updated":"2024-10-02T13:42:29Z","published":"2024-07-19T08:59:00Z","title":"Comparing and Contrasting Deep Learning Weather Prediction Backbones on\n  Navier-Stokes and Atmospheric Dynamics","summary":"  Remarkable progress in the development of Deep Learning Weather Prediction\n(DLWP) models positions them to become competitive with traditional numerical\nweather prediction (NWP) models. Indeed, a wide number of DLWP architectures --\nbased on various backbones, including U-Net, Transformer, Graph Neural Network\n(GNN), and Fourier Neural Operator (FNO) -- have demonstrated their potential\nat forecasting atmospheric states. However, due to differences in training\nprotocols, forecast horizons, and data choices, it remains unclear which (if\nany) of these methods and architectures are most suitable for weather\nforecasting and for future model development. Here, we step back and provide a\ndetailed empirical analysis, under controlled conditions, comparing and\ncontrasting the most prominent DLWP models, along with their backbones. We\naccomplish this by predicting synthetic two-dimensional incompressible\nNavier-Stokes and real-world global weather dynamics. In terms of accuracy,\nmemory consumption, and runtime, our results illustrate various tradeoffs. For\nexample, on synthetic data, we observe favorable performance of FNO; and on the\nreal-world WeatherBench dataset, our results demonstrate the suitability of\nConvLSTM and SwinTransformer for short-to-mid-ranged forecasts. For long-ranged\nweather rollouts of up to 365 days, we observe superior stability and physical\nsoundness in architectures that formulate a spherical data representation,\ni.e., GraphCast and Spherical FNO. In addition, we observe that all of these\nmodel backbones \"saturate,\" i.e., none of them exhibit so-called neural\nscaling, which highlights an important direction for future work on these and\nrelated models. The code is available at\nhttps://github.com/amazon-science/dlwp-benchmark.\n","authors":["Matthias Karlbauer","Danielle C. Maddix","Abdul Fatir Ansari","Boran Han","Gaurav Gupta","Yuyang Wang","Andrew Stuart","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2407.14129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10079v3","updated":"2024-10-02T13:40:53Z","published":"2023-04-20T04:12:50Z","title":"Dynamic Graph Representation Learning via Edge Temporal States Modeling\n  and Structure-reinforced Transformer","summary":"  Dynamic graph representation learning has emerged as a crucial research area,\ndriven by the growing need for analyzing time-evolving graph data in real-world\napplications. While recent approaches leveraging recurrent neural networks\n(RNNs) and graph neural networks (GNNs) have shown promise, they often fail to\nadequately capture the impact of temporal edge states on inter-node\nrelationships, consequently overlooking the dynamic changes in node features\ninduced by these evolving relationships. Furthermore, these methods suffer from\nGNNs' inherent over-smoothing problem, which hinders the extraction of global\nstructural features. To address these challenges, we introduce the Recurrent\nStructure-reinforced Graph Transformer (RSGT), a novel framework for dynamic\ngraph representation learning. It first designs a heuristic method to\nexplicitly model edge temporal states by employing different edge types and\nweights based on the differences between consecutive snapshots, thereby\nintegrating varying edge temporal states into the graph's topological\nstructure. We then propose a structure-reinforced graph transformer that\ncaptures temporal node representations encoding both graph topology and\nevolving dynamics through a recurrent learning paradigm, enabling the\nextraction of both local and global structural features. Comprehensive\nexperiments on four real-world datasets demonstrate RSGT's superior performance\nin discrete dynamic graph representation learning, consistently outperforming\nexisting methods in dynamic link prediction tasks.\n","authors":["Shengxiang Hu","Guobing Zou","Song Yang","Shiyi Lin","Yanglan Gan","Bofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.10079v3.pdf","comment":"This work has been submitted to the Elsevier for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2406.07107v3","updated":"2024-10-02T13:38:28Z","published":"2024-06-11T09:49:00Z","title":"Agnostic Sharpness-Aware Minimization","summary":"  Sharpness-aware minimization (SAM) has been instrumental in improving deep\nneural network training by minimizing both the training loss and the sharpness\nof the loss landscape, leading the model into flatter minima that are\nassociated with better generalization properties. In another aspect,\nModel-Agnostic Meta-Learning (MAML) is a framework designed to improve the\nadaptability of models. MAML optimizes a set of meta-models that are\nspecifically tailored for quick adaptation to multiple tasks with minimal\nfine-tuning steps and can generalize well with limited data. In this work, we\nexplore the connection between SAM and MAML in enhancing model generalization.\nWe introduce Agnostic-SAM, a novel approach that combines the principles of\nboth SAM and MAML. Agnostic-SAM adapts the core idea of SAM by optimizing the\nmodel toward wider local minima using training data, while concurrently\nmaintaining low loss values on validation data. By doing so, it seeks flatter\nminima that are not only robust to small perturbations but also less vulnerable\nto data distributional shift problems. Our experimental results demonstrate\nthat Agnostic-SAM significantly improves generalization over baselines across a\nrange of datasets and under challenging conditions such as noisy labels or data\nlimitation.\n","authors":["Van-Anh Nguyen","Quyen Tran","Tuan Truong","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2406.07107v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.03179v2","updated":"2024-10-02T13:32:56Z","published":"2024-07-03T14:59:46Z","title":"Motion meets Attention: Video Motion Prompts","summary":"  Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose a modified\nSigmoid function with learnable slope and shift parameters as an attention\nmechanism to modulate motion signals from frame differencing maps. This\napproach generates a sequence of attention maps that enhance the processing of\nmotion-related video content. To ensure temporal continuity and smoothness of\nthe attention maps, we apply pair-wise temporal attention variation\nregularization to remove unwanted motions (e.g., noise) while preserving\nimportant ones. We then perform Hadamard product between each pair of attention\nmaps and the original video frames to highlight the evolving motions of\ninterest over time. These highlighted motions, termed video motion prompts, are\nsubsequently used as inputs to the model instead of the original video frames.\nWe formalize this process as a motion prompt layer and incorporate the\nregularization term into the loss function to learn better motion prompts. This\nlayer serves as an adapter between the model and the video data, bridging the\ngap between traditional 'blind motion extraction' and the extraction of\nrelevant motions of interest. We show that our lightweight, plug-and-play\nmotion prompt layer seamlessly integrates into models like SlowFast, X3D, and\nTimeSformer, enhancing performance on benchmarks such as FineGym and MPII\nCooking 2.\n","authors":["Qixiang Chen","Lei Wang","Piotr Koniusz","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2407.03179v2.pdf","comment":"Accepted at the 16th Asian Conference on Machine Learning (ACML 2024)"},{"id":"http://arxiv.org/abs/2410.01545v1","updated":"2024-10-02T13:31:06Z","published":"2024-10-02T13:31:06Z","title":"Lines of Thought in Large Language Models","summary":"  Large Language Models achieve next-token prediction by transporting a\nvectorized piece of text (prompt) across an accompanying embedding space under\nthe action of successive transformer layers. The resulting high-dimensional\ntrajectories realize different contextualization, or 'thinking', steps, and\nfully determine the output probability distribution. We aim to characterize the\nstatistical properties of ensembles of these 'lines of thought.' We observe\nthat independent trajectories cluster along a low-dimensional, non-Euclidean\nmanifold, and that their path can be well approximated by a stochastic equation\nwith few parameters extracted from data. We find it remarkable that the vast\ncomplexity of such large models can be reduced to a much simpler form, and we\nreflect on implications.\n","authors":["Raphal Sarfati","Toni J. B. Liu","Nicolas Boull","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2410.01545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01540v1","updated":"2024-10-02T13:29:52Z","published":"2024-10-02T13:29:52Z","title":"Edge-preserving noise for diffusion models","summary":"  Classical generative diffusion models learn an isotropic Gaussian denoising\nprocess, treating all spatial regions uniformly, thus neglecting potentially\nvaluable structural information in the data. Inspired by the long-established\nwork on anisotropic diffusion in image processing, we present a novel\nedge-preserving diffusion model that is a generalization of denoising diffusion\nprobablistic models (DDPM). In particular, we introduce an edge-aware noise\nscheduler that varies between edge-preserving and isotropic Gaussian noise. We\nshow that our model's generative process converges faster to results that more\nclosely match the target distribution. We demonstrate its capability to better\nlearn the low-to-mid frequencies within the dataset, which plays a crucial role\nin representing shapes and structural information. Our edge-preserving\ndiffusion process consistently outperforms state-of-the-art baselines in\nunconditional image generation. It is also more robust for generative tasks\nguided by a shape-based prior, such as stroke-to-image generation. We present\nqualitative and quantitative results showing consistent improvements (FID\nscore) of up to 30% for both tasks.\n","authors":["Jente Vandersanden","Sascha Holl","Xingchang Huang","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2410.01540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01537v1","updated":"2024-10-02T13:28:02Z","published":"2024-10-02T13:28:02Z","title":"Attention layers provably solve single-location regression","summary":"  Attention-based models, such as Transformer, excel across various tasks but\nlack a comprehensive theoretical understanding, especially regarding token-wise\nsparsity and internal linear representations. To address this gap, we introduce\nthe single-location regression task, where only one token in a sequence\ndetermines the output, and its position is a latent random variable,\nretrievable via a linear projection of the input. To solve this task, we\npropose a dedicated predictor, which turns out to be a simplified version of a\nnon-linear self-attention layer. We study its theoretical properties, by\nshowing its asymptotic Bayes optimality and analyzing its training dynamics. In\nparticular, despite the non-convex nature of the problem, the predictor\neffectively learns the underlying structure. This work highlights the capacity\nof attention mechanisms to handle sparse token information and internal linear\nstructures.\n","authors":["Pierre Marion","Raphal Berthier","Grard Biau","Claire Boyer"],"pdf_url":"https://arxiv.org/pdf/2410.01537v1.pdf","comment":"41 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.14341v2","updated":"2024-10-02T13:24:42Z","published":"2024-06-20T14:09:00Z","title":"HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?","summary":"  Accurately forecasting multiple future events within a given time horizon is\ncrucial for finance, retail, social networks, and healthcare applications.\nEvent timing and labels are typically modeled using Marked Temporal Point\nProcesses (MTPP), with evaluations often focused on next-event prediction\nquality. While some studies have extended evaluations to a fixed number of\nfuture events, we demonstrate that this approach leads to inaccuracies in\nhandling false positives and false negatives. To address these issues, we\npropose a novel evaluation method inspired by object detection techniques from\ncomputer vision. Specifically, we introduce Temporal mean Average Precision\n(T-mAP), a temporal variant of mAP, which overcomes the limitations of existing\nlong-horizon evaluation metrics. Our extensive experiments demonstrate that\nmodels with strong next-event prediction accuracy can yield poor long-horizon\nforecasts and vice versa, indicating that specialized methods are needed for\neach task. To support further research, we release HoTPP, the first benchmark\ndesigned explicitly for evaluating long-horizon MTPP predictions. HoTPP\nincludes large-scale datasets with up to 43 million events and provides\noptimized procedures for both autoregressive and parallel inference, paving the\nway for future advancements in the field.\n","authors":["Ivan Karpukhin","Foma Shipilov","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2406.14341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01531v1","updated":"2024-10-02T13:24:24Z","published":"2024-10-02T13:24:24Z","title":"TiVaT: Joint-Axis Attention for Time Series Forecasting with Lead-Lag\n  Dynamics","summary":"  Multivariate time series (MTS) forecasting plays a crucial role in various\nreal-world applications, yet simultaneously capturing both temporal and\ninter-variable dependencies remains a challenge. Conventional Channel-Dependent\n(CD) models handle these dependencies separately, limiting their ability to\nmodel complex interactions such as lead-lag dynamics. To address these\nlimitations, we propose TiVaT (Time-Variable Transformer), a novel architecture\nthat integrates temporal and variate dependencies through its Joint-Axis (JA)\nattention mechanism. TiVaT's ability to capture intricate variate-temporal\ndependencies, including asynchronous interactions, is further enhanced by the\nincorporation of Distance-aware Time-Variable (DTV) Sampling, which reduces\nnoise and improves accuracy through a learned 2D map that focuses on key\ninteractions. TiVaT effectively models both temporal and variate dependencies,\nconsistently delivering strong performance across diverse datasets. Notably, it\nexcels in capturing complex patterns within multivariate time series, enabling\nit to surpass or remain competitive with state-of-the-art methods. This\npositions TiVaT as a new benchmark in MTS forecasting, particularly in handling\ndatasets characterized by intricate and challenging dependencies.\n","authors":["Junwoo Ha","Hyukjae Kwon","Sungsoo Kim","Kisu Lee","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01531v1.pdf","comment":"15pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.16587v2","updated":"2024-10-02T13:22:27Z","published":"2024-05-26T14:38:24Z","title":"Cost-Effective Online Multi-LLM Selection with Versatile Reward Models","summary":"  With the rapid advancement of large language models (LLMs), the diversity of\nmulti-LLM tasks and the variability in their pricing structures have become\nincreasingly important, as costs can vary greatly between different LLMs. To\ntackle these challenges, we introduce the \\textit{C2MAB-V}, a\n\\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed\n\\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM\nselection and usage. This online model differs from traditional static\napproaches or those reliant on a single LLM without cost consideration. With\nmultiple LLMs deployed on a scheduling cloud and a local server dedicated to\nhandling user queries, \\textit{C2MAB-V} facilitates the selection of multiple\nLLMs over a combinatorial search space, specifically tailored for various\ncollaborative task types with different reward models. Based on our designed\nonline feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can\neffectively address the multi-LLM selection challenge by managing the\nexploration-exploitation trade-off across different models, while also\nbalancing cost and reward for diverse tasks. The NP-hard integer linear\nprogramming problem for selecting multiple LLMs with trade-off dilemmas is\naddressed by: i) decomposing the integer problem into a relaxed form by the\nlocal server, ii) utilizing a discretization rounding scheme that provides\noptimal LLM combinations by the scheduling cloud, and iii) continual online\nupdates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers\nstrict guarantees over versatile reward models, matching state-of-the-art\nresults for regret and violations in some degenerate cases. Empirically, we\nshow that \\textit{C2MAB-V} effectively balances performance and cost-efficiency\nwith nine LLMs for three application scenarios.\n","authors":["Xiangxiang Dai","Jin Li","Xutong Liu","Anqi Yu","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2405.16587v2.pdf","comment":"32 pages, 14 figures, conference"},{"id":"http://arxiv.org/abs/2408.13131v2","updated":"2024-10-02T13:21:50Z","published":"2024-08-23T14:57:46Z","title":"DeTPP: Leveraging Object Detection for Robust Long-Horizon Event\n  Prediction","summary":"  Long-horizon event forecasting is critical across various domains, including\nretail, finance, healthcare, and social networks. Traditional methods, such as\nMarked Temporal Point Processes (MTPP), often rely on autoregressive models to\npredict multiple future events. However, these models frequently suffer from\nissues like converging to constant or repetitive outputs, which limits their\neffectiveness and general applicability. To address these challenges, we\nintroduce DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection techniques from computer vision. DeTPP employs a\nunique matching-based loss function that selectively prioritizes reliably\npredictable events, improving the accuracy and diversity of predictions during\ninference. Our method establishes a new state-of-the-art in long-horizon event\nforecasting, achieving up to a 77% relative improvement over existing MTPP and\nnext-K methods. The proposed hybrid approach enhances the accuracy of next\nevent prediction by up to 2.7% on a large transactional dataset. Notably, DeTPP\nis also among the fastest methods for inference. The implementation of DeTPP is\npublicly available on GitHub.\n","authors":["Ivan Karpukhin","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2408.13131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09335v2","updated":"2024-10-02T13:13:06Z","published":"2024-08-18T02:31:55Z","title":"Exploratory Optimal Stopping: A Singular Control Formulation","summary":"  This paper explores continuous-time and state-space optimal stopping problems\nfrom a reinforcement learning perspective. We begin by formulating the stopping\nproblem using randomized stopping times, where the decision maker's control is\nrepresented by the probability of stopping within a given time--specifically, a\nbounded, non-decreasing, c\\`adl\\`ag control process. To encourage exploration\nand facilitate learning, we introduce a regularized version of the problem by\npenalizing it with the cumulative residual entropy of the randomized stopping\ntime. The regularized problem takes the form of an (n+1)-dimensional degenerate\nsingular stochastic control with finite-fuel. We address this through the\ndynamic programming principle, which enables us to identify the unique optimal\nexploratory strategy. For the specific case of a real option problem, we derive\na semi-explicit solution to the regularized problem, allowing us to assess the\nimpact of entropy regularization and analyze the vanishing entropy limit.\nFinally, we propose a reinforcement learning algorithm based on policy\niteration. We show both policy improvement and policy convergence results for\nour proposed algorithm.\n","authors":["Jodi Dianetti","Giorgio Ferrari","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.09335v2.pdf","comment":"49 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.02135v2","updated":"2024-10-02T13:11:08Z","published":"2024-09-02T12:55:27Z","title":"Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based\n  Sampling","summary":"  Learning-based methods have gained attention as general-purpose solvers due\nto their ability to automatically learn problem-specific heuristics, reducing\nthe need for manually crafted heuristics. However, these methods often face\nscalability challenges. To address these issues, the improved Sampling\nalgorithm for Combinatorial Optimization (iSCO), using discrete Langevin\ndynamics, has been proposed, demonstrating better performance than several\nlearning-based solvers. This study proposes a different approach that\nintegrates gradient-based update through continuous relaxation, combined with\nQuasi-Quantum Annealing (QQA). QQA smoothly transitions the objective function,\nstarting from a simple convex function, minimized at half-integral values, to\nthe original objective function, where the relaxed variables are minimized only\nin the discrete space. Furthermore, we incorporate parallel run communication\nleveraging GPUs to enhance exploration capabilities and accelerate convergence.\nNumerical experiments demonstrate that our method is a competitive\ngeneral-purpose solver, achieving performance comparable to iSCO and\nlearning-based solvers across various benchmark problems. Notably, our method\nexhibits superior speed-quality trade-offs for large-scale instances compared\nto iSCO, learning-based solvers, commercial solvers, and specialized\nalgorithms.\n","authors":["Yuma Ichikawa","Yamato Arai"],"pdf_url":"https://arxiv.org/pdf/2409.02135v2.pdf","comment":"21 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.01516v1","updated":"2024-10-02T13:05:09Z","published":"2024-10-02T13:05:09Z","title":"Bounds on $L_p$ Errors in Density Ratio Estimation via $f$-Divergence\n  Loss Functions","summary":"  Density ratio estimation (DRE) is a fundamental machine learning technique\nfor identifying relationships between two probability distributions.\n$f$-divergence loss functions, derived from variational representations of\n$f$-divergence, are commonly employed in DRE to achieve state-of-the-art\nresults. This study presents a novel perspective on DRE using $f$-divergence\nloss functions by deriving the upper and lower bounds on $L_p$ errors. These\nbounds apply to any estimator within a class of Lipschitz continuous\nestimators, irrespective of the specific $f$-divergence loss functions\nutilized. The bounds are formulated as a product of terms that include the data\ndimension and the expected value of the density ratio raised to the power of\n$p$. Notably, the lower bound incorporates an exponential term dependent on the\nKullback--Leibler divergence, indicating that the $L_p$ error significantly\nincreases with the Kullback--Leibler divergence for $p > 1$, and this increase\nbecomes more pronounced as $p$ increases. Furthermore, these theoretical\nfindings are substantiated through numerical experiments.\n","authors":["Yoshiaki Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2410.01516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00535v2","updated":"2024-10-02T13:02:06Z","published":"2024-10-01T09:21:29Z","title":"Optimal Causal Representations and the Causal Information Bottleneck","summary":"  To effectively study complex causal systems, it is often useful to construct\nrepresentations that simplify parts of the system by discarding irrelevant\ndetails while preserving key features. The Information Bottleneck (IB) method\nis a widely used approach in representation learning that compresses random\nvariables while retaining information about a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces representations which are causally interpretable, and which can\nbe used when reasoning about interventions. We present experimental results\ndemonstrating that the learned representations accurately capture causality as\nintended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v2.pdf","comment":"Submitted to ICLR 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2402.02041v3","updated":"2024-10-02T12:57:41Z","published":"2024-02-03T05:33:01Z","title":"$$-Divergence Loss Function for Neural Density Ratio Estimation","summary":"  Density ratio estimation (DRE) is a fundamental machine learning technique\nfor capturing relationships between two probability distributions.\nState-of-the-art DRE methods estimate the density ratio using neural networks\ntrained with loss functions derived from variational representations of\n$f$-divergence. However, existing methods face optimization challenges, such as\noverfitting due to lower-unbounded loss functions, biased mini-batch gradients,\nvanishing training loss gradients, and high sample requirements for\nKullback-Leibler (KL) divergence loss functions. To address these issues, we\nfocus on $\\alpha$-divergence, which provides a suitable variational\nrepresentation of $f$-divergence. Subsequently, a novel loss function for DRE,\nthe $\\alpha$-divergence loss function ($\\alpha$-Div), is derived. $\\alpha$-Div\nis concise but offers stable and effective optimization for DRE. The\nboundedness of $\\alpha$-divergence provides the potential for successful DRE\nwith data exhibiting high KL-divergence. Our numerical experiments demonstrate\nthe effectiveness in optimization using $\\alpha$-Div. However, the experiments\nalso show that the proposed loss function offers no significant advantage over\nthe KL-divergence loss function in terms of RMSE for DRE. This indicates that\nthe accuracy of DRE is primarily determined by the amount of KL-divergence in\nthe data and is less dependent on $\\alpha$-divergence.\n","authors":["Yoshiaki Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2402.02041v3.pdf","comment":"$\\mathcal{T}_{\\text{Lip}}$ in Theorem 7.1 (Theorem B.15.) was changed\n  to the set of all locally Lipschitz continuous functions. In the previous\n  version, $\\mathcal{T}_{\\text{Lip}}$ was defined as the set of all Lipschitz\n  continuous functions, which is unsuitable for the statement of case (ii) in\n  the theorem"},{"id":"http://arxiv.org/abs/2402.05569v4","updated":"2024-10-02T12:57:32Z","published":"2024-02-08T11:10:39Z","title":"Training-Free Message Passing for Learning on Hypergraphs","summary":"  Hypergraphs are crucial for modelling higher-order interactions in real-world\ndata. Hypergraph neural networks (HNNs) effectively utilise these structures by\nmessage passing to generate informative node features for various downstream\ntasks like node classification. However, the message passing module in existing\nHNNs typically requires a computationally intensive training process, which\nlimits their practical use. To tackle this challenge, we propose an alternative\napproach by decoupling the usage of hypergraph structural information from the\nmodel learning stage. This leads to a novel training-free message passing\nmodule, named TF-MP-Module, which can be precomputed in the data preprocessing\nstage, thereby reducing the computational burden. We refer to the hypergraph\nneural network equipped with our TF-MP-Module as TF-HNN. We theoretically\nsupport the efficiency and effectiveness of TF-HNN by showing that: 1) It is\nmore training-efficient compared to existing HNNs; 2) It utilises as much\ninformation as existing HNNs for node feature generation; and 3) It is robust\nagainst the oversmoothing issue while using long-range interactions.\nExperiments based on seven real-world hypergraph benchmarks in node\nclassification and hyperlink prediction show that, compared to state-of-the-art\nHNNs, TF-HNN exhibits both competitive performance and superior training\nefficiency. Specifically, on the large-scale benchmark, Trivago, TF-HNN\noutperforms the node classification accuracy of the best baseline by 10% with\njust 1% of the training time of that baseline.\n","authors":["Bohan Tang","Zexi Liu","Keyue Jiang","Siheng Chen","Xiaowen Dong"],"pdf_url":"https://arxiv.org/pdf/2402.05569v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00488v2","updated":"2024-10-02T12:55:53Z","published":"2024-08-31T15:47:31Z","title":"Rapid Gyroscope Calibration: A Deep Learning Approach","summary":"  Low-cost gyroscope calibration is essential for ensuring the accuracy and\nreliability of gyroscope measurements. Stationary calibration estimates the\ndeterministic parts of measurement errors. To this end, a common practice is to\naverage the gyroscope readings during a predefined period and estimate the\ngyroscope bias. Calibration duration plays a crucial role in performance,\ntherefore, longer periods are preferred. However, some applications require\nquick startup times and calibration is therefore allowed only for a short time.\nIn this work, we focus on reducing low-cost gyroscope calibration time using\ndeep learning methods. We propose a deep-learning framework and explore the\npossibilities of using multiple real and virtual gyroscopes to improve the\ncalibration performance of single gyroscopes. To train and validate our\napproach, we recorded a dataset consisting of 169 hours of gyroscope readings,\nusing 24 gyroscopes of two different brands. We also created a virtual dataset\nconsisting of simulated gyroscope readings. The two datasets were used to\nevaluate our proposed approach. One of our key achievements in this work is\nreducing gyroscope calibration time by up to 89% using three low-cost\ngyroscopes.\n","authors":["Yair Stolero","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2409.00488v2.pdf","comment":"10 Pages, 14 Figures"},{"id":"http://arxiv.org/abs/2410.01500v1","updated":"2024-10-02T12:51:25Z","published":"2024-10-02T12:51:25Z","title":"Discrete Diffusion Schrdinger Bridge Matching for Graph\n  Transformation","summary":"  Transporting between arbitrary distributions is a fundamental goal in\ngenerative modeling. Recently proposed diffusion bridge models provide a\npotential solution, but they rely on a joint distribution that is difficult to\nobtain in practice. Furthermore, formulations based on continuous domains limit\ntheir applicability to discrete domains such as graphs. To overcome these\nlimitations, we propose Discrete Diffusion Schr\\\"odinger Bridge Matching\n(DDSBM), a novel framework that utilizes continuous-time Markov chains to solve\nthe SB problem in a high-dimensional discrete state space. Our approach extends\nIterative Markovian Fitting to discrete domains, and we have proved its\nconvergence to the SB. Furthermore, we adapt our framework for the graph\ntransformation and show that our design choice of underlying dynamics\ncharacterized by independent modifications of nodes and edges can be\ninterpreted as the entropy-regularized version of optimal transport with a cost\nfunction described by the graph edit distance. To demonstrate the effectiveness\nof our framework, we have applied DDSBM to molecular optimization in the field\nof chemistry. Experimental results demonstrate that DDSBM effectively optimizes\nmolecules' property-of-interest with minimal graph transformation, successfully\nretaining other features.\n","authors":["Jun Hyeong Kim","Seonghwan Kim","Seokhyun Moon","Hyeongwoo Kim","Jeheon Woo","Woo Youn Kim"],"pdf_url":"https://arxiv.org/pdf/2410.01500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04370v2","updated":"2024-10-02T12:49:18Z","published":"2024-06-01T02:08:44Z","title":"Large Language Model Confidence Estimation via Black-Box Access","summary":"  Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b and Mistral-7b on four benchmark Q\\&A tasks as well as of\nPegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.\n","authors":["Tejaswini Pedapati","Amit Dhurandhar","Soumya Ghosh","Soham Dan","Prasanna Sattigeri"],"pdf_url":"https://arxiv.org/pdf/2406.04370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01497v1","updated":"2024-10-02T12:45:52Z","published":"2024-10-02T12:45:52Z","title":"DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,\n  Lightweight Plugin for Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.\n","authors":["Yuxuan Zhang","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2410.01497v1.pdf","comment":"Preprint under review, 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.14219v2","updated":"2024-10-02T12:45:50Z","published":"2024-05-23T06:28:44Z","title":"Understanding the Training and Generalization of Pretrained Transformer\n  for Sequential Decision Making","summary":"  In this paper, we consider the supervised pre-trained transformer for a class\nof sequential decision-making problems. The class of considered problems is a\nsubset of the general formulation of reinforcement learning in that there is no\ntransition probability matrix; though seemingly restrictive, the subset class\nof problems covers bandits, dynamic pricing, and newsvendor problems as special\ncases. Such a structure enables the use of optimal actions/decisions in the\npre-training phase, and the usage also provides new insights for the training\nand generalization of the pre-trained transformer. We first note the training\nof the transformer model can be viewed as a performative prediction problem,\nand the existing methods and theories largely ignore or cannot resolve an\nout-of-distribution issue. We propose a natural solution that includes the\ntransformer-generated action sequences in the training procedure, and it enjoys\nbetter properties both numerically and theoretically. The availability of the\noptimal actions in the considered tasks also allows us to analyze the\nproperties of the pre-trained transformer as an algorithm and explains why it\nmay lack exploration and how this can be automatically resolved. Numerically,\nwe categorize the advantages of pre-trained transformers over the structured\nalgorithms such as UCB and Thompson sampling into three cases: (i) it better\nutilizes the prior knowledge in the pre-training data; (ii) it can elegantly\nhandle the misspecification issue suffered by the structured algorithms; (iii)\nfor short time horizon such as $T\\le50$, it behaves more greedy and enjoys much\nbetter regret than the structured algorithms designed for asymptotic\noptimality.\n","authors":["Hanzhao Wang","Yu Pan","Fupeng Sun","Shang Liu","Kalyan Talluri","Guanting Chen","Xiaocheng Li"],"pdf_url":"https://arxiv.org/pdf/2405.14219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01879v2","updated":"2024-10-02T12:42:56Z","published":"2024-02-02T20:08:11Z","title":"$$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial\n  Examples","summary":"  Evaluating the adversarial robustness of deep networks to gradient-based\nattacks is challenging. While most attacks consider $\\ell_2$- and\n$\\ell_\\infty$-norm constraints to craft input perturbations, only a few\ninvestigate sparse $\\ell_1$- and $\\ell_0$-norm attacks. In particular,\n$\\ell_0$-norm attacks remain the least studied due to the inherent complexity\nof optimizing over a non-convex and non-differentiable constraint. However,\nevaluating adversarial robustness under these attacks could reveal weaknesses\notherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm\nattacks. In this work, we propose a novel $\\ell_0$-norm attack, called\n$\\sigma$-zero, which leverages a differentiable approximation of the $\\ell_0$\nnorm to facilitate gradient-based optimization, and an adaptive projection\noperator to dynamically adjust the trade-off between loss minimization and\nperturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet\ndatasets, involving robust and non-robust models, show that $\\sigma$-zero finds\nminimum $\\ell_0$-norm adversarial examples without requiring any time-consuming\nhyperparameter tuning, and that it outperforms all competing sparse attacks in\nterms of success rate, perturbation size, and efficiency.\n","authors":["Antonio Emanuele Cin","Francesco Villani","Maura Pintor","Lea Schnherr","Battista Biggio","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2402.01879v2.pdf","comment":"Code available at\n  https://github.com/Cinofix/sigma-zero-adversarial-attack"},{"id":"http://arxiv.org/abs/2407.04528v2","updated":"2024-10-02T12:38:39Z","published":"2024-07-05T14:16:47Z","title":"GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning","summary":"  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n","authors":["Aleksander Ficek","Jiaqi Zeng","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2407.04528v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.01483v1","updated":"2024-10-02T12:34:32Z","published":"2024-10-02T12:34:32Z","title":"Foldable SuperNets: Scalable Merging of Transformers with Different\n  Initializations and Tasks","summary":"  Many recent methods aim to merge neural networks (NNs) with identical\narchitectures trained on different tasks to obtain a single multi-task model.\nMost existing works tackle the simpler setup of merging NNs initialized from a\ncommon pre-trained network, where simple heuristics like weight averaging work\nwell. This work targets a more challenging goal: merging large transformers\ntrained on different tasks from distinct initializations. First, we demonstrate\nthat traditional merging methods fail catastrophically in this setup. To\novercome this challenge, we propose Foldable SuperNet Merge (FS-Merge), a\nmethod that optimizes a SuperNet to fuse the original models using a feature\nreconstruction loss. FS-Merge is simple, data-efficient, and capable of merging\nmodels of varying widths. We test FS-Merge against existing methods, including\nknowledge distillation, on MLPs and transformers across various settings,\nsizes, tasks, and modalities. FS-Merge consistently outperforms them, achieving\nSOTA results, particularly in limited data scenarios.\n","authors":["Edan Kinderman","Itay Hubara","Haggai Maron","Daniel Soudry"],"pdf_url":"https://arxiv.org/pdf/2410.01483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01482v1","updated":"2024-10-02T12:34:04Z","published":"2024-10-02T12:34:04Z","title":"One Wave to Explain Them All: A Unifying Perspective on Post-hoc\n  Explainability","summary":"  Despite the growing use of deep neural networks in safety-critical\ndecision-making, their inherent black-box nature hinders transparency and\ninterpretability. Explainable AI (XAI) methods have thus emerged to understand\na model's internal workings, and notably attribution methods also called\nsaliency maps. Conventional attribution methods typically identify the\nlocations -- the where -- of significant regions within an input. However,\nbecause they overlook the inherent structure of the input data, these methods\noften fail to interpret what these regions represent in terms of structural\ncomponents (e.g., textures in images or transients in sounds). Furthermore,\nexisting methods are usually tailored to a single data modality, limiting their\ngeneralizability. In this paper, we propose leveraging the wavelet domain as a\nrobust mathematical foundation for attribution. Our approach, the Wavelet\nAttribution Method (WAM) extends the existing gradient-based feature\nattributions into the wavelet domain, providing a unified framework for\nexplaining classifiers across images, audio, and 3D shapes. Empirical\nevaluations demonstrate that WAM matches or surpasses state-of-the-art methods\nacross faithfulness metrics and models in image, audio, and 3D explainability.\nFinally, we show how our method explains not only the where -- the important\nparts of the input -- but also the what -- the relevant patterns in terms of\nstructural components.\n","authors":["Gabriel Kasmi","Amandine Brunetto","Thomas Fel","Jayneel Parekh"],"pdf_url":"https://arxiv.org/pdf/2410.01482v1.pdf","comment":"main: 10 pages, appendix: 14 pages, 5 Tables, 25 Figures"},{"id":"http://arxiv.org/abs/2410.01480v1","updated":"2024-10-02T12:33:16Z","published":"2024-10-02T12:33:16Z","title":"Introducing Flexible Monotone Multiple Choice Item Response Theory\n  Models and Bit Scales","summary":"  Item Response Theory (IRT) is a powerful statistical approach for evaluating\ntest items and determining test taker abilities through response analysis. An\nIRT model that better fits the data leads to more accurate latent trait\nestimates. In this study, we present a new model for multiple choice data, the\nmonotone multiple choice (MMC) model, which we fit using autoencoders. Using\nboth simulated scenarios and real data from the Swedish Scholastic Aptitude\nTest, we demonstrate empirically that the MMC model outperforms the traditional\nnominal response IRT model in terms of fit. Furthermore, we illustrate how the\nlatent trait scale from any fitted IRT model can be transformed into a ratio\nscale, aiding in score interpretation and making it easier to compare different\ntypes of IRT models. We refer to these new scales as bit scales. Bit scales are\nespecially useful for models for which minimal or no assumptions are made for\nthe latent trait scale distributions, such as for the autoencoder fitted models\nin this study.\n","authors":["Joakim Wallmark","Maria Josefsson","Marie Wiberg"],"pdf_url":"https://arxiv.org/pdf/2410.01480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01569v2","updated":"2024-10-02T12:31:11Z","published":"2024-04-02T02:03:28Z","title":"Evaluating Large Language Models Using Contrast Sets: An Experimental\n  Approach","summary":"  In the domain of Natural Language Inference (NLI), especially in tasks\ninvolving the classification of multiple input texts, the Cross-Entropy Loss\nmetric is widely employed as a standard for error measurement. However, this\nmetric falls short in effectively evaluating a model's capacity to understand\nlanguage entailments. In this study, we introduce an innovative technique for\ngenerating a contrast set for the Stanford Natural Language Inference (SNLI)\ndataset. Our strategy involves the automated substitution of verbs, adverbs,\nand adjectives with their synonyms to preserve the original meaning of\nsentences. This method aims to assess whether a model's performance is based on\ngenuine language comprehension or simply on pattern recognition. We conducted\nour analysis using the ELECTRA-small model. The model achieved an accuracy of\n89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%\non our contrast set, indicating a substantial 17% decline. This outcome led us\nto conduct a detailed examination of the model's learning behaviors. Following\nthis, we improved the model's resilience by fine-tuning it with a\ncontrast-enhanced training dataset specifically designed for SNLI, which\nincreased its accuracy to 85.5% on the contrast sets. Our findings highlight\nthe importance of incorporating diverse linguistic expressions into datasets\nfor NLI tasks. We hope that our research will encourage the creation of more\ninclusive datasets, thereby contributing to the development of NLI models that\nare both more sophisticated and effective.\n","authors":["Manish Sanwal"],"pdf_url":"https://arxiv.org/pdf/2404.01569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01476v1","updated":"2024-10-02T12:30:05Z","published":"2024-10-02T12:30:05Z","title":"Reducing Variance in Meta-Learning via Laplace Approximation for\n  Regression Tasks","summary":"  Given a finite set of sample points, meta-learning algorithms aim to learn an\noptimal adaptation strategy for new, unseen tasks. Often, this data can be\nambiguous as it might belong to different tasks concurrently. This is\nparticularly the case in meta-regression tasks. In such cases, the estimated\nadaptation strategy is subject to high variance due to the limited amount of\nsupport data for each task, which often leads to sub-optimal generalization\nperformance. In this work, we address the problem of variance reduction in\ngradient-based meta-learning and formalize the class of problems prone to this,\na condition we refer to as \\emph{task overlap}. Specifically, we propose a\nnovel approach that reduces the variance of the gradient estimate by weighing\neach support point individually by the variance of its posterior over the\nparameters. To estimate the posterior, we utilize the Laplace approximation,\nwhich allows us to express the variance in terms of the curvature of the loss\nlandscape of our meta-learner. Experimental results demonstrate the\neffectiveness of the proposed method and highlight the importance of variance\nreduction in meta-learning.\n","authors":["Alfredo Reichlin","Gustaf Tegnr","Miguel Vasco","Hang Yin","Mrten Bjrkman","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2410.01476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10780v2","updated":"2024-10-02T12:27:13Z","published":"2024-07-15T14:59:43Z","title":"Correlations Are Ruining Your Gradient Descent","summary":"  Herein the topics of (natural) gradient descent, data decorrelation, and\napproximate methods for backpropagation are brought into a common discussion.\nNatural gradient descent illuminates how gradient vectors, pointing at\ndirections of steepest descent, can be improved by considering the local\ncurvature of loss landscapes. We extend this perspective and show that to fully\nsolve the problem illuminated by natural gradients in neural networks, one must\nrecognise that correlations in the data at any linear transformation, including\nnode responses at every layer of a neural network, cause a non-orthonormal\nrelationship between the model's parameters. To solve this requires a method\nfor decorrelating inputs at each individual layer of a neural network. We\ndescribe a range of methods which have been proposed for decorrelation and\nwhitening of node output, and expand on these to provide a novel method\nspecifically useful for distributed computing and computational neuroscience.\nImplementing decorrelation within multi-layer neural networks, we can show that\nnot only is training via backpropagation sped up significantly but also\nexisting approximations of backpropagation, which have failed catastrophically\nin the past, benefit significantly in their accuracy and convergence speed.\nThis has the potential to provide a route forward for approximate gradient\ndescent methods which have previously been discarded, training approaches for\nanalogue and neuromorphic hardware, and potentially insights as to the efficacy\nand utility of decorrelation processes in the brain.\n","authors":["Nasir Ahmad"],"pdf_url":"https://arxiv.org/pdf/2407.10780v2.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19531v2","updated":"2024-10-02T12:22:51Z","published":"2024-06-27T21:12:26Z","title":"Off-policy Evaluation with Deeply-abstracted States","summary":"  Off-policy evaluation (OPE) is crucial for assessing a target policy's impact\noffline before its deployment. However, achieving accurate OPE in large state\nspaces remains challenging. This paper studies state abstractions -- originally\ndesigned for policy learning -- in the context of OPE. Our contributions are\nthree-fold: (i) We define a set of irrelevance conditions central to learning\nstate abstractions for OPE, and derive a backward-model-irrelevance condition\nfor achieving irrelevance in %sequential and (marginalized) importance sampling\nratios by constructing a time-reversed Markov decision process (MDP). (ii) We\npropose a novel iterative procedure that sequentially projects the original\nstate space into a smaller space, resulting in a deeply-abstracted state, which\nsubstantially simplifies the sample complexity of OPE arising from high\ncardinality. (iii) We prove the Fisher consistencies of various OPE estimators\nwhen applied to our proposed abstract state spaces.\n","authors":["Meiling Hao","Pingfan Su","Liyuan Hu","Zoltan Szabo","Qingyuan Zhao","Chengchun Shi"],"pdf_url":"https://arxiv.org/pdf/2406.19531v2.pdf","comment":"56 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.09858v2","updated":"2024-10-02T12:22:10Z","published":"2024-08-19T10:03:14Z","title":"ShortCircuit: AlphaZero-Driven Circuit Design","summary":"  Chip design relies heavily on generating Boolean circuits, such as\nAND-Inverter Graphs (AIGs), from functional descriptions like truth tables.\nThis generation operation is a key process in logic synthesis, a primary chip\ndesign stage. While recent advances in deep learning have aimed to accelerate\ncircuit design, these efforts have mostly focused on tasks other than\nsynthesis, and traditional heuristic methods have plateaued. In this paper, we\nintroduce ShortCircuit, a novel transformer-based architecture that leverages\nthe structural properties of AIGs and performs efficient space exploration.\nContrary to prior approaches attempting end-to-end generation of logic circuits\nusing deep networks, ShortCircuit employs a two-phase process combining\nsupervised with reinforcement learning to enhance generalization to unseen\ntruth tables. We also propose an AlphaZero variant to handle the double\nexponentially large state space and the reward sparsity, enabling the discovery\nof near-optimal designs. To evaluate the generative performance of our model ,\nwe extract 500 truth tables from a set of 20 real-world circuits. ShortCircuit\nsuccessfully generates AIGs for $98\\%$ of the 8-input test truth tables, and\noutperforms the state-of-the-art logic synthesis tool, ABC, by $18.62\\%$ in\nterms of circuits size.\n","authors":["Dimitrios Tsaras","Antoine Grosnit","Lei Chen","Zhiyao Xie","Haitham Bou-Ammar","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.09858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01464v1","updated":"2024-10-02T12:16:46Z","published":"2024-10-02T12:16:46Z","title":"Flow Matching for Accelerated Simulation of Atomic Transport in\n  Materials","summary":"  We introduce LiFlow, a generative framework to accelerate molecular dynamics\n(MD) simulations for crystalline materials that formulates the task as\nconditional generation of atomic displacements. The model uses flow matching,\nwith a Propagator submodel to generate atomic displacements and a Corrector to\nlocally correct unphysical geometries, and incorporates an adaptive prior based\non the Maxwell-Boltzmann distribution to account for chemical and thermal\nconditions. We benchmark LiFlow on a dataset comprising 25-ps trajectories of\nlithium diffusion across 4,186 solid-state electrolyte (SSE) candidates at four\ntemperatures. The model obtains a consistent Spearman rank correlation of\n0.7-0.8 for lithium mean squared displacement (MSD) predictions on unseen\ncompositions. Furthermore, LiFlow generalizes from short training trajectories\nto larger supercells and longer simulations while maintaining high accuracy.\nWith speed-ups of up to 600,000$\\times$ compared to first-principles methods,\nLiFlow enables scalable simulations at significantly larger length and time\nscales.\n","authors":["Juno Nam","Sulin Liu","Gavin Winter","KyuJung Jun","Soojung Yang","Rafael Gmez-Bombarelli"],"pdf_url":"https://arxiv.org/pdf/2410.01464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01463v1","updated":"2024-10-02T12:14:36Z","published":"2024-10-02T12:14:36Z","title":"Selective Aggregation for Low-Rank Adaptation in Federated Learning","summary":"  We investigate LoRA in federated learning through the lens of the asymmetry\nanalysis of the learned $A$ and $B$ matrices. In doing so, we uncover that $A$\nmatrices are responsible for learning general knowledge, while $B$ matrices\nfocus on capturing client-specific knowledge. Based on this finding, we\nintroduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two\nlow-rank trainable matrices $A$ and $B$ to model the weight update, but only\n$A$ matrices are shared with the server for aggregation. Moreover, we delve\ninto the relationship between the learned $A$ and $B$ matrices in other LoRA\nvariants, such as rsLoRA and VeRA, revealing a consistent pattern.\nConsequently, we extend our FedSA-LoRA method to these LoRA variants, resulting\nin FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm\nfor integrating LoRA with FL, offering guidance for future work on subsequent\nLoRA variants combined with FL. Extensive experimental results on natural\nlanguage understanding and generation tasks demonstrate the effectiveness of\nthe proposed method.\n","authors":["Pengxin Guo","Shuang Zeng","Yanran Wang","Huijie Fan","Feifei Wang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2410.01463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07656v4","updated":"2024-10-02T12:12:31Z","published":"2024-01-15T12:52:56Z","title":"Learning Explainable and Better Performing Representations of POMDP\n  Strategies","summary":"  Strategies for partially observable Markov decision processes (POMDP)\ntypically require memory. One way to represent this memory is via automata. We\npresent a method to learn an automaton representation of a strategy using a\nmodification of the L*-algorithm. Compared to the tabular representation of a\nstrategy, the resulting automaton is dramatically smaller and thus also more\nexplainable. Moreover, in the learning process, our heuristics may even improve\nthe strategy's performance. In contrast to approaches that synthesize an\nautomaton directly from the POMDP thereby solving it, our approach is\nincomparably more scalable.\n","authors":["Alexander Bork","Debraj Chakraborty","Kush Grover","Jan Kretinsky","Stefanie Mohr"],"pdf_url":"https://arxiv.org/pdf/2401.07656v4.pdf","comment":"Technical report for the submission to TACAS 24"},{"id":"http://arxiv.org/abs/2410.01458v1","updated":"2024-10-02T12:10:07Z","published":"2024-10-02T12:10:07Z","title":"From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with\n  LLM-Guided Knowledge","summary":"  Q-shaping is an extension of Q-value initialization and serves as an\nalternative to reward shaping for incorporating domain knowledge to accelerate\nagent training, thereby improving sample efficiency by directly shaping\nQ-values. This approach is both general and robust across diverse tasks,\nallowing for immediate impact assessment while guaranteeing optimality. We\nevaluated Q-shaping across 20 different environments using a large language\nmodel (LLM) as the heuristic provider. The results demonstrate that Q-shaping\nsignificantly enhances sample efficiency, achieving a \\textbf{16.87\\%}\nimprovement over the best baseline in each environment and a \\textbf{253.80\\%}\nimprovement compared to LLM-based reward shaping methods. These findings\nestablish Q-shaping as a superior and unbiased alternative to conventional\nreward shaping in reinforcement learning.\n","authors":["Xiefeng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.01458v1.pdf","comment":"q-shaping, reinforcement learning, reward shaping"},{"id":"http://arxiv.org/abs/2410.01457v1","updated":"2024-10-02T12:07:47Z","published":"2024-10-02T12:07:47Z","title":"Verbalized Graph Representation Learning: A Fully Interpretable Graph\n  Model Based on Large Language Models Throughout the Entire Process","summary":"  Representation learning on text-attributed graphs (TAGs) has attracted\nsignificant interest due to its wide-ranging real-world applications,\nparticularly through Graph Neural Networks (GNNs). Traditional GNN methods\nfocus on encoding the structural information of graphs, often using shallow\ntext embeddings for node or edge attributes. This limits the model to\nunderstand the rich semantic information in the data and its reasoning ability\nfor complex downstream tasks, while also lacking interpretability. With the\nrise of large language models (LLMs), an increasing number of studies are\ncombining them with GNNs for graph representation learning and downstream\ntasks. While these approaches effectively leverage the rich semantic\ninformation in TAGs datasets, their main drawback is that they are only\npartially interpretable, which limits their application in critical fields. In\nthis paper, we propose a verbalized graph representation learning (VGRL) method\nwhich is fully interpretable. In contrast to traditional graph machine learning\nmodels, which are usually optimized within a continuous parameter space, VGRL\nconstrains this parameter space to be text description which ensures complete\ninterpretability throughout the entire process, making it easier for users to\nunderstand and trust the decisions of the model. We conduct several studies to\nempirically evaluate the effectiveness of VGRL and we believe these method can\nserve as a stepping stone in graph representation learning.\n","authors":["Xingyu Ji","Jiale Liu","Lu Li","Maojun Wang","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01457v1.pdf","comment":"under review. corresponding author: Zeyu Zhang"},{"id":"http://arxiv.org/abs/2410.01452v1","updated":"2024-10-02T12:02:43Z","published":"2024-10-02T12:02:43Z","title":"Ensembles provably learn equivariance through data augmentation","summary":"  Recently, it was proved that group equivariance emerges in ensembles of\nneural networks as the result of full augmentation in the limit of infinitely\nwide neural networks (neural tangent kernel limit). In this paper, we extend\nthis result significantly. We provide a proof that this emergence does not\ndepend on the neural tangent kernel limit at all. We also consider stochastic\nsettings, and furthermore general architectures. For the latter, we provide a\nsimple sufficient condition on the relation between the architecture and the\naction of the group for our results to hold. We validate our findings through\nsimple numeric experiments.\n","authors":["Oskar Nordenfors","Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2410.01452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10115v2","updated":"2024-10-02T11:59:27Z","published":"2024-04-15T20:07:44Z","title":"Multiple-Input Fourier Neural Operator (MIFNO) for source-dependent 3D\n  elastodynamics","summary":"  Numerical simulations are essential tools to evaluate the solution of the\nwave equation in complex settings, such as three-dimensional (3D) domains with\nheterogeneous properties. However, their application is limited by high\ncomputational costs and existing surrogate models lack the flexibility of\nnumerical solvers. This work introduces the Multiple-Input Fourier Neural\nOperator (MIFNO) to deal with structured 3D fields representing material\nproperties as well as vectors describing the source characteristics. The MIFNO\nis applied to the problem of elastic wave propagation in the Earth's crust. It\nis trained on the HEMEW^S-3D database containing 30000 earthquake simulations\nin different heterogeneous domains with random source positions and\norientations. Outputs are time- and space-dependent surface wavefields. The\nMIFNO predictions are assessed as good to excellent based on Goodness-Of-Fit\n(GOF) criteria. Wave arrival times and wave fronts' propagation are very\naccurate since 80% of the predictions have an excellent phase GOF. The\nfluctuations amplitudes are good for 87% of the predictions. The envelope score\nis hindered by the small-scale fluctuations that are challenging to capture due\nto the complex physical phenomena associated with high-frequency features.\nNevertheless, the MIFNO can generalize to sources located outside the training\ndomain and it shows good generalization ability to a real complex overthrust\ngeology. When focusing on a region of interest, transfer learning improves the\naccuracy with limited additional costs, since GOF scores improved by more than\n1 GOF unit with only 500 additional specific samples. The MIFNO is the first\nsurrogate model offering the flexibility of an earthquake simulator with\nvarying sources and material properties. Its good accuracy and massive speed-up\noffer new perspectives to replace numerical simulations in many-query problems.\n","authors":["Fanny Lehmann","Filippo Gatti","Didier Clouteau"],"pdf_url":"https://arxiv.org/pdf/2404.10115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01444v1","updated":"2024-10-02T11:54:06Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  Compositionality, the notion that the meaning of an expression is constructed\nfrom the meaning of its parts and syntactic rules, permits the infinite\nproductivity of human language. For the first time, artificial language models\n(LMs) are able to match human performance in a number of compositional\ngeneralization tasks. However, much remains to be understood about the\nrepresentational mechanisms underlying these abilities. We take a high-level\ngeometric approach to this problem by relating the degree of compositionality\nin a dataset to the intrinsic dimensionality of its representations under an\nLM, a measure of feature complexity. We find not only that the degree of\ndataset compositionality is reflected in representations' intrinsic\ndimensionality, but that the relationship between compositionality and\ngeometric complexity arises due to learned linguistic features over training.\nFinally, our analyses reveal a striking contrast between linear and nonlinear\ndimensionality, showing that they respectively encode formal and semantic\naspects of linguistic composition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.09549v2","updated":"2024-10-02T11:44:26Z","published":"2024-06-13T19:30:32Z","title":"Urdu Dependency Parsing and Treebank Development: A Syntactic and\n  Morphological Perspective","summary":"  Parsing is the process of analyzing a sentence's syntactic structure by\nbreaking it down into its grammatical components. and is critical for various\nlinguistic applications. Urdu is a low-resource, free word-order language and\nexhibits complex morphology. Literature suggests that dependency parsing is\nwell-suited for such languages. Our approach begins with a basic feature model\nencompassing word location, head word identification, and dependency relations,\nfollowed by a more advanced model integrating part-of-speech (POS) tags and\nmorphological attributes (e.g., suffixes, gender). We manually annotated a\ncorpus of news articles of varying complexity. Using Maltparser and the\nNivreEager algorithm, we achieved a best-labeled accuracy (LA) of 70% and an\nunlabeled attachment score (UAS) of 84%, demonstrating the feasibility of\ndependency parsing for Urdu.\n","authors":["Nudrat Habib"],"pdf_url":"https://arxiv.org/pdf/2406.09549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01440v1","updated":"2024-10-02T11:42:49Z","published":"2024-10-02T11:42:49Z","title":"Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence\n  Modeling","summary":"  In the endeavor to make autonomous robots take actions, task planning is a\nmajor challenge that requires translating high-level task descriptions into\nlong-horizon action sequences. Despite recent advances in language model\nagents, they remain prone to planning errors and limited in their ability to\nplan ahead. To address these limitations in robotic planning, we advocate a\nself-refining scheme that iteratively refines a draft plan until an equilibrium\nis reached. Remarkably, this process can be optimized end-to-end from an\nanalytical perspective without the need to curate additional verifiers or\nreward models, allowing us to train self-refining planners in a simple\nsupervised learning fashion. Meanwhile, a nested equilibrium sequence modeling\nprocedure is devised for efficient closed-loop planning that incorporates\nuseful feedback from the environment (or an internal world model). Our method\nis evaluated on the VirtualHome-Env benchmark, showing advanced performance\nwith better scaling for inference computation. Code is available at\nhttps://github.com/Singularity0104/equilibrium-planner.\n","authors":["Jinghan Li","Zhicheng Sun","Fei Li","Cao Sheng","Jiazhong Yu","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2410.01440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01438v1","updated":"2024-10-02T11:40:49Z","published":"2024-10-02T11:40:49Z","title":"Information-Theoretical Principled Trade-off between Jailbreakability\n  and Stealthiness on Vision Language Models","summary":"  In recent years, Vision-Language Models (VLMs) have demonstrated significant\nadvancements in artificial intelligence, transforming tasks across various\ndomains. Despite their capabilities, these models are susceptible to jailbreak\nattacks, which can compromise their safety and reliability. This paper explores\nthe trade-off between jailbreakability and stealthiness in VLMs, presenting a\nnovel algorithm to detect non-stealthy jailbreak attacks and enhance model\nrobustness. We introduce a stealthiness-aware jailbreak attack using diffusion\nmodels, highlighting the challenge of detecting AI-generated content. Our\napproach leverages Fano's inequality to elucidate the relationship between\nattack success rates and stealthiness scores, providing an explainable\nframework for evaluating these threats. Our contributions aim to fortify AI\nsystems against sophisticated attacks, ensuring their outputs remain aligned\nwith ethical standards and user expectations.\n","authors":["Ching-Chia Kao","Chia-Mu Yu","Chun-Shien Lu","Chu-Song Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01434v1","updated":"2024-10-02T11:36:45Z","published":"2024-10-02T11:36:45Z","title":"Circuit Compositions: Exploring Modular Structures in Transformer-Based\n  Language Models","summary":"  A fundamental question in interpretability research is to what extent neural\nnetworks, particularly language models, implement reusable functions via\nsubnetworks that can be composed to perform more complex tasks. Recent\ndevelopments in mechanistic interpretability have made progress in identifying\nsubnetworks, often referred to as circuits, which represent the minimal\ncomputational subgraph responsible for a model's behavior on specific tasks.\nHowever, most studies focus on identifying circuits for individual tasks\nwithout investigating how functionally similar circuits relate to each other.\nTo address this gap, we examine the modularity of neural networks by analyzing\ncircuits for highly compositional subtasks within a transformer-based language\nmodel. Specifically, given a probabilistic context-free grammar, we identify\nand compare circuits responsible for ten modular string-edit operations. Our\nresults indicate that functionally similar circuits exhibit both notable node\noverlap and cross-task faithfulness. Moreover, we demonstrate that the circuits\nidentified can be reused and combined through subnetwork set operations to\nrepresent more complex functional capabilities of the model.\n","authors":["Philipp Mondorf","Sondre Wold","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.01434v1.pdf","comment":"24 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.11085v4","updated":"2024-10-02T11:35:45Z","published":"2023-10-17T09:10:27Z","title":"Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained\n  Language Models","summary":"  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\nconduct an extensive benchmark demonstrating the effectiveness of our\nframework, achieving state-of-the-art results across six relation extraction\ndatasets and outperforming more than 30 baseline methods. Unlike our framework,\nthe baseline methods have large computational overhead (e.g., from\nfine-tuning). To the best of our knowledge, we are the first to reformulate the\ndocument-level relation extraction task as a tailored in-context few-shot\nlearning paradigm.\n","authors":["Yilmazcan Ozyurt","Stefan Feuerriegel","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.11085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01432v1","updated":"2024-10-02T11:33:13Z","published":"2024-10-02T11:33:13Z","title":"Adaptive teachers for amortized samplers","summary":"  Amortized inference is the task of training a parametric model, such as a\nneural network, to approximate a distribution with a given unnormalized density\nwhere exact sampling is intractable. When sampling is implemented as a\nsequential decision-making process, reinforcement learning (RL) methods, such\nas generative flow networks, can be used to train the sampling policy.\nOff-policy RL training facilitates the discovery of diverse, high-reward\ncandidates, but existing methods still face challenges in efficient\nexploration. We propose to use an adaptive training distribution (the Teacher)\nto guide the training of the primary amortized sampler (the Student) by\nprioritizing high-loss regions. The Teacher, an auxiliary behavior model, is\ntrained to sample high-error regions of the Student and can generalize across\nunexplored modes, thereby enhancing mode coverage by providing an efficient\ntraining curriculum. We validate the effectiveness of this approach in a\nsynthetic environment designed to present an exploration challenge, two\ndiffusion-based sampling tasks, and four biochemical discovery tasks\ndemonstrating its ability to improve sample efficiency and mode coverage.\n","authors":["Minsu Kim","Sanghyeok Choi","Taeyoung Yun","Emmanuel Bengio","Leo Feng","Jarrid Rector-Brooks","Sungsoo Ahn","Jinkyoo Park","Nikolay Malkin","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2410.01432v1.pdf","comment":"26 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.01431v1","updated":"2024-10-02T11:31:48Z","published":"2024-10-02T11:31:48Z","title":"Scalable Reinforcement Learning-based Neural Architecture Search","summary":"  In this publication, we assess the ability of a novel Reinforcement\nLearning-based solution to the problem of Neural Architecture Search, where a\nReinforcement Learning (RL) agent learns to search for good architectures,\nrather than to return a single optimal architecture. We consider both the\nNAS-Bench-101 and NAS- Bench-301 settings, and compare against various known\nstrong baselines, such as local search and random search. We conclude that our\nReinforcement Learning agent displays strong scalability with regards to the\nsize of the search space, but limited robustness to hyperparameter changes.\n","authors":["Amber Cassimon","Siegfried Mercelis","Kevin Mets"],"pdf_url":"https://arxiv.org/pdf/2410.01431v1.pdf","comment":"33 Pages, 19 Figures"},{"id":"http://arxiv.org/abs/2410.01426v1","updated":"2024-10-02T11:23:09Z","published":"2024-10-02T11:23:09Z","title":"Approximation by Steklov Neural Network Operators","summary":"  The present paper deals with construction of newly family of Neural Network\noperators, that is,Steklov Neural Network operators. By using Steklov type\nintegral, we introduce a new version of Neural Network operators and we obtain\nsome convergence theorems for the family, such as, pointwise and uniform\nconvergence,rate of convergence via moduli of smoothness of order $r$.\n","authors":["S. N. Karaman","M. Turgay","T. Acar"],"pdf_url":"https://arxiv.org/pdf/2410.01426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.15432v2","updated":"2024-10-02T11:20:22Z","published":"2021-06-29T14:01:40Z","title":"On exploring the potential of quantum auto-encoder for learning quantum\n  systems","summary":"  The frequent interactions between quantum computing and machine learning\nrevolutionize both fields. One prototypical achievement is the quantum\nauto-encoder (QAE), as the leading strategy to relieve the curse of\ndimensionality ubiquitous in the quantum world. Despite its attractive\ncapabilities, practical applications of QAE have yet largely unexplored. To\nnarrow this knowledge gap, here we devise three effective QAE-based learning\nprotocols to address three classically computational hard learning problems\nwhen learning quantum systems, which are low-rank state fidelity estimation,\nquantum Fisher information estimation, and Gibbs state preparation. Attributed\nto the versatility of QAE, our proposals can be readily executed on near-term\nquantum machines. Besides, we analyze the error bounds of the trained protocols\nand showcase the necessary conditions to provide practical utility from the\nperspective of complexity theory. We conduct numerical simulations to confirm\nthe effectiveness of the proposed three protocols. Our work sheds new light on\ndeveloping advanced quantum learning algorithms to accomplish hard quantum\nphysics and quantum information processing tasks.\n","authors":["Yuxuan Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2106.15432v2.pdf","comment":"Accepted to IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2410.01423v1","updated":"2024-10-02T11:16:11Z","published":"2024-10-02T11:16:11Z","title":"Fair4Free: Generating High-fidelity Fair Synthetic Samples using Data\n  Free Distillation","summary":"  This work presents Fair4Free, a novel generative model to generate synthetic\nfair data using data-free distillation in the latent space. Fair4Free can work\non the situation when the data is private or inaccessible. In our approach, we\nfirst train a teacher model to create fair representation and then distil the\nknowledge to a student model (using a smaller architecture). The process of\ndistilling the student model is data-free, i.e. the student model does not have\naccess to the training dataset while distilling. After the distillation, we use\nthe distilled model to generate fair synthetic samples. Our extensive\nexperiments show that our synthetic samples outperform state-of-the-art models\nin all three criteria (fairness, utility and synthetic quality) with a\nperformance increase of 5% for fairness, 8% for utility and 12% in synthetic\nquality for both tabular and image datasets.\n","authors":["Md Fahim Sikder","Daniel de Leng","Fredrik Heintz"],"pdf_url":"https://arxiv.org/pdf/2410.01423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06835v4","updated":"2024-10-02T11:15:25Z","published":"2023-11-12T13:25:28Z","title":"Open-Set Graph Anomaly Detection via Normal Structure Regularisation","summary":"  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). Those labelled training data provide crucial prior\nknowledge about abnormalities for GAD models, enabling substantially reduced\ndetection errors. However, current supervised GAD methods tend to\nover-emphasise fitting the seen anomalies, leading to many errors of detecting\nthe unseen anomalies as normal nodes. Further, existing open-set AD models were\nintroduced to handle Euclidean data, failing to effectively capture\ndiscriminative features from graph structure and node attributes for GAD. In\nthis work, we propose a novel open-set GAD approach, namely normal structure\nregularisation (NSReg), to achieve generalised detection ability to unseen\nanomalies, while maintaining its effectiveness on detecting seen anomalies. The\nkey idea in NSReg is to introduce a regularisation term that enforces the\nlearning of compact, semantically-rich representations of normal nodes based on\ntheir structural relations to other nodes. When being optimised with supervised\nanomaly detection losses, the regularisation term helps incorporate strong\nnormality into the modelling, and thus, it effectively avoids over-fitting the\nseen anomalies and learns a better normality decision boundary, largely\nreducing the false negatives of detecting unseen anomalies as normal. Extensive\nempirical results on seven real-world datasets show that NSReg significantly\noutperforms state-of-the-art competing methods by at least 14% AUC-ROC on the\nunseen anomaly classes and by 10% AUC-ROC on all anomaly classes.\n","authors":["Qizhou Wang","Guansong Pang","Mahsa Salehi","Xiaokun Xia","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2311.06835v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.01737v1","updated":"2024-10-02T16:47:55Z","published":"2024-10-02T16:47:55Z","title":"RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection","summary":"  Multimodal Industrial Anomaly Detection (MIAD), utilizing 3D point clouds and\n2D RGB images to identify the abnormal region of products, plays a crucial role\nin industrial quality inspection. However, the conventional MIAD setting\npresupposes that all 2D and 3D modalities are paired, overlooking the fact that\nmultimodal data collected from the real world is often imperfect due to missing\nmodalities. Consequently, MIAD models that demonstrate robustness against\nmodal-incomplete data are highly desirable in practice. To address this\npractical challenge, we introduce a first-of-its-kind study that\ncomprehensively investigates Modality-Incomplete Industrial Anomaly Detection\n(MIIAD), to consider the imperfect learning environment in which the multimodal\ninformation may be incomplete. Not surprisingly, we discovered that most\nexisting MIAD approaches are inadequate for addressing MIIAD challenges,\nleading to significant performance degradation on the MIIAD benchmark we\ndeveloped. In this paper, we propose a novel two-stage Robust\nmodAlity-imcomplete fusing and Detecting frAmewoRk, abbreviated as RADAR. Our\nbootstrapping philosophy is to enhance two stages in MIIAD, improving the\nrobustness of the Multimodal Transformer: i) In feature fusion, we first\nexplore learning modality-incomplete instruction, guiding the pre-trained\nMultimodal Transformer to robustly adapt to various modality-incomplete\nscenarios, and implement adaptive parameter learning based on a HyperNetwork;\nii) In anomaly detection, we construct a real-pseudo hybrid module to highlight\nthe distinctiveness of modality combinations, further enhancing the robustness\nof the MIIAD model. Our experimental results demonstrate that the proposed\nRADAR significantly surpasses conventional MIAD methods in terms of\neffectiveness and robustness on our newly created MIIAD dataset, underscoring\nits practical application value.\n","authors":["Bingchen Miao","Wenqiao Zhang","Juncheng Li","Siliang Tang","Zhaocheng Li","Haochen Shi","Jun Xiao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.01737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13049v2","updated":"2024-10-02T13:04:02Z","published":"2024-09-19T18:55:13Z","title":"DiffSSD: A Diffusion-Based Dataset For Speech Forensics","summary":"  Diffusion-based speech generators are ubiquitous. These methods can generate\nvery high quality synthetic speech and several recent incidents report their\nmalicious use. To counter such misuse, synthetic speech detectors have been\ndeveloped. Many of these detectors are trained on datasets which do not include\ndiffusion-based synthesizers. In this paper, we demonstrate that existing\ndetectors trained on one such dataset, ASVspoof2019, do not perform well in\ndetecting synthetic speech from recent diffusion-based synthesizers. We propose\nthe Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of\nabout 200 hours of labeled speech, including synthetic speech generated by 8\ndiffusion-based open-source and 2 commercial generators. We also examine the\nperformance of existing synthetic speech detectors on DiffSSD in both\nclosed-set and open-set scenarios. The results highlight the importance of this\ndataset in detecting synthetic speech generated from recent open-source and\ncommercial speech generators.\n","authors":["Kratika Bhagtani","Amit Kumar Singh Yadav","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2409.13049v2.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP) 2025"},{"id":"http://arxiv.org/abs/2410.01366v1","updated":"2024-10-02T09:28:21Z","published":"2024-10-02T09:28:21Z","title":"Harnessing the Latent Diffusion Model for Training-Free Image Style\n  Transfer","summary":"  Diffusion models have recently shown the ability to generate high-quality\nimages. However, controlling its generation process still poses challenges. The\nimage style transfer task is one of those challenges that transfers the visual\nattributes of a style image to another content image. Typical obstacle of this\ntask is the requirement of additional training of a pre-trained model. We\npropose a training-free style transfer algorithm, Style Tracking Reverse\nDiffusion Process (STRDP) for a pretrained Latent Diffusion Model (LDM). Our\nalgorithm employs Adaptive Instance Normalization (AdaIN) function in a\ndistinct manner during the reverse diffusion process of an LDM while tracking\nthe encoding history of the style image. This algorithm enables style transfer\nin the latent space of LDM for reduced computational cost, and provides\ncompatibility for various LDM models. Through a series of experiments and a\nuser study, we show that our method can quickly transfer the style of an image\nwithout additional training. The speed, compatibility, and training-free aspect\nof our algorithm facilitates agile experiments with combinations of styles and\nLDMs for extensive application.\n","authors":["Kento Masui","Mayu Otani","Masahiro Nomura","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2410.01366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18680v2","updated":"2024-10-02T01:45:40Z","published":"2024-09-27T12:06:53Z","title":"Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models","summary":"  Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.\n","authors":["Yiming Chen","Xianghu Yue","Xiaoxue Gao","Chen Zhang","Luis Fernando D'Haro","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2409.18680v2.pdf","comment":"EMNLP24 Findings"},{"id":"http://arxiv.org/abs/2410.01906v1","updated":"2024-10-02T18:05:03Z","published":"2024-10-02T18:05:03Z","title":"Social Media Authentication and Combating Deepfakes using Semi-fragile\n  Invisible Image Watermarking","summary":"  With the significant advances in deep generative models for image and video\nsynthesis, Deepfakes and manipulated media have raised severe societal\nconcerns. Conventional machine learning classifiers for deepfake detection\noften fail to cope with evolving deepfake generation technology and are\nsusceptible to adversarial attacks. Alternatively, invisible image watermarking\nis being researched as a proactive defense technique that allows media\nauthentication by verifying an invisible secret message embedded in the image\npixels. A handful of invisible image watermarking techniques introduced for\nmedia authentication have proven vulnerable to basic image processing\noperations and watermark removal attacks. In response, we have proposed a\nsemi-fragile image watermarking technique that embeds an invisible secret\nmessage into real images for media authentication. Our proposed watermarking\nframework is designed to be fragile to facial manipulations or tampering while\nbeing robust to benign image-processing operations and watermark removal\nattacks. This is facilitated through a unique architecture of our proposed\ntechnique consisting of critic and adversarial networks that enforce high image\nquality and resiliency to watermark removal efforts, respectively, along with\nthe backbone encoder-decoder and the discriminator networks. Thorough\nexperimental investigations on SOTA facial Deepfake datasets demonstrate that\nour proposed model can embed a $64$-bit secret as an imperceptible image\nwatermark that can be recovered with a high-bit recovery accuracy when benign\nimage processing operations are applied while being non-recoverable when unseen\nDeepfake manipulations are applied. In addition, our proposed watermarking\ntechnique demonstrates high resilience to several white-box and black-box\nwatermark removal attacks. Thus, obtaining state-of-the-art performance.\n","authors":["Aakash Varma Nadimpalli","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2410.01906v1.pdf","comment":"ACM Transactions (Digital Threats: Research and Practice)"},{"id":"http://arxiv.org/abs/2303.08336v3","updated":"2024-10-02T18:01:46Z","published":"2023-03-15T02:54:27Z","title":"Progressive Frame Patching for FoV-based Point Cloud Video Streaming","summary":"  Many XR applications require the delivery of volumetric video to users with\nsix degrees of freedom (6-DoF) movements. Point Cloud has become a popular\nvolumetric video format. A dense point cloud consumes much higher bandwidth\nthan a 2D/360 degree video frame. User Field of View (FoV) is more dynamic with\n6-DoF movement than 3-DoF movement. To save bandwidth, FoV-adaptive streaming\npredicts a user's FoV and only downloads point cloud data falling in the\npredicted FoV. However, it is vulnerable to FoV prediction errors, which can be\nsignificant when a long buffer is utilized for smoothed streaming. In this\nwork, we propose a multi-round progressive refinement framework for point cloud\nvideo streaming. Instead of sequentially downloading point cloud frames, our\nsolution simultaneously downloads/patches multiple frames falling into a\nsliding time-window, leveraging the inherent scalability of octree-based\npoint-cloud coding. The optimal rate allocation among all tiles of active\nframes are solved analytically using the heterogeneous tile rate-quality\nfunctions calibrated by the predicted user FoV. Multi-frame\ndownloading/patching simultaneously takes advantage of the streaming smoothness\nresulting from long buffer and the FoV prediction accuracy at short buffer\nlength. We evaluate our streaming solution using simulations driven by real\npoint cloud videos, real bandwidth traces, and 6-DoF FoV traces of real users.\nOur solution is robust against the bandwidth/FoV prediction errors, and can\ndeliver high and smooth view quality in the face of bandwidth variations and\ndynamic user and point cloud movements.\n","authors":["Tongyu Zong","Yixiang Mao","Chen Li","Yong Liu","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.08336v3.pdf","comment":"Transactions on Multimedia (under review)"}]},"2024-10-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.01769v2","updated":"2024-10-03T15:30:12Z","published":"2024-10-02T17:25:37Z","title":"Quantifying Generalization Complexity for Large Language Models","summary":"  While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.\n","authors":["Zhenting Qi","Hongyin Luo","Xuliang Huang","Zhuokai Zhao","Yibo Jiang","Xiangjun Fan","Himabindu Lakkaraju","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.01769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01744v2","updated":"2024-10-03T15:57:05Z","published":"2024-10-02T16:55:01Z","title":"Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks","summary":"  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose Leopard, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n","authors":["Mengzhao Jia","Wenhao Yu","Kaixin Ma","Tianqing Fang","Zhihan Zhang","Siru Ouyang","Hongming Zhang","Meng Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01744v2.pdf","comment":"Our code is available at https://github.com/Jill0001/Leopard"},{"id":"http://arxiv.org/abs/2410.01556v2","updated":"2024-10-03T03:11:24Z","published":"2024-10-02T13:52:55Z","title":"Integrative Decoding: Improve Factuality via Implicit Self-consistency","summary":"  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n","authors":["Yi Cheng","Xiao Liang","Yeyun Gong","Wen Xiao","Song Wang","Yuji Zhang","Wenjun Hou","Kaishuai Xu","Wenge Liu","Wenjie Li","Jian Jiao","Qi Chen","Peng Cheng","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.01556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01490v2","updated":"2024-10-03T05:43:30Z","published":"2024-10-02T12:40:11Z","title":"Extending Context Window of Large Language Models from a Distributional\n  Perspective","summary":"  Scaling the rotary position embedding (RoPE) has become a common method for\nextending the context window of RoPE-based large language models (LLMs).\nHowever, existing scaling methods often rely on empirical approaches and lack a\nprofound understanding of the internal distribution within RoPE, resulting in\nsuboptimal performance in extending the context window length. In this paper,\nwe propose to optimize the context window extending task from the view of\nrotary angle distribution. Specifically, we first estimate the distribution of\nthe rotary angles within the model and analyze the extent to which length\nextension perturbs this distribution. Then, we present a novel extension\nstrategy that minimizes the disturbance between rotary angle distributions to\nmaintain consistency with the pre-training phase, enhancing the model's\ncapability to generalize to longer sequences. Experimental results compared to\nthe strong baseline methods demonstrate that our approach reduces by up to 72%\nof the distributional disturbance when extending LLaMA2's context window to 8k,\nand reduces by up to 32% when extending to 16k. On the LongBench-E benchmark,\nour method achieves an average improvement of up to 4.33% over existing\nstate-of-the-art methods. Furthermore, Our method maintains the model's\nperformance on the Hugging Face Open LLM benchmark after context window\nextension, with only an average performance fluctuation ranging from -0.12 to\n+0.22.\n","authors":["Yingsheng Wu","Yuxuan Gu","Xiaocheng Feng","Weihong Zhong","Dongliang Xu","Qing Yang","Hongtao Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2410.01490v2.pdf","comment":"14 pages, 8 figures, Accepted to EMNLP2024"},{"id":"http://arxiv.org/abs/2409.18511v3","updated":"2024-10-03T01:44:40Z","published":"2024-09-27T07:46:06Z","title":"Do We Need Domain-Specific Embedding Models? An Empirical Investigation","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era. FinMTEB comes with\nopen-source code at https://github.com/yixuantt/FinMTEB\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18511v3.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2406.18164v3","updated":"2024-10-03T15:46:16Z","published":"2024-06-26T08:24:44Z","title":"Nebula: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.18256v3","updated":"2024-10-03T15:48:31Z","published":"2024-06-26T11:08:17Z","title":"Llamipa: An Incremental Discourse Parser","summary":"  This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.\n","authors":["Kate Thompson","Akshay Chaturvedi","Julie Hunter","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18256v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.01288v2","updated":"2024-10-03T07:13:43Z","published":"2024-10-02T07:18:16Z","title":"Mitigating Copy Bias in In-Context Learning through Neuron Pruning","summary":"  Large language models (LLMs) have demonstrated impressive few-shot in-context\nlearning (ICL) abilities. Still, we show that they are sometimes prone to a\n`copying bias', where they copy answers from provided examples instead of\nlearning the underlying patterns. In this work, we propose a novel and simple\nmethod to mitigate such copying bias. First, we create a synthetic task and use\nthe Integrated Gradients method to identify neurons that prioritize copying\nover generalization. We demonstrate that pruning these neurons consistently\nimproves performance across a diverse set of ICL tasks. We also show that our\nmethod is applicable across various LLM architectures, including Transformers\nand State-Space Models, without requiring modifications. In our analysis, we\nadopt a task-recognition perspective on ICL and examine task vectors (Hendel et\nal., 2023) induced by the model. We find that pruning enhances the quality of\nthese vectors, suggesting that the pruned neurons previously hindered effective\ntask recognition.\n","authors":["Ameen Ali","Lior Wolf","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2410.01288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01242v2","updated":"2024-10-03T13:12:24Z","published":"2024-10-02T05:07:02Z","title":"RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance","summary":"  Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.\n","authors":["Haolin Jin","Zechao Sun","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02763v1","updated":"2024-10-03T17:59:58Z","published":"2024-10-03T17:59:58Z","title":"Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos","summary":"  There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02763v1.pdf","comment":"Project Page: https://vinoground.github.io"},{"id":"http://arxiv.org/abs/2404.10917v2","updated":"2024-10-03T17:59:55Z","published":"2024-04-16T21:33:05Z","title":"Which questions should I answer? Salience Prediction of Inquisitive\n  Questions","summary":"  Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.\n","authors":["Yating Wu","Ritika Mangla","Alexandros G. Dimakis","Greg Durrett","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2404.10917v2.pdf","comment":"Camera Ready for EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02760v1","updated":"2024-10-03T17:59:30Z","published":"2024-10-03T17:59:30Z","title":"Erasing Conceptual Knowledge from Language Models","summary":"  Concept erasure in language models has traditionally lacked a comprehensive\nevaluation framework, leading to incomplete assessments of effectiveness of\nerasure methods. We propose an evaluation paradigm centered on three critical\ncriteria: innocence (complete knowledge removal), seamlessness (maintaining\nconditional fluent generation), and specificity (preserving unrelated task\nperformance). Our evaluation metrics naturally motivate the development of\nErasure of Language Memory (ELM), a new method designed to address all three\ndimensions. ELM employs targeted low-rank updates to alter output distributions\nfor erased concepts while preserving overall model capabilities including\nfluency when prompted for an erased concept. We demonstrate ELM's efficacy on\nbiosecurity, cybersecurity, and literary domain erasure tasks. Comparative\nanalysis shows that ELM achieves superior performance across our proposed\nmetrics, including near-random scores on erased topic assessments, generation\nfluency, maintained accuracy on unrelated benchmarks, and robustness under\nadversarial attacks. Our code, data, and trained models are available at\nhttps://elm.baulab.info\n","authors":["Rohit Gandikota","Sheridan Feucht","Samuel Marks","David Bau"],"pdf_url":"https://arxiv.org/pdf/2410.02760v1.pdf","comment":"Project Page: https://elm.baulab.info"},{"id":"http://arxiv.org/abs/2410.02756v1","updated":"2024-10-03T17:58:55Z","published":"2024-10-03T17:58:55Z","title":"CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text","summary":"  We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on\nMultilingual Coreference Resolution. In this third iteration of the shared\ntask, a novel objective is to also predict empty nodes needed for zero\ncoreference mentions (while the empty nodes were given on input in previous\nyears). This way, coreference resolution can be performed on raw text. We\nevaluate two model variants: a~two-stage approach (where the empty nodes are\npredicted first using a pretrained encoder model and then processed together\nwith sentence words by another pretrained model) and a single-stage approach\n(where a single pretrained encoder model generates empty nodes, coreference\nmentions, and coreference links jointly). In both settings, CorPipe surpasses\nother participants by a large margin of 3.9 and 2.8 percent points,\nrespectively. The source code and the trained model are available at\nhttps://github.com/ufal/crac2024-corpipe .\n","authors":["Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2410.02756v1.pdf","comment":"Accepted to CRAC 2024"},{"id":"http://arxiv.org/abs/2410.02755v1","updated":"2024-10-03T17:58:29Z","published":"2024-10-03T17:58:29Z","title":"SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost","summary":"  Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.\n","authors":["Jifan Zhang","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2410.02755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v1","updated":"2024-10-03T17:57:22Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18957v2","updated":"2024-10-03T17:57:07Z","published":"2024-09-27T17:58:50Z","title":"LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction","summary":"  Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2409.18957v2.pdf","comment":"Updated title, abstract, and images"},{"id":"http://arxiv.org/abs/2410.02748v1","updated":"2024-10-03T17:57:01Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11687v2","updated":"2024-10-03T17:56:34Z","published":"2024-06-17T16:05:32Z","title":"Tokenization Falling Short: The Curse of Tokenization","summary":"  Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.\n","authors":["Yekun Chai","Yewei Fang","Qiwei Peng","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2406.11687v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2404.07840v3","updated":"2024-10-03T17:56:12Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We make our code and data publicly available at\nhttps://github.com/ernie-research/gptfluence.\n","authors":["Yekun Chai","Qingyi Liu","Shuohuan Wang","Yu Sun","Qiwei Peng","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2307.10432v3","updated":"2024-10-03T17:55:29Z","published":"2023-07-19T19:40:34Z","title":"PharmacyGPT: The AI Pharmacist","summary":"  In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.\n","authors":["Zhengliang Liu","Zihao Wu","Mengxuan Hu","Bokai Zhao","Lin Zhao","Tianyi Zhang","Haixing Dai","Xianyan Chen","Ye Shen","Sheng Li","Quanzheng Li","Xiang Li","Brian Murray","Tianming Liu","Andrea Sikora"],"pdf_url":"https://arxiv.org/pdf/2307.10432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02744v1","updated":"2024-10-03T17:55:17Z","published":"2024-10-03T17:55:17Z","title":"Neutral residues: revisiting adapters for model extension","summary":"  We address the problem of extending a pretrained large language model to a\nnew domain that was not seen at training time, like adding a language for which\nthe original model has seen no or little training data. Popular solutions like\nfine-tuning or low-rank adaptation are successful at domain adaptation, but\nformally they do not add any extra capacity and degrade the performance in the\noriginal domain.\n  Our paper analyzes this extension problem under three angles: data,\narchitecture and training procedure, which are advantageously considered\njointly. In particular, we improve adapters and make it possible to learn an\nentire new language while ensuring that the output of the neural network is\nalmost unchanged in the original domain. For this purpose, we modify the new\nresidual blocks in a way that leads each new residual block to output\nnear-zeros in the original domain.\n  This solution of neutral residues, which borrows architectural components\nfrom mixture of experts, is effective: with only 20% extra learnable weights\ncompared to an original model trained on English, we get results that are\nsignificantly better than concurrent approaches (fine-tuning, low-rank or\nvanilla adapters) in terms of the trade-off between learning a new language and\nnot forgetting English.\n","authors":["Franck Signe Talla","Herve Jegou","Edouard Grave"],"pdf_url":"https://arxiv.org/pdf/2410.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02743v1","updated":"2024-10-03T17:55:13Z","published":"2024-10-03T17:55:13Z","title":"MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions","summary":"  Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF .\n","authors":["Yekun Chai","Haoran Sun","Huang Fang","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2410.02743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02742v1","updated":"2024-10-03T17:55:09Z","published":"2024-10-03T17:55:09Z","title":"Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models","summary":"  Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.\n","authors":["Haolan Liu","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.02742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02741v1","updated":"2024-10-03T17:54:56Z","published":"2024-10-03T17:54:56Z","title":"Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Lei Xu","Mohammed Asad Karim","Saket Dingliwal","Aparna Elangovan"],"pdf_url":"https://arxiv.org/pdf/2410.02741v1.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.02736v1","updated":"2024-10-03T17:53:30Z","published":"2024-10-03T17:53:30Z","title":"Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge","summary":"  LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.\n","authors":["Jiayi Ye","Yanbo Wang","Yue Huang","Dongping Chen","Qihui Zhang","Nuno Moniz","Tian Gao","Werner Geyer","Chao Huang","Pin-Yu Chen","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02730v1","updated":"2024-10-03T17:49:28Z","published":"2024-10-03T17:49:28Z","title":"DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes\n  and Objects","summary":"  Object navigation in unknown environments is crucial for deploying embodied\nagents in real-world applications. While we have witnessed huge progress due to\nlarge-scale scene datasets, faster simulators, and stronger models, previous\nstudies mainly focus on limited scene types and target objects. In this paper,\nwe study a new task of navigating to diverse target objects in a large number\nof scene types. To benchmark the problem, we present a large-scale scene\ndataset, DivScene, which contains 4,614 scenes across 81 different types. With\nthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a\nLarge Vision Language Model (LVLM) through imitation learning. The LVLM is\ntrained to take previous observations from the environment and generate the\nnext actions. We also introduce CoT explanation traces of the action prediction\nfor better performance when tuning LVLMs. Our extensive experiments find that\nwe can build a performant LVLM-based agent through imitation learning on the\nshortest paths constructed by a BFS planner without any human supervision. Our\nagent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we\ncarry out various analyses showing the generalization ability of our agent.\n","authors":["Zhaowei Wang","Hongming Zhang","Tianqing Fang","Ye Tian","Yue Yang","Kaixin Ma","Xiaoman Pan","Yangqiu Song","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02730v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2410.02729v1","updated":"2024-10-03T17:49:09Z","published":"2024-10-03T17:49:09Z","title":"Unified Multi-Modal Interleaved Document Representation for Information\n  Retrieval","summary":"  Information Retrieval (IR) methods aim to identify relevant documents in\nresponse to a given query, which have gained remarkable attention due to their\nsuccessful application in various natural language tasks. However, existing\napproaches typically consider only the textual information within the\ndocuments, which overlooks the fact that documents can contain multiple\nmodalities, including texts, images, and tables. Further, they often segment\neach long document into multiple discrete passages for embedding, preventing\nthem from capturing the overall document context and interactions between\nparagraphs. We argue that these two limitations lead to suboptimal document\nrepresentations for retrieval. In this work, to address them, we aim to produce\nmore comprehensive and nuanced document representations by holistically\nembedding documents interleaved with different modalities. Specifically, we\nachieve this by leveraging the capability of recent vision-language models that\nenable the processing and integration of text, images, and tables into a\nunified format and representation. Moreover, to mitigate the information loss\nfrom segmenting documents into passages, instead of representing and retrieving\npassages individually, we further merge the representations of segmented\npassages into one single document representation, while we additionally\nintroduce a reranking strategy to decouple and identify the relevant passage\nwithin the document if necessary. Then, through extensive experiments on\ndiverse information retrieval scenarios considering both the textual and\nmultimodal queries, we show that our approach substantially outperforms\nrelevant baselines, thanks to the consideration of the multimodal information\ninterleaved within the documents in a unified way.\n","authors":["Jaewoo Lee","Joonho Ko","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.02729v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02725v1","updated":"2024-10-03T17:47:29Z","published":"2024-10-03T17:47:29Z","title":"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation","summary":"  Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.\n","authors":["Rohin Manvi","Anikait Singh","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.02725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10710v3","updated":"2024-10-03T17:46:40Z","published":"2024-04-16T16:36:50Z","title":"Autoregressive Pre-Training on Pixels and Texts","summary":"  The integration of visual and textual information represents a promising\ndirection in the advancement of language models. In this paper, we explore the\ndual modality of language--both visual and textual--within an autoregressive\nframework, pre-trained on both document images and texts. Our method employs a\nmultimodal training strategy, utilizing visual data through next patch\nprediction with a regression head and/or textual data through next token\nprediction with a classification head. We focus on understanding the\ninteraction between these two modalities and their combined impact on model\nperformance. Our extensive evaluation across a wide range of benchmarks shows\nthat incorporating both visual and textual data significantly improves the\nperformance of pixel-based language models. Remarkably, we find that a\nunidirectional pixel-based model trained solely on visual data can achieve\ncomparable results to state-of-the-art bidirectional models on several language\nunderstanding tasks. This work uncovers the untapped potential of integrating\nvisual and textual modalities for more effective language modeling. We release\nour code, data, and model checkpoints at\n\\url{https://github.com/ernie-research/pixelgpt}.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02724v1","updated":"2024-10-03T17:45:31Z","published":"2024-10-03T17:45:31Z","title":"Large Language Models as Markov Chains","summary":"  Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.\n","authors":["Oussama Zekri","Ambroise Odonnat","Abdelhakim Benechehab","Linus Bleistein","Nicolas Boull","Ievgen Redko"],"pdf_url":"https://arxiv.org/pdf/2410.02724v1.pdf","comment":"49 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.02721v1","updated":"2024-10-03T17:40:55Z","published":"2024-10-03T17:40:55Z","title":"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization","summary":"  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.\n","authors":["Ryan C. Barron","Ves Grantcharov","Selma Wanna","Maksim E. Eren","Manish Bhattarai","Nicholas Solovyev","George Tompkins","Charles Nicholas","Kim . Rasmussen","Cynthia Matuszek","Boian S. Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2410.02721v1.pdf","comment":"9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024"},{"id":"http://arxiv.org/abs/2410.02719v1","updated":"2024-10-03T17:39:38Z","published":"2024-10-03T17:39:38Z","title":"UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling\n  for Retrieval-Augmented Generation","summary":"  We present UncertaintyRAG, a novel approach for long-context\nRetrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio\n(SNR)-based span uncertainty to estimate similarity between text chunks. This\nspan uncertainty enhances model calibration, improving robustness and\nmitigating semantic inconsistencies introduced by random chunking. Leveraging\nthis insight, we propose an efficient unsupervised learning technique to train\nthe retrieval model, alongside an effective data sampling and scaling strategy.\nUncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving\nstate-of-the-art results while using only 4% of the training data compared to\nother advanced open-source retrieval models under distribution shift settings.\nOur method demonstrates strong calibration through span uncertainty, leading to\nimproved generalization and robustness in long-context RAG tasks. Additionally,\nUncertaintyRAG provides a lightweight retrieval model that can be integrated\ninto any large language model with varying context window lengths, without the\nneed for fine-tuning, showcasing the flexibility of our approach.\n","authors":["Zixuan Li","Jing Xiong","Fanghua Ye","Chuanyang Zheng","Xun Wu","Jianqiao Lu","Zhongwei Wan","Xiaodan Liang","Chengming Li","Zhenan Sun","Lingpeng Kong","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.02719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02713v1","updated":"2024-10-03T17:36:49Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v1.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02712v1","updated":"2024-10-03T17:36:33Z","published":"2024-10-03T17:36:33Z","title":"LLaVA-Critic: Learning to Evaluate Multimodal Models","summary":"  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)\ndesigned as a generalist evaluator to assess performance across a wide range of\nmultimodal tasks. LLaVA-Critic is trained using a high-quality critic\ninstruction-following dataset that incorporates diverse evaluation criteria and\nscenarios. Our experiments demonstrate the model's effectiveness in two key\nareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation\nscores, performing on par with or surpassing GPT models on multiple evaluation\nbenchmarks; and (2) Preference Learning, where it generates reward signals for\npreference learning, enhancing model alignment capabilities. This work\nunderscores the potential of open-source LMMs in self-critique and evaluation,\nsetting the stage for future research into scalable, superhuman alignment\nfeedback mechanisms for LMMs.\n","authors":["Tianyi Xiong","Xiyao Wang","Dong Guo","Qinghao Ye","Haoqi Fan","Quanquan Gu","Heng Huang","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02712v1.pdf","comment":"Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic"},{"id":"http://arxiv.org/abs/2410.02707v1","updated":"2024-10-03T17:31:31Z","published":"2024-10-03T17:31:31Z","title":"LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations","summary":"  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n","authors":["Hadas Orgad","Michael Toker","Zorik Gekhman","Roi Reichart","Idan Szpektor","Hadas Kotek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.02707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02703v1","updated":"2024-10-03T17:27:30Z","published":"2024-10-03T17:27:30Z","title":"Selective Attention Improves Transformer","summary":"  Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.\n","authors":["Yaniv Leviathan","Matan Kalman","Yossi Matias"],"pdf_url":"https://arxiv.org/pdf/2410.02703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12683v2","updated":"2024-10-03T17:27:28Z","published":"2023-12-20T00:49:52Z","title":"Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?","summary":"  The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning.\n","authors":["Tannon Kew","Florian Schottmann","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2312.12683v2.pdf","comment":"Accepted at Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.07071v2","updated":"2024-10-03T17:26:48Z","published":"2024-07-09T17:44:34Z","title":"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps","summary":"  When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.\n","authors":["Yung-Sung Chuang","Linlu Qiu","Cheng-Yu Hsieh","Ranjay Krishna","Yoon Kim","James Glass"],"pdf_url":"https://arxiv.org/pdf/2407.07071v2.pdf","comment":"EMNLP 2024 main conference long paper. The source code is available\n  at https://github.com/voidism/Lookback-Lens"},{"id":"http://arxiv.org/abs/2311.00237v3","updated":"2024-10-03T17:25:02Z","published":"2023-11-01T02:40:42Z","title":"The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis","summary":"  Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey.\n","authors":["Yuxiang Zhou","Jiazheng Li","Yanzheng Xiang","Hanqi Yan","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2311.00237v3.pdf","comment":"Accepted to the main conference of EMNLP 2024. Resources are\n  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources"},{"id":"http://arxiv.org/abs/2410.02694v1","updated":"2024-10-03T17:20:11Z","published":"2024-10-03T17:20:11Z","title":"HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly","summary":"  There have been many benchmarks for evaluating long-context language models\n(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack\n(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate\nto the diverse downstream applications of LCLMs, and the inconsistency further\ncomplicates model comparison. We investigate the underlying reasons behind\ncurrent practices and find that existing benchmarks often provide noisy signals\ndue to low coverage of applications, insufficient lengths, unreliable metrics,\nand incompatibility with base models. In this work, we present HELMET (How to\nEvaluate Long-context Models Effectively and Thoroughly), a comprehensive\nbenchmark encompassing seven diverse, application-centric categories. We also\naddress many issues in previous benchmarks by adding controllable lengths up to\n128k tokens, model-based evaluation for reliable metrics, and few-shot\nprompting for robustly evaluating base models. Consequently, we demonstrate\nthat HELMET offers more reliable and consistent rankings of frontier LCLMs.\nThrough a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks\nlike NIAH are not good predictors of downstream performance; (2) the diverse\ncategories in HELMET exhibit distinct trends and low correlation with each\nother; and (3) while most LCLMs achieve perfect NIAH scores, open-source models\nsignificantly lag behind closed ones when the task requires full-context\nreasoning or following complex instructions -- the gap widens with increased\nlengths. Finally, we recommend using our RAG tasks for fast model development,\nas they are easy to run and more predictive of other downstream performance;\nultimately, we advocate for a holistic evaluation across diverse tasks.\n","authors":["Howard Yen","Tianyu Gao","Minmin Hou","Ke Ding","Daniel Fleischer","Peter Izasak","Moshe Wasserblat","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02694v1.pdf","comment":"Code and data are available here:\n  https://github.com/princeton-nlp/HELMET"},{"id":"http://arxiv.org/abs/2410.02691v1","updated":"2024-10-03T17:18:03Z","published":"2024-10-03T17:18:03Z","title":"On the Proper Treatment of Tokenization in Psycholinguistics","summary":"  Language models are widely used in computational psycholinguistics to test\ntheories that relate the negative log probability (the surprisal) of a region\nof interest (a substring of characters) under a language model to its cognitive\ncost experienced by readers, as operationalized, for example, by gaze duration\non the region. However, the application of modern language models to\npsycholinguistic studies is complicated by the practice of using tokenization\nas an intermediate step in training a model. Doing so results in a language\nmodel over token strings rather than one over character strings. Vexingly,\nregions of interest are generally misaligned with these token strings. The\npaper argues that token-level language models should be (approximately)\nmarginalized into character-level language models before they are used in\npsycholinguistic studies to compute the surprisal of a region of interest;\nthen, the marginalized character-level language model can be used to compute\nthe surprisal of an arbitrary character substring, which we term a focal area,\nthat the experimenter may wish to use as a predictor. Our proposal of\nmarginalizing a token-level model into a character-level one solves this\nmisalignment issue independently of the tokenization scheme. Empirically, we\ndiscover various focal areas whose surprisal is a better psychometric predictor\nthan the surprisal of the region of interest itself.\n","authors":["Mario Giulianelli","Luca Malagutti","Juan Luis Gastaldi","Brian DuSell","Tim Vieira","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.02691v1.pdf","comment":"Main conference long paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.03741v2","updated":"2024-10-03T17:15:24Z","published":"2024-01-08T09:01:29Z","title":"Enhanced Automated Code Vulnerability Repair using Large Language Models","summary":"  This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.\n","authors":["David de-Fitero-Dominguez","Eva Garcia-Lopez","Antonio Garcia-Cabot","Jose-Javier Martinez-Herraiz"],"pdf_url":"https://arxiv.org/pdf/2401.03741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03650v2","updated":"2024-10-03T17:13:04Z","published":"2024-09-05T16:08:19Z","title":"On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.\n","authors":["Yong Lin","Skyler Seto","Maartje ter Hoeve","Katherine Metcalf","Barry-John Theobald","Xuan Wang","Yizhe Zhang","Chen Huang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.03650v2.pdf","comment":"12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.02684v1","updated":"2024-10-03T17:10:41Z","published":"2024-10-03T17:10:41Z","title":"HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router","summary":"  As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses.\n","authors":["Lingrui Mei","Shenghua Liu","Yiwei Wang","Baolong Bi","Ruibin Yuan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.02684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18725v2","updated":"2024-10-03T17:10:09Z","published":"2024-06-26T19:48:48Z","title":"Jailbreaking LLMs with Arabic Transliteration and Arabizi","summary":"  This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.\n","authors":["Mansour Al Ghanim","Saleh Almohaimeed","Mengxin Zheng","Yan Solihin","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2406.18725v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02683v1","updated":"2024-10-03T17:08:52Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2311.09756v2","updated":"2024-10-03T17:04:50Z","published":"2023-11-16T10:30:26Z","title":"StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for\n  Children's Story-Based Learning","summary":"  Interactive story reading is a common parent-child activity, where parents\nexpect to teach both language skills and real-world knowledge beyond the story.\nWhile increasing storytelling and reading systems have been developed for this\nactivity, they often fail to infuse real-world knowledge into the conversation.\nThis limitation can be attributed to the existing question-answering (QA)\ndatasets used for children's education, upon which the systems are built,\nfailing to capture the nuances of how education experts think when conducting\ninteractive story reading activities. To bridge this gap, we design an\nannotation framework, empowered by existing knowledge graph to capture experts'\nannotations and thinking process, and leverage this framework to construct\nStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with\nreal-world knowledge. We conduct automated and human expert evaluations across\nvarious QA pair generation settings to demonstrate that our StorySparkQA can\neffectively support models in generating QA pairs that target real-world\nknowledge beyond story content. StorySparkQA is available at\nhttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.\n","authors":["Jiaju Chen","Yuxuan Lu","Shao Zhang","Bingsheng Yao","Yuanzhe Dong","Ying Xu","Yunyao Li","Qianwen Wang","Dakuo Wang","Yuling Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09756v2.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02678v1","updated":"2024-10-03T17:04:48Z","published":"2024-10-03T17:04:48Z","title":"Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data","summary":"  Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.\n","authors":["William Held","Ella Li","Michael Ryan","Weiyan Shi","Yanzhe Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02677v1","updated":"2024-10-03T17:04:31Z","published":"2024-10-03T17:04:31Z","title":"CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs","summary":"  To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.\n","authors":["Yu Ying Chiu","Liwei Jiang","Bill Yuchen Lin","Chan Young Park","Shuyue Stella Li","Sahithya Ravi","Mehar Bhatia","Maria Antoniak","Yulia Tsvetkov","Vered Shwartz","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02677v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.02675v1","updated":"2024-10-03T17:02:21Z","published":"2024-10-03T17:02:21Z","title":"FAN: Fourier Analysis Networks","summary":"  Despite the remarkable success achieved by neural networks, particularly\nthose represented by MLP and Transformer, we reveal that they exhibit potential\nflaws in the modeling and reasoning of periodicity, i.e., they tend to memorize\nthe periodic data rather than genuinely understanding the underlying principles\nof periodicity. However, periodicity is a crucial trait in various forms of\nreasoning and generalization, underpinning predictability across natural and\nengineered systems through recurring patterns in observations. In this paper,\nwe propose FAN, a novel network architecture based on Fourier Analysis, which\nempowers the ability to efficiently model and reason about periodic phenomena.\nBy introducing Fourier Series, the periodicity is naturally integrated into the\nstructure and computational processes of the neural network, thus achieving a\nmore accurate expression and prediction of periodic patterns. As a promising\nsubstitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in\nvarious models with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the effectiveness of FAN in modeling and reasoning about\nperiodic functions, and the superiority and generalizability of FAN across a\nrange of real-world tasks, including symbolic formula representation, time\nseries forecasting, and language modeling.\n","authors":["Yihong Dong","Ge Li","Yongding Tao","Xue Jiang","Kechi Zhang","Jia Li","Jing Su","Jun Zhang","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02674v1","updated":"2024-10-03T16:58:21Z","published":"2024-10-03T16:58:21Z","title":"Examining Language Modeling Assumptions Using an Annotated Literary\n  Dialect Corpus","summary":"  We present a dataset of 19th century American literary orthovariant tokens\nwith a novel layer of human-annotated dialect group tags designed to serve as\nthe basis for computational experiments exploring literarily meaningful\northographic variation. We perform an initial broad set of experiments over\nthis dataset using both token (BERT) and character (CANINE)-level contextual\nlanguage models. We find indications that the \"dialect effect\" produced by\nintentional orthographic variation employs multiple linguistic channels, and\nthat these channels are able to be surfaced to varied degrees given particular\nlanguage modelling assumptions. Specifically, we find evidence showing that\nchoice of tokenization scheme meaningfully impact the type of orthographic\ninformation a model is able to surface.\n","authors":["Craig Messner","Tom Lippincott"],"pdf_url":"https://arxiv.org/pdf/2410.02674v1.pdf","comment":"Accepted to NLP4DH@EMNLP2024"},{"id":"http://arxiv.org/abs/2407.07950v2","updated":"2024-10-03T16:54:59Z","published":"2024-07-10T18:00:05Z","title":"Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance","summary":"  The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.\n","authors":["Kaitlyn Zhou","Jena D. Hwang","Xiang Ren","Nouha Dziri","Dan Jurafsky","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2407.07950v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.07565v3","updated":"2024-10-03T16:48:55Z","published":"2024-07-10T11:50:20Z","title":"On Leakage of Code Generation Evaluation Datasets","summary":"  In this paper, we consider contamination by code generation test sets, in\nparticular in their use in modern large language models. We discuss three\npossible sources of such contamination and show findings supporting each of\nthem: (i) direct data leakage, (ii) indirect data leakage through the use of\nsynthetic data and (iii) overfitting to evaluation sets during model selection.\nTo address this, we release Less Basic Python Problems (LBPP): an\nuncontaminated new benchmark of 161 prompts with their associated Python\nsolutions. LBPP is released at https://huggingface.co/datasets/CohereForAI/lbpp .\n","authors":["Alexandre Matton","Tom Sherborne","Dennis Aumiller","Elena Tommasone","Milad Alizadeh","Jingyi He","Raymond Ma","Maxime Voisin","Ellen Gilsenan-McMahon","Matthias Gall"],"pdf_url":"https://arxiv.org/pdf/2407.07565v3.pdf","comment":"EMNLP 2024 Findings. 5 main pages, 9 in total"},{"id":"http://arxiv.org/abs/2410.02660v1","updated":"2024-10-03T16:46:52Z","published":"2024-10-03T16:46:52Z","title":"How to Train Long-Context Language Models (Effectively)","summary":"  We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- Instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context tasks, and we evaluate models after SFT with instruction data\nas this better reveals long-context abilities. Supported by our robust\nevaluations, we run thorough experiments to decide the data mix for continued\npre-training, the instruction tuning dataset, and many other design choices. We\nfind that (1) code repositories and books are excellent sources of long data,\nbut it is crucial to combine them with high-quality short data; (2) training\nwith a sequence length beyond the evaluation length boosts long-context\nperformance; (3) for SFT, using only short instruction datasets yields strong\nperformance on long-context tasks. Our final model, ProLong-8B, which is\ninitialized from Llama-3 and trained on 40B tokens, demonstrates\nstate-of-the-art long-context performance among similarly sized models at a\nlength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of\nlong-context tasks despite having seen only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.\n","authors":["Tianyu Gao","Alexander Wettig","Howard Yen","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02660v1.pdf","comment":"Our code, data, and models are available at\n  https://github.com/princeton-nlp/ProLong"},{"id":"http://arxiv.org/abs/2407.11969v3","updated":"2024-10-03T16:46:09Z","published":"2024-07-16T17:59:55Z","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","summary":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.\n","authors":["Maksym Andriushchenko","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2407.11969v3.pdf","comment":"Update in v3: o1-mini and o1-preview results (on top of GPT-4o and\n  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at\n  https://github.com/tml-epfl/llm-past-tense"},{"id":"http://arxiv.org/abs/2410.02657v1","updated":"2024-10-03T16:43:17Z","published":"2024-10-03T16:43:17Z","title":"Hate Personified: Investigating the role of LLMs in content moderation","summary":"  For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases.\n","authors":["Sarah Masud","Sahajpreet Singh","Viktor Hangya","Alexander Fraser","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2410.02657v1.pdf","comment":"17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains"},{"id":"http://arxiv.org/abs/2402.16382v2","updated":"2024-10-03T16:39:32Z","published":"2024-02-26T08:08:03Z","title":"Immunization against harmful fine-tuning attacks","summary":"  Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","ukasz Bartoszcze","Jan Batzner","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2402.16382v2.pdf","comment":"Published in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02653v1","updated":"2024-10-03T16:36:35Z","published":"2024-10-03T16:36:35Z","title":"Measuring and Improving Persuasiveness of Generative Models","summary":"  LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.\n","authors":["Somesh Singh","Yaman K Singla","Harini SI","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2410.02653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02650v1","updated":"2024-10-03T16:34:46Z","published":"2024-10-03T16:34:46Z","title":"Undesirable Memorization in Large Language Models: A Survey","summary":"  While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.\n","authors":["Ali Satvaty","Suzan Verberne","Fatih Turkmen"],"pdf_url":"https://arxiv.org/pdf/2410.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02647v1","updated":"2024-10-03T16:33:35Z","published":"2024-10-03T16:33:35Z","title":"Immunogenicity Prediction with Dual Attention Enables Vaccine Target\n  Selection","summary":"  Immunogenicity prediction is a central topic in reverse vaccinology for\nfinding candidate vaccines that can trigger protective immune responses.\nExisting approaches typically rely on highly compressed features and simple\nmodel architectures, leading to limited prediction accuracy and poor\ngeneralizability. To address these challenges, we introduce ProVaccine, a novel\ndeep learning solution with a dual attention mechanism that integrates\npre-trained latent vector representations of protein sequences and structures.\nWe also compile the most comprehensive immunogenicity dataset to date,\nencompassing over 9,500 antigen sequences, structures, and immunogenicity\nlabels from bacteria, viruses, and tumors. Extensive experiments demonstrate\nthat ProVaccine outperforms existing methods across a wide range of evaluation\nmetrics. Furthermore, we establish a post-hoc validation protocol to assess the\npractical significance of deep learning models in tackling vaccine design\nchallenges. Our work provides an effective tool for vaccine design and sets\nvaluable benchmarks for future research.\n","authors":["Song Li","Yang Tan","Song Ke","Liang Hong","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.02647v1.pdf","comment":"18 pages, 11 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.02026v2","updated":"2024-10-03T16:31:59Z","published":"2024-09-03T16:20:22Z","title":"Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization","summary":"  In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.\n","authors":["Sean I. Young"],"pdf_url":"https://arxiv.org/pdf/2409.02026v2.pdf","comment":"Preprint. 17 pages, 4 figures, 5 appendices"},{"id":"http://arxiv.org/abs/2409.11295v2","updated":"2024-10-03T16:30:43Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2410.02642v1","updated":"2024-10-03T16:25:37Z","published":"2024-10-03T16:25:37Z","title":"Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers","summary":"  Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.\n","authors":["Shijie Chen","Bernal Jimnez Gutirrez","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.02642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02631v1","updated":"2024-10-03T16:15:04Z","published":"2024-10-03T16:15:04Z","title":"Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning","summary":"  Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests.\n","authors":["Tianxiang Hu","Pei Zhang","Baosong Yang","Jun Xie","Derek F. Wong","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.02631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08702v4","updated":"2024-10-03T16:11:43Z","published":"2024-02-13T16:38:01Z","title":"PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling","summary":"  Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST.\n","authors":["Yongchao Chen","Jacob Arkin","Yilun Hao","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2402.08702v4.pdf","comment":"62 pages, 14 figures, Published in EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2403.13681v2","updated":"2024-10-03T16:01:01Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?","summary":"  In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05197v2","updated":"2024-10-03T15:55:40Z","published":"2024-09-08T19:22:58Z","title":"Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?","summary":"  State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.\n","authors":["Neeladri Bhuiya","Viktor Schlegel","Stefan Winkler"],"pdf_url":"https://arxiv.org/pdf/2409.05197v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.12191v2","updated":"2024-10-03T15:54:49Z","published":"2024-09-18T17:59:32Z","title":"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at\n  Any Resolution","summary":"  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .\n","authors":["Peng Wang","Shuai Bai","Sinan Tan","Shijie Wang","Zhihao Fan","Jinze Bai","Keqin Chen","Xuejing Liu","Jialin Wang","Wenbin Ge","Yang Fan","Kai Dang","Mengfei Du","Xuancheng Ren","Rui Men","Dayiheng Liu","Chang Zhou","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2409.12191v2.pdf","comment":"Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin\n  note: text overlap with arXiv:2408.15262 by other authors"},{"id":"http://arxiv.org/abs/2410.02613v1","updated":"2024-10-03T15:51:36Z","published":"2024-10-03T15:51:36Z","title":"NL-Eye: Abductive NLI for Images","summary":"  Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.\n","authors":["Mor Ventura","Michael Toker","Nitay Calderon","Zorik Gekhman","Yonatan Bitton","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2410.02613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02611v1","updated":"2024-10-03T15:50:08Z","published":"2024-10-03T15:50:08Z","title":"IndicSentEval: How Effectively do Multilingual Transformer Models encode\n  Linguistic Properties for Indic Languages?","summary":"  Transformer-based models have revolutionized the field of natural language\nprocessing. To understand why they perform so well and to assess their\nreliability, several studies have focused on questions such as: Which\nlinguistic properties are encoded by these models, and to what extent? How\nrobust are these models in encoding linguistic properties when faced with\nperturbations in the input text? However, these studies have mainly focused on\nBERT and the English language. In this paper, we investigate similar questions\nregarding encoding capability and robustness for 8 linguistic properties across\n13 different perturbations in 6 Indic languages, using 9 multilingual\nTransformer models (7 universal and 2 Indic-specific). To conduct this study,\nwe introduce a novel multilingual benchmark dataset, IndicSentEval, containing\napproximately $\\sim$47K sentences. Surprisingly, our probing analysis of\nsurface, syntactic, and semantic properties reveals that while almost all\nmultilingual models demonstrate consistent encoding performance for English,\nthey show mixed results for Indic languages. As expected, Indic-specific\nmultilingual models capture linguistic properties in Indic languages better\nthan universal models. Intriguingly, universal models broadly exhibit better\nrobustness compared to Indic-specific models, particularly under perturbations\nsuch as dropping both nouns and verbs, dropping only verbs, or keeping only\nnouns. Overall, this study provides valuable insights into probing and\nperturbation-specific strengths and weaknesses of popular multilingual\nTransformer-based models for different Indic languages. We make our code and\ndataset publicly available [https://tinyurl.com/IndicSentEval}].\n","authors":["Akhilesh Aravapalli","Mounika Marreddy","Subba Reddy Oota","Radhika Mamidi","Manish Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.02611v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.02609v1","updated":"2024-10-03T15:49:35Z","published":"2024-10-03T15:49:35Z","title":"Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in\n  Under-Resourced Languages Using Explainable AI","summary":"  The proliferation of fake news has emerged as a significant threat to the\nintegrity of information dissemination, particularly on social media platforms.\nMisinformation can spread quickly due to the ease of creating and disseminating\ncontent, affecting public opinion and sociopolitical events. Identifying false\ninformation is therefore essential to reducing its negative consequences and\nmaintaining the reliability of online news sources. Traditional approaches to\nfake news detection often rely solely on content-based features, overlooking\nthe crucial role of social context in shaping the perception and propagation of\nnews articles. In this paper, we propose a comprehensive approach that\nintegrates social context-based features with news content features to enhance\nthe accuracy of fake news detection in under-resourced languages. We perform\nseveral experiments utilizing a variety of methodologies, including traditional\nmachine learning, neural networks, ensemble learning, and transfer learning.\nAssessment of the outcomes of the experiments shows that the ensemble learning\napproach has the highest accuracy, achieving a 0.99 F1 score. Additionally,\nwhen compared with monolingual models, the fine-tuned model with the target\nlanguage outperformed others, achieving a 0.94 F1 score. We analyze the\nfunctioning of the models, considering the important features that contribute\nto model performance, using explainable AI techniques.\n","authors":["Mesay Gemeda Yigezu","Melkamu Abay Mersha","Girma Yohannis Bade","Jugal Kalita","Olga Kolesnikova","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2410.02609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10960v3","updated":"2024-10-03T15:48:45Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v3.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2304.08460v3","updated":"2024-10-03T15:46:13Z","published":"2023-04-17T17:36:35Z","title":"LongForm: Effective Instruction Tuning with Reverse Instructions","summary":"  Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.\n","authors":["Abdullatif Kksal","Timo Schick","Anna Korhonen","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2304.08460v3.pdf","comment":"EMNLP 2024 Findings. This version extends the training with recent\n  LLMs, evaluation with new metrics, and NLU tasks"},{"id":"http://arxiv.org/abs/2407.12402v2","updated":"2024-10-03T15:45:52Z","published":"2024-07-17T08:28:55Z","title":"TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish","summary":"  Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.\n","authors":["Arda Yksel","Abdullatif Kksal","Ltfi Kerem enel","Anna Korhonen","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2407.12402v2.pdf","comment":"EMNLP 2024 - Findings"},{"id":"http://arxiv.org/abs/2404.14741v2","updated":"2024-10-03T15:44:59Z","published":"2024-04-23T04:47:22Z","title":"Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering","summary":"  To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.\n","authors":["Yao Xu","Shizhu He","Jiabei Chen","Zihao Wang","Yangqiu Song","Hanghang Tong","Guang Liu","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.14741v2.pdf","comment":"Accepted by EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.02603v1","updated":"2024-10-03T15:44:42Z","published":"2024-10-03T15:44:42Z","title":"Agents' Room: Narrative Generation through Multi-step Collaboration","summary":"  Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.\n","authors":["Fantine Huot","Reinald Kim Amplayo","Jennimaria Palomaki","Alice Shoshana Jakobovits","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.02603v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.02584v1","updated":"2024-10-03T15:28:05Z","published":"2024-10-03T15:28:05Z","title":"Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions","summary":"  As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful.\n","authors":["Angana Borah","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2410.02584v1.pdf","comment":"Accepted to EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2310.04484v3","updated":"2024-10-03T15:20:17Z","published":"2023-10-06T13:28:04Z","title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning","summary":"  Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions.\n","authors":["Wanyun Cui","Qianle Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04484v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11194v2","updated":"2024-10-03T15:13:58Z","published":"2024-06-17T04:00:04Z","title":"In-Context Editing: Learning Knowledge from Self-Induced Distributions","summary":"  In scenarios where language models must incorporate new information\nefficiently without extensive retraining, traditional fine-tuning methods are\nprone to overfitting, degraded generalization, and unnatural language\ngeneration. To address these limitations, we introduce Consistent In-Context\nEditing (ICE), a novel approach leveraging the model's in-context learning\ncapability to optimize toward a contextual distribution rather than a one-hot\ntarget. ICE introduces a simple yet effective optimization framework for the\nmodel to internalize new knowledge by aligning its output distributions with\nand without additional context. This method enhances the robustness and\neffectiveness of gradient-based tuning methods, preventing overfitting and\npreserving the model's integrity. We analyze ICE across four critical aspects\nof knowledge editing: accuracy, locality, generalization, and linguistic\nquality, demonstrating its advantages. Experimental results confirm the\neffectiveness of ICE and demonstrate its potential for continual editing,\nensuring that the integrity of the model is preserved while updating\ninformation.\n","authors":["Siyuan Qi","Bangcheng Yang","Kailin Jiang","Xiaobo Wang","Jiaqi Li","Yifan Zhong","Yaodong Yang","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.11194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02560v1","updated":"2024-10-03T15:04:27Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Yakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v1.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.02558v1","updated":"2024-10-03T15:04:00Z","published":"2024-10-03T15:04:00Z","title":"Improving Unsupervised Constituency Parsing via Maximizing Semantic\n  Information","summary":"  Unsupervised constituency parsers organize phrases within a sentence into a\ntree-shaped syntactic constituent structure that reflects the organization of\nsentence semantics. However, the traditional objective of maximizing sentence\nlog-likelihood (LL) does not explicitly account for the close relationship\nbetween the constituent structure and the semantics, resulting in a weak\ncorrelation between LL values and parsing accuracy. In this paper, we introduce\na novel objective for training unsupervised parsers: maximizing the information\nbetween constituent structures and sentence semantics (SemInfo). We introduce a\nbag-of-substrings model to represent the semantics and apply the\nprobability-weighted information metric to estimate the SemInfo. Additionally,\nwe develop a Tree Conditional Random Field (TreeCRF)-based model to apply the\nSemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)\ninduction, the state-of-the-art method for unsupervised constituency parsing.\nExperiments demonstrate that SemInfo correlates more strongly with parsing\naccuracy than LL. Our algorithm significantly enhances parsing accuracy by an\naverage of 7.85 points across five PCFG variants and in four languages,\nachieving new state-of-the-art results in three of the four languages.\n","authors":["Junjie Chen","Xiangheng He","Yusuke Miyao","Danushka Bollegala"],"pdf_url":"https://arxiv.org/pdf/2410.02558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12471v2","updated":"2024-10-03T14:56:29Z","published":"2024-06-18T10:20:36Z","title":"Fighting Randomness with Randomness: Mitigating Optimisation Instability\n  of Fine-Tuning using Delayed Ensemble and Noisy Interpolation","summary":"  While fine-tuning of pre-trained language models generally helps to overcome\nthe lack of labelled training samples, it also displays model performance\ninstability. This instability mainly originates from randomness in\ninitialisation or data shuffling. To address this, researchers either modify\nthe training process or augment the available samples, which typically results\nin increased computational costs. We propose a new mitigation strategy, called\nDelayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths\nof ensembling, noise regularisation and model interpolation, while retaining\ncomputational efficiency. We compare DENI with 9 representative mitigation\nstrategies across 3 models, 4 tuning strategies and 7 text classification\ndatasets. We show that: 1) DENI outperforms the best performing mitigation\nstrategy (Ensemble), while using only a fraction of its cost; 2) the mitigation\nstrategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,\noutperforming full fine-tuning in specific cases; and 3) combining DENI with\ndata augmentation often leads to even more effective instability mitigation.\n","authors":["Branislav Pecher","Jan Cegin","Robert Belanec","Jakub Simko","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2406.12471v2.pdf","comment":"Accepted to the Findings of the EMNLP'24 Conference"},{"id":"http://arxiv.org/abs/2402.12817v2","updated":"2024-10-03T14:56:24Z","published":"2024-02-20T08:38:19Z","title":"On Sensitivity of Learning with Limited Labelled Data to the Effects of\n  Randomness: Impact of Interactions and Systematic Choices","summary":"  While learning with limited labelled data can improve performance when the\nlabels are lacking, it is also sensitive to the effects of uncontrolled\nrandomness introduced by so-called randomness factors (e.g., varying order of\ndata). We propose a method to systematically investigate the effects of\nrandomness factors while taking the interactions between them into\nconsideration. To measure the true effects of an individual randomness factor,\nour method mitigates the effects of other factors and observes how the\nperformance varies across multiple runs. Applying our method to multiple\nrandomness factors across in-context learning and fine-tuning approaches on 7\nrepresentative text classification tasks and meta-learning on 3 tasks, we show\nthat: 1) disregarding interactions between randomness factors in existing works\ncaused inconsistent findings due to incorrect attribution of the effects of\nrandomness factors, such as disproving the consistent sensitivity of in-context\nlearning to sample order even with random sample selection; and 2) besides\nmutual interactions, the effects of randomness factors, especially sample\norder, are also dependent on more systematic choices unexplored in existing\nworks, such as number of classes, samples per class or choice of prompt format.\n","authors":["Branislav Pecher","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2402.12817v2.pdf","comment":"Accepted to the EMNLP'24 Main Conference"},{"id":"http://arxiv.org/abs/2409.19700v2","updated":"2024-10-03T14:56:02Z","published":"2024-09-29T13:16:37Z","title":"2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models","summary":"  Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02551v1","updated":"2024-10-03T14:55:22Z","published":"2024-10-03T14:55:22Z","title":"ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration","summary":"  We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app.\n","authors":["Zixiang Wang","Yinghao Zhu","Huiya Zhao","Xiaochen Zheng","Tianlong Wang","Wen Tang","Yasha Wang","Chengwei Pan","Ewen M. Harrison","Junyi Gao","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02458v1","updated":"2024-10-03T14:50:33Z","published":"2024-10-03T14:50:33Z","title":"MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation","summary":"  Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n","authors":["Gurucharan Marthi Krishna Kumar","Aman Chadha","Janine Mendola","Amir Shmuel"],"pdf_url":"https://arxiv.org/pdf/2410.02458v1.pdf","comment":"Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2409.15977v3","updated":"2024-10-03T14:45:55Z","published":"2024-09-24T11:18:09Z","title":"TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://tcsinger.github.io/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v3.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.18045v3","updated":"2024-10-03T14:44:44Z","published":"2024-02-28T04:43:46Z","title":"Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore","summary":"  Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics.\n","authors":["Sheikh Shafayat","Eunsu Kim","Juhyun Oh","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2402.18045v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02538v1","updated":"2024-10-03T14:43:43Z","published":"2024-10-03T14:43:43Z","title":"Algorithms For Automatic Accentuation And Transcription Of Russian Texts\n  In Speech Recognition Systems","summary":"  This paper presents an overview of rule-based system for automatic\naccentuation and phonemic transcription of Russian texts for speech connected\ntasks, such as Automatic Speech Recognition (ASR). Two parts of the developed\nsystem, accentuation and transcription, use different approaches to achieve\ncorrect phonemic representations of input phrases. Accentuation is based on\n\"Grammatical dictionary of the Russian language\" of A.A. Zaliznyak and\nwiktionary corpus. To distinguish homographs, the accentuation system also\nutilises morphological information of the sentences based on Recurrent Neural\nNetworks (RNN). Transcription algorithms apply the rules presented in the\nmonograph of B.M. Lobanov and L.I. Tsirulnik \"Computer Synthesis and Voice\nCloning\". The rules described in the present paper are implemented in an\nopen-source module, which can be of use to any scientific study connected to\nASR or Speech To Text (STT) tasks. Automatically marked up text annotations of\nthe Russian Voxforge database were used as training data for an acoustic model\nin CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,\nmean Word Accuracy being 71.2%. The developed toolkit is written in the Python\nlanguage and is accessible on GitHub for any researcher interested.\n","authors":["Olga Iakovenko","Ivan Bondarenko","Mariya Borovikova","Daniil Vodolazsky"],"pdf_url":"https://arxiv.org/pdf/2410.02538v1.pdf","comment":"Speech and Computer 20th International Conference, SPECOM 2018,\n  Leipzig, Germany, Proceedings 20"},{"id":"http://arxiv.org/abs/2402.17512v3","updated":"2024-10-03T14:41:43Z","published":"2024-02-27T13:54:48Z","title":"Latte: Latent Attention for Linear Time Transformers","summary":"  The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.\n","authors":["Rares Dolga","Marius Cobzarenco","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.17512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02525v1","updated":"2024-10-03T14:33:34Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17954v3","updated":"2024-10-03T14:29:11Z","published":"2024-02-28T00:24:29Z","title":"Twists, Humps, and Pebbles: Multilingual Speech Recognition Models\n  Exhibit Gender Performance Gaps","summary":"  Current automatic speech recognition (ASR) models are designed to be used\nacross many languages and tasks without substantial changes. However, this\nbroad language coverage hides performance gaps within languages, for example,\nacross genders. Our study systematically evaluates the performance of two\nwidely used multilingual ASR models on three datasets, encompassing 19\nlanguages from eight language families and two speaking conditions. Our\nfindings reveal clear gender disparities, with the advantaged group varying\nacross languages and models. Surprisingly, those gaps are not explained by\nacoustic or lexical properties. However, probing internal model states reveals\na correlation with gendered performance gap. That is, the easier it is to\ndistinguish speaker gender in a language using probes, the more the gap\nreduces, favoring female speakers. Our results show that gender disparities\npersist even in state-of-the-art models. Our findings have implications for the\nimprovement of multilingual ASR systems, underscoring the importance of\naccessibility to training data and nuanced evaluation to predict and mitigate\ngender gaps. We release all code and artifacts at\nhttps://github.com/g8a9/multilingual-asr-gender-gap.\n","authors":["Giuseppe Attanasio","Beatrice Savoldi","Dennis Fucci","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2402.17954v3.pdf","comment":"Accepted at EMNLP 2024. Code and artifacts at\n  https://github.com/g8a9/multilingual-asr-gender-gap"},{"id":"http://arxiv.org/abs/2410.02521v1","updated":"2024-10-03T14:28:40Z","published":"2024-10-03T14:28:40Z","title":"Methods for Automatic Matrix Language Determination of Code-Switched\n  Speech","summary":"  Code-switching (CS) is the process of speakers interchanging between two or\nmore languages which in the modern world becomes increasingly common. In order\nto better describe CS speech the Matrix Language Frame (MLF) theory introduces\nthe concept of a Matrix Language, which is the language that provides the\ngrammatical structure for a CS utterance. In this work the MLF theory was used\nto develop systems for Matrix Language Identity (MLID) determination. The MLID\nof English/Mandarin and English/Spanish CS text and speech was compared to\nacoustic language identity (LID), which is a typical way to identify a language\nin monolingual utterances. MLID predictors from audio show higher correlation\nwith the textual principles than LID in all cases while also outperforming LID\nin an MLID recognition task based on F1 macro (60\\%) and correlation score\n(0.38). This novel approach has identified that non-English languages (Mandarin\nand Spanish) are preferred over the English language as the ML contrary to the\nmonolingual choice of LID.\n","authors":["Olga Iakovenko","Thomas Hain"],"pdf_url":"https://arxiv.org/pdf/2410.02521v1.pdf","comment":"Accepted at EMNLP"},{"id":"http://arxiv.org/abs/2309.15656v2","updated":"2024-10-03T14:27:14Z","published":"2023-09-27T13:45:38Z","title":"Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis","summary":"  Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback.\n","authors":["Ildik Piln","Laurent Prvot","Hendrik Buschmeier","Pierre Lison"],"pdf_url":"https://arxiv.org/pdf/2309.15656v2.pdf","comment":"Updated version for SIGdial 2024"},{"id":"http://arxiv.org/abs/2408.03350v2","updated":"2024-10-03T14:20:40Z","published":"2024-08-05T20:19:18Z","title":"miniCTX: Neural Theorem Proving with (Long-)Contexts","summary":"  Real-world formal theorem proving often depends on a wealth of context,\nincluding definitions, lemmas, comments, file structure, and other information.\nWe introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new context that is not seen during\ntraining. miniCTX contains theorems sourced from real Lean projects and\ntextbooks, each associated with a context that can span tens of thousands of\ntokens. Models are tasked with proving a theorem given access to code from the\ntheorem's repository, which contains context that is needed for the proof. As a\nbaseline for miniCTX, we tested fine-tuning and prompting methods that\ncondition theorem proving on preceding context. Both approaches substantially\noutperform traditional methods that rely solely on state information. We found\nthat this ability to use context is not captured by previous benchmarks such as\nminiF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting\nand annotating theorem proving data, making it easy to add new projects into\nminiCTX to ensure that contexts are not seen during training. miniCTX offers a\nchallenging and realistic evaluation of neural theorem provers.\n","authors":["Jiewen Hu","Thomas Zhu","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2408.03350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02507v1","updated":"2024-10-03T14:15:00Z","published":"2024-10-03T14:15:00Z","title":"Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration","summary":"  Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.\n","authors":["Weikang Yuan","Junjie Cao","Zhuoren Jiang","Yangyang Kang","Jun Lin","Kaisong Song","tianqianjin lin","Pengwei Yan","Changlong Sun","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18028v2","updated":"2024-10-03T14:11:23Z","published":"2024-09-26T16:34:35Z","title":"Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective","summary":"  A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.\n","authors":["Yotam Wolf","Binyamin Rothberg","Dorin Shteyman","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2409.18028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02503v1","updated":"2024-10-03T14:06:43Z","published":"2024-10-03T14:06:43Z","title":"Mixed-Session Conversation with Egocentric Memory","summary":"  Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.\n","authors":["Jihyoung Jang","Taeyoung Kim","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02503v1.pdf","comment":"EMNLP Findings 2024 (30 pages); Project website:\n  https://mixed-session.github.io/"},{"id":"http://arxiv.org/abs/2407.03277v2","updated":"2024-10-03T14:05:14Z","published":"2024-07-03T17:04:17Z","title":"Evaluating Automatic Metrics with Incremental Machine Translation\n  Systems","summary":"  We introduce a dataset comprising commercial machine translations, gathered\nweekly over six years across 12 translation directions. Since human A/B testing\nis commonly used, we assume commercial systems improve over time, which enables\nus to evaluate machine translation (MT) metrics based on their preference for\nmore recent translations. Our study not only confirms several prior findings,\nsuch as the advantage of neural metrics over non-neural ones, but also explores\nthe debated issue of how MT quality affects metric reliability--an\ninvestigation that smaller datasets in previous research could not sufficiently\nexplore. Overall, our research demonstrates the dataset's value as a testbed\nfor metric evaluation. We release our code at https://github.com/gjwubyron/Evo\n","authors":["Guojun Wu","Shay B. Cohen","Rico Sennrich"],"pdf_url":"https://arxiv.org/pdf/2407.03277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02499v1","updated":"2024-10-03T14:01:01Z","published":"2024-10-03T14:01:01Z","title":"Defining Knowledge: Bridging Epistemology and Large Language Models","summary":"  Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions.\n","authors":["Constanza Fierro","Ruchira Dhar","Filippos Stamatiou","Nicolas Garneau","Anders Sgaard"],"pdf_url":"https://arxiv.org/pdf/2410.02499v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02498v1","updated":"2024-10-03T14:00:44Z","published":"2024-10-03T14:00:44Z","title":"Dynamic Gradient Alignment for Online Data Mixing","summary":"  The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.\n","authors":["Simin Fan","David Grangier","Pierre Ablin"],"pdf_url":"https://arxiv.org/pdf/2410.02498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02492v1","updated":"2024-10-03T13:57:07Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2404.07103v3","updated":"2024-10-03T13:55:08Z","published":"2024-04-10T15:41:53Z","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs","summary":"  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n","authors":["Bowen Jin","Chulin Xie","Jiawei Zhang","Kashob Kumar Roy","Yu Zhang","Zheng Li","Ruirui Li","Xianfeng Tang","Suhang Wang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2404.07103v3.pdf","comment":"21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT"},{"id":"http://arxiv.org/abs/2405.13448v2","updated":"2024-10-03T13:53:59Z","published":"2024-05-22T08:38:26Z","title":"Distilling Instruction-following Abilities of Large Language Models with\n  Task-aware Curriculum Planning","summary":"  Instruction tuning aims to align large language models (LLMs) with\nopen-domain instructions and human-preferred responses. While several studies\nhave explored autonomous approaches to distilling and annotating instructions\nfrom powerful proprietary LLMs, such as ChatGPT, they often neglect the impact\nof the distributions and characteristics of tasks, together with the varying\ndifficulty of instructions in training sets. This oversight can lead to\nimbalanced knowledge capabilities and poor generalization powers of student\nLLMs. To address these challenges, we introduce Task-Aware Curriculum Planning\nfor Instruction Refinement (TAPIR), a multi-round distillation framework that\nutilizes an oracle LLM to select instructions that are difficult for a student\nLLM to follow. To balance the student's capabilities, task distributions in\ntraining sets are adjusted with responses automatically refined according to\ntheir corresponding tasks. In addition, by incorporating curriculum planning,\nour approach systematically escalates the difficulty levels of tasks,\nprogressively enhancing the student LLM's capabilities. We rigorously evaluate\nTAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,\nMT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that\nstudent LLMs, trained with our method and less training data, outperform larger\ninstruction-tuned models and strong distillation baselines.\n","authors":["Yuanhao Yue","Chengyu Wang","Jun Huang","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13448v2.pdf","comment":"emnlp 2024 findings"},{"id":"http://arxiv.org/abs/2407.04069v2","updated":"2024-10-03T13:51:53Z","published":"2024-07-04T17:15:37Z","title":"A Systematic Survey and Critical Review on Evaluating Large Language\n  Models: Challenges, Limitations, and Recommendations","summary":"  Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust.\n","authors":["Md Tahmid Rahman Laskar","Sawsan Alqahtani","M Saiful Bari","Mizanur Rahman","Mohammad Abdullah Matin Khan","Haidar Khan","Israt Jahan","Amran Bhuiyan","Chee Wei Tan","Md Rizwan Parvez","Enamul Hoque","Shafiq Joty","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2407.04069v2.pdf","comment":"Accepted at EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2312.02783v3","updated":"2024-10-03T13:47:02Z","published":"2023-12-05T14:14:27Z","title":"Large Language Models on Graphs: A Comprehensive Survey","summary":"  Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.\n","authors":["Bowen Jin","Gang Liu","Chi Han","Meng Jiang","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2312.02783v3.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2401.16332v4","updated":"2024-10-03T13:40:39Z","published":"2024-01-29T17:38:14Z","title":"Tradeoffs Between Alignment and Helpfulness in Language Models with\n  Representation Engineering","summary":"  Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment.\n","authors":["Yotam Wolf","Noam Wies","Dorin Shteyman","Binyamin Rothberg","Yoav Levine","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2401.16332v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17041v4","updated":"2024-10-03T13:31:39Z","published":"2023-11-28T18:53:06Z","title":"Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties","summary":"  A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.\n","authors":["Keunwoo Peter Yu","Zheyuan Zhang","Fengyuan Hu","Shane Storks","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2311.17041v4.pdf","comment":"16 pages, LaTeX; Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2404.15206v3","updated":"2024-10-03T13:23:59Z","published":"2024-04-23T16:39:03Z","title":"Does Instruction Tuning Make LLMs More Consistent?","summary":"  The purpose of instruction tuning is enabling zero-shot performance, but\ninstruction tuning has also been shown to improve chain-of-thought reasoning\nand value alignment (Si et al., 2023). Here we consider the impact on\n$\\textit{consistency}$, i.e., the sensitivity of language models to small\nperturbations in the input. We compare 10 instruction-tuned LLaMA models to the\noriginal LLaMA-7b model and show that almost across-the-board they become more\nconsistent, both in terms of their representations and their predictions in\nzero-shot and downstream tasks. We explain these improvements through\nmechanistic analyses of factual recall.\n","authors":["Constanza Fierro","Jiaang Li","Anders Sgaard"],"pdf_url":"https://arxiv.org/pdf/2404.15206v3.pdf","comment":"We need to run extra experiments to ensure some of the claims in the\n  paper are fully correct"},{"id":"http://arxiv.org/abs/2410.02465v1","updated":"2024-10-03T13:15:19Z","published":"2024-10-03T13:15:19Z","title":"Response Tuning: Aligning Large Language Models without Instruction","summary":"  Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs.\n","authors":["Seokhyun An","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02465v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2409.07431v2","updated":"2024-10-03T13:07:25Z","published":"2024-09-11T17:21:59Z","title":"Synthetic continued pretraining","summary":"  Pretraining on large-scale, unstructured internet text enables language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient--to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining with EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If, instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.\n","authors":["Zitong Yang","Neil Band","Shuangping Li","Emmanuel Cands","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2409.07431v2.pdf","comment":"Updated organization of experimental results and methods\n  introduction. Released the dataset and model weights artifact"},{"id":"http://arxiv.org/abs/2406.19999v2","updated":"2024-10-03T13:02:11Z","published":"2024-06-28T15:34:26Z","title":"The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models","summary":"  Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models.\n","authors":["Xinyi Chen","Baohao Liao","Jirui Qi","Panagiotis Eustratiadis","Christof Monz","Arianna Bisazza","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2406.19999v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02441v1","updated":"2024-10-03T12:39:14Z","published":"2024-10-03T12:39:14Z","title":"Embedded Topic Models Enhanced by Wikification","summary":"  Topic modeling analyzes a collection of documents to learn meaningful\npatterns of words. However, previous topic models consider only the spelling of\nwords and do not take into consideration the homography of words. In this\nstudy, we incorporate the Wikipedia knowledge into a neural topic model to make\nit aware of named entities. We evaluate our method on two datasets, 1) news\narticles of \\textit{New York Times} and 2) the AIDA-CoNLL dataset. Our\nexperiments show that our method improves the performance of neural topic\nmodels in generalizability. Moreover, we analyze frequent terms in each topic\nand the temporal dependencies between topics to demonstrate that our\nentity-aware topic models can capture the time-series development of topics\nwell.\n","authors":["Takashi Shibuya","Takehito Utsuro"],"pdf_url":"https://arxiv.org/pdf/2410.02441v1.pdf","comment":"Accepted at EMNLP 2024 Workshop NLP for Wikipedia"},{"id":"http://arxiv.org/abs/2410.02433v1","updated":"2024-10-03T12:28:13Z","published":"2024-10-03T12:28:13Z","title":"Better Call SAUL: Fluent and Consistent Language Model Editing with\n  Generation Regularization","summary":"  To ensure large language models contain up-to-date knowledge, they need to be\nupdated regularly. However, model editing is challenging as it might also\naffect knowledge that is unrelated to the new data. State-of-the-art methods\nidentify parameters associated with specific knowledge and then modify them via\ndirect weight updates. However, these locate-and-edit methods suffer from heavy\ncomputational overhead and lack theoretical validation. In contrast, directly\nfine-tuning the model on requested edits affects the model's behavior on\nunrelated knowledge, and significantly damages the model's generation fluency\nand consistency. To address these challenges, we propose SAUL, a streamlined\nmodel editing method that uses sentence concatenation with augmented random\nfacts for generation regularization. Evaluations on three model editing\nbenchmarks show that SAUL is a practical and reliable solution for model\nediting outperforming state-of-the-art methods while maintaining generation\nquality and reducing computational overhead.\n","authors":["Mingyang Wang","Lukas Lange","Heike Adel","Jannik Strtgen","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2410.02433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02429v1","updated":"2024-10-03T12:24:18Z","published":"2024-10-03T12:24:18Z","title":"IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.\n","authors":["Tuo An","Yunjiao Zhou","Han Zou","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02429v1.pdf","comment":"21 pages, 10 figures, submitted to ICLR 2025 Conference"},{"id":"http://arxiv.org/abs/2410.02428v1","updated":"2024-10-03T12:21:17Z","published":"2024-10-03T12:21:17Z","title":"Collective Critics for Creative Story Generation","summary":"  Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.\n","authors":["Minwook Bae","Hyounghun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02428v1.pdf","comment":"EMNLP 2024 (36 pages)"},{"id":"http://arxiv.org/abs/2406.13092v2","updated":"2024-10-03T12:20:10Z","published":"2024-06-18T22:44:50Z","title":"Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language\n  Story Understanding","summary":"  Story video-text alignment, a core task in computational story understanding,\naims to align video clips with corresponding sentences in their descriptions.\nHowever, progress on the task has been held back by the scarcity of manually\nannotated video-text correspondence and the heavy concentration on English\nnarrations of Hollywood movies. To address these issues, in this paper, we\nconstruct a large-scale multilingual video story dataset named Multilingual\nSynopses of Movie Narratives (M-SYMON), containing 13,166 movie summary videos\nfrom 7 languages, as well as manual annotation of fine-grained video-text\ncorrespondences for 101.5 hours of video. Training on the human annotated data\nfrom SyMoN outperforms the SOTA methods by 15.7 and 16.2 percentage points on\nClip Accuracy and Sentence IoU scores, respectively, demonstrating the\neffectiveness of the annotations. As benchmarks for future research, we create\n6 baseline approaches with different multilingual training strategies, compare\ntheir performance in both intra-lingual and cross-lingual setups, exemplifying\nthe challenges of multilingual video-text alignment. The dataset is released\nat: https://github.com/insundaycathy/M-SyMoN\n","authors":["Yidan Sun","Jianfei Yu","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2406.13092v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.02426v1","updated":"2024-10-03T12:19:49Z","published":"2024-10-03T12:19:49Z","title":"Learning the Latent Rules of a Game from Data: A Chess Story","summary":"  We demonstrate that small pretrained foundational generative language models\nwith millions of parameters can learn the latent rules of a process from data\nassociated with the process. Inspired by Stefan Zweig's novella\n\"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M\nand 125M parameter pretrained foundational small language models (SLMs) can be\ninstruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of\nchess, propose legal moves, and accurately solve chess problems. We also\nexplore the impact of successive language model fine-tuning epochs on improved\noutcomes and demonstrate reductions in model hallucinations by increasing the\nnumber of instruction fine-tuning examples.\n","authors":["Ben Fauber"],"pdf_url":"https://arxiv.org/pdf/2410.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02425v1","updated":"2024-10-03T12:19:06Z","published":"2024-10-03T12:19:06Z","title":"LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services","summary":"  As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.\n","authors":["Magorzata azuka","Andreea Anghel","Thomas Parnell"],"pdf_url":"https://arxiv.org/pdf/2410.02425v1.pdf","comment":"Accepted to the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC '24)"},{"id":"http://arxiv.org/abs/2410.02417v1","updated":"2024-10-03T12:07:34Z","published":"2024-10-03T12:07:34Z","title":"MenakBERT -- Hebrew Diacriticizer","summary":"  Diacritical marks in the Hebrew language give words their vocalized form. The\ntask of adding diacritical marks to plain Hebrew text is still dominated by a\nsystem that relies heavily on human-curated resources. Recent models trained on\ndiacritized Hebrew texts still present a gap in performance. We use a recently\ndeveloped char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a\ncharacter level transformer pretrained on Hebrew text and fine-tuned to produce\ndiacritical marks for Hebrew sentences. We continue to show how finetuning a\nmodel for diacritizing transfers to a task such as part of speech tagging.\n","authors":["Ido Cohen","Jacob Gidron","Idan Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.02417v1.pdf","comment":"Published at ISCOL2022 as a poster"},{"id":"http://arxiv.org/abs/2406.11096v3","updated":"2024-10-03T11:57:00Z","published":"2024-06-16T22:59:18Z","title":"The Potential and Challenges of Evaluating Attitudes, Opinions, and\n  Values in Large Language Models","summary":"  Recent advances in Large Language Models (LLMs) have sparked wide interest in\nvalidating and comprehending the human-like cognitive-behavioral traits LLMs\nmay capture and convey. These cognitive-behavioral traits include typically\nAttitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within\nLLMs remains opaque, and different evaluation methods may yield different\nresults. This has led to a lack of clarity on how different studies are related\nto each other and how they can be interpreted. This paper aims to bridge this\ngap by providing a comprehensive overview of recent works on the evaluation of\nAOVs in LLMs. Moreover, we survey related approaches in different stages of the\nevaluation pipeline in these works. By doing so, we address the potential and\nchallenges with respect to understanding the model, human-AI alignment, and\ndownstream application in social sciences. Finally, we provide practical\ninsights into evaluation methods, model enhancement, and interdisciplinary\ncollaboration, thereby contributing to the evolving landscape of evaluating\nAOVs in LLMs.\n","authors":["Bolei Ma","Xinpeng Wang","Tiancheng Hu","Anna-Carolina Haensch","Michael A. Hedderich","Barbara Plank","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2406.11096v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2212.00596v2","updated":"2024-10-03T11:42:43Z","published":"2022-12-01T15:48:51Z","title":"Language models and brains align due to more than next-word prediction\n  and word-level information","summary":"  Pretrained language models have been shown to significantly predict brain\nrecordings of people comprehending language. Recent work suggests that the\nprediction of the next word is a key mechanism that contributes to this\nalignment. What is not yet understood is whether prediction of the next word is\nnecessary for this observed alignment or simply sufficient, and whether there\nare other shared mechanisms or information that are similarly important. In\nthis work, we take a step towards understanding the reasons for brain alignment\nvia two simple perturbations in popular pretrained language models. These\nperturbations help us design contrasts that can control for different types of\ninformation. By contrasting the brain alignment of these differently perturbed\nmodels, we show that improvements in alignment with brain recordings are due to\nmore than improvements in next-word prediction and word-level information.\n","authors":["Gabriele Merlin","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2212.00596v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02396v1","updated":"2024-10-03T11:17:58Z","published":"2024-10-03T11:17:58Z","title":"Parameter Competition Balancing for Model Merging","summary":"  While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.\n","authors":["Guodong Du","Junlin Lee","Jing Li","Runhua Jiang","Yifei Guo","Shuyang Yu","Hanting Liu","Sim Kuan Goh","Ho-Kin Tang","Daojing He","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02396v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2406.13560v2","updated":"2024-10-03T11:17:43Z","published":"2024-06-19T13:48:19Z","title":"Lexically Grounded Subword Segmentation","summary":"  We present three innovations in tokenization and subword segmentation. First,\nwe propose to use unsupervised morphological analysis with Morfessor as\npre-tokenization. Second, we present an algebraic method for obtaining subword\nembeddings grounded in a word embedding space. Based on that, we design a novel\nsubword segmentation algorithm that uses the embeddings, ensuring that the\nprocedure considers lexical meaning. Third, we introduce an efficient\nsegmentation algorithm based on a subword bigram model that can be initialized\nwith the lexically aware segmentation method to avoid using Morfessor and large\nembedding tables at inference time. We evaluate the proposed approaches using\ntwo intrinsic metrics and measure their performance on two downstream tasks:\npart-of-speech tagging and machine translation. Our experiments show\nsignificant improvements in the morphological plausibility of the segmentation\nwhen evaluated using segmentation precision on morpheme boundaries and improved\nR\\'enyi efficiency in 8 languages. Although the proposed tokenization methods\ndo not have a large impact on automatic translation quality, we observe\nconsistent performance gains in the arguably more morphological task of\npart-of-speech tagging.\n","authors":["Jindich Libovick","Jindich Helcl"],"pdf_url":"https://arxiv.org/pdf/2406.13560v2.pdf","comment":"Camera-ready, EMNLP Main conf"},{"id":"http://arxiv.org/abs/2406.13663v3","updated":"2024-10-03T11:03:22Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernndez","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v3.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2410.02381v1","updated":"2024-10-03T11:01:25Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.02889v2","updated":"2024-10-03T11:01:14Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v2.pdf","comment":"20 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2410.02378v1","updated":"2024-10-03T10:51:02Z","published":"2024-10-03T10:51:02Z","title":"Towards Comprehensive Detection of Chinese Harmful Memes","summary":"  This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes\nhave proliferated on the Chinese Internet, while research on detecting Chinese\nharmful memes significantly lags behind due to the absence of reliable datasets\nand effective detectors. To this end, we focus on the comprehensive detection\nof Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful\nmeme dataset, which consists of 12,000 samples with fine-grained annotations\nfor various meme types. Additionally, we propose a baseline detector,\nMultimodal Knowledge Enhancement (MKE), incorporating contextual information of\nmeme content generated by the LLM to enhance the understanding of Chinese\nmemes. During the evaluation phase, we conduct extensive quantitative\nexperiments and qualitative analyses on multiple baselines, including LLMs and\nour MKE. The experimental results indicate that detecting Chinese harmful memes\nis challenging for existing models while demonstrating the effectiveness of\nMKE. The resources for this paper are available at\nhttps://github.com/DUT-lujunyu/ToxiCN_MM.\n","authors":["Junyu Lu","Bo Xu","Xiaokun Zhang","Hongbo Wang","Haohao Zhu","Dongyu Zhang","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2410.02378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02365v1","updated":"2024-10-03T10:24:24Z","published":"2024-10-03T10:24:24Z","title":"From Concrete to Abstract: A Multimodal Generative Approach to Abstract\n  Concept Learning","summary":"  Understanding and manipulating concrete and abstract concepts is fundamental\nto human intelligence. Yet, they remain challenging for artificial agents. This\npaper introduces a multimodal generative approach to high order abstract\nconcept learning, which integrates visual and categorical linguistic\ninformation from concrete ones. Our model initially grounds subordinate level\nconcrete concepts, combines them to form basic level concepts, and finally\nabstracts to superordinate level concepts via the grounding of basic-level\nconcepts. We evaluate the model language learning ability through\nlanguage-to-visual and visual-to-language tests with high order abstract\nconcepts. Experimental results demonstrate the proficiency of the model in both\nlanguage understanding and language naming tasks.\n","authors":["Haodong Xie","Rahul Singh Maharjan","Federico Tavella","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2410.02365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02355v1","updated":"2024-10-03T10:06:27Z","published":"2024-10-03T10:06:27Z","title":"AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models","summary":"  Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.\n","authors":["Junfeng Fang","Houcheng Jiang","Kun Wang","Yunshan Ma","Xiang Wang","Xiangnan He","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.02355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02343v1","updated":"2024-10-03T09:53:48Z","published":"2024-10-03T09:53:48Z","title":"Listening to the Wise Few: Select-and-Copy Attention Heads for\n  Multiple-Choice QA","summary":"  A standard way to evaluate the abilities of LLM involves presenting a\nmultiple-choice question and selecting the option with the highest logit as the\nmodel's predicted answer. However, such a format for evaluating LLMs has\nlimitations, since even if the model knows the correct answer, it may struggle\nto select the corresponding letter simply due to difficulties in following this\nrigid format. To address this, we introduce new scores that better capture and\nreveal model's underlying knowledge: the Query-Key Score (QK-score), derived\nfrom the interaction between query and key representations in attention heads,\nand the Attention Score, based on attention weights. These scores are extracted\nfrom specific \\textit{select-and-copy} heads, which show consistent performance\nacross popular Multi-Choice Question Answering (MCQA) datasets. Based on these\nscores, our method improves knowledge extraction, yielding up to 16\\% gain for\nLLaMA2-7B and up to 10\\% for larger models on popular MCQA benchmarks. At the\nsame time, the accuracy on a simple synthetic dataset, where the model\nexplicitly knows the right answer, increases by almost 60\\%, achieving nearly\nperfect accuracy, therefore demonstrating the method's efficiency in mitigating\nMCQA format limitations. To support our claims, we conduct experiments on\nmodels ranging from 7 billion to 70 billion parameters in both zero- and\nfew-shot setups.\n","authors":["Eduard Tulchinskii","Laida Kushnareva","Kristian Kuznetsov","Anastasia Voznyuk","Andrei Andriiainen","Irina Piontkovskaya","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2410.02343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02338v1","updated":"2024-10-03T09:48:09Z","published":"2024-10-03T09:48:09Z","title":"How Much Can RAG Help the Reasoning of LLM?","summary":"  Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.\n","authors":["Jingyu Liu","Jiaen Lin","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18369v2","updated":"2024-10-03T09:45:47Z","published":"2024-05-28T17:08:31Z","title":"PromptWizard: Task-Aware Prompt Optimization Framework","summary":"  Large language models (LLMs) have transformed AI across diverse domains, with\nprompting being central to their success in guiding model outputs. However,\nmanual prompt engineering is both labor-intensive and domain-specific,\nnecessitating the need for automated solutions. We introduce PromptWizard, a\nnovel, fully automated framework for discrete prompt optimization, utilizing a\nself-evolving, self-adapting mechanism. Through a feedback-driven critique and\nsynthesis process, PromptWizard achieves an effective balance between\nexploration and exploitation, iteratively refining both prompt instructions and\nin-context examples to generate human-readable, task-specific prompts. This\nguided approach systematically improves prompt quality, resulting in superior\nperformance across 45 tasks. PromptWizard excels even with limited training\ndata, smaller LLMs, and various LLM architectures. Additionally, our cost\nanalysis reveals a substantial reduction in API calls, token usage, and overall\ncost, demonstrating PromptWizard's efficiency, scalability, and advantages over\nexisting prompt optimization strategies.\n","authors":["Eshaan Agarwal","Joykirat Singh","Vivek Dani","Raghav Magazine","Tanuja Ganu","Akshay Nambi"],"pdf_url":"https://arxiv.org/pdf/2405.18369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12319v2","updated":"2024-10-03T09:38:48Z","published":"2024-06-18T06:43:04Z","title":"On the Adversarial Vulnerability of Pairwise Evaluation Using Large\n  Language Models","summary":"  Pairwise evaluation using large language models (LLMs) is widely adopted for\nevaluating generated outputs. However, the reliability of LLM evaluators is\noften compromised by their biased preferences, such as favoring verbosity and\nan authoritative tone. In this work, we find that the evaluation setup itself\ncan significantly amplify these biases, where pairwise evaluators exhibit more\nundesirable tendencies than pointwise evaluators. Our analysis further reveals\nthat even when pairwise evaluators make incorrect judgments, they can still\naccurately identify shortcomings in low-quality outputs. As a simple remedy, we\nalso propose incorporating pointwise reasoning into pairwise evaluation.\nExperimental results show that our method improves the performance of pairwise\nevaluators on adversarial samples across various models. We hope our findings\nencourage further exploration into the reliability of LLM evaluators.\n","authors":["Hawon Jeong","ChaeHun Park","Jimin Hong","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.12319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04459v3","updated":"2024-10-03T09:32:31Z","published":"2024-07-05T12:09:40Z","title":"Generalists vs. Specialists: Evaluating Large Language Models for Urdu","summary":"  In this paper, we compare general-purpose models, GPT-4-Turbo and Llama-3-8b,\nwith special-purpose models--XLM-Roberta-large, mT5-large, and Llama-3-8b--that\nhave been fine-tuned on specific tasks. We focus on seven classification and\nseven generation tasks to evaluate the performance of these models on Urdu\nlanguage. Urdu has 70 million native speakers, yet it remains underrepresented\nin Natural Language Processing (NLP). Despite the frequent advancements in\nLarge Language Models (LLMs), their performance in low-resource languages,\nincluding Urdu, still needs to be explored. We also conduct a human evaluation\nfor the generation tasks and compare the results with the evaluations performed\nby GPT-4-Turbo, Llama-3-8b and Claude 3.5 Sonnet. We find that special-purpose\nmodels consistently outperform general-purpose models across various tasks. We\nalso find that the evaluation done by GPT-4-Turbo for generation tasks aligns\nmore closely with human evaluation compared to the evaluation the evaluation\ndone by Llama-3-8b. This paper contributes to the NLP community by providing\ninsights into the effectiveness of general and specific-purpose LLMs for\nlow-resource languages.\n","authors":["Samee Arif","Abdul Hameed Azeemi","Agha Ali Raza","Awais Athar"],"pdf_url":"https://arxiv.org/pdf/2407.04459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02330v1","updated":"2024-10-03T09:28:59Z","published":"2024-10-03T09:28:59Z","title":"Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection","summary":"  As a manner to augment pre-trained large language models (LLM), knowledge\ninjection is critical to develop vertical domain large models and has been\nwidely studied. Although most current approaches, including parameter-efficient\nfine-tuning (PEFT) and block expansion methods, uniformly apply knowledge\nacross all LLM layers, it raises the question: are all layers equally crucial\nfor knowledge injection? We begin by evaluating the importance of each layer in\nfinding the optimal layer range for knowledge injection. Intuitively, the more\nimportant layers should play a more critical role in knowledge injection and\ndeserve a denser injection. We observe performance dips in question-answering\nbenchmarks after the removal or expansion of the shallow layers, and the\ndegradation shrinks as the layer gets deeper, indicating that the shallow\nlayers hold the key to knowledge injection. This insight leads us to propose\nthe S strategy, a post-pretraining strategy of selectively enhancing shallow\nlayers while pruning the less effective deep ones. Based on this strategy, we\nintroduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the\ncorpus of code $\\&$ math and demonstrated the effectiveness of our strategy.\nFurther experiments across different LLM, Mistral-7B, and a legal corpus\nconfirmed the general applicability of the approach, underscoring its\nwide-ranging efficacy. Our code is available at:\n\\https://github.com/txchen-USTC/Llama-Slayer\n","authors":["Tianxiang Chen","Zhentao Tan","Tao Gong","Yue Wu","Qi Chu","Bin Liu","Jieping Ye","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16050v2","updated":"2024-10-03T09:24:56Z","published":"2024-02-25T10:27:46Z","title":"Efficient Temporal Extrapolation of Multimodal Large Language Models\n  with Temporal Grounding Bridge","summary":"  Despite progress in multimodal large language models (MLLMs), the challenge\nof interpreting long-form videos in response to linguistic queries persists,\nlargely due to the inefficiency in temporal grounding and limited pre-trained\ncontext window size. In this work, we introduce Temporal Grounding Bridge\n(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding\ncapabilities and broadens their contextual scope. Our framework significantly\nenhances the temporal capabilities of current MLLMs through three key\ninnovations: an efficient multi-span temporal grounding algorithm applied to\nlow-dimension temporal features projected from flow; a multimodal length\nextrapolation training paradigm that utilizes low-dimension temporal features\nto extend the training context window size; and a bootstrapping framework that\nbridges our model with pluggable MLLMs without requiring annotation. We\nvalidate TGB across seven video benchmarks and demonstrate substantial\nperformance improvements compared with prior MLLMs. Notably, our model,\ninitially trained on sequences of four frames, effectively handles sequences up\nto 16 longer without sacrificing performance, highlighting its scalability and\neffectiveness in real-world applications. Our code is publicly available at\nhttps://github.com/bigai-nlco/VideoTGB\n","authors":["Yuxuan Wang","Yueqian Wang","Pengfei Wu","Jianxin Liang","Dongyan Zhao","Yang Liu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.16050v2.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.12924v2","updated":"2024-10-03T09:21:57Z","published":"2024-09-04T03:17:19Z","title":"WaveletGPT: Wavelets Meet Large Language Models","summary":"  Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. They\nare trained on a simple objective: to predict the next token given the previous\ncontext. We live in a world where most of the data around us, e.g., text,\naudio, and music, has a multi-scale structure associated with it. This paper\ninfuses LLMs with traditional signal processing ideas, namely wavelets, during\npre-training to take advantage of the structure. Without adding \\textbf{any\nextra parameters} to a GPT-style LLM architecture, we achieve the same\npre-training performance almost twice as fast in text, raw audio, and symbolic\nmusic. This is achieved by imposing a structure on intermediate embeddings.\nWhen trained for the same number of training steps, we achieve significant\ngains in performance, which is comparable to pre-training a larger neural\narchitecture. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every Transformer\ndecoder block. This work will hopefully pave the way for incorporating\nmulti-rate signal processing ideas into traditional LLM pre-training. Further,\nwe showcase pushing model performance by improving internal structure instead\nof just going after scale.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2409.12924v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.09589v4","updated":"2024-10-03T09:00:35Z","published":"2024-05-15T10:16:25Z","title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video\n  and Audio Foundation Models","summary":"  The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.\n","authors":["Pranab Sahoo","Prabhash Meharia","Akash Ghosh","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2405.09589v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02320v1","updated":"2024-10-03T08:56:29Z","published":"2024-10-03T08:56:29Z","title":"Post-edits Are Preferences Too","summary":"  Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors \\emph{create} $s_1$ and know that it should be better\nthan $s_2$. We attempt to use these implicit preferences for PO and show that\nit helps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.\n","authors":["Nathaniel Berger","Stefan Riezler","Miriam Exel","Matthias Huck"],"pdf_url":"https://arxiv.org/pdf/2410.02320v1.pdf","comment":"To appear at the Ninth Conference on Machine Translation (WMT24)"},{"id":"http://arxiv.org/abs/2406.02069v3","updated":"2024-10-03T08:46:42Z","published":"2024-06-04T07:51:30Z","title":"PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling","summary":"  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.\n","authors":["Zefan Cai","Yichi Zhang","Bofei Gao","Yuliang Liu","Tianyu Liu","Keming Lu","Wayne Xiong","Yue Dong","Baobao Chang","Junjie Hu","Wen Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.02069v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02308v1","updated":"2024-10-03T08:44:17Z","published":"2024-10-03T08:44:17Z","title":"Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large\n  Language Models","summary":"  Phrases are fundamental linguistic units through which humans convey\nsemantics. This study critically examines the capacity of API-based large\nlanguage models (LLMs) to comprehend phrase semantics, utilizing three\nhuman-annotated datasets. We assess the performance of LLMs in executing phrase\nsemantic reasoning tasks guided by natural language instructions and explore\nthe impact of common prompting techniques, including few-shot demonstrations\nand Chain-of-Thought reasoning. Our findings reveal that LLMs greatly\noutperform traditional embedding methods across the datasets; however, they do\nnot show a significant advantage over fine-tuned methods. The effectiveness of\nadvanced prompting strategies shows variability. We conduct detailed error\nanalyses to interpret the limitations faced by LLMs in comprehending phrase\nsemantics. Code and data can be found at\nhttps://github.com/memray/llm_phrase_semantics.\n","authors":["Rui Meng","Ye Liu","Lifu Tu","Daqing He","Yingbo Zhou","Semih Yavuz"],"pdf_url":"https://arxiv.org/pdf/2410.02308v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.17233v2","updated":"2024-10-03T08:43:25Z","published":"2024-06-25T02:37:53Z","title":"Self-Constructed Context Decompilation with Fined-grained Alignment\n  Enhancement","summary":"  Decompilation transforms compiled code back into a high-level programming\nlanguage for analysis when source code is unavailable. Previous work has\nprimarily focused on enhancing decompilation performance by increasing the\nscale of model parameters or training data for pre-training. Based on the\ncharacteristics of the decompilation task, we propose two methods: (1) Without\nfine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method\nrecompiles the LLM's decompilation results to construct pairs for in-context\nlearning, helping the model improve decompilation performance. (2) Fine-grained\nAlignment Enhancement (FAE), which meticulously aligns assembly code with\nsource code at the statement level by leveraging debugging information, is\nemployed during the fine-tuning phase to achieve further improvements in\ndecompilation. By integrating these two methods, we achieved a Re-Executability\nperformance improvement of approximately 3.90% on the Decompile-Eval benchmark,\nestablishing a new state-of-the-art performance of 52.41%. The code, data, and\nmodels are available at https://github.com/AlongWY/sccdec.\n","authors":["Yunlong Feng","Dechuan Teng","Yang Xu","Honglin Mu","Xiao Xu","Libo Qin","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2406.17233v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02298v1","updated":"2024-10-03T08:34:17Z","published":"2024-10-03T08:34:17Z","title":"Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models","summary":"  As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.\n","authors":["Guobin Shen","Dongcheng Zhao","Yiting Dong","Xiang He","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.02298v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02297v1","updated":"2024-10-03T08:27:59Z","published":"2024-10-03T08:27:59Z","title":"Make Compound Sentences Simple to Analyze: Learning to Split Sentences\n  for Aspect-based Sentiment Analysis","summary":"  In the domain of Aspect-Based Sentiment Analysis (ABSA), generative methods\nhave shown promising results and achieved substantial advancements. However,\ndespite these advancements, the tasks of extracting sentiment quadruplets,\nwhich capture the nuanced sentiment expressions within a sentence, remain\nsignificant challenges. In particular, compound sentences can potentially\ncontain multiple quadruplets, making the extraction task increasingly difficult\nas sentence complexity grows. To address this issue, we are focusing on\nsimplifying sentence structures to facilitate the easier recognition of these\nelements and crafting a model that integrates seamlessly with various ABSA\ntasks. In this paper, we propose Aspect Term Oriented Sentence Splitter\n(ATOSS), which simplifies compound sentence into simpler and clearer forms,\nthereby clarifying their structure and intent. As a plug-and-play module, this\napproach retains the parameters of the ABSA model while making it easier to\nidentify essential intent within input sentences. Extensive experimental\nresults show that utilizing ATOSS outperforms existing methods in both ASQP and\nACOS tasks, which are the primary tasks for extracting sentiment quadruplets.\n","authors":["Yongsik Seo","Sungwon Song","Ryang Heo","Jieyong Kim","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02297v1.pdf","comment":"Accepted at EMNLP 2024 (Findings, long paper)"},{"id":"http://arxiv.org/abs/2410.02296v1","updated":"2024-10-03T08:27:54Z","published":"2024-10-03T08:27:54Z","title":"Language Models are Graph Learners","summary":"  Language Models (LMs) are increasingly challenging the dominance of\ndomain-specific models, including Graph Neural Networks (GNNs) and Graph\nTransformers (GTs), in graph learning tasks. Following this trend, we propose a\nnovel approach that empowers off-the-shelf LMs to achieve performance\ncomparable to state-of-the-art GNNs on node classification tasks, without\nrequiring any architectural modification. By preserving the LM's original\narchitecture, our approach retains a key benefit of LM instruction tuning: the\nability to jointly train on diverse datasets, fostering greater flexibility and\nefficiency. To achieve this, we introduce two key augmentation strategies: (1)\nEnriching LMs' input using topological and semantic retrieval methods, which\nprovide richer contextual information, and (2) guiding the LMs' classification\nprocess through a lightweight GNN classifier that effectively prunes class\ncandidates. Our experiments on real-world datasets show that backbone Flan-T5\nmodels equipped with these augmentation strategies outperform state-of-the-art\ntext-output node classifiers and are comparable to top-performing vector-output\nnode classifiers. By bridging the gap between specialized task-specific node\nclassifiers and general LMs, this work paves the way for more versatile and\nwidely applicable graph learning models. We will open-source the code upon\npublication.\n","authors":["Zhe Xu","Kaveh Hassani","Si Zhang","Hanqing Zeng","Michihiro Yasunaga","Limei Wang","Dongqi Fu","Ning Yao","Bo Long","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2410.02296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18430v2","updated":"2024-10-03T08:24:40Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Ral Toral","David Snchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2410.02293v1","updated":"2024-10-03T08:23:06Z","published":"2024-10-03T08:23:06Z","title":"Efficient Second-Order Neural Network Optimization via Adaptive Trust\n  Region Methods","summary":"  Second-order optimization methods offer notable advantages in training deep\nneural networks by utilizing curvature information to achieve faster\nconvergence. However, traditional second-order techniques are computationally\nprohibitive, primarily due to the large matrix inversions and high memory\ndemands they require. While adaptive trust-region methods have been developed\nto mitigate these issues, their performance is often hindered by conservative\nestimates of key parameters, such as the Lipschitz constant of the Hessian,\nresulting in suboptimal outcomes. In this paper, we introduce\nSecondOrderAdaptiveAdam (SOAA), a novel optimization algorithm designed to\novercome these limitations. SOAA approximates the Fisher information matrix\nusing a diagonal representation, reducing computational complexity from\n\\(O(n^{2})\\) to \\(O(n)\\), thereby making it suitable for large-scale deep\nlearning models, including large language models (LLMs). Additionally, the\nalgorithm integrates an adaptive trust-region mechanism that dynamically\nadjusts the trust region size based on observed loss reduction, ensuring both\nrobust convergence and computational efficiency. We empirically demonstrate\nthat SOAA achieves faster and more stable convergence compared to first-order\noptimizers, such as Adam, under similar computational constraints. However, the\ndiagonal approximation of the Fisher information matrix may be less effective\nin capturing higher-order interactions between gradients, suggesting potential\nareas for further refinement and future research.\n","authors":["James Vo"],"pdf_url":"https://arxiv.org/pdf/2410.02293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02284v1","updated":"2024-10-03T08:07:55Z","published":"2024-10-03T08:07:55Z","title":"Correlation and Navigation in the Vocabulary Key Representation Space of\n  Language Models","summary":"  Language model (LM) decoding is based on the next-token prediction (NTP)\nprobability distribution. For neural LMs (e.g., Transformer-based), NTP\ndistribution is essentially a softmax-regularized dot product between an\nencoded input context (query) and fixed vocabulary representations (keys). In\nthis paper, we study the effect of the key distribution on the NTP\ndistribution, with a focus on whether the similarity between keys will trigger\nspurious correlations in NTP. Through knowledge-probing tasks, we show that in\nthe NTP distribution, the few top-ranked tokens are typically accurate.\nHowever, the middle-ranked prediction is highly biased towards the tokens that\nare distributionally (not necessarily semantically) similar to these top ones.\nFor instance, if \"P\" is predicted as the top-1 token, \"A\"-\"Z\" will all be\nranked high in NTP, no matter whether they can lead to correct decoding\nresults. This hurts the sampling diversity and makes the sampling of correct,\nlong-tail results hopeless and noisy. We attempt to alleviate this issue via a\nnovel in-context method that iteratively pushes the query representation away\nfrom explored regions. Specifically, we include the explored decoding results\nin the context and prompt the LM to generate something else, which encourages\nthe LM to produce a query representation that has small dot products with\nexplored keys. Experiments on knowledge-probing tasks show that our method\nleads to efficient navigation away from explored keys to correct new keys. We\nfurther extend our method to open-ended and chain-of-thought (for reasoning)\ngeneration. Experiment results show that ICN contributes to better generation\ndiversity and improved self-consistency voting performance. Finally, we discuss\npotential training issues caused by the fixed key space together with the\nchallenges and possible ways to address them in future research.\n","authors":["Letian Peng","Chenyang An","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2410.02284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02283v1","updated":"2024-10-03T08:07:14Z","published":"2024-10-03T08:07:14Z","title":"Morphological evaluation of subwords vocabulary used by BETO language\n  model","summary":"  Subword tokenization algorithms used by Large Language Models are\nsignificantly more efficient and can independently build the necessary\nvocabulary of words and subwords without human intervention. However, those\nsubwords do not always align with real morphemes, potentially impacting the\nmodels' performance, though it remains uncertain when this might occur. In\nprevious research, we proposed a method to assess the morphological quality of\nvocabularies, focusing on the overlap between these vocabularies and the\nmorphemes of a given language. Our evaluation method was built on three quality\nmeasures, relevance, cohesion, and morphological accuracy, and a procedure for\ntheir assessment. By applying this method to vocabularies created by three\nsubword tokenization algorithms, BPE, Wordpiece, and Unigram, we concluded that\nthese vocabularies generally exhibit very low morphological quality. In this\narticle, we apply this evaluation to the tokenizer of BETO, a BERT language\nmodel trained on large Spanish corpora. This evaluation, along with our\nprevious results, helped us conclude that its vocabulary has a low\nmorphological quality, and we also found that training the tokenizer in a\nlarger corpus does not improve the morphological quality of the generated\nvocabulary. Additionally, this evaluation helps clarify the algorithm used by\nthe tokenizer, that is, Wordpiece, given the inconsistencies between the\nauthors' claims and the model's configuration.\n","authors":["scar Garca-Sierra","Ana Fernndez-Pampilln Cesteros","Miguel Ortega-Martn"],"pdf_url":"https://arxiv.org/pdf/2410.02283v1.pdf","comment":"in Spanish language"},{"id":"http://arxiv.org/abs/2406.11341v2","updated":"2024-10-03T08:07:01Z","published":"2024-06-17T08:59:04Z","title":"A Systematic Analysis of Large Language Models as Soft Reasoners: The\n  Case of Syllogistic Inferences","summary":"  The reasoning abilities of Large Language Models (LLMs) are becoming a\ncentral focus of study in NLP. In this paper, we consider the case of\nsyllogistic reasoning, an area of deductive reasoning studied extensively in\nlogic and cognitive psychology. Previous research has shown that pre-trained\nLLMs exhibit reasoning biases, such as $\\textit{content effects}$, avoid\nanswering that $\\textit{no conclusion follows}$, display human-like\ndifficulties, and struggle with multi-step reasoning. We contribute to this\nresearch line by systematically investigating the effects of chain-of-thought\nreasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on\nsyllogistic reasoning, considering syllogisms with conclusions that support or\nviolate world knowledge, as well as ones with multiple premises. Crucially, we\ngo beyond the standard focus on accuracy, with an in-depth analysis of the\nconclusions generated by the models. Our results suggest that the behavior of\npre-trained LLMs can be explained by heuristics studied in cognitive science\nand that both ICL and SFT improve model performance on valid inferences,\nalthough only the latter mitigates most reasoning biases without harming model\nconsistency.\n","authors":["Leonardo Bertolazzi","Albert Gatt","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.11341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11016v2","updated":"2024-10-03T08:05:14Z","published":"2024-06-16T17:19:23Z","title":"Optimized Speculative Sampling for GPU Hardware Accelerators","summary":"  In this work, we optimize speculative sampling for parallel hardware\naccelerators to improve sampling speed. We notice that substantial portions of\nthe intermediate matrices necessary for speculative sampling can be computed\nconcurrently. This allows us to distribute the workload across multiple GPU\nthreads, enabling simultaneous operations on matrix segments within thread\nblocks. This results in profiling time improvements ranging from 6% to 13%\nrelative to the baseline implementation, without compromising accuracy. To\nfurther accelerate speculative sampling, probability distributions\nparameterized by softmax are approximated by sigmoid. This approximation\napproach results in significantly greater relative improvements in profiling\ntime, ranging from 37% to 94%, with a minor decline in accuracy. We conduct\nextensive experiments on both automatic speech recognition and summarization\ntasks to validate the effectiveness of our optimization methods.\n","authors":["Dominik Wagner","Seanie Lee","Ilja Baumann","Philipp Seeberger","Korbinian Riedhammer","Tobias Bocklet"],"pdf_url":"https://arxiv.org/pdf/2406.11016v2.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02281v1","updated":"2024-10-03T08:03:40Z","published":"2024-10-03T08:03:40Z","title":"Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity\n  Recognition","summary":"  The Novelties corpus is a collection of novels (and parts of novels)\nannotated for Named Entity Recognition (NER) among other tasks. This document\ndescribes the guidelines applied during its annotation. It contains the\ninstructions used by the annotators, as well as a number of examples retrieved\nfrom the annotated novels, and illustrating expressions that should be marked\nas entities as well as expressions that should not.\n","authors":["Arthur Amalvy","Vincent Labatut"],"pdf_url":"https://arxiv.org/pdf/2410.02281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19335v2","updated":"2024-10-03T07:59:36Z","published":"2024-04-30T08:01:49Z","title":"StablePT: Towards Stable Prompting for Few-shot Learning via Input\n  Separation","summary":"  Large language models have shown their ability to become effective few-shot\nlearners with prompting, revolutionizing the paradigm of learning with data\nscarcity. However, this approach largely depends on the quality of prompt\ninitialization, and always exhibits large variability among different runs.\nSuch property makes prompt tuning highly unreliable and vulnerable to poorly\nconstructed prompts, which limits its extension to more real-world\napplications. To tackle this issue, we propose to treat the hard prompt and\nsoft prompt as separate inputs to mitigate noise brought by the prompt\ninitialization. Furthermore, we optimize soft prompts with contrastive learning\nfor utilizing class-aware information in the training process to maintain model\nperformance. Experimental results demonstrate that \\sysname outperforms\nstate-of-the-art methods by 6.97% in accuracy and reduces the standard\ndeviation by 1.92 on average. Furthermore, extensive experiments underscore its\nrobustness and stability across 8 datasets covering various tasks. Codes are\navailable at https://github.com/lccc0528/Stable/tree/main.\n","authors":["Xiaoming Liu","Chen Liu","Zhaohan Zhang","Chengzhengxu Li","Longtian Wang","Yu Lan","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2404.19335v2.pdf","comment":"EMNLP 2024 Findings"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.01744v2","updated":"2024-10-03T15:57:05Z","published":"2024-10-02T16:55:01Z","title":"Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks","summary":"  Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose Leopard, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.\n","authors":["Mengzhao Jia","Wenhao Yu","Kaixin Ma","Tianqing Fang","Zhihan Zhang","Siru Ouyang","Hongming Zhang","Meng Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01744v2.pdf","comment":"Our code is available at https://github.com/Jill0001/Leopard"},{"id":"http://arxiv.org/abs/2410.01697v2","updated":"2024-10-03T09:28:48Z","published":"2024-10-02T16:05:03Z","title":"MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning","summary":"  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n","authors":["Sedjro Salomon Hotegni","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2410.01697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01654v2","updated":"2024-10-03T12:43:14Z","published":"2024-10-02T15:19:31Z","title":"Releasing the Parameter Latency of Neural Representation for\n  High-Efficiency Video Compression","summary":"  For decades, video compression technology has been a prominent research area.\nTraditional hybrid video compression framework and end-to-end frameworks\ncontinue to explore various intra- and inter-frame reference and prediction\nstrategies based on discrete transforms and deep learning techniques. However,\nthe emerging implicit neural representation (INR) technique models entire\nvideos as basic units, automatically capturing intra-frame and inter-frame\ncorrelations and obtaining promising performance. INR uses a compact neural\nnetwork to store video information in network parameters, effectively\neliminating spatial and temporal redundancy in the original video. However, in\nthis paper, our exploration and verification reveal that current INR video\ncompression methods do not fully exploit their potential to preserve\ninformation. We investigate the potential of enhancing network parameter\nstorage through parameter reuse. By deepening the network, we designed a\nfeasible INR parameter reuse scheme to further improve compression performance.\nExtensive experimental results show that our method significantly enhances the\nrate-distortion performance of INR video compression.\n","authors":["Gai Zhang","Xinfeng Zhang","Lv Tang","Yue Li","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01620v2","updated":"2024-10-03T02:29:12Z","published":"2024-10-02T14:57:58Z","title":"LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large\n  Vision-Language Models","summary":"  Ophthalmology relies heavily on detailed image analysis for diagnosis and\ntreatment planning. While large vision-language models (LVLMs) have shown\npromise in understanding complex visual information, their performance on\nophthalmology images remains underexplored. We introduce LMOD, a dataset and\nbenchmark for evaluating LVLMs on ophthalmology images, covering anatomical\nunderstanding, diagnostic analysis, and demographic extraction. LMODincludes\n21,993 images spanning optical coherence tomography, scanning laser\nophthalmoscopy, eye photos, surgical scenes, and color fundus photographs. We\nbenchmark 13 state-of-the-art LVLMs and find that they are far from perfect for\ncomprehending ophthalmology images. Models struggle with diagnostic analysis\nand demographic extraction, reveal weaknesses in spatial reasoning, diagnostic\nanalysis, handling out-of-domain queries, and safeguards for handling\nbiomarkers of ophthalmology images.\n","authors":["Zhenyue Qin","Yu Yin","Dylan Campbell","Xuansheng Wu","Ke Zou","Yih-Chung Tham","Ninghao Liu","Xiuzhen Zhang","Qingyu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01574v2","updated":"2024-10-03T10:11:53Z","published":"2024-10-02T14:11:29Z","title":"Fake It Until You Break It: On the Adversarial Robustness of\n  AI-generated Image Detectors","summary":"  While generative AI (GenAI) offers countless possibilities for creative and\nproductive tasks, artificially generated media can be misused for fraud,\nmanipulation, scams, misinformation campaigns, and more. To mitigate the risks\nassociated with maliciously generated media, forensic classifiers are employed\nto identify AI-generated content. However, current forensic classifiers are\noften not evaluated in practically relevant scenarios, such as the presence of\nan attacker or when real-world artifacts like social media degradations affect\nimages. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)\ndetectors under different attack scenarios. We demonstrate that forensic\nclassifiers can be effectively attacked in realistic settings, even when the\nattacker does not have access to the target model and post-processing occurs\nafter the adversarial examples are created, which is standard on social media\nplatforms. These attacks can significantly reduce detection accuracy to the\nextent that the risks of relying on detectors outweigh their benefits. Finally,\nwe propose a simple defense mechanism to make CLIP-based detectors, which are\ncurrently the best-performing detectors, robust against these attacks.\n","authors":["Sina Mavali","Jonas Ricker","David Pape","Yash Sharma","Asja Fischer","Lea Schnherr"],"pdf_url":"https://arxiv.org/pdf/2410.01574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01536v2","updated":"2024-10-03T10:57:59Z","published":"2024-10-02T13:26:53Z","title":"EUFCC-CIR: a Composed Image Retrieval Dataset for GLAM Collections","summary":"  The intersection of Artificial Intelligence and Digital Humanities enables\nresearchers to explore cultural heritage collections with greater depth and\nscale. In this paper, we present EUFCC-CIR, a dataset designed for Composed\nImage Retrieval (CIR) within Galleries, Libraries, Archives, and Museums (GLAM)\ncollections. Our dataset is built on top of the EUFCC-340K image labeling\ndataset and contains over 180K annotated CIR triplets. Each triplet is composed\nof a multi-modal query (an input image plus a short text describing the desired\nattribute manipulations) and a set of relevant target images. The EUFCC-CIR\ndataset fills an existing gap in CIR-specific resources for Digital Humanities.\nWe demonstrate the value of the EUFCC-CIR dataset by highlighting its unique\nqualities in comparison to other existing CIR datasets and evaluating the\nperformance of several zero-shot CIR baselines.\n","authors":["Francesc Net","Lluis Gomez"],"pdf_url":"https://arxiv.org/pdf/2410.01536v2.pdf","comment":"ECCV Workshop (AI4DH2024)"},{"id":"http://arxiv.org/abs/2410.01506v2","updated":"2024-10-03T05:50:09Z","published":"2024-10-02T12:58:55Z","title":"LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature\n  Fusion","summary":"  In computer vision tasks, features often come from diverse representations,\ndomains, and modalities, such as text, images, and videos. Effectively fusing\nthese features is essential for robust performance, especially with the\navailability of powerful pre-trained models like vision-language models.\nHowever, common fusion methods, such as concatenation, element-wise operations,\nand non-linear techniques, often fail to capture structural relationships, deep\nfeature interactions, and suffer from inefficiency or misalignment of features\nacross domains. In this paper, we shift from high-dimensional feature space to\na lower-dimensional, interpretable graph space by constructing similarity\ngraphs that encode feature relationships at different levels, e.g., clip,\nframe, patch, token, etc. To capture deeper interactions, we use graph power\nexpansions and introduce a learnable graph fusion operator to combine these\ngraph powers for more effective fusion. Our approach is relationship-centric,\noperates in a homogeneous space, and is mathematically principled, resembling\nelement-wise similarity score aggregation via multilinear polynomials. We\ndemonstrate the effectiveness of our graph-based fusion method on video anomaly\ndetection, showing strong performance across multi-representational,\nmulti-modal, and multi-domain feature fusion tasks.\n","authors":["Dexuan Ding","Lei Wang","Liyun Zhu","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2410.01506v2.pdf","comment":"Research paper"},{"id":"http://arxiv.org/abs/2410.01443v2","updated":"2024-10-03T14:14:29Z","published":"2024-10-02T11:53:28Z","title":"SurgPointTransformer: Vertebrae Shape Completion with RGB-D Data","summary":"  State-of-the-art computer- and robot-assisted surgery systems heavily depend\non intraoperative imaging technologies such as CT and fluoroscopy to generate\ndetailed 3D visualization of the patient's anatomy. While imaging techniques\nare highly accurate, they are based on ionizing radiation and expose patients\nand clinicians. This study introduces an alternative, radiation-free approach\nfor reconstructing the 3D spine anatomy using RGB-D data. Drawing inspiration\nfrom the 3D \"mental map\" that surgeons form during surgeries, we introduce\nSurgPointTransformer, a shape completion approach for surgical applications\nthat can accurately reconstruct the unexposed spine regions from sparse\nobservations of the exposed surface.\n  Our method involves two main steps: segmentation and shape completion. The\nsegmentation step includes spinal column localization and segmentation,\nfollowed by vertebra-wise segmentation. The segmented vertebra point clouds are\nthen subjected to SurgPointTransformer, which leverages an attention mechanism\nto learn patterns between visible surface features and the underlying anatomy.\nFor evaluation, we utilize an ex-vivo dataset of nine specimens. Their CT data\nis used to establish ground truth data that were used to compare to the outputs\nof our methods. Our method significantly outperforms the state-of-the-art\nbaselines, achieving an average Chamfer Distance of 5.39, an F-Score of 0.85,\nan Earth Mover's Distance of 0.011, and a Signal-to-Noise Ratio of 22.90 dB.\n  This study demonstrates the potential of our reconstruction method for 3D\nvertebral shape completion. It enables 3D reconstruction of the entire lumbar\nspine and surgical guidance without ionizing radiation or invasive imaging. Our\nwork contributes to computer-aided and robot-assisted surgery, advancing the\nperception and intelligence of these systems.\n","authors":["Aidana Massalimova","Florentin Liebmann","Sascha Jecklin","Fabio Carrillo","Farshad Mazda","Philipp Frnstahl"],"pdf_url":"https://arxiv.org/pdf/2410.01443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02764v1","updated":"2024-10-03T17:59:59Z","published":"2024-10-03T17:59:59Z","title":"Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats","summary":"  We introduce a simple yet effective approach for separating transmitted and\nreflected light. Our key insight is that the powerful novel view synthesis\ncapabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian\nsplatting) allow one to perform flash/no-flash reflection separation using\nunpaired measurements -- this relaxation dramatically simplifies image\nacquisition over conventional paired flash/no-flash reflection separation\nmethods. Through extensive real-world experiments, we demonstrate our method,\nFlash-Splat, accurately reconstructs both transmitted and reflected scenes in\n3D. Our method outperforms existing 3D reflection separation methods, which do\nnot leverage illumination control, by a large margin. Our project webpage is at\nhttps://flash-splat.github.io/.\n","authors":["Mingyang Xie","Haoming Cai","Sachin Shah","Yiran Xu","Brandon Y. Feng","Jia-Bin Huang","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2410.02764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02763v1","updated":"2024-10-03T17:59:58Z","published":"2024-10-03T17:59:58Z","title":"Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos","summary":"  There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02763v1.pdf","comment":"Project Page: https://vinoground.github.io"},{"id":"http://arxiv.org/abs/2410.02762v1","updated":"2024-10-03T17:59:57Z","published":"2024-10-03T17:59:57Z","title":"Interpreting and Editing Vision-Language Representations to Mitigate\n  Hallucinations","summary":"  We investigate the internal representations of vision-language models (VLMs)\nto address hallucinations, a persistent challenge despite advances in model\nsize and training. We project VLMs' internal image representations to their\nlanguage vocabulary and observe more confident output probabilities on real\nobjects than hallucinated objects. We additionally use these output\nprobabilities to spatially localize real objects. Building on this approach, we\nintroduce a knowledge erasure algorithm that removes hallucinations by linearly\northogonalizing image features with respect to hallucinated object features. We\nshow that targeted edits to a model's latent representations can reduce\nhallucinations by up to 25.7% on the COCO2014 dataset while preserving\nperformance. Our findings demonstrate how a deeper understanding of VLMs'\nlatent representations can enhance reliability and enable novel capabilities,\nsuch as zero-shot segmentation.\n","authors":["Nick Jiang","Anish Kachinthaya","Suzie Petryk","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2410.02762v1.pdf","comment":"Project page and code: http://anishk23733.github.io/vl-interp/"},{"id":"http://arxiv.org/abs/2410.02761v1","updated":"2024-10-03T17:59:34Z","published":"2024-10-03T17:59:34Z","title":"FakeShield: Explainable Image Forgery Detection and Localization via\n  Multi-modal Large Language Models","summary":"  The rapid development of generative AI is a double-edged sword, which not\nonly facilitates content creation but also makes image manipulation easier and\nmore difficult to detect. Although current image forgery detection and\nlocalization (IFDL) methods are generally effective, they tend to face two\nchallenges: \\textbf{1)} black-box nature with unknown detection principle,\n\\textbf{2)} limited generalization across diverse tampering methods (e.g.,\nPhotoshop, DeepFake, AIGC-Editing). To address these issues, we propose the\nexplainable IFDL task and design FakeShield, a multi-modal framework capable of\nevaluating image authenticity, generating tampered region masks, and providing\na judgment basis based on pixel-level and image-level tampering clues.\nAdditionally, we leverage GPT-4o to enhance existing IFDL datasets, creating\nthe Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's\ntampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided\nExplainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery\nLocalization Module (MFLM) to address various types of tamper detection\ninterpretation and achieve forgery localization guided by detailed textual\ndescriptions. Extensive experiments demonstrate that FakeShield effectively\ndetects and localizes various tampering techniques, offering an explainable and\nsuperior solution compared to previous IFDL methods.\n","authors":["Zhipei Xu","Xuanyu Zhang","Runyi Li","Zecheng Tang","Qing Huang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17916v2","updated":"2024-10-03T17:59:25Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as model input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of\nour method in cooperative perception, tracking, and motion prediction. In\nparticular, CMP reduces the average prediction error by 16.4\\% with fewer\nmissing detections compared with the no cooperation setting and by 12.3\\%\ncompared with the strongest baseline. Our work marks a significant step forward\nin the cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios. The code can be found on the project website:\nhttps://cmp-cooperative-prediction.github.io/.\n","authors":["Zehao Wang","Yuping Wang","Zhuoyuan Wu","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v2.pdf","comment":"Project website: https://cmp-cooperative-prediction.github.io/"},{"id":"http://arxiv.org/abs/2410.02757v1","updated":"2024-10-03T17:59:02Z","published":"2024-10-03T17:59:02Z","title":"Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models","summary":"  It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://epiphqny.github.io/Loong-video.\n","authors":["Yuqing Wang","Tianwei Xiong","Daquan Zhou","Zhijie Lin","Yang Zhao","Bingyi Kang","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02757v1.pdf","comment":"Project page: https://epiphqny.github.io/Loong-video/"},{"id":"http://arxiv.org/abs/2307.08695v3","updated":"2024-10-03T17:58:03Z","published":"2023-07-17T17:57:01Z","title":"NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth\n  Estimation","summary":"  Video depth estimation aims to infer temporally consistent depth. One\napproach is to finetune a single-image model on each video with geometry\nconstraints, which proves inefficient and lacks robustness. An alternative is\nlearning to enforce consistency from data, which requires well-designed models\nand sufficient video depth data. To address both challenges, we introduce NVDS+\nthat stabilizes inconsistent depth estimated by various single-image models in\na plug-and-play manner. We also elaborate a large-scale Video Depth in the Wild\n(VDW) dataset, which contains 14,203 videos with over two million frames,\nmaking it the largest natural-scene video depth dataset. Additionally, a\nbidirectional inference strategy is designed to improve consistency by\nadaptively fusing forward and backward predictions. We instantiate a model\nfamily ranging from small to large scales for different applications. The\nmethod is evaluated on VDW dataset and three public benchmarks. To further\nprove the versatility, we extend NVDS+ to video semantic segmentation and\nseveral downstream applications like bokeh rendering, novel view synthesis, and\n3D reconstruction. Experimental results show that our method achieves\nsignificant improvements in consistency, accuracy, and efficiency. Our work\nserves as a solid baseline and data foundation for learning-based video depth\nestimation. Code and dataset are available at:\nhttps://github.com/RaymondWang987/NVDS\n","authors":["Yiran Wang","Min Shi","Jiaqi Li","Chaoyi Hong","Zihao Huang","Juewen Peng","Zhiguo Cao","Jianming Zhang","Ke Xian","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2307.08695v3.pdf","comment":"V1/V2: ICCV 2023 accepted; V3: the journal extension accepted by IEEE\n  TPAMI 2024"},{"id":"http://arxiv.org/abs/2404.05717v3","updated":"2024-10-03T17:56:42Z","published":"2024-04-08T17:52:29Z","title":"SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual\n  Editing","summary":"  Effective editing of personal content holds a pivotal role in enabling\nindividuals to express their creativity, weaving captivating narratives within\ntheir visual stories, and elevate the overall quality and impact of their\nvisual content. Therefore, in this work, we introduce SwapAnything, a novel\nframework that can swap any objects in an image with personalized concepts\ngiven by the reference, while keeping the context unchanged. Compared with\nexisting methods for personalized subject swapping, SwapAnything has three\nunique advantages: (1) precise control of arbitrary objects and parts rather\nthan the main subject, (2) more faithful preservation of context pixels, (3)\nbetter adaptation of the personalized concept to the image. First, we propose\ntargeted variable swapping to apply region control over latent feature maps and\nswap masked variables for faithful context preservation and initial semantic\nconcept swapping. Then, we introduce appearance adaptation, to seamlessly adapt\nthe semantic concept into the original image in terms of target location,\nshape, style, and content during the image generation process. Extensive\nresults on both human and automatic evaluation demonstrate significant\nimprovements of our approach over baseline methods on personalized swapping.\nFurthermore, SwapAnything shows its precise and faithful swapping abilities\nacross single object, multiple objects, partial object, and cross-domain\nswapping tasks. SwapAnything also achieves great performance on text-based\nswapping and tasks beyond swapping such as object insertion.\n","authors":["Jing Gu","Nanxuan Zhao","Wei Xiong","Qing Liu","Zhifei Zhang","He Zhang","Jianming Zhang","HyunJoon Jung","Yilin Wang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2404.05717v3.pdf","comment":"ECCV 2024, 23 pages, 14 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.02746v1","updated":"2024-10-03T17:56:09Z","published":"2024-10-03T17:56:09Z","title":"Contrastive Localized Language-Image Pre-Training","summary":"  Contrastive Language-Image Pre-training (CLIP) has been a celebrated method\nfor training vision encoders to generate image/text representations\nfacilitating various applications. Recently, CLIP has been widely adopted as\nthe vision backbone of multimodal large language models (MLLMs) to connect\nimage inputs for language interactions. The success of CLIP as a\nvision-language foundation model relies on aligning web-crawled noisy text\nannotations at image levels. Nevertheless, such criteria may become\ninsufficient for downstream tasks in need of fine-grained vision\nrepresentations, especially when region-level understanding is demanding for\nMLLMs. In this paper, we improve the localization capability of CLIP with\nseveral advances. We propose a pre-training method called Contrastive Localized\nLanguage-Image Pre-training (CLOC) by complementing CLIP with region-text\ncontrastive loss and modules. We formulate a new concept, promptable\nembeddings, of which the encoder produces image embeddings easy to transform\ninto region representations given spatial hints. To support large-scale\npre-training, we design a visually-enriched and spatially-localized captioning\nframework to effectively generate region-text pseudo-labels at scale. By\nscaling up to billions of annotated images, CLOC enables high-quality regional\nembeddings for image region recognition and retrieval tasks, and can be a\ndrop-in replacement of CLIP to enhance MLLMs, especially on referring and\ngrounding tasks.\n","authors":["Hong-You Chen","Zhengfeng Lai","Haotian Zhang","Xinze Wang","Marcin Eichner","Keen You","Meng Cao","Bowen Zhang","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2410.02746v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02740v1","updated":"2024-10-03T17:54:52Z","published":"2024-10-03T17:54:52Z","title":"Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models","summary":"  Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.\n","authors":["Zhengfeng Lai","Vasileios Saveris","Chen Chen","Hong-You Chen","Haotian Zhang","Bowen Zhang","Juan Lao Tebar","Wenze Hu","Zhe Gan","Peter Grasch","Meng Cao","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02740v1.pdf","comment":"CV/ML"},{"id":"http://arxiv.org/abs/2303.17051v3","updated":"2024-10-03T17:53:04Z","published":"2023-03-29T22:50:05Z","title":"Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning\n  for Volumetric Organ Segmentation","summary":"  The recent popularity of foundation models and the pre-train-and-adapt\nparadigm, where a large-scale model is transferred to downstream tasks, is\ngaining attention for volumetric medical image segmentation. However, current\ntransfer learning strategies devoted to full fine-tuning for transfer learning\nmay require significant resources and yield sub-optimal results when the\nlabeled data of the target task is scarce. This makes its applicability in real\nclinical settings challenging since these institutions are usually constrained\non data and computational resources to develop proprietary solutions. To\naddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a\nnovel and realistic scenario for adapting medical image segmentation foundation\nmodels. This setting considers the key role of both data- and parameter-\nefficiency during adaptation. Building on a foundation model pre-trained on\nopen-access CT organ segmentation sources, we propose leveraging\nParameter-Efficient Fine-Tuning and black-box Adapters to address such\nchallenges. Furthermore, novel efficient adaptation methodologies are\nintroduced in this work, which include Spatial black-box Adapters that are more\nappropriate for dense prediction tasks and constrained transductive inference,\nleveraging task-specific prior knowledge. Our comprehensive transfer learning\nexperiments confirm the suitability of foundation models in medical image\nsegmentation and unveil the limitations of popular fine-tuning strategies in\nfew-shot scenarios.\n","authors":["Julio Silva-Rodrguez","Jose Dolz","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2303.17051v3.pdf","comment":"Journal Extension of MICCAI - MedAGI Workshop 2023. Code in\n  https://github.com/jusiro/fewshot-finetuning"},{"id":"http://arxiv.org/abs/2410.02730v1","updated":"2024-10-03T17:49:28Z","published":"2024-10-03T17:49:28Z","title":"DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes\n  and Objects","summary":"  Object navigation in unknown environments is crucial for deploying embodied\nagents in real-world applications. While we have witnessed huge progress due to\nlarge-scale scene datasets, faster simulators, and stronger models, previous\nstudies mainly focus on limited scene types and target objects. In this paper,\nwe study a new task of navigating to diverse target objects in a large number\nof scene types. To benchmark the problem, we present a large-scale scene\ndataset, DivScene, which contains 4,614 scenes across 81 different types. With\nthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a\nLarge Vision Language Model (LVLM) through imitation learning. The LVLM is\ntrained to take previous observations from the environment and generate the\nnext actions. We also introduce CoT explanation traces of the action prediction\nfor better performance when tuning LVLMs. Our extensive experiments find that\nwe can build a performant LVLM-based agent through imitation learning on the\nshortest paths constructed by a BFS planner without any human supervision. Our\nagent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we\ncarry out various analyses showing the generalization ability of our agent.\n","authors":["Zhaowei Wang","Hongming Zhang","Tianqing Fang","Ye Tian","Yue Yang","Kaixin Ma","Xiaoman Pan","Yangqiu Song","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02730v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2404.10710v3","updated":"2024-10-03T17:46:40Z","published":"2024-04-16T16:36:50Z","title":"Autoregressive Pre-Training on Pixels and Texts","summary":"  The integration of visual and textual information represents a promising\ndirection in the advancement of language models. In this paper, we explore the\ndual modality of language--both visual and textual--within an autoregressive\nframework, pre-trained on both document images and texts. Our method employs a\nmultimodal training strategy, utilizing visual data through next patch\nprediction with a regression head and/or textual data through next token\nprediction with a classification head. We focus on understanding the\ninteraction between these two modalities and their combined impact on model\nperformance. Our extensive evaluation across a wide range of benchmarks shows\nthat incorporating both visual and textual data significantly improves the\nperformance of pixel-based language models. Remarkably, we find that a\nunidirectional pixel-based model trained solely on visual data can achieve\ncomparable results to state-of-the-art bidirectional models on several language\nunderstanding tasks. This work uncovers the untapped potential of integrating\nvisual and textual modalities for more effective language modeling. We release\nour code, data, and model checkpoints at\n\\url{https://github.com/ernie-research/pixelgpt}.\n","authors":["Yekun Chai","Qingyi Liu","Jingwu Xiao","Shuohuan Wang","Yu Sun","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.10710v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02720v1","updated":"2024-10-03T17:39:55Z","published":"2024-10-03T17:39:55Z","title":"Curvature Diversity-Driven Deformation and Domain Alignment for Point\n  Cloud","summary":"  Unsupervised Domain Adaptation (UDA) is crucial for reducing the need for\nextensive manual data annotation when training deep networks on point cloud\ndata. A significant challenge of UDA lies in effectively bridging the domain\ngap. To tackle this challenge, we propose \\textbf{C}urvature\n\\textbf{D}iversity-Driven \\textbf{N}uclear-Norm Wasserstein \\textbf{D}omain\nAlignment (CDND). Our approach first introduces a \\textit{\\textbf{Curv}ature\nDiversity-driven Deformation \\textbf{Rec}onstruction (CurvRec)} task, which\neffectively mitigates the gap between the source and target domains by enabling\nthe model to extract salient features from semantically rich regions of a given\npoint cloud. We then propose \\textit{\\textbf{D}eformation-based\n\\textbf{N}uclear-norm \\textbf{W}asserstein \\textbf{D}iscrepancy (D-NWD)}, which\napplies the Nuclear-norm Wasserstein Discrepancy to both \\textit{deformed and\noriginal} data samples to align the source and target domains. Furthermore, we\ncontribute a theoretical justification for the effectiveness of D-NWD in\ndistribution alignment and demonstrate that it is \\textit{generic} enough to be\napplied to \\textbf{any} deformations. To validate our method, we conduct\nextensive experiments on two public domain adaptation datasets for point cloud\nclassification and segmentation tasks. Empirical experiment results show that\nour CDND achieves state-of-the-art performance by a noticeable margin over\nexisting approaches.\n","authors":["Mengxi Wu","Hao Huang","Yi Fang","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2410.02720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02714v1","updated":"2024-10-03T17:37:18Z","published":"2024-10-03T17:37:18Z","title":"AlzhiNet: Traversing from 2DCNN to 3DCNN, Towards Early Detection and\n  Diagnosis of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) is a progressive neurodegenerative disorder with\nincreasing prevalence among the aging population, necessitating early and\naccurate diagnosis for effective disease management. In this study, we present\na novel hybrid deep learning framework that integrates both 2D Convolutional\nNeural Networks (2D-CNN) and 3D Convolutional Neural Networks (3D-CNN), along\nwith a custom loss function and volumetric data augmentation, to enhance\nfeature extraction and improve classification performance in AD diagnosis.\nAccording to extensive experiments, AlzhiNet outperforms standalone 2D and 3D\nmodels, highlighting the importance of combining these complementary\nrepresentations of data. The depth and quality of 3D volumes derived from the\naugmented 2D slices also significantly influence the model's performance. The\nresults indicate that carefully selecting weighting factors in hybrid\npredictions is imperative for achieving optimal results. Our framework has been\nvalidated on the Magnetic Resonance Imaging (MRI) from Kaggle and MIRIAD\ndatasets, obtaining accuracies of 98.9% and 99.99%, respectively, with an AUC\nof 100%. Furthermore, AlzhiNet was studied under a variety of perturbation\nscenarios on the Alzheimer's Kaggle dataset, including Gaussian noise,\nbrightness, contrast, salt and pepper noise, color jitter, and occlusion. The\nresults obtained show that AlzhiNet is more robust to perturbations than\nResNet-18, making it an excellent choice for real-world applications. This\napproach represents a promising advancement in the early diagnosis and\ntreatment planning for Alzheimer's disease.\n","authors":["Romoke Grace Akindele","Samuel Adebayo","Paul Shekonya Kanda","Ming Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02713v1","updated":"2024-10-03T17:36:49Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v1.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02712v1","updated":"2024-10-03T17:36:33Z","published":"2024-10-03T17:36:33Z","title":"LLaVA-Critic: Learning to Evaluate Multimodal Models","summary":"  We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)\ndesigned as a generalist evaluator to assess performance across a wide range of\nmultimodal tasks. LLaVA-Critic is trained using a high-quality critic\ninstruction-following dataset that incorporates diverse evaluation criteria and\nscenarios. Our experiments demonstrate the model's effectiveness in two key\nareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation\nscores, performing on par with or surpassing GPT models on multiple evaluation\nbenchmarks; and (2) Preference Learning, where it generates reward signals for\npreference learning, enhancing model alignment capabilities. This work\nunderscores the potential of open-source LMMs in self-critique and evaluation,\nsetting the stage for future research into scalable, superhuman alignment\nfeedback mechanisms for LMMs.\n","authors":["Tianyi Xiong","Xiyao Wang","Dong Guo","Qinghao Ye","Haoqi Fan","Quanquan Gu","Heng Huang","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02712v1.pdf","comment":"Project Page: https://llava-vl.github.io/blog/2024-10-03-llava-critic"},{"id":"http://arxiv.org/abs/2410.02710v1","updated":"2024-10-03T17:34:55Z","published":"2024-10-03T17:34:55Z","title":"SteerDiff: Steering towards Safe Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have drawn attention for their ability\nto generate high-quality images with precise text alignment. However, these\nmodels can also be misused to produce inappropriate content. Existing safety\nmeasures, which typically rely on text classifiers or ControlNet-like\napproaches, are often insufficient. Traditional text classifiers rely on\nlarge-scale labeled datasets and can be easily bypassed by rephrasing. As\ndiffusion models continue to scale, fine-tuning these safeguards becomes\nincreasingly challenging and lacks flexibility. Recent red-teaming attack\nresearches further underscore the need for a new paradigm to prevent the\ngeneration of inappropriate content. In this paper, we introduce SteerDiff, a\nlightweight adaptor module designed to act as an intermediary between user\ninput and the diffusion model, ensuring that generated images adhere to ethical\nand safety standards with little to no impact on usability. SteerDiff\nidentifies and manipulates inappropriate concepts within the text embedding\nspace to guide the model away from harmful outputs. We conduct extensive\nexperiments across various concept unlearning tasks to evaluate the\neffectiveness of our approach. Furthermore, we benchmark SteerDiff against\nmultiple red-teaming strategies to assess its robustness. Finally, we explore\nthe potential of SteerDiff for concept forgetting tasks, demonstrating its\nversatility in text-conditioned image generation.\n","authors":["Hongxiang Zhang","Yifeng He","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02705v1","updated":"2024-10-03T17:28:07Z","published":"2024-10-03T17:28:07Z","title":"ControlAR: Controllable Image Generation with Autoregressive Models","summary":"  Autoregressive (AR) models have reformulated image generation as next-token\nprediction, demonstrating remarkable potential and emerging as strong\ncompetitors to diffusion models. However, control-to-image generation, akin to\nControlNet, remains largely unexplored within AR models. Although a natural\napproach, inspired by advancements in Large Language Models, is to tokenize\ncontrol images into tokens and prefill them into the autoregressive model\nbefore decoding image tokens, it still falls short in generation quality\ncompared to ControlNet and suffers from inefficiency. To this end, we introduce\nControlAR, an efficient and effective framework for integrating spatial\ncontrols into autoregressive image generation models. Firstly, we explore\ncontrol encoding for AR models and propose a lightweight control encoder to\ntransform spatial inputs (e.g., canny edges or depth maps) into control tokens.\nThen ControlAR exploits the conditional decoding method to generate the next\nimage token conditioned on the per-token fusion between control and image\ntokens, similar to positional encodings. Compared to prefilling tokens, using\nconditional decoding significantly strengthens the control capability of AR\nmodels but also maintains the model's efficiency. Furthermore, the proposed\nControlAR surprisingly empowers AR models with arbitrary-resolution image\ngeneration via conditional decoding and specific controls. Extensive\nexperiments can demonstrate the controllability of the proposed ControlAR for\nthe autoregressive control-to-image generation across diverse inputs, including\nedges, depths, and segmentation masks. Furthermore, both quantitative and\nqualitative results indicate that ControlAR surpasses previous state-of-the-art\ncontrollable diffusion models, e.g., ControlNet++. Code, models, and demo will\nsoon be available at https://github.com/hustvl/ControlAR.\n","authors":["Zongming Li","Tianheng Cheng","Shoufa Chen","Peize Sun","Haocheng Shen","Longjin Ran","Xiaoxin Chen","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.02705v1.pdf","comment":"Preprint. Work in progress"},{"id":"http://arxiv.org/abs/2406.03520v2","updated":"2024-10-03T17:24:40Z","published":"2024-06-05T17:53:55Z","title":"VideoPhy: Evaluating Physical Commonsense for Video Generation","summary":"  Recent advances in internet-scale video data pretraining have led to the\ndevelopment of text-to-video generative models that can create high-quality\nvideos across a broad range of visual concepts, synthesize realistic motions\nand render complex objects. Hence, these generative models have the potential\nto become general-purpose simulators of the physical world. However, it is\nunclear how far we are from this goal with the existing text-to-video\ngenerative models. To this end, we present VideoPhy, a benchmark designed to\nassess whether the generated videos follow physical commonsense for real-world\nactivities (e.g. marbles will roll down when placed on a slanted surface).\nSpecifically, we curate diverse prompts that involve interactions between\nvarious material types in the physical world (e.g., solid-solid, solid-fluid,\nfluid-fluid). We then generate videos conditioned on these captions from\ndiverse state-of-the-art text-to-video generative models, including open models\n(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human\nevaluation reveals that the existing models severely lack the ability to\ngenerate videos adhering to the given text prompts, while also lack physical\ncommonsense. Specifically, the best performing model, CogVideoX-5B, generates\nvideos that adhere to the caption and physical laws for 39.6% of the instances.\nVideoPhy thus highlights that the video generative models are far from\naccurately simulating the physical world. Finally, we propose an\nauto-evaluator, VideoCon-Physics, to assess the performance reliably for the\nnewly released models.\n","authors":["Hritik Bansal","Zongyu Lin","Tianyi Xie","Zeshun Zong","Michal Yarom","Yonatan Bitton","Chenfanfu Jiang","Yizhou Sun","Kai-Wei Chang","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.03520v2.pdf","comment":"43 pages, 29 figures, 12 tables. Added CogVideo and Dream Machine in\n  v2"},{"id":"http://arxiv.org/abs/2410.02698v1","updated":"2024-10-03T17:21:30Z","published":"2024-10-03T17:21:30Z","title":"Lie Algebra Canonicalization: Equivariant Neural Operators under\n  arbitrary Lie Groups","summary":"  The quest for robust and generalizable machine learning models has driven\nrecent interest in exploiting symmetries through equivariant neural networks.\nIn the context of PDE solvers, recent works have shown that Lie point\nsymmetries can be a useful inductive bias for Physics-Informed Neural Networks\n(PINNs) through data and loss augmentation. Despite this, directly enforcing\nequivariance within the model architecture for these problems remains elusive.\nThis is because many PDEs admit non-compact symmetry groups, oftentimes not\nstudied beyond their infinitesimal generators, making them incompatible with\nmost existing equivariant architectures. In this work, we propose Lie aLgebrA\nCanonicalization (LieLAC), a novel approach that exploits only the action of\ninfinitesimal generators of the symmetry group, circumventing the need for\nknowledge of the full group structure. To achieve this, we address existing\ntheoretical issues in the canonicalization literature, establishing connections\nwith frame averaging in the case of continuous non-compact groups. Operating\nwithin the framework of canonicalization, LieLAC can easily be integrated with\nunconstrained pre-trained models, transforming inputs to a canonical form\nbefore feeding them into the existing model, effectively aligning the input for\nmodel inference according to allowed symmetries. LieLAC utilizes standard Lie\ngroup descent schemes, achieving equivariance in pre-trained models. Finally,\nwe showcase LieLAC's efficacy on tasks of invariant image classification and\nLie point symmetry equivariant neural PDE solvers using pre-trained models.\n","authors":["Zakhar Shumaylov","Peter Zaika","James Rowbottom","Ferdia Sherry","Melanie Weber","Carola-Bibiane Schnlieb"],"pdf_url":"https://arxiv.org/pdf/2410.02698v1.pdf","comment":"40 pages; preprint"},{"id":"http://arxiv.org/abs/2310.10224v4","updated":"2024-10-03T17:13:41Z","published":"2023-10-16T09:34:06Z","title":"Generalizing Medical Image Representations via Quaternion Wavelet\n  Networks","summary":"  Neural network generalizability is becoming a broad research field due to the\nincreasing availability of datasets from different sources and for various\ntasks. This issue is even wider when processing medical data, where a lack of\nmethodological standards causes large variations being provided by different\nimaging centers or acquired with various devices and cofactors. To overcome\nthese limitations, we introduce a novel, generalizable, data- and task-agnostic\nframework able to extract salient features from medical images. The proposed\nquaternion wavelet network (QUAVE) can be easily integrated with any\npre-existing medical image analysis or synthesis task, and it can be involved\nwith real, quaternion, or hypercomplex-valued models, generalizing their\nadoption to single-channel data. QUAVE first extracts different sub-bands\nthrough the quaternion wavelet transform, resulting in both\nlow-frequency/approximation bands and high-frequency/fine-grained features.\nThen, it weighs the most representative set of sub-bands to be involved as\ninput to any other neural model for image processing, replacing standard data\nsamples. We conduct an extensive experimental evaluation comprising different\ndatasets, diverse image analysis, and synthesis tasks including reconstruction,\nsegmentation, and modality translation. We also evaluate QUAVE in combination\nwith both real and quaternion-valued models. Results demonstrate the\neffectiveness and the generalizability of the proposed framework that improves\nnetwork performance while being flexible to be adopted in manifold scenarios\nand robust to domain shifts. The full code is available at:\nhttps://github.com/ispamm/QWT.\n","authors":["Luigi Sigillo","Eleonora Grassucci","Aurelio Uncini","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.10224v4.pdf","comment":"This paper is currently under review"},{"id":"http://arxiv.org/abs/2403.10390v2","updated":"2024-10-03T17:10:22Z","published":"2024-03-15T15:21:04Z","title":"Evaluating Perceptual Distance Models by Fitting Binomial Distributions\n  to Two-Alternative Forced Choice Data","summary":"  The two-alternative forced choice (2AFC) experimental method is popular in\nthe visual perception literature, where practitioners aim to understand how\nhuman observers perceive distances within triplets made of a reference image\nand two distorted versions. In the past, this had been conducted in controlled\nenvironments, with triplets sharing images, so it was possible to rank the\nperceived quality. This ranking would then be used to evaluate perceptual\ndistance models against the experimental data. Recently, crowd-sourced\nperceptual datasets have emerged, with no images shared between triplets,\nmaking ranking infeasible. Evaluating perceptual distance models using this\ndata reduces the judgements on a triplet to a binary decision, namely, whether\nthe distance model agrees with the human decision - which is suboptimal and\nprone to misleading conclusions. Instead, we statistically model the underlying\ndecision-making process during 2AFC experiments using a binomial distribution.\nHaving enough empirical data, we estimate a smooth and consistent distribution\nof the judgements on the reference-distorted distance plane, according to each\ndistance model. By applying maximum likelihood, we estimate the parameter of\nthe local binomial distribution, and a global measurement of the expected\nlog-likelihood of the measured responses. We calculate meaningful and\nwell-founded metrics for the distance model, beyond the mere prediction\naccuracy as percentage agreement, even with variable numbers of judgements per\ntriplet -- key advantages over both classical and neural network methods.\n","authors":["Alexander Hepburn","Raul Santos-Rodriguez","Javier Portilla"],"pdf_url":"https://arxiv.org/pdf/2403.10390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02671v1","updated":"2024-10-03T16:54:35Z","published":"2024-10-03T16:54:35Z","title":"Unsupervised Point Cloud Completion through Unbalanced Optimal Transport","summary":"  Unpaired point cloud completion explores methods for learning a completion\nmap from unpaired incomplete and complete point cloud data. In this paper, we\npropose a novel approach for unpaired point cloud completion using the\nunbalanced optimal transport map, called Unbalanced Optimal Transport Map for\nUnpaired Point Cloud Completion (UOT-UPC). We demonstrate that the unpaired\npoint cloud completion can be naturally interpreted as the Optimal Transport\n(OT) problem and introduce the Unbalanced Optimal Transport (UOT) approach to\naddress the class imbalance problem, which is prevalent in unpaired point cloud\ncompletion datasets. Moreover, we analyze the appropriate cost function for\nunpaired completion tasks. This analysis shows that the InfoCD cost function is\nparticularly well-suited for this task. Our model is the first attempt to\nleverage UOT for unpaired point cloud completion, achieving competitive or\nsuperior results on both single-category and multi-category datasets. In\nparticular, our model is especially effective in scenarios with class\nimbalance, where the proportions of categories are different between the\nincomplete and complete point cloud datasets.\n","authors":["Taekyung Lee","Jaemoo Choi","Jaewoong Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02671v1.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.02653v1","updated":"2024-10-03T16:36:35Z","published":"2024-10-03T16:36:35Z","title":"Measuring and Improving Persuasiveness of Generative Models","summary":"  LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.\n","authors":["Somesh Singh","Yaman K Singla","Harini SI","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2410.02653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02646v1","updated":"2024-10-03T16:31:28Z","published":"2024-10-03T16:31:28Z","title":"Learning 3D Perception from Others' Predictions","summary":"  Accurate 3D object detection in real-world environments requires a huge\namount of annotated data with high quality. Acquiring such data is tedious and\nexpensive, and often needs repeated effort when a new sensor is adopted or when\nthe detector is deployed in a new environment. We investigate a new scenario to\nconstruct 3D object detectors: learning from the predictions of a nearby unit\nthat is equipped with an accurate detector. For example, when a self-driving\ncar enters a new area, it may learn from other traffic participants whose\ndetectors have been optimized for that area. This setting is label-efficient,\nsensor-agnostic, and communication-efficient: nearby units only need to share\nthe predictions with the ego agent (e.g., car). Naively using the received\npredictions as ground-truths to train the detector for the ego car, however,\nleads to inferior performance. We systematically study the problem and identify\nviewpoint mismatches and mislocalization (due to synchronization and GPS\nerrors) as the main causes, which unavoidably result in false positives, false\nnegatives, and inaccurate pseudo labels. We propose a distance-based\ncurriculum, first learning from closer units with similar viewpoints and\nsubsequently improving the quality of other units' predictions via\nself-training. We further demonstrate that an effective pseudo label refinement\nmodule can be trained with a handful of annotated data, largely reducing the\ndata quantity necessary to train an object detector. We validate our approach\non the recently released real-world collaborative driving dataset, using\nreference cars' predictions as pseudo labels for the ego car. Extensive\nexperiments including several scenarios (e.g., different sensors, detectors,\nand domains) demonstrate the effectiveness of our approach toward\nlabel-efficient learning of 3D perception from other units' predictions.\n","authors":["Jinsu Yoo","Zhenyang Feng","Tai-Yu Pan","Yihong Sun","Cheng Perng Phoo","Xiangyu Chen","Mark Campbell","Kilian Q. Weinberger","Bharath Hariharan","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2410.02646v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.02643v1","updated":"2024-10-03T16:29:47Z","published":"2024-10-03T16:29:47Z","title":"Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based\n  Place Recognition","summary":"  Recent advances in robotics are pushing real-world autonomy, enabling robots\nto perform long-term and large-scale missions. A crucial component for\nsuccessful missions is the incorporation of loop closures through place\nrecognition, which effectively mitigates accumulated pose estimation drift.\nDespite computational advancements, optimizing performance for real-time\ndeployment remains challenging, especially in resource-constrained mobile\nrobots and multi-robot systems since, conventional keyframe sampling practices\nin place recognition often result in retaining redundant information or\noverlooking relevant data, as they rely on fixed sampling intervals or work\ndirectly in the 3D space instead of the feature space. To address these\nconcerns, we introduce the concept of sample space in place recognition and\ndemonstrate how different sampling techniques affect the query process and\noverall performance. We then present a novel keyframe sampling approach for\nLiDAR-based place recognition, which focuses on redundancy minimization and\ninformation preservation in the hyper-dimensional descriptor space. This\napproach is applicable to both learning-based and handcrafted descriptors, and\nthrough the experimental validation across multiple datasets and descriptor\nframeworks, we demonstrate the effectiveness of our proposed method, showing it\ncan jointly minimize redundancy and preserve essential information in\nreal-time. The proposed approach maintains robust performance across various\ndatasets without requiring parameter tuning, contributing to more efficient and\nreliable place recognition for a wide range of robotic applications.\n","authors":["Nikolaos Stathoulopoulos","Vidya Sumathy","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.02643v1.pdf","comment":"20 pages, 15 figures. Submitted"},{"id":"http://arxiv.org/abs/2405.13675v2","updated":"2024-10-03T16:26:46Z","published":"2024-05-22T14:16:30Z","title":"Context and Geometry Aware Voxel Transformer for Semantic Scene\n  Completion","summary":"  Vision-based Semantic Scene Completion (SSC) has gained much attention due to\nits widespread applications in various 3D perception tasks. Existing\nsparse-to-dense approaches typically employ shared context-independent queries\nacross various input images, which fails to capture distinctions among them as\nthe focal regions of different inputs vary and may result in undirected feature\naggregation of cross-attention. Additionally, the absence of depth information\nmay lead to points projected onto the image plane sharing the same 2D position\nor similar sampling points in the feature map, resulting in depth ambiguity. In\nthis paper, we present a novel context and geometry aware voxel transformer. It\nutilizes a context aware query generator to initialize context-dependent\nqueries tailored to individual input images, effectively capturing their unique\ncharacteristics and aggregating information within the region of interest.\nFurthermore, it extend deformable cross-attention from 2D to 3D pixel space,\nenabling the differentiation of points with similar image coordinates based on\ntheir depth coordinates. Building upon this module, we introduce a neural\nnetwork named CGFormer to achieve semantic scene completion. Simultaneously,\nCGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost\nthe semantic and geometric representation abilities of the transformed 3D\nvolume from both local and global perspectives. Experimental results\ndemonstrate that CGFormer achieves state-of-the-art performance on the\nSemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and\n20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer\neven outperforms approaches employing temporal images as inputs or much larger\nimage backbone networks.\n","authors":["Zhu Yu","Runmin Zhang","Jiacheng Ying","Junchen Yu","Xiaohai Hu","Lun Luo","Si-Yuan Cao","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2405.13675v2.pdf","comment":"NIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.02640v1","updated":"2024-10-03T16:24:20Z","published":"2024-10-03T16:24:20Z","title":"Diffusion-based Extreme Image Compression with Compressed Feature\n  Initialization","summary":"  Diffusion-based extreme image compression methods have achieved impressive\nperformance at extremely low bitrates. However, constrained by the iterative\ndenoising process that starts from pure noise, these methods are limited in\nboth fidelity and efficiency. To address these two issues, we present Relay\nResidual Diffusion Extreme Image Compression (RDEIC), which leverages\ncompressed feature initialization and residual diffusion. Specifically, we\nfirst use the compressed latent features of the image with added noise, instead\nof pure noise, as the starting point to eliminate the unnecessary initial\nstages of the denoising process. Second, we design a novel relay residual\ndiffusion that reconstructs the raw image by iteratively removing the added\nnoise and the residual between the compressed and target latent features.\nNotably, our relay residual diffusion network seamlessly integrates pre-trained\nstable diffusion to leverage its robust generative capability for high-quality\nreconstruction. Third, we propose a fixed-step fine-tuning strategy to\neliminate the discrepancy between the training and inference phases, further\nimproving the reconstruction quality. Extensive experiments demonstrate that\nthe proposed RDEIC achieves state-of-the-art visual quality and outperforms\nexisting diffusion-based extreme image compression methods in both fidelity and\nefficiency. The source code will be provided in\nhttps://github.com/huai-chang/RDEIC.\n","authors":["Zhiyuan Li","Yanhui Zhou","Hao Wei","Chenyang Ge","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2410.02640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02638v1","updated":"2024-10-03T16:23:33Z","published":"2024-10-03T16:23:33Z","title":"Spatial-Temporal Multi-Cuts for Online Multiple-Camera Vehicle Tracking","summary":"  Accurate online multiple-camera vehicle tracking is essential for intelligent\ntransportation systems, autonomous driving, and smart city applications. Like\nsingle-camera multiple-object tracking, it is commonly formulated as a graph\nproblem of tracking-by-detection. Within this framework, existing online\nmethods usually consist of two-stage procedures that cluster temporally first,\nthen spatially, or vice versa. This is computationally expensive and prone to\nerror accumulation. We introduce a graph representation that allows\nspatial-temporal clustering in a single, combined step: New detections are\nspatially and temporally connected with existing clusters. By keeping sparse\nappearance and positional cues of all detections in a cluster, our method can\ncompare clusters based on the strongest available evidence. The final tracks\nare obtained online using a simple multicut assignment procedure. Our method\ndoes not require any training on the target scene, pre-extraction of\nsingle-camera tracks, or additional annotations. Notably, we outperform the\nonline state-of-the-art on the CityFlow dataset in terms of IDF1 by more than\n14%, and on the Synthehicle dataset by more than 25%, respectively. The code is\npublicly available.\n","authors":["Fabian Herzog","Johannes Gilg","Philipp Wolters","Torben Teepe","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2410.02638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02637v1","updated":"2024-10-03T16:23:13Z","published":"2024-10-03T16:23:13Z","title":"Plots Unlock Time-Series Understanding in Multimodal Models","summary":"  While multimodal foundation models can now natively work with data beyond\ntext, they remain underutilized in analyzing the considerable amounts of\nmulti-dimensional time-series data in fields like healthcare, finance, and\nsocial sciences, representing a missed opportunity for richer, data-driven\ninsights. This paper proposes a simple but effective method that leverages the\nexisting vision encoders of these models to \"see\" time-series data via plots,\navoiding the need for additional, potentially costly, model training. Our\nempirical evaluations show that this approach outperforms providing the raw\ntime-series data as text, with the additional benefit that visual time-series\nrepresentations demonstrate up to a 90% reduction in model API costs. We\nvalidate our hypothesis through synthetic data tasks of increasing complexity,\nprogressing from simple functional form identification on clean data, to\nextracting trends from noisy scatter plots. To demonstrate generalizability\nfrom synthetic tasks with clear reasoning steps to more complex, real-world\nscenarios, we apply our approach to consumer health tasks - specifically fall\ndetection, activity recognition, and readiness assessment - which involve\nheterogeneous, noisy data and multi-step reasoning. The overall success in plot\nperformance over text performance (up to an 120% performance increase on\nzero-shot synthetic tasks, and up to 150% performance increase on real-world\ntasks), across both GPT and Gemini model families, highlights our approach's\npotential for making the best use of the native capabilities of foundation\nmodels.\n","authors":["Mayank Daswani","Mathias M. J. Bellaiche","Marc Wilson","Desislav Ivanov","Mikhail Papkov","Eva Schnider","Jing Tang","Kay Lamerigts","Gabriela Botea","Michael A. Sanchez","Yojan Patel","Shruthi Prabhakara","Shravya Shetty","Umesh Telang"],"pdf_url":"https://arxiv.org/pdf/2410.02637v1.pdf","comment":"49 pages"},{"id":"http://arxiv.org/abs/2410.02630v1","updated":"2024-10-03T16:14:22Z","published":"2024-10-03T16:14:22Z","title":"Metrics Revolutions: Groundbreaking Insights into the Implementation of\n  Metrics for Biomedical Image Segmentation","summary":"  The evaluation of segmentation performance is a common task in biomedical\nimage analysis, with its importance emphasized in the recently released metrics\nselection guidelines and computing frameworks. To quantitatively evaluate the\nalignment of two segmentations, researchers commonly resort to counting\nmetrics, such as the Dice similarity coefficient, or distance-based metrics,\nsuch as the Hausdorff distance, which are usually computed by publicly\navailable open-source tools with an inherent assumption that these tools\nprovide consistent results. In this study we questioned this assumption, and\nperformed a systematic implementation analysis along with quantitative\nexperiments on real-world clinical data to compare 11 open-source tools for\ndistance-based metrics computation against our highly accurate mesh-based\nreference implementation. The results revealed that statistically significant\ndifferences among all open-source tools are both surprising and concerning,\nsince they question the validity of existing studies. Besides identifying the\nmain sources of variation, we also provide recommendations for distance-based\nmetrics computation.\n","authors":["Gaper Podobnik","Toma Vrtovec"],"pdf_url":"https://arxiv.org/pdf/2410.02630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02619v1","updated":"2024-10-03T15:58:18Z","published":"2024-10-03T15:58:18Z","title":"GI-GS: Global Illumination Decomposition on Gaussian Splatting for\n  Inverse Rendering","summary":"  We present GI-GS, a novel inverse rendering framework that leverages 3D\nGaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel\nview synthesis and relighting. In inverse rendering, accurately modeling the\nshading processes of objects is essential for achieving high-fidelity results.\nTherefore, it is critical to incorporate global illumination to account for\nindirect lighting that reaches an object after multiple bounces across the\nscene. Previous 3DGS-based methods have attempted to model indirect lighting by\ncharacterizing indirect illumination as learnable lighting volumes or\nadditional attributes of each Gaussian, while using baked occlusion to\nrepresent shadow effects. These methods, however, fail to accurately model the\ncomplex physical interactions between light and objects, making it impossible\nto construct realistic indirect illumination during relighting. To address this\nlimitation, we propose to calculate indirect lighting using efficient path\ntracing with deferred shading. In our framework, we first render a G-buffer to\ncapture the detailed geometry and material properties of the scene. Then, we\nperform physically-based rendering (PBR) only for direct lighting. With the\nG-buffer and previous rendering results, the indirect lighting can be\ncalculated through a lightweight path tracing. Our method effectively models\nindirect lighting under any given lighting conditions, thereby achieving better\nnovel view synthesis and relighting. Quantitative and qualitative results show\nthat our GI-GS outperforms existing baselines in both rendering quality and\nefficiency.\n","authors":["Hongze Chen","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12191v2","updated":"2024-10-03T15:54:49Z","published":"2024-09-18T17:59:32Z","title":"Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at\n  Any Resolution","summary":"  We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .\n","authors":["Peng Wang","Shuai Bai","Sinan Tan","Shijie Wang","Zhihao Fan","Jinze Bai","Keqin Chen","Xuejing Liu","Jialin Wang","Wenbin Ge","Yang Fan","Kai Dang","Mengfei Du","Xuancheng Ren","Rui Men","Dayiheng Liu","Chang Zhou","Jingren Zhou","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2409.12191v2.pdf","comment":"Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin\n  note: text overlap with arXiv:2408.15262 by other authors"},{"id":"http://arxiv.org/abs/2410.02613v1","updated":"2024-10-03T15:51:36Z","published":"2024-10-03T15:51:36Z","title":"NL-Eye: Abductive NLI for Images","summary":"  Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.\n","authors":["Mor Ventura","Michael Toker","Nitay Calderon","Zorik Gekhman","Yonatan Bitton","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2410.02613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05662v4","updated":"2024-10-03T15:50:48Z","published":"2024-04-08T16:46:25Z","title":"BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models","summary":"  With the advancement of diffusion models (DMs) and the substantially\nincreased computational requirements, quantization emerges as a practical\nsolution to obtain compact and efficient low-bit DMs. However, the highly\ndiscrete representation leads to severe accuracy degradation, hindering the\nquantization of diffusion models to ultra-low bit-widths. This paper proposes a\nnovel weight binarization approach for DMs, namely BinaryDM, pushing binarized\nDMs to be accurate and efficient by improving the representation and\noptimization. From the representation perspective, we present an\nEvolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from\nfull-precision to accurately binarized. EBB enhances information representation\nin the initial stage through the flexible combination of multiple binary bases\nand applies regularization to evolve into efficient single-basis binarization.\nThe evolution only occurs in the head and tail of the DM architecture to retain\nthe stability of training. From the optimization perspective, a Low-rank\nRepresentation Mimicking (LRM) is applied to assist the optimization of\nbinarized DMs. The LRM mimics the representations of full-precision DMs in\nlow-rank space, alleviating the direction ambiguity of the optimization process\ncaused by fine-grained alignment. Comprehensive experiments demonstrate that\nBinaryDM achieves significant accuracy and efficiency gains compared to SOTA\nquantization methods of DMs under ultra-low bit-widths. With 1-bit weight and\n4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the\nperformance from collapse (baseline FID 10.87). As the first binarization\nmethod for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and\n29.2x model size savings, showcasing its substantial potential for edge\ndeployment.\n","authors":["Xingyu Zheng","Xianglong Liu","Haotong Qin","Xudong Ma","Mingyuan Zhang","Haojie Hao","Jiakai Wang","Zixiang Zhao","Jinyang Guo","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2404.05662v4.pdf","comment":"The code is available at https://github.com/Xingyu-Zheng/BinaryDM"},{"id":"http://arxiv.org/abs/2410.02598v1","updated":"2024-10-03T15:40:58Z","published":"2024-10-03T15:40:58Z","title":"High-Efficiency Neural Video Compression via Hierarchical Predictive\n  Learning","summary":"  The enhanced Deep Hierarchical Video Compression-DHVC 2.0-has been\nintroduced. This single-model neural video codec operates across a broad range\nof bitrates, delivering not only superior compression performance to\nrepresentative methods but also impressive complexity efficiency, enabling\nreal-time processing with a significantly smaller memory footprint on standard\nGPUs. These remarkable advancements stem from the use of hierarchical\npredictive coding. Each video frame is uniformly transformed into multiscale\nrepresentations through hierarchical variational autoencoders. For a specific\nscale's feature representation of a frame, its corresponding latent residual\nvariables are generated by referencing lower-scale spatial features from the\nsame frame and then conditionally entropy-encoded using a probabilistic model\nwhose parameters are predicted using same-scale temporal reference from\nprevious frames and lower-scale spatial reference of the current frame. This\nfeature-space processing operates from the lowest to the highest scale of each\nframe, completely eliminating the need for the complexity-intensive motion\nestimation and compensation techniques that have been standard in video codecs\nfor decades. The hierarchical approach facilitates parallel processing,\naccelerating both encoding and decoding, and supports transmission-friendly\nprogressive decoding, making it particularly advantageous for networked video\napplications in the presence of packet loss. Source codes will be made\navailable.\n","authors":["Ming Lu","Zhihao Duan","Wuyang Cong","Dandan Ding","Fengqing Zhu","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02592v1","updated":"2024-10-03T15:34:41Z","published":"2024-10-03T15:34:41Z","title":"IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of\n  Both Driver and Passengers","summary":"  Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.\n","authors":["Zihan Fang","Zheng Lin","Senkang Hu","Hangcheng Cao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.02592v1.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.02587v1","updated":"2024-10-03T15:29:43Z","published":"2024-10-03T15:29:43Z","title":"An Improved Variational Method for Image Denoising","summary":"  The total variation (TV) method is an image denoising technique that aims to\nreduce noise by minimizing the total variation of the image, which measures the\nvariation in pixel intensities. The TV method has been widely applied in image\nprocessing and computer vision for its ability to preserve edges and enhance\nimage quality. In this paper, we propose an improved TV model for image\ndenoising and the associated numerical algorithm to carry out the procedure,\nwhich is particularly effective in removing several types of noises and their\ncombinations. Our improved model admits a unique solution and the associated\nnumerical algorithm guarantees the convergence. Numerical experiments are\ndemonstrated to show improved effectiveness and denoising quality compared to\nother TV models. Such encouraging results further enhance the utility of the TV\nmethod in image processing.\n","authors":["Jing-En Huang","Jia-Wei Liao","Ku-Te Lin","Yu-Ju Tsai","Mei-Heng Yueh"],"pdf_url":"https://arxiv.org/pdf/2410.02587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02572v1","updated":"2024-10-03T15:20:19Z","published":"2024-10-03T15:20:19Z","title":"Combining Pre- and Post-Demosaicking Noise Removal for RAW Video","summary":"  Denoising is one of the fundamental steps of the processing pipeline that\nconverts data captured by a camera sensor into a display-ready image or video.\nIt is generally performed early in the pipeline, usually before demosaicking,\nalthough studies swapping their order or even conducting them jointly have been\nproposed. With the advent of deep learning, the quality of denoising algorithms\nhas steadily increased. Even so, modern neural networks still have a hard time\nadapting to new noise levels and scenes, which is indispensable for real-world\napplications. With those in mind, we propose a self-similarity-based denoising\nscheme that weights both a pre- and a post-demosaicking denoiser for\nBayer-patterned CFA video data. We show that a balance between the two leads to\nbetter image quality, and we empirically find that higher noise levels benefit\nfrom a higher influence pre-demosaicking. We also integrate temporal trajectory\nprefiltering steps before each denoiser, which further improve texture\nreconstruction. The proposed method only requires an estimation of the noise\nmodel at the sensor, accurately adapts to any noise level, and is competitive\nwith the state of the art, making it suitable for real-world videography.\n","authors":["Marco Snchez-Beeckman","Antoni Buades","Nicola Brandonisio","Bilel Kanoun"],"pdf_url":"https://arxiv.org/pdf/2410.02572v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.02571v1","updated":"2024-10-03T15:18:28Z","published":"2024-10-03T15:18:28Z","title":"SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field\n  and Gradient-guided Splitting","summary":"  Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis\nwith its real-time rendering capabilities and superior quality. However, it\nfaces challenges for high-resolution novel view synthesis (HRNVS) due to the\ncoarse nature of primitives derived from low-resolution input views. To address\nthis issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion\nof 3DGS designed with a two-stage coarse-to-fine training framework, utilizing\npretrained low-resolution scene representation as an initialization for\nsuper-resolution optimization. Moreover, we introduce Multi-resolution Feature\nGaussian Splatting (MFGS) to incorporates a latent feature field for flexible\nfeature sampling and Gradient-guided Selective Splitting (GSS) for effective\nGaussian upsampling. By integrating these strategies within the coarse-to-fine\nframework ensure both high fidelity and memory efficiency. Extensive\nexperiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods\non challenging real-world datasets using only low-resolution inputs.\n","authors":["Shiyun Xie","Zhiru Wang","Yinghao Zhu","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2410.02571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14407v2","updated":"2024-10-03T15:07:52Z","published":"2024-02-22T09:48:47Z","title":"Learning an Actionable Discrete Diffusion Policy via Large-Scale\n  Actionless Video Pre-Training","summary":"  Learning a generalist embodied agent capable of completing multiple tasks\nposes challenges, primarily stemming from the scarcity of action-labeled\nrobotic datasets. In contrast, a vast amount of human videos exist, capturing\nintricate tasks and interactions with the physical world. Promising prospects\narise for utilizing actionless human videos for pre-training and transferring\nthe knowledge to facilitate robot policy learning through limited robot\ndemonstrations. However, it remains a challenge due to the domain gap between\nhumans and robots. Moreover, it is difficult to extract useful information\nrepresenting the dynamic world from human videos, because of its noisy and\nmultimodal data structure. In this paper, we introduce a novel framework to\ntackle these challenges, which leverages a unified discrete diffusion to\ncombine generative pre-training on human videos and policy fine-tuning on a\nsmall number of action-labeled robot videos. We start by compressing both human\nand robot videos into unified video tokens. In the pre-training stage, we\nemploy a discrete diffusion model with a mask-and-replace diffusion strategy to\npredict future video tokens in the latent space. In the fine-tuning stage, we\nharness the imagined future videos to guide low-level action learning with a\nlimited set of robot data. Experiments demonstrate that our method generates\nhigh-fidelity future videos for planning and enhances the fine-tuned policies\ncompared to previous state-of-the-art approaches with superior performance. Our\nproject website is available at https://video-diff.github.io/.\n","authors":["Haoran He","Chenjia Bai","Ling Pan","Weinan Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2402.14407v2.pdf","comment":"Accepted by NeurIPS 2024. 24 pages"},{"id":"http://arxiv.org/abs/2410.02550v1","updated":"2024-10-03T14:53:42Z","published":"2024-10-03T14:53:42Z","title":"NestedMorph: Enhancing Deformable Medical Image Registration with Nested\n  Attention Mechanisms","summary":"  Deformable image registration is crucial for aligning medical images in a\nnon-linear fashion across different modalities, allowing for precise spatial\ncorrespondence between varying anatomical structures. This paper presents\nNestedMorph, a novel network utilizing a Nested Attention Fusion approach to\nimprove intra-subject deformable registration between T1-weighted (T1w) MRI and\ndiffusion MRI (dMRI) data. NestedMorph integrates high-resolution spatial\ndetails from an encoder with semantic information from a decoder using a\nmulti-scale framework, enhancing both local and global feature extraction. Our\nmodel notably outperforms existing methods, including CNN-based approaches like\nVoxelMorph, MIDIR, and CycleMorph, as well as Transformer-based models such as\nTransMorph and ViT-V-Net, and traditional techniques like NiftyReg and SyN.\nEvaluations on the HCP dataset demonstrate that NestedMorph achieves superior\nperformance across key metrics, including SSIM, HD95, and SDlogJ, with the\nhighest SSIM of 0.89, and the lowest HD95 of 2.5 and SDlogJ of 0.22. These\nresults highlight NestedMorph's ability to capture both local and global image\nfeatures effectively, leading to superior registration performance. The\npromising outcomes of this study underscore NestedMorph's potential to\nsignificantly advance deformable medical image registration, providing a robust\nframework for future research and clinical applications. The source code and\nour implementation are available at: https://bit.ly/3zdVqcg\n","authors":["Gurucharan Marthi Krishna Kumar","Janine Mendola","Amir Shmuel"],"pdf_url":"https://arxiv.org/pdf/2410.02550v1.pdf","comment":"Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2410.02458v1","updated":"2024-10-03T14:50:33Z","published":"2024-10-03T14:50:33Z","title":"MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation","summary":"  Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n","authors":["Gurucharan Marthi Krishna Kumar","Aman Chadha","Janine Mendola","Amir Shmuel"],"pdf_url":"https://arxiv.org/pdf/2410.02458v1.pdf","comment":"Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2403.09850v2","updated":"2024-10-03T14:44:51Z","published":"2024-03-14T20:18:08Z","title":"MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation","summary":"  Tasks such as autonomous navigation, 3D reconstruction, and object\nrecognition near the water surfaces are crucial in marine robotics\napplications. However, challenges arise due to dynamic disturbances, e.g.,\nlight reflections and refraction from the random air-water interface, irregular\nliquid flow, and similar factors, which can lead to potential failures in\nperception and navigation systems. Traditional computer vision algorithms\nstruggle to differentiate between real and virtual image regions, significantly\ncomplicating tasks. A virtual image region is an apparent representation formed\nby the redirection of light rays, typically through reflection or refraction,\ncreating the illusion of an object's presence without its actual physical\nlocation. This work proposes a novel approach for segmentation on real and\nvirtual image regions, exploiting synthetic images combined with\ndomain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric\nConsistency. Our segmentation network does not need to be re-trained if the\ndomain changes. We show this by deploying the same segmentation network in two\ndifferent domains: simulation and the real world. By creating realistic\nsynthetic images that mimic the complexities of the water surface, we provide\nfine-grained training data for our network (MARVIS) to discern between real and\nvirtual images effectively. By motion & geometry-aware design choices and\nthrough comprehensive experimental analysis, we achieve state-of-the-art\nreal-virtual image segmentation performance in unseen real world domain,\nachieving an IoU over 78% and a F1-Score over 86% while ensuring a small\ncomputational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a\nsingle GPU (CPU core). Our code and dataset are available here\nhttps://github.com/jiayi-wu-umd/MARVIS.\n","authors":["Jiayi Wu","Xiaomin Lin","Shahriar Negahdaripour","Cornelia Fermller","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2403.09850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02534v1","updated":"2024-10-03T14:40:17Z","published":"2024-10-03T14:40:17Z","title":"Pseudo-Stereo Inputs: A Solution to the Occlusion Challenge in\n  Self-Supervised Stereo Matching","summary":"  Self-supervised stereo matching holds great promise for application and\nresearch due to its independence from expensive labeled data. However, direct\nself-supervised stereo matching paradigms based on photometric loss functions\nhave consistently struggled with performance issues due to the occlusion\nchallenge. The crux of the occlusion challenge lies in the fact that the\npositions of occluded pixels consistently align with the epipolar search\ndirection defined by the input stereo images, leading to persistent information\nloss and erroneous feedback at fixed locations during self-supervised training.\nIn this work, we propose a simple yet highly effective pseudo-stereo inputs\nstrategy to address the core occlusion challenge. This strategy decouples the\ninput and feedback images, compelling the network to probabilistically sample\ninformation from both sides of the occluding objects. As a result, the\npersistent lack of information in the aforementioned fixed occlusion areas is\nmitigated. Building upon this, we further address feedback conflicts and\noverfitting issues arising from the strategy. By integrating these components,\nour method achieves stable and significant performance improvements compared to\nexisting methods. Quantitative experiments are conducted to evaluate the\nperformance. Qualitative experiments further demonstrate accurate disparity\ninference even at occluded regions. These results demonstrate a significant\nadvancement over previous methods in the field of direct self-supervised stereo\nmatching based on photometric loss. The proposed pseudo-stereo inputs strategy,\ndue to its simplicity and effectiveness, has the potential to serve as a new\nparadigm for direct self-supervised stereo matching. Code is available at\nhttps://github.com/qrzyang/Pseudo-Stereo.\n","authors":["Ruizhi Yang","Xingqiang Li","Jiajun Bai","Jinsong Du"],"pdf_url":"https://arxiv.org/pdf/2410.02534v1.pdf","comment":"Submitted to IEEE Transactions on Image Processing (TIP)"},{"id":"http://arxiv.org/abs/2410.02530v1","updated":"2024-10-03T14:36:32Z","published":"2024-10-03T14:36:32Z","title":"A Foundation Model for the Solar Dynamics Observatory","summary":"  SDO-FM is a foundation model using data from NASA's Solar Dynamics\nObservatory (SDO) spacecraft; integrating three separate instruments to\nencapsulate the Sun's complex physical interactions into a multi-modal\nembedding space. This model can be used to streamline scientific investigations\ninvolving SDO by making the enormous datasets more computationally accessible\nfor heliophysics research and enable investigations that require instrument\nfusion. We discuss four key components: an ingestion pipeline to create machine\nlearning ready datasets, the model architecture and training approach,\nresultant embeddings and fine-tunable models, and finally downstream fine-tuned\napplications. A key component of this effort has been to include subject matter\nspecialists at each stage of development; reviewing the scientific value and\nproviding guidance for model architecture, dataset, and training paradigm\ndecisions. This paper marks release of our pretrained models and embedding\ndatasets, available to the community on Hugging Face and sdofm.org.\n","authors":["James Walsh","Daniel G. Gass","Raul Ramos Pollan","Paul J. Wright","Richard Galvez","Noah Kasmanoff","Jason Naradowsky","Anne Spalding","James Parr","Atlm Gne Baydin"],"pdf_url":"https://arxiv.org/pdf/2410.02530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02528v1","updated":"2024-10-03T14:36:22Z","published":"2024-10-03T14:36:22Z","title":"HiFiSeg: High-Frequency Information Enhanced Polyp Segmentation with\n  Global-Local Vision Transformer","summary":"  Numerous studies have demonstrated the strong performance of Vision\nTransformer (ViT)-based methods across various computer vision tasks. However,\nViT models often struggle to effectively capture high-frequency components in\nimages, which are crucial for detecting small targets and preserving edge\ndetails, especially in complex scenarios. This limitation is particularly\nchallenging in colon polyp segmentation, where polyps exhibit significant\nvariability in structure, texture, and shape. High-frequency information, such\nas boundary details, is essential for achieving precise semantic segmentation\nin this context. To address these challenges, we propose HiFiSeg, a novel\nnetwork for colon polyp segmentation that enhances high-frequency information\nprocessing through a global-local vision transformer framework. HiFiSeg\nleverages the pyramid vision transformer (PVT) as its encoder and introduces\ntwo key modules: the global-local interaction module (GLIM) and the selective\naggregation module (SAM). GLIM employs a parallel structure to fuse global and\nlocal information at multiple scales, effectively capturing fine-grained\nfeatures. SAM selectively integrates boundary details from low-level features\nwith semantic information from high-level features, significantly improving the\nmodel's ability to accurately detect and segment polyps. Extensive experiments\non five widely recognized benchmark datasets demonstrate the effectiveness of\nHiFiSeg for polyp segmentation. Notably, the mDice scores on the challenging\nCVC-ColonDB and ETIS datasets reached 0.826 and 0.822, respectively,\nunderscoring the superior performance of HiFiSeg in handling the specific\ncomplexities of this task.\n","authors":["Jingjing Ren","Xiaoyong Zhang","Lina Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02527v1","updated":"2024-10-03T14:35:35Z","published":"2024-10-03T14:35:35Z","title":"Learning from Offline Foundation Features with Tensor Augmentations","summary":"  We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.\n","authors":["Emir Konuk","Christos Matsoukas","Moein Sorkhei","Phitchapha Lertsiravaramet","Kevin Smith"],"pdf_url":"https://arxiv.org/pdf/2410.02527v1.pdf","comment":"Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.02523v1","updated":"2024-10-03T14:29:46Z","published":"2024-10-03T14:29:46Z","title":"Med-TTT: Vision Test-Time Training model for Medical Image Segmentation","summary":"  Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning. Although models based on convolutional neural networks\n(CNNs) and Transformers have achieved remarkable success in medical image\nsegmentation tasks, they still face challenges such as high computational\ncomplexity and the loss of local features when capturing long-range\ndependencies. To address these limitations, we propose Med-TTT, a visual\nbackbone network integrated with Test-Time Training (TTT) layers, which\nincorporates dynamic adjustment capabilities. Med-TTT introduces the Vision-TTT\nlayer, which enables effective modeling of long-range dependencies with linear\ncomputational complexity and adaptive parameter adjustment during inference.\nFurthermore, we designed a multi-resolution fusion mechanism to combine image\nfeatures at different scales, facilitating the identification of subtle lesion\ncharacteristics in complex backgrounds. At the same time, we adopt a frequency\ndomain feature enhancement strategy based on high pass filtering, which can\nbetter capture texture and fine-grained details in images. Experimental results\ndemonstrate that Med-TTT significantly outperforms existing methods on multiple\nmedical image datasets, exhibiting strong segmentation capabilities,\nparticularly in complex image backgrounds. The model achieves leading\nperformance in terms of accuracy, sensitivity, and Dice coefficient, providing\nan efficient and robust solution for the field of medical image\nsegmentation.The code is available at https://github.com/Jiashu-Xu/Med-TTT .\n","authors":["Jiashu Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20409v2","updated":"2024-10-03T14:26:43Z","published":"2024-09-30T15:36:14Z","title":"Physics-Regularized Multi-Modal Image Assimilation for Brain Tumor\n  Localization","summary":"  Physical models in the form of partial differential equations represent an\nimportant prior for many under-constrained problems. One example is tumor\ntreatment planning, which heavily depends on accurate estimates of the spatial\ndistribution of tumor cells in a patient's anatomy. Medical imaging scans can\nidentify the bulk of the tumor, but they cannot reveal its full spatial\ndistribution. Tumor cells at low concentrations remain undetectable, for\nexample, in the most frequent type of primary brain tumors, glioblastoma.\nDeep-learning-based approaches fail to estimate the complete tumor cell\ndistribution due to a lack of reliable training data. Most existing works\ntherefore rely on physics-based simulations to match observed tumors, providing\nanatomically and physiologically plausible estimations. However, these\napproaches struggle with complex and unknown initial conditions and are limited\nby overly rigid physical models. In this work, we present a novel method that\nbalances data-driven and physics-based cost functions. In particular, we\npropose a unique discretization scheme that quantifies the adherence of our\nlearned spatiotemporal tumor and brain tissue distributions to their\ncorresponding growth and elasticity equations. This quantification, serving as\na regularization term rather than a hard constraint, enables greater\nflexibility and proficiency in assimilating patient data than existing models.\nWe demonstrate improved coverage of tumor recurrence areas compared to existing\ntechniques on real-world data from a cohort of patients. The method holds the\npotential to enhance clinical adoption of model-driven treatment planning for\nglioblastoma.\n","authors":["Michal Balcerak","Tamaz Amiranashvili","Andreas Wagner","Jonas Weidner","Petr Karnakov","Johannes C. Paetzold","Ivan Ezhov","Petros Koumoutsakos","Benedikt Wiestler","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2409.20409v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.02954v3","updated":"2024-10-03T14:25:07Z","published":"2024-05-05T14:48:13Z","title":"Source-Free Domain Adaptation Guided by Vision and Vision-Language\n  Pre-Training","summary":"  Source-free domain adaptation (SFDA) aims to adapt a source model trained on\na fully-labeled source domain to a related but unlabeled target domain. While\nthe source model is a key avenue for acquiring target pseudolabels, the\ngenerated pseudolabels may exhibit source bias. In the conventional SFDA\npipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to\ninitialize the source model at the start of source training, and subsequently\ndiscarded. Despite having diverse features important for generalization, the\npre-trained feature extractor can overfit to the source data distribution\nduring source training and forget relevant target domain knowledge. Rather than\ndiscarding this valuable knowledge, we introduce an integrated framework to\nincorporate pre-trained networks into the target adaptation process. The\nproposed framework is flexible and allows us to plug modern pre-trained\nnetworks into the adaptation process to leverage their stronger representation\nlearning capabilities. For adaptation, we propose the Co-learn algorithm to\nimprove target pseudolabel quality collaboratively through the source model and\na pre-trained feature extractor. Building on the recent success of the\nvision-language model CLIP in zero-shot image recognition, we present an\nextension Co-learn++ to further incorporate CLIP's zero-shot classification\ndecisions. We evaluate on 4 benchmark datasets and include more challenging\nscenarios such as open-set, partial-set and open-partial SFDA. Experimental\nresults demonstrate that our proposed strategy improves adaptation performance\nand can be successfully integrated with existing SFDA methods. Project code is\navailable at https://github.com/zwenyu/colearn-plus.\n","authors":["Wenyu Zhang","Li Shen","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2405.02954v3.pdf","comment":"Extension of ICCV paper arXiv:2212.07585; Published at IJCV"},{"id":"http://arxiv.org/abs/2410.02505v1","updated":"2024-10-03T14:14:21Z","published":"2024-10-03T14:14:21Z","title":"Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality\n  Assessment","summary":"  Image quality assessment (IQA) serves as the golden standard for all models'\nperformance in nearly all computer vision fields. However, it still suffers\nfrom poor out-of-distribution generalization ability and expensive training\ncosts. To address these problems, we propose Dog-IQA, a standard-guided\nzero-shot mix-grained IQA method, which is training-free and utilizes the\nexceptional prior knowledge of multimodal large language models (MLLMs). To\nobtain accurate IQA scores, namely scores consistent with humans, we design an\nMLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA\napplies two techniques. First, Dog-IQA objectively scores with specific\nstandards that utilize MLLM's behavior pattern and minimize the influence of\nsubjective factors. Second, Dog-IQA comprehensively takes local semantic\nobjects and the whole image as input and aggregates their scores, leveraging\nlocal and global information. Our proposed Dog-IQA achieves state-of-the-art\n(SOTA) performance compared with training-free methods, and competitive\nperformance compared with training-based methods in cross-dataset scenarios.\nOur code and models will be available at https://github.com/Kai-Liu001/Dog-IQA.\n","authors":["Kai Liu","Ziqing Zhang","Wenbo Li","Renjing Pei","Fenglong Song","Xiaohong Liu","Linghe Kong","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02505v1.pdf","comment":"10 pages, 5 figures. The code and models will be available at\n  https://github.com/Kai-Liu001/Dog-IQA"},{"id":"http://arxiv.org/abs/2306.11528v3","updated":"2024-10-03T14:02:10Z","published":"2023-06-20T13:31:33Z","title":"TransRef: Multi-Scale Reference Embedding Transformer for\n  Reference-Guided Image Inpainting","summary":"  Image inpainting for completing complicated semantic environments and diverse\nhole patterns of corrupted images is challenging even for state-of-the-art\nlearning-based inpainting methods trained on large-scale data. A reference\nimage capturing the same scene of a corrupted image offers informative guidance\nfor completing the corrupted image as it shares similar texture and structure\npriors to that of the holes of the corrupted image. In this work, we propose a\ntransformer-based encoder-decoder network, named TransRef, for reference-guided\nimage inpainting. Specifically, the guidance is conducted progressively through\na reference embedding procedure, in which the referencing features are\nsubsequently aligned and fused with the features of the corrupted image. For\nprecise utilization of the reference features for guidance, a reference-patch\nalignment (Ref-PA) module is proposed to align the patch features of the\nreference and corrupted images and harmonize their style differences, while a\nreference-patch transformer (Ref-PT) module is proposed to refine the embedded\nreference feature. Moreover, to facilitate the research of reference-guided\nimage restoration tasks, we construct a publicly accessible benchmark dataset\ncontaining 50K pairs of input and reference images. Both quantitative and\nqualitative evaluations demonstrate the efficacy of the reference information\nand the proposed method over the state-of-the-art methods in completing complex\nholes. Code and dataset can be accessed at https://github.com/Cameltr/TransRef.\n","authors":["Taorong Liu","Liang Liao","Delin Chen","Jing Xiao","Zheng Wang","Chia-Wen Lin","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2306.11528v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.19365v2","updated":"2024-10-03T14:01:03Z","published":"2024-09-28T14:36:38Z","title":"Conditional Image Synthesis with Diffusion Models: A Survey","summary":"  Conditional image synthesis based on user-specified requirements is a key\ncomponent in creating complex visual content. In recent years, diffusion-based\ngenerative modeling has become a highly effective way for conditional image\nsynthesis, leading to exponential growth in the literature. However, the\ncomplexity of diffusion-based modeling, the wide range of image synthesis\ntasks, and the diversity of conditioning mechanisms present significant\nchallenges for researchers to keep up with rapid developments and understand\nthe core concepts on this topic. In this survey, we categorize existing works\nbased on how conditions are integrated into the two fundamental components of\ndiffusion-based modeling, i.e., the denoising network and the sampling process.\nWe specifically highlight the underlying principles, advantages, and potential\nchallenges of various conditioning approaches in the training, re-purposing,\nand specialization stages to construct a desired denoising network. We also\nsummarize six mainstream conditioning mechanisms in the essential sampling\nprocess. All discussions are centered around popular applications. Finally, we\npinpoint some critical yet still open problems to be solved in the future and\nsuggest some possible solutions. Our reviewed works are itemized at\nhttps://github.com/zju-pi/Awesome-Conditional-Diffusion-Models.\n","authors":["Zheyuan Zhan","Defang Chen","Jian-Ping Mei","Zhenghe Zhao","Jiawei Chen","Chun Chen","Siwei Lyu","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2409.19365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02492v1","updated":"2024-10-03T13:57:07Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v1.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2409.20195v2","updated":"2024-10-03T13:50:29Z","published":"2024-09-30T11:11:35Z","title":"Forecasting Disease Progression with Parallel Hyperplanes in\n  Longitudinal Retinal OCT","summary":"  Predicting future disease progression risk from medical images is challenging\ndue to patient heterogeneity, and subtle or unknown imaging biomarkers.\nMoreover, deep learning (DL) methods for survival analysis are susceptible to\nimage domain shifts across scanners. We tackle these issues in the task of\npredicting late dry Age-related Macular Degeneration (dAMD) onset from retinal\nOCT scans. We propose a novel DL method for survival prediction to jointly\npredict from the current scan a risk score, inversely related to\ntime-to-conversion, and the probability of conversion within a time interval\n$t$. It uses a family of parallel hyperplanes generated by parameterizing the\nbias term as a function of $t$. In addition, we develop unsupervised losses\nbased on intra-subject image pairs to ensure that risk scores increase over\ntime and that future conversion predictions are consistent with AMD stage\nprediction using actual scans of future visits. Such losses enable\ndata-efficient fine-tuning of the trained model on new unlabeled datasets\nacquired with a different scanner. Extensive evaluation on two large datasets\nacquired with different scanners resulted in a mean AUROCs of 0.82 for\nDataset-1 and 0.83 for Dataset-2, across prediction intervals of 6,12 and 24\nmonths.\n","authors":["Arunava Chakravarty","Taha Emre","Dmitrii Lachinov","Antoine Rivail","Hendrik Scholl","Lars Fritsche","Sobha Sivaprasad","Daniel Rueckert","Andrew Lotery","Ursula Schmidt-Erfurth","Hrvoje Bogunovi"],"pdf_url":"https://arxiv.org/pdf/2409.20195v2.pdf","comment":"accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.02483v1","updated":"2024-10-03T13:41:58Z","published":"2024-10-03T13:41:58Z","title":"Event-Customized Image Generation","summary":"  Customized Image Generation, generating customized images with user-specified\nconcepts, has raised significant attention due to its creativity and novelty.\nWith impressive progress achieved in subject customization, some pioneer works\nfurther explored the customization of action and interaction beyond entity\n(i.e., human, animal, and object) appearance. However, these approaches only\nfocus on basic actions and interactions between two entities, and their effects\nare limited by insufficient ''exactly same'' reference images. To extend\ncustomized image generation to more complex scenes for general real-world\napplications, we propose a new task: event-customized image generation. Given a\nsingle reference image, we define the ''event'' as all specific actions, poses,\nrelations, or interactions between different entities in the scene. This task\naims at accurately capturing the complex event and generating customized images\nwith various target entities. To solve this task, we proposed a novel\ntraining-free event customization method: FreeEvent. Specifically, FreeEvent\nintroduces two extra paths alongside the general diffusion denoising process:\n1) Entity switching path: it applies cross-attention guidance and regulation\nfor target entity generation. 2) Event transferring path: it injects the\nspatial feature and self-attention maps from the reference image to the target\nimage for event generation. To further facilitate this new task, we collected\ntwo evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and\nablations have demonstrated the effectiveness of FreeEvent.\n","authors":["Zhen Wang","Yilei Jiang","Dong Zheng","Jun Xiao","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17041v4","updated":"2024-10-03T13:31:39Z","published":"2023-11-28T18:53:06Z","title":"Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties","summary":"  A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.\n","authors":["Keunwoo Peter Yu","Zheyuan Zhang","Fengyuan Hu","Shane Storks","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2311.17041v4.pdf","comment":"16 pages, LaTeX; Accepted to EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.02467v1","updated":"2024-10-03T13:17:06Z","published":"2024-10-03T13:17:06Z","title":"Towards a Theoretical Understanding of Memorization in Diffusion Models","summary":"  As diffusion probabilistic models (DPMs) are being employed as mainstream\nmodels for Generative Artificial Intelligence (GenAI), the study of their\nmemorization of training data has attracted growing attention. Existing works\nin this direction aim to establish an understanding of whether or to what\nextent DPMs learn via memorization. Such an understanding is crucial for\nidentifying potential risks of data leakage and copyright infringement in\ndiffusion models and, more importantly, for trustworthy application of GenAI.\nExisting works revealed that conditional DPMs are more prone to training data\nmemorization than unconditional DPMs, and the motivated data extraction methods\nare mostly for conditional DPMs. However, these understandings are primarily\nempirical, and extracting training data from unconditional models has been\nfound to be extremely challenging. In this work, we provide a theoretical\nunderstanding of memorization in both conditional and unconditional DPMs under\nthe assumption of model convergence. Our theoretical analysis indicates that\nextracting data from unconditional models can also be effective by constructing\na proper surrogate condition. Based on this result, we propose a novel data\nextraction method named \\textbf{Surrogate condItional Data Extraction (SIDE)}\nthat leverages a time-dependent classifier trained on the generated data as a\nsurrogate condition to extract training data from unconditional DPMs. Empirical\nresults demonstrate that our SIDE can extract training data in challenging\nscenarios where previous methods fail, and it is, on average, over 50\\% more\neffective across different scales of the CelebA dataset.\n","authors":["Yunhao Chen","Xingjun Ma","Difan Zou","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.02467v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.12752"},{"id":"http://arxiv.org/abs/2407.07053v5","updated":"2024-10-03T13:12:06Z","published":"2024-07-09T17:18:27Z","title":"Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model","summary":"  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n","authors":["Wenqi Zhang","Zhenglin Cheng","Yuanyu He","Mengna Wang","Yongliang Shen","Zeqi Tan","Guiyang Hou","Mingqian He","Yanna Ma","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.07053v5.pdf","comment":"The paper is accepted by EMNLP-24. Code:\n  https://github.com/zwq2018/Multi-modal-Self-instruct dataset:\n  https://huggingface.co/datasets/zwq2018/Multi-modal-Self-instruct\n  Leaderboard: https://multi-modal-self-instruct.github.io/"},{"id":"http://arxiv.org/abs/2410.02456v1","updated":"2024-10-03T13:05:27Z","published":"2024-10-03T13:05:27Z","title":"Recurrent Few-Shot model for Document Verification","summary":"  General-purpose ID, or travel, document image- and video-based verification\nsystems have yet to achieve good enough performance to be considered a solved\nproblem. There are several factors that negatively impact their performance,\nincluding low-resolution images and videos and a lack of sufficient data to\ntrain the models. This task is particularly challenging when dealing with\nunseen class of ID, or travel, documents. In this paper we address this task by\nproposing a recurrent-based model able to detect forged documents in a few-shot\nscenario. The recurrent architecture makes the model robust to document\nresolution variability. Moreover, the few-shot approach allow the model to\nperform well even for unseen class of documents. Preliminary results on the\nSIDTD and Findit datasets show good performance of this model for this task.\n","authors":["Maxime Talarmain","Carlos Boned","Sanket Biswas","Oriol Ramos"],"pdf_url":"https://arxiv.org/pdf/2410.02456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11682v2","updated":"2024-10-03T13:03:39Z","published":"2024-09-18T03:47:24Z","title":"SRIF: Semantic Shape Registration Empowered by Diffusion-based Image\n  Morphing and Flow Estimation","summary":"  In this paper, we propose SRIF, a novel Semantic shape Registration framework\nbased on diffusion-based Image morphing and Flow estimation. More concretely,\ngiven a pair of extrinsically aligned shapes, we first render them from\nmulti-views, and then utilize an image interpolation framework based on\ndiffusion models to generate sequences of intermediate images between them. The\nimages are later fed into a dynamic 3D Gaussian splatting framework, with which\nwe reconstruct and post-process for intermediate point clouds respecting the\nimage morphing processing. In the end, tailored for the above, we propose a\nnovel registration module to estimate continuous normalizing flow, which\ndeforms source shape consistently towards the target, with intermediate point\nclouds as weak guidance. Our key insight is to leverage large vision models\n(LVMs) to associate shapes and therefore obtain much richer semantic\ninformation on the relationship between shapes than the ad-hoc feature\nextraction and alignment. As a consequence, SRIF achieves high-quality dense\ncorrespondences on challenging shape pairs, but also delivers smooth,\nsemantically meaningful interpolation in between. Empirical evidence justifies\nthe effectiveness and superiority of our method as well as specific design\nchoices. The code is released at https://github.com/rqhuang88/SRIF.\n","authors":["Mingze Sun","Chen Guo","Puhua Jiang","Shiwei Mao","Yurun Chen","Ruqi Huang"],"pdf_url":"https://arxiv.org/pdf/2409.11682v2.pdf","comment":"Accepted as a conference paper of SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2406.06050v2","updated":"2024-10-03T12:52:10Z","published":"2024-06-10T06:38:11Z","title":"Generalizable Human Gaussians from Single-View Image","summary":"  In this work, we tackle the task of learning generalizable 3D human Gaussians\nfrom a single image. The main challenge for this task is to recover detailed\ngeometry and appearance, especially for the unobserved regions. To this end, we\npropose single-view generalizable Human Gaussian model (HGM), a\ndiffusion-guided framework for 3D human modeling from a single image. We design\na diffusion-based coarse-to-fine pipeline, where the diffusion model is adapted\nto refine novel-view images rendered from a coarse human Gaussian model. The\nrefined images are then used together with the input image to learn a refined\nhuman Gaussian model. Although effective in hallucinating the unobserved views,\nthe approach may generate unrealistic human pose and shapes due to the lack of\nsupervision. We circumvent this problem by further encoding the geometric\npriors from SMPL model. Specifically, we propagate geometric features from SMPL\nvolume to the predicted Gaussians via sparse convolution and attention\nmechanism. We validate our approach on publicly available datasets and\ndemonstrate that it significantly surpasses state-of-the-art methods in terms\nof PSNR and SSIM. Additionally, our method exhibits strong generalization for\nin-the-wild images.\n","authors":["Jinnan Chen","Chen Li","Jianfeng Zhang","Lingting Zhu","Buzhen Huang","Hanlin Chen","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.06050v2.pdf","comment":"https://jinnan-chen.github.io/projects/HGM/"},{"id":"http://arxiv.org/abs/2410.02443v1","updated":"2024-10-03T12:40:52Z","published":"2024-10-03T12:40:52Z","title":"Clinnova Federated Learning Proof of Concept: Key Takeaways from a\n  Cross-border Collaboration","summary":"  Clinnova, a collaborative initiative involving France, Germany, Switzerland,\nand Luxembourg, is dedicated to unlocking the power of precision medicine\nthrough data federation, standardization, and interoperability. This European\nGreater Region initiative seeks to create an interoperable European standard\nusing artificial intelligence (AI) and data science to enhance healthcare\noutcomes and efficiency. Key components include multidisciplinary research\ncenters, a federated biobanking strategy, a digital health innovation platform,\nand a federated AI strategy. It targets inflammatory bowel disease, rheumatoid\ndiseases, and multiple sclerosis (MS), emphasizing data quality to develop AI\nalgorithms for personalized treatment and translational research.\n  The IHU Strasbourg (Institute of Minimal-invasive Surgery) has the lead in\nthis initiative to develop the federated learning (FL) proof of concept (POC)\nthat will serve as a foundation for advancing AI in healthcare. At its core,\nClinnova-MS aims to enhance MS patient care by using FL to develop more\naccurate models that detect disease progression, guide interventions, and\nvalidate digital biomarkers across multiple sites. This technical report\npresents insights and key takeaways from the first cross-border federated POC\non MS segmentation of MRI images within the Clinnova framework. While our work\nmarks a significant milestone in advancing MS segmentation through cross-border\ncollaboration, it also underscores the importance of addressing technical,\nlogistical, and ethical considerations to realize the full potential of FL in\nhealthcare settings.\n","authors":["Julia Alekseenko","Bram Stieltjes","Michael Bach","Melanie Boerries","Oliver Opitz","Alexandros Karargyris","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2410.02443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02430v1","updated":"2024-10-03T12:25:01Z","published":"2024-10-03T12:25:01Z","title":"Predictive Attractor Models","summary":"  Sequential memory, the ability to form and accurately recall a sequence of\nevents or stimuli in the correct order, is a fundamental prerequisite for\nbiological and artificial intelligence as it underpins numerous cognitive\nfunctions (e.g., language comprehension, planning, episodic memory formation,\netc.) However, existing methods of sequential memory suffer from catastrophic\nforgetting, limited capacity, slow iterative learning procedures, low-order\nMarkov memory, and, most importantly, the inability to represent and generate\nmultiple valid future possibilities stemming from the same context. Inspired by\nbiologically plausible neuroscience theories of cognition, we propose\n\\textit{Predictive Attractor Models (PAM)}, a novel sequence memory\narchitecture with desirable generative properties. PAM is a streaming model\nthat learns a sequence in an online, continuous manner by observing each input\n\\textit{only once}. Additionally, we find that PAM avoids catastrophic\nforgetting by uniquely representing past context through lateral inhibition in\ncortical minicolumns, which prevents new memories from overwriting previously\nlearned knowledge. PAM generates future predictions by sampling from a union\nset of predicted possibilities; this generative ability is realized through an\nattractor model trained alongside the predictor. We show that PAM is trained\nwith local computations through Hebbian plasticity rules in a biologically\nplausible framework. Other desirable traits (e.g., noise tolerance, CPU-based\nlearning, capacity scaling) are discussed throughout the paper. Our findings\nsuggest that PAM represents a significant step forward in the pursuit of\nbiologically plausible and computationally efficient sequential memory models,\nwith broad implications for cognitive science and artificial intelligence\nresearch.\n","authors":["Ramy Mounir","Sudeep Sarkar"],"pdf_url":"https://arxiv.org/pdf/2410.02430v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.02423v1","updated":"2024-10-03T12:13:56Z","published":"2024-10-03T12:13:56Z","title":"PnP-Flow: Plug-and-Play Image Restoration with Flow Matching","summary":"  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm\nfor solving imaging inverse problems. PnP methods leverage the strength of\npre-trained denoisers, often deep neural networks, by integrating them in\noptimization schemes. While they achieve state-of-the-art performance on\nvarious inverse problems in imaging, PnP approaches face inherent limitations\non more generative tasks like inpainting. On the other hand, generative models\nsuch as Flow Matching pushed the boundary in image sampling yet lack a clear\nmethod for efficient use in image restoration. We propose to combine the PnP\nframework with Flow Matching (FM) by defining a time-dependent denoiser using a\npre-trained FM model. Our algorithm alternates between gradient descent steps\non the data-fidelity term, reprojections onto the learned FM path, and\ndenoising. Notably, our method is computationally efficient and\nmemory-friendly, as it avoids backpropagation through ODEs and trace\ncomputations. We evaluate its performance on denoising, super-resolution,\ndeblurring, and inpainting tasks, demonstrating superior results compared to\nexisting PnP algorithms and Flow Matching based state-of-the-art methods.\n","authors":["Sgolne Martin","Anne Gagneux","Paul Hagemann","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2410.02423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02420v1","updated":"2024-10-03T12:11:22Z","published":"2024-10-03T12:11:22Z","title":"LoGDesc: Local geometric features aggregation for robust point cloud\n  registration","summary":"  This paper introduces a new hybrid descriptor for 3D point matching and point\ncloud registration, combining local geometrical properties and learning-based\nfeature propagation for each point's neighborhood structure description. The\nproposed architecture first extracts prior geometrical information by computing\neach point's planarity, anisotropy, and omnivariance using a Principal\nComponents Analysis (PCA). This prior information is completed by a descriptor\nbased on the normal vectors estimated thanks to constructing a neighborhood\nbased on triangles. The final geometrical descriptor is propagated between the\npoints using local graph convolutions and attention mechanisms. The new feature\nextractor is evaluated on ModelNet40, Bunny Stanford dataset, KITTI and MVP\n(Multi-View Partial)-RG for point cloud registration and shows interesting\nresults, particularly on noisy and low overlapping point clouds.\n","authors":["Karim Slimani","Brahim Tamadazte","Catherine Achard"],"pdf_url":"https://arxiv.org/pdf/2410.02420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02416v1","updated":"2024-10-03T12:06:29Z","published":"2024-10-03T12:06:29Z","title":"Eliminating Oversaturation and Artifacts of High Guidance Scales in\n  Diffusion Models","summary":"  Classifier-free guidance (CFG) is crucial for improving both generation\nquality and alignment between the input condition and final output in diffusion\nmodels. While a high guidance scale is generally required to enhance these\naspects, it also causes oversaturation and unrealistic artifacts. In this\npaper, we revisit the CFG update rule and introduce modifications to address\nthis issue. We first decompose the update term in CFG into parallel and\northogonal components with respect to the conditional model prediction and\nobserve that the parallel component primarily causes oversaturation, while the\northogonal component enhances image quality. Accordingly, we propose\ndown-weighting the parallel component to achieve high-quality generations\nwithout oversaturation. Additionally, we draw a connection between CFG and\ngradient ascent and introduce a new rescaling and momentum method for the CFG\nupdate rule based on this insight. Our approach, termed adaptive projected\nguidance (APG), retains the quality-boosting advantages of CFG while enabling\nthe use of higher guidance scales without oversaturation. APG is easy to\nimplement and introduces practically no additional computational overhead to\nthe sampling process. Through extensive experiments, we demonstrate that APG is\ncompatible with various conditional diffusion models and samplers, leading to\nimproved FID, recall, and saturation scores while maintaining precision\ncomparable to CFG, making our method a superior plug-and-play alternative to\nstandard classifier-free guidance.\n","authors":["Seyedmorteza Sadat","Otmar Hilliges","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2410.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02401v1","updated":"2024-10-03T11:29:09Z","published":"2024-10-03T11:29:09Z","title":"SynCo: Synthetic Hard Negatives in Contrastive Learning for Better\n  Unsupervised Visual Representations","summary":"  Contrastive learning has become a dominant approach in self-supervised visual\nrepresentation learning, with hard negatives-samples that closely resemble the\nanchor-being key to enhancing the discriminative power of learned\nrepresentations. However, efficiently leveraging hard negatives remains a\nchallenge due to the difficulty in identifying and incorporating them without\nsignificantly increasing computational costs. To address this, we introduce\nSynCo (Synthetic Negatives in Contrastive learning), a novel contrastive\nlearning approach that improves model performance by generating synthetic hard\nnegatives. Built on the MoCo framework, SynCo introduces six novel strategies\nfor creating diverse synthetic hard negatives that can be generated on-the-fly\nwith minimal computational overhead. SynCo achieves faster training and better\nrepresentation learning, achieving a top-1 accuracy of 68.1% in ImageNet linear\nevaluation after only 200 epochs on pretraining, surpassing MoCo's 67.5% with\nthe same ResNet-50 encoder. Additionally, it transfers more effectively to\ndetection tasks: on the PASCAL VOC, it outperforms both the supervised baseline\nand MoCo, achieving an AP of 82.5%; on the COCO dataset, it sets a new\nbenchmark with 40.4% AP for bounding box detection and 35.4% AP for instance\nsegmentation. Our synthetic hard negative generation procedure significantly\nenhances the quality of visual representations learned through self-supervised\ncontrastive learning. Code is available at\nhttps://github.com/giakoumoglou/synco.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.02401v1.pdf","comment":"10 pages, 6 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:2010.01028 by other authors"},{"id":"http://arxiv.org/abs/2410.02396v1","updated":"2024-10-03T11:17:58Z","published":"2024-10-03T11:17:58Z","title":"Parameter Competition Balancing for Model Merging","summary":"  While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.\n","authors":["Guodong Du","Junlin Lee","Jing Li","Runhua Jiang","Yifei Guo","Shuyang Yu","Hanting Liu","Sim Kuan Goh","Ho-Kin Tang","Daojing He","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02396v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.02381v1","updated":"2024-10-03T11:01:25Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.02889v2","updated":"2024-10-03T11:01:14Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v2.pdf","comment":"20 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2410.02369v1","updated":"2024-10-03T10:33:49Z","published":"2024-10-03T10:33:49Z","title":"Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation","summary":"  The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.\n","authors":["Muzhi Zhu","Yang Liu","Zekai Luo","Chenchen Jing","Hao Chen","Guangkai Xu","Xinlong Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2410.02369v1.pdf","comment":"Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2410.02362v1","updated":"2024-10-03T10:23:03Z","published":"2024-10-03T10:23:03Z","title":"A Comprehensive Survey of Mamba Architectures for Medical Image\n  Analysis: Classification, Segmentation, Restoration and Beyond","summary":"  Mamba, a special case of the State Space Model, is gaining popularity as an\nalternative to template-based deep learning approaches in medical image\nanalysis. While transformers are powerful architectures, they have drawbacks,\nincluding quadratic computational complexity and an inability to address\nlong-range dependencies efficiently. This limitation affects the analysis of\nlarge and complex datasets in medical imaging, where there are many spatial and\ntemporal relationships. In contrast, Mamba offers benefits that make it\nwell-suited for medical image analysis. It has linear time complexity, which is\na significant improvement over transformers. Mamba processes longer sequences\nwithout attention mechanisms, enabling faster inference and requiring less\nmemory. Mamba also demonstrates strong performance in merging multimodal data,\nimproving diagnosis accuracy and patient outcomes. The organization of this\npaper allows readers to appreciate the capabilities of Mamba in medical imaging\nstep by step. We begin by defining core concepts of SSMs and models, including\nS4, S5, and S6, followed by an exploration of Mamba architectures such as pure\nMamba, U-Net variants, and hybrid models with convolutional neural networks,\ntransformers, and Graph Neural Networks. We also cover Mamba optimizations,\ntechniques and adaptations, scanning, datasets, applications, experimental\nresults, and conclude with its challenges and future directions in medical\nimaging. This review aims to demonstrate the transformative potential of Mamba\nin overcoming existing barriers within medical imaging while paving the way for\ninnovative advancements in the field. A comprehensive list of Mamba\narchitectures applied in the medical field, reviewed in this work, is available\nat Github.\n","authors":["Shubhi Bansal","Sreeharish A","Madhava Prasath J","Manikandan S","Sreekanth Madisetty","Mohammad Zia Ur Rehman","Chandravardhan Singh Raghaw","Gaurav Duggal","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.02362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02352v1","updated":"2024-10-03T10:05:27Z","published":"2024-10-03T10:05:27Z","title":"ProtoSeg: A Prototype-Based Point Cloud Instance Segmentation Method","summary":"  3D instance segmentation is crucial for obtaining an understanding of a point\ncloud scene. This paper presents a novel neural network architecture for\nperforming instance segmentation on 3D point clouds. We propose to jointly\nlearn coefficients and prototypes in parallel which can be combined to obtain\nthe instance predictions. The coefficients are computed using an overcomplete\nset of sampled points with a novel multi-scale module, dubbed dilated point\ninception. As the set of obtained instance mask predictions is overcomplete, we\nemploy a non-maximum suppression algorithm to retrieve the final predictions.\nThis approach allows to omit the time-expensive clustering step and leads to a\nmore stable inference time. The proposed method is not only 28% faster than the\nstate-of-the-art, it also exhibits the lowest standard deviation. Our\nexperiments have shown that the standard deviation of the inference time is\nonly 1.0% of the total time while it ranges between 10.8 and 53.1% for the\nstate-of-the-art methods. Lastly, our method outperforms the state-of-the-art\nboth on S3DIS-blocks (4.9% in mRec on Fold-5) and PartNet (2.0% on average in\nmAP).\n","authors":["Remco Royen","Leon Denis","Adrian Munteanu"],"pdf_url":"https://arxiv.org/pdf/2410.02352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13991v3","updated":"2024-10-03T10:05:24Z","published":"2023-08-27T02:59:59Z","title":"Optimal Projections for Discriminative Dictionary Learning using the\n  JL-lemma","summary":"  Dimensionality reduction-based dictionary learning methods in the literature\nhave often used iterative random projections. The dimensionality of such a\nrandom projection matrix is a random number that might not lead to a separable\nsubspace structure in the transformed space. The convergence of such methods\nhighly depends on the initial seed values used. Also, gradient descent-based\nupdates might result in local minima. This paper proposes a constructive\napproach to derandomize the projection matrix using the Johnson-Lindenstrauss\nlemma. Rather than reducing dimensionality via random projections, a projection\nmatrix derived from the proposed Modified Supervised PC analysis is used. A\nheuristic is proposed to decide the data perturbation levels and the dictionary\natom's corresponding suitable description length. The projection matrix is\nderived in a single step, provides maximum feature-label consistency of the\ntransformed space, and preserves the geometry of the original data. The\nprojection matrix thus constructed is proved to be a JL-embedding. Despite\nconfusing classes in the OCR datasets, the dictionary trained in the\ntransformed space generates discriminative sparse coefficients with reduced\ncomplexity. Empirical study demonstrates that the proposed method performs well\neven when the number of classes and dimensionality increase. Experimentation on\nOCR and face recognition datasets shows better classification performance than\nother algorithms.\n","authors":["G. Madhuri","Atul Negi","Kaluri V. Rangarao"],"pdf_url":"https://arxiv.org/pdf/2308.13991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02331v1","updated":"2024-10-03T09:29:28Z","published":"2024-10-03T09:29:28Z","title":"Self-eXplainable AI for Medical Image Analysis: A Survey and New\n  Outlooks","summary":"  The increasing demand for transparent and reliable models, particularly in\nhigh-stakes decision-making areas such as medical image analysis, has led to\nthe emergence of eXplainable Artificial Intelligence (XAI). Post-hoc XAI\ntechniques, which aim to explain black-box models after training, have been\ncontroversial in recent works concerning their fidelity to the models'\npredictions. In contrast, Self-eXplainable AI (S-XAI) offers a compelling\nalternative by incorporating explainability directly into the training process\nof deep learning models. This approach allows models to generate inherent\nexplanations that are closely aligned with their internal decision-making\nprocesses. Such enhanced transparency significantly supports the\ntrustworthiness, robustness, and accountability of AI systems in real-world\nmedical applications. To facilitate the development of S-XAI methods for\nmedical image analysis, this survey presents an comprehensive review across\nvarious image modalities and clinical applications. It covers more than 200\npapers from three key perspectives: 1) input explainability through the\nintegration of explainable feature engineering and knowledge graph, 2) model\nexplainability via attention-based learning, concept-based learning, and\nprototype-based learning, and 3) output explainability by providing\ncounterfactual explanation and textual explanation. Additionally, this paper\noutlines the desired characteristics of explainability and existing evaluation\nmethods for assessing explanation quality. Finally, it discusses the major\nchallenges and future research directions in developing S-XAI for medical image\nanalysis.\n","authors":["Junlin Hou","Sicen Liu","Yequan Bie","Hongmei Wang","Andong Tan","Luyang Luo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16050v2","updated":"2024-10-03T09:24:56Z","published":"2024-02-25T10:27:46Z","title":"Efficient Temporal Extrapolation of Multimodal Large Language Models\n  with Temporal Grounding Bridge","summary":"  Despite progress in multimodal large language models (MLLMs), the challenge\nof interpreting long-form videos in response to linguistic queries persists,\nlargely due to the inefficiency in temporal grounding and limited pre-trained\ncontext window size. In this work, we introduce Temporal Grounding Bridge\n(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding\ncapabilities and broadens their contextual scope. Our framework significantly\nenhances the temporal capabilities of current MLLMs through three key\ninnovations: an efficient multi-span temporal grounding algorithm applied to\nlow-dimension temporal features projected from flow; a multimodal length\nextrapolation training paradigm that utilizes low-dimension temporal features\nto extend the training context window size; and a bootstrapping framework that\nbridges our model with pluggable MLLMs without requiring annotation. We\nvalidate TGB across seven video benchmarks and demonstrate substantial\nperformance improvements compared with prior MLLMs. Notably, our model,\ninitially trained on sequences of four frames, effectively handles sequences up\nto 16 longer without sacrificing performance, highlighting its scalability and\neffectiveness in real-world applications. Our code is publicly available at\nhttps://github.com/bigai-nlco/VideoTGB\n","authors":["Yuxuan Wang","Yueqian Wang","Pengfei Wu","Jianxin Liang","Dongyan Zhao","Yang Liu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.16050v2.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2311.10794v2","updated":"2024-10-03T09:10:47Z","published":"2023-11-17T03:00:29Z","title":"Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human\n  Expression","summary":"  We introduce Style Tailoring, a recipe to finetune Latent Diffusion Models\n(LDMs) in a distinct domain with high visual quality, prompt alignment and\nscene diversity. We choose sticker image generation as the target domain, as\nthe images significantly differ from photorealistic samples typically generated\nby large-scale LDMs. We start with a competent text-to-image model, like Emu,\nand show that relying on prompt engineering with a photorealistic model to\ngenerate stickers leads to poor prompt alignment and scene diversity. To\novercome these drawbacks, we first finetune Emu on millions of sticker-like\nimages collected using weak supervision to elicit diversity. Next, we curate\nhuman-in-the-loop (HITL) Alignment and Style datasets from model generations,\nand finetune to improve prompt alignment and style alignment respectively.\nSequential finetuning on these datasets poses a tradeoff between better style\nalignment and prompt alignment gains. To address this tradeoff, we propose a\nnovel fine-tuning method called Style Tailoring, which jointly fits the content\nand style distribution and achieves best tradeoff. Evaluation results show our\nmethod improves visual quality by 14%, prompt alignment by 16.2% and scene\ndiversity by 15.3%, compared to prompt engineering the base Emu model for\nstickers generation.\n","authors":["Animesh Sinha","Bo Sun","Anmol Kalia","Arantxa Casanova","Elliot Blanchard","David Yan","Winnie Zhang","Tony Nelli","Jiahui Chen","Hardik Shah","Licheng Yu","Mitesh Kumar Singh","Ankit Ramchandani","Maziar Sanjabi","Sonal Gupta","Amy Bearman","Dhruv Mahajan"],"pdf_url":"https://arxiv.org/pdf/2311.10794v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.02323v1","updated":"2024-10-03T09:10:42Z","published":"2024-10-03T09:10:42Z","title":"RESSCAL3D++: Joint Acquisition and Semantic Segmentation of 3D Point\n  Clouds","summary":"  3D scene understanding is crucial for facilitating seamless interaction\nbetween digital devices and the physical world. Real-time capturing and\nprocessing of the 3D scene are essential for achieving this seamless\nintegration. While existing approaches typically separate acquisition and\nprocessing for each frame, the advent of resolution-scalable 3D sensors offers\nan opportunity to overcome this paradigm and fully leverage the otherwise\nwasted acquisition time to initiate processing. In this study, we introduce\nVX-S3DIS, a novel point cloud dataset accurately simulating the behavior of a\nresolution-scalable 3D sensor. Additionally, we present RESSCAL3D++, an\nimportant improvement over our prior work, RESSCAL3D, by incorporating an\nupdate module and processing strategy. By applying our method to the new\ndataset, we practically demonstrate the potential of joint acquisition and\nsemantic segmentation of 3D point clouds. Our resolution-scalable approach\nsignificantly reduces scalability costs from 2% to just 0.2% in mIoU while\nachieving impressive speed-ups of 15.6 to 63.9% compared to the non-scalable\nbaseline. Furthermore, our scalable approach enables early predictions, with\nthe first one occurring after only 7% of the total inference time of the\nbaseline. The new VX-S3DIS dataset is available at\nhttps://github.com/remcoroyen/vx-s3dis.\n","authors":["Remco Royen","Kostas Pataridis","Ward van der Tempel","Adrian Munteanu"],"pdf_url":"https://arxiv.org/pdf/2410.02323v1.pdf","comment":"2024 IEEE International Conference on Image Processing (ICIP). IEEE,\n  2024"},{"id":"http://arxiv.org/abs/2105.14711v4","updated":"2024-10-03T09:08:20Z","published":"2021-05-31T05:34:27Z","title":"CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in\n  Computed Tomography","summary":"  Spine-related diseases have high morbidity and cause a huge burden of social\ncost. Spine imaging is an essential tool for noninvasively visualizing and\nassessing spinal pathology. Segmenting vertebrae in computed tomography (CT)\nimages is the basis of quantitative medical image analysis for clinical\ndiagnosis and surgery planning of spine diseases. Current publicly available\nannotated datasets on spinal vertebrae are small in size. Due to the lack of a\nlarge-scale annotated spine image dataset, the mainstream deep learning-based\nsegmentation methods, which are data-driven, are heavily restricted. In this\npaper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated\nfrom multiple sources for vertebra segmentation, which contains 1,005 CT\nvolumes with over 11,100 labeled vertebrae belonging to different spinal\nconditions. Based on this dataset, we conduct several spinal vertebrae\nsegmentation experiments to set the first benchmark. We believe that this\nlarge-scale dataset will facilitate further research in many spine-related\nimage analysis tasks, including but not limited to vertebrae segmentation,\nlabeling, 3D spine reconstruction from biplanar radiographs, image\nsuper-resolution, and enhancement.\n","authors":["Yang Deng","Ce Wang","Yuan Hui","Qian Li","Jun Li","Shiwei Luo","Mengke Sun","Quan Quan","Shuxin Yang","You Hao","Pengbo Liu","Honghu Xiao","Chunpeng Zhao","Xinbao Wu","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2105.14711v4.pdf","comment":"Accepted by MICCAI2024 Open Data for oral presentation and will be\n  published as a part of the journal MELBA special issue"},{"id":"http://arxiv.org/abs/2405.09589v4","updated":"2024-10-03T09:00:35Z","published":"2024-05-15T10:16:25Z","title":"A Comprehensive Survey of Hallucination in Large Language, Image, Video\n  and Audio Foundation Models","summary":"  The rapid advancement of foundation models (FMs) across language, image,\naudio, and video domains has shown remarkable capabilities in diverse tasks.\nHowever, the proliferation of FMs brings forth a critical challenge: the\npotential to generate hallucinated outputs, particularly in high-stakes\napplications. The tendency of foundation models to produce hallucinated content\narguably represents the biggest hindrance to their widespread adoption in\nreal-world scenarios, especially in domains where reliability and accuracy are\nparamount. This survey paper presents a comprehensive overview of recent\ndevelopments that aim to identify and mitigate the problem of hallucination in\nFMs, spanning text, image, video, and audio modalities. By synthesizing recent\nadvancements in detecting and mitigating hallucination across various\nmodalities, the paper aims to provide valuable insights for researchers,\ndevelopers, and practitioners. Essentially, it establishes a clear framework\nencompassing definition, taxonomy, and detection strategies for addressing\nhallucination in multimodal foundation models, laying the foundation for future\nresearch in this pivotal area.\n","authors":["Pranab Sahoo","Prabhash Meharia","Akash Ghosh","Sriparna Saha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2405.09589v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.15735v2","updated":"2024-10-03T08:52:55Z","published":"2024-06-22T04:56:16Z","title":"Identifying and Solving Conditional Image Leakage in Image-to-Video\n  Diffusion Model","summary":"  Diffusion models have obtained substantial progress in image-to-video\ngeneration. However, in this paper, we find that these models tend to generate\nvideos with less motion than expected. We attribute this to the issue called\nconditional image leakage, where the image-to-video diffusion models (I2V-DMs)\ntend to over-rely on the conditional image at large time steps. We further\naddress this challenge from both inference and training aspects. First, we\npropose to start the generation process from an earlier time step to avoid the\nunreliable large-time steps of I2V-DMs, as well as an initial noise\ndistribution with optimal analytic expressions (Analytic-Init) by minimizing\nthe KL divergence between it and the actual marginal distribution to bridge the\ntraining-inference gap. Second, we design a time-dependent noise distribution\n(TimeNoise) for the conditional image during training, applying higher noise\nlevels at larger time steps to disrupt it and reduce the model's dependency on\nit. We validate these general strategies on various I2V-DMs on our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results show that\nour methods outperform baselines by producing higher motion scores with lower\nerrors while maintaining image alignment and temporal consistency, thereby\nyielding superior overall performance and enabling more accurate motion\ncontrol. The project page: \\url{https://cond-image-leak.github.io/}.\n","authors":["Min Zhao","Hongzhou Zhu","Chendong Xiang","Kaiwen Zheng","Chongxuan Li","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.15735v2.pdf","comment":"NeurIPS 2024. Project page: https://cond-image-leak.github.io/"},{"id":"http://arxiv.org/abs/2410.02316v1","updated":"2024-10-03T08:52:21Z","published":"2024-10-03T08:52:21Z","title":"CTARR: A fast and robust method for identifying anatomical regions on CT\n  images via atlas registration","summary":"  Medical image analysis tasks often focus on regions or structures located in\na particular location within the patient's body. Often large parts of the image\nmay not be of interest for the image analysis task. When using deep-learning\nbased approaches, this causes an unnecessary increases the computational burden\nduring inference and raises the chance of errors. In this paper, we introduce\nCTARR, a novel generic method for CT Anatomical Region Recognition. The method\nserves as a pre-processing step for any deep learning-based CT image analysis\npipeline by automatically identifying the pre-defined anatomical region that is\nrelevant for the follow-up task and removing the rest. It can be used in (i)\nimage segmentation to prevent false positives in anatomically implausible\nregions and speeding up the inference, (ii) image classification to produce\nimage crops that are consistent in their anatomical context, and (iii) image\nregistration by serving as a fast pre-registration step. Our proposed method is\nbased on atlas registration and provides a fast and robust way to crop any\nanatomical region encoded as one or multiple bounding box(es) from any\nunlabeled CT scan of the brain, chest, abdomen and/or pelvis. We demonstrate\nthe utility and robustness of the proposed method in the context of medical\nimage segmentation by evaluating it on six datasets of public segmentation\nchallenges. The foreground voxels in the regions of interest are preserved in\nthe vast majority of cases and tasks (97.45-100%) while taking only fractions\nof a seconds to compute (0.1-0.21s) on a deep learning workstation and greatly\nreducing the segmentation runtime (2.0-12.7x). Our code is available at\nhttps://github.com/ThomasBudd/ctarr.\n","authors":["Thomas Buddenkotte","Roland Opfer","Julia Krger","Alessa Hering","Mireia Crispin-Ortuzar"],"pdf_url":"https://arxiv.org/pdf/2410.02316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02309v1","updated":"2024-10-03T08:46:17Z","published":"2024-10-03T08:46:17Z","title":"Decoupling Layout from Glyph in Online Chinese Handwriting Generation","summary":"  Text plays a crucial role in the transmission of human civilization, and\nteaching machines to generate online handwritten text in various styles\npresents an interesting and significant challenge. However, most prior work has\nconcentrated on generating individual Chinese fonts, leaving {complete text\nline generation largely unexplored}. In this paper, we identify that text lines\ncan naturally be divided into two components: layout and glyphs. Based on this\ndivision, we designed a text line layout generator coupled with a\ndiffusion-based stylized font synthesizer to address this challenge\nhierarchically. More concretely, the layout generator performs in-context-like\nlearning based on the text content and the provided style references to\ngenerate positions for each glyph autoregressively. Meanwhile, the font\nsynthesizer which consists of a character embedding dictionary, a multi-scale\ncalligraphy style encoder, and a 1D U-Net based diffusion denoiser will\ngenerate each font on its position while imitating the calligraphy style\nextracted from the given style references. Qualitative and quantitative\nexperiments on the CASIA-OLHWDB demonstrate that our method is capable of\ngenerating structurally correct and indistinguishable imitation samples.\n","authors":["Ren-Min Si","Yan-Ming Zhang","Yi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02305v1","updated":"2024-10-03T08:40:14Z","published":"2024-10-03T08:40:14Z","title":"The Comparison of Individual Cat Recognition Using Neural Networks","summary":"  Facial recognition using deep learning has been widely used in social life\nfor applications such as authentication, smart door locks, and photo grouping,\netc. More and more networks have been developed to facilitate computer vision\ntasks, such as ResNet, DenseNet, EfficientNet, ConvNeXt, and Siamese networks.\nHowever, few studies have systematically compared the advantages and\ndisadvantages of such neural networks in identifying individuals from images,\nespecially for pet animals like cats. In the present study, by systematically\ncomparing the efficacy of different neural networks in cat recognition, we\nfound traditional CNNs trained with transfer learning have better performance\nthan models trained with the fine-tuning method or Siamese networks in\nindividual cat recognition. In addition, ConvNeXt and DenseNet yield\nsignificant results which could be further optimized for individual cat\nrecognition in pet stores and in the wild. These results provide a method to\nimprove cat management in pet stores and monitoring of cats in the wild.\n","authors":["Mingxuan Li","Kai Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.02305v1.pdf","comment":"13 pages,7 figures"},{"id":"http://arxiv.org/abs/2410.02304v1","updated":"2024-10-03T08:39:06Z","published":"2024-10-03T08:39:06Z","title":"A Novel Method for Accurate & Real-time Food Classification: The\n  Synergistic Integration of EfficientNetB7, CBAM, Transfer Learning, and Data\n  Augmentation","summary":"  Integrating artificial intelligence into modern society is profoundly\ntransformative, significantly enhancing productivity by streamlining various\ndaily tasks. AI-driven recognition systems provide notable advantages in the\nfood sector, including improved nutrient tracking, tackling food waste, and\nboosting food production and consumption efficiency. Accurate food\nclassification is a crucial initial step in utilizing advanced AI models, as\nthe effectiveness of this process directly influences the success of subsequent\noperations; therefore, achieving high accuracy at a reasonable speed is\nessential. Despite existing research efforts, a gap persists in improving\nperformance while ensuring rapid processing times, prompting researchers to\npursue cost-effective and precise models. This study addresses this gap by\nemploying the state-of-the-art EfficientNetB7 architecture, enhanced through\ntransfer learning, data augmentation, and the CBAM attention module. This\nmethodology results in a robust model that surpasses previous studies in\naccuracy while maintaining rapid processing suitable for real-world\napplications. The Food11 dataset from Kaggle was utilized, comprising 16643\nimbalanced images across 11 diverse classes with significant intra-category\ndiversities and inter-category similarities. Furthermore, the proposed\nmethodology, bolstered by various deep learning techniques, consistently\nachieves an impressive average accuracy of 96.40%. Notably, it can classify\nover 60 images within one second during inference on unseen data, demonstrating\nits ability to deliver high accuracy promptly. This underscores its potential\nfor practical applications in accurate food classification and enhancing\nefficiency in subsequent processes.\n","authors":["Shayan Rokhva","Babak Teimourpour"],"pdf_url":"https://arxiv.org/pdf/2410.02304v1.pdf","comment":"20 pages, six figures, two tables"},{"id":"http://arxiv.org/abs/2406.00093v2","updated":"2024-10-03T08:20:17Z","published":"2024-05-31T17:59:56Z","title":"Bootstrap3D: Improving Multi-view Diffusion Model with Synthetic Data","summary":"  Recent years have witnessed remarkable progress in multi-view diffusion\nmodels for 3D content creation. However, there remains a significant gap in\nimage quality and prompt-following ability compared to 2D diffusion models. A\ncritical bottleneck is the scarcity of high-quality 3D objects with detailed\ncaptions. To address this challenge, we propose Bootstrap3D, a novel framework\nthat automatically generates an arbitrary quantity of multi-view images to\nassist in training multi-view diffusion models. Specifically, we introduce a\ndata generation pipeline that employs (1) 2D and video diffusion models to\ngenerate multi-view images based on constructed text prompts, and (2) our\nfine-tuned 3D-aware MV-LLaVA for filtering high-quality data and rewriting\ninaccurate captions. Leveraging this pipeline, we have generated 1 million\nhigh-quality synthetic multi-view images with dense descriptive captions to\naddress the shortage of high-quality 3D data. Furthermore, we present a\nTraining Timestep Reschedule (TTR) strategy that leverages the denoising\nprocess to learn multi-view consistency while maintaining the original 2D\ndiffusion prior. Extensive experiments demonstrate that Bootstrap3D can\ngenerate high-quality multi-view images with superior aesthetic quality,\nimage-text alignment, and maintained view consistency.\n","authors":["Zeyi Sun","Tong Wu","Pan Zhang","Yuhang Zang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.00093v2.pdf","comment":"Project Page: https://sunzey.github.io/Bootstrap3D/"},{"id":"http://arxiv.org/abs/2410.02288v1","updated":"2024-10-03T08:13:26Z","published":"2024-10-03T08:13:26Z","title":"Computer-aided Colorization State-of-the-science: A Survey","summary":"  This paper reviews published research in the field of computer-aided\ncolorization technology. We argue that the colorization task originates from\ncomputer graphics, prospers by introducing computer vision, and tends to the\nfusion of vision and graphics, so we put forward our taxonomy and organize the\nwhole paper chronologically. We extend the existing reconstruction-based\ncolorization evaluation techniques, considering that aesthetic assessment of\ncolored images should be introduced to ensure that colorization satisfies human\nvisual-related requirements and emotions more closely. We perform the\ncolorization aesthetic assessment on seven representative unconditional\ncolorization models and discuss the difference between our assessment and the\nexisting reconstruction-based metrics. Finally, this paper identifies\nunresolved issues and proposes fruitful areas for future research and\ndevelopment. Access to the project associated with this survey can be obtained\nat https://github.com/DanielCho-HK/Colorization.\n","authors":["Yu Cao","Xin Duan","Xiangqiao Meng","P. Y. Mok","Ping Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02268v1","updated":"2024-10-03T07:40:14Z","published":"2024-10-03T07:40:14Z","title":"Structural-Entropy-Based Sample Selection for Efficient and Effective\n  Learning","summary":"  Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.\n","authors":["Tianchi Xie","Jiangning Zhu","Guozu Ma","Minzhi Lin","Wei Chen","Weikai Yang","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02268v1.pdf","comment":"Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2404.19460v2","updated":"2024-10-03T07:07:25Z","published":"2024-04-30T11:19:05Z","title":"AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples","summary":"  Adversarial examples are typically optimized with gradient-based attacks.\nWhile novel attacks are continuously proposed, each is shown to outperform its\npredecessors using different experimental setups, hyperparameter settings, and\nnumber of forward and backward calls to the target models. This provides\noverly-optimistic and even biased evaluations that may unfairly favor one\nparticular attack over the others. In this work, we aim to overcome these\nlimitations by proposing AttackBench, i.e., the first evaluation framework that\nenables a fair comparison among different attacks. To this end, we first\npropose a categorization of gradient-based attacks, identifying their main\ncomponents and differences. We then introduce our framework, which evaluates\ntheir effectiveness and efficiency. We measure these characteristics by (i)\ndefining an optimality metric that quantifies how close an attack is to the\noptimal solution, and (ii) limiting the number of forward and backward queries\nto the model, such that all attacks are compared within a given maximum query\nbudget. Our extensive experimental analysis compares more than $100$ attack\nimplementations with a total of over $800$ different configurations against\nCIFAR-10 and ImageNet models, highlighting that only very few attacks\noutperform all the competing approaches. Within this analysis, we shed light on\nseveral implementation issues that prevent many attacks from finding better\nsolutions or running at all. We release AttackBench as a publicly-available\nbenchmark, aiming to continuously update it to include and evaluate novel\ngradient-based attacks for optimizing adversarial examples.\n","authors":["Antonio Emanuele Cin","Jrme Rony","Maura Pintor","Luca Demetrio","Ambra Demontis","Battista Biggio","Ismail Ben Ayed","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2404.19460v2.pdf","comment":"https://attackbench.github.io"},{"id":"http://arxiv.org/abs/2410.02250v1","updated":"2024-10-03T06:43:09Z","published":"2024-10-03T06:43:09Z","title":"Probabilistic road classification in historical maps using synthetic\n  data and deep learning","summary":"  Historical maps are invaluable for analyzing long-term changes in\ntransportation and spatial development, offering a rich source of data for\nevolutionary studies. However, digitizing and classifying road networks from\nthese maps is often expensive and time-consuming, limiting their widespread\nuse. Recent advancements in deep learning have made automatic road extraction\nfrom historical maps feasible, yet these methods typically require large\namounts of labeled training data. To address this challenge, we introduce a\nnovel framework that integrates deep learning with geoinformation,\ncomputer-based painting, and image processing methodologies. This framework\nenables the extraction and classification of roads from historical maps using\nonly road geometries without needing road class labels for training. The\nprocess begins with training of a binary segmentation model to extract road\ngeometries, followed by morphological operations, skeletonization,\nvectorization, and filtering algorithms. Synthetic training data is then\ngenerated by a painting function that artificially re-paints road segments\nusing predefined symbology for road classes. Using this synthetic data, a deep\nensemble is trained to generate pixel-wise probabilities for road classes to\nmitigate distribution shift. These predictions are then discretized along the\nextracted road geometries. Subsequently, further processing is employed to\nclassify entire roads, enabling the identification of potential changes in road\nclasses and resulting in a labeled road class dataset. Our method achieved\ncompleteness and correctness scores of over 94% and 92%, respectively, for road\nclass 2, the most prevalent class in the two Siegfried Map sheets from\nSwitzerland used for testing. This research offers a powerful tool for urban\nplanning and transportation decision-making by efficiently extracting and\nclassifying roads from historical maps.\n","authors":["Dominik J. Mhlematter","Sebastian Schweizer","Chenjing Jiao","Xue Xia","Magnus Heitzler","Lorenz Hurni"],"pdf_url":"https://arxiv.org/pdf/2410.02250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02249v1","updated":"2024-10-03T06:41:10Z","published":"2024-10-03T06:41:10Z","title":"Spiking Neural Network as Adaptive Event Stream Slicer","summary":"  Event-based cameras are attracting significant interest as they provide rich\nedge information, high dynamic range, and high temporal resolution. Many\nstate-of-the-art event-based algorithms rely on splitting the events into fixed\ngroups, resulting in the omission of crucial temporal information, particularly\nwhen dealing with diverse motion scenarios (e.g., high/low speed). In this\nwork, we propose SpikeSlicer, a novel-designed plug-and-play event processing\nmethod capable of splitting events stream adaptively. SpikeSlicer utilizes a\nlightweight (0.41M) and low-energy spiking neural network (SNN) to trigger\nevent slicing. To guide the SNN to fire spikes at optimal time steps, we\npropose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's\nstate. Additionally, we develop a Feedback-Update training strategy that\nrefines the slicing decisions using feedback from the downstream artificial\nneural network (ANN). Extensive experiments demonstrate that our method yields\nsignificant performance improvements in event-based object tracking and\nrecognition. Notably, SpikeSlicer provides a brand-new SNN-ANN cooperation\nparadigm, where the SNN acts as an efficient, low-energy data processor to\nassist the ANN in improving downstream performance, injecting new perspectives\nand potential avenues of exploration.\n","authors":["Jiahang Cao","Mingyuan Sun","Ziqing Wang","Hao Cheng","Qiang Zhang","Shibo Zhou","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02249v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.02230v2","updated":"2024-10-03T06:36:14Z","published":"2024-06-04T11:48:44Z","title":"I4VGen: Image as Free Stepping Stone for Text-to-Video Generation","summary":"  Text-to-video generation has trailed behind text-to-image generation in terms\nof quality and diversity, primarily due to the inherent complexities of\nspatio-temporal modeling and the limited availability of video-text datasets.\nRecent text-to-video diffusion models employ the image as an intermediate step,\nsignificantly enhancing overall performance but incurring high training costs.\nIn this paper, we present I4VGen, a novel video diffusion inference pipeline to\nleverage advanced image techniques to enhance pre-trained text-to-video\ndiffusion models, which requires no additional training. Instead of the vanilla\ntext-to-video inference pipeline, I4VGen consists of two stages: anchor image\nsynthesis and anchor image-augmented text-to-video synthesis. Correspondingly,\na simple yet effective generation-selection strategy is employed to achieve\nvisually-realistic and semantically-faithful anchor image, and an innovative\nnoise-invariant video score distillation sampling (NI-VSDS) is developed to\nanimate the image to a dynamic video by distilling motion knowledge from video\ndiffusion models, followed by a video regeneration process to refine the video.\nExtensive experiments show that the proposed method produces videos with higher\nvisual realism and textual fidelity. Furthermore, I4VGen also supports being\nseamlessly integrated into existing image-to-video diffusion models, thereby\nimproving overall video quality.\n","authors":["Xiefan Guo","Jinlin Liu","Miaomiao Cui","Liefeng Bo","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2406.02230v2.pdf","comment":"Project page: https://xiefan-guo.github.io/i4vgen"},{"id":"http://arxiv.org/abs/2410.02244v1","updated":"2024-10-03T06:33:43Z","published":"2024-10-03T06:33:43Z","title":"Visual Prompting in LLMs for Enhancing Emotion Recognition","summary":"  Vision Large Language Models (VLLMs) are transforming the intersection of\ncomputer vision and natural language processing. Nonetheless, the potential of\nusing visual prompts for emotion recognition in these models remains largely\nunexplored and untapped. Traditional methods in VLLMs struggle with spatial\nlocalization and often discard valuable global context. To address this\nproblem, we propose a Set-of-Vision prompting (SoV) approach that enhances\nzero-shot emotion recognition by using spatial information, such as bounding\nboxes and facial landmarks, to mark targets precisely. SoV improves accuracy in\nface count and emotion categorization while preserving the enriched image\ncontext. Through a battery of experimentation and analysis of recent commercial\nor open-source VLLMs, we evaluate the SoV model's ability to comprehend facial\nexpressions in natural environments. Our findings demonstrate the effectiveness\nof integrating spatial visual prompts into VLLMs for improving emotion\nrecognition performance.\n","authors":["Qixuan Zhang","Zhifeng Wang","Dylan Zhang","Wenjia Niu","Sabrina Caldwell","Tom Gedeon","Yang Liu","Zhenyue Qin"],"pdf_url":"https://arxiv.org/pdf/2410.02244v1.pdf","comment":"Accepted by EMNLP2024 (Main, Long paper)"},{"id":"http://arxiv.org/abs/2409.18147v2","updated":"2024-10-03T06:29:20Z","published":"2024-09-25T02:41:58Z","title":"SSP-RACL: Classification of Noisy Fundus Images with Self-Supervised\n  Pretraining and Robust Adaptive Credal Loss","summary":"  Fundus image classification is crucial in the computer aided diagnosis tasks,\nbut label noise significantly impairs the performance of deep neural networks.\nTo address this challenge, we propose a robust framework, Self-Supervised\nPre-training with Robust Adaptive Credal Loss (SSP-RACL), for handling label\nnoise in fundus image datasets. First, we use Masked Autoencoders (MAE) for\npre-training to extract features, unaffected by label noise. Subsequently, RACL\nemploy a superset learning framework, setting confidence thresholds and\nadaptive label relaxation parameter to construct possibility distributions and\nprovide more reliable ground-truth estimates, thus effectively suppressing the\nmemorization effect. Additionally, we introduce clinical knowledge-based\nasymmetric noise generation to simulate real-world noisy fundus image datasets.\nExperimental results demonstrate that our proposed method outperforms existing\napproaches in handling label noise, showing superior performance.\n","authors":["Mengwen Ye","Yingzi Huangfu","You Li","Zekuan Yu"],"pdf_url":"https://arxiv.org/pdf/2409.18147v2.pdf","comment":"IEEE BioCAS 2024"},{"id":"http://arxiv.org/abs/2406.13527v3","updated":"2024-10-03T06:26:49Z","published":"2024-06-19T13:11:02Z","title":"4K4DGen: Panoramic 4D Generation at 4K Resolution","summary":"  The blooming of virtual reality and augmented reality (VR/AR) technologies\nhas driven an increasing demand for the creation of high-quality, immersive,\nand dynamic environments. However, existing generative techniques either focus\nsolely on dynamic objects or perform outpainting from a single perspective\nimage, failing to meet the requirements of VR/AR applications that need\nfree-viewpoint, 360$^{\\circ}$ virtual views where users can move in all\ndirections. In this work, we tackle the challenging task of elevating a single\npanorama to an immersive 4D experience. For the first time, we demonstrate the\ncapability to generate omnidirectional dynamic scenes with 360$^{\\circ}$ views\nat 4K (4096 $\\times$ 2048) resolution, thereby providing an immersive user\nexperience. Our method introduces a pipeline that facilitates natural scene\nanimations and optimizes a set of dynamic Gaussians using efficient splatting\ntechniques for real-time exploration. To overcome the lack of scene-scale\nannotated 4D data and models, especially in panoramic formats, we propose a\nnovel \\textbf{Panoramic Denoiser} that adapts generic 2D diffusion priors to\nanimate consistently in 360$^{\\circ}$ images, transforming them into panoramic\nvideos with dynamic scenes at targeted regions. Subsequently, we propose\n\\textbf{Dynamic Panoramic Lifting} to elevate the panoramic video into a 4D\nimmersive environment while preserving spatial and temporal consistency. By\ntransferring prior knowledge from 2D models in the perspective domain to the\npanoramic domain and the 4D lifting with spatial appearance and geometry\nregularization, we achieve high-quality Panorama-to-4D generation at a\nresolution of 4K for the first time.\n","authors":["Renjie Li","Panwang Pan","Bangbang Yang","Dejia Xu","Shijie Zhou","Xuanyang Zhang","Zeming Li","Achuta Kadambi","Zhangyang Wang","Zhengzhong Tu","Zhiwen Fan"],"pdf_url":"https://arxiv.org/pdf/2406.13527v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02240v1","updated":"2024-10-03T06:25:53Z","published":"2024-10-03T06:25:53Z","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial\n  Attack","summary":"  Unrestricted adversarial attacks typically manipulate the semantic content of\nan image (e.g., color or texture) to create adversarial examples that are both\neffective and photorealistic. Recent works have utilized the diffusion\ninversion process to map images into a latent space, where high-level semantics\nare manipulated by introducing perturbations. However, they often results in\nsubstantial semantic distortions in the denoised output and suffers from low\nefficiency. In this study, we propose a novel framework called\nSemantic-Consistent Unrestricted Adversarial Attacks (SCA), which employs an\ninversion method to extract edit-friendly noise maps and utilizes Multimodal\nLarge Language Model (MLLM) to provide semantic guidance throughout the\nprocess. Under the condition of rich semantic information provided by MLLM, we\nperform the DDPM denoising process of each step using a series of edit-friendly\nnoise maps, and leverage DPM Solver++ to accelerate this process, enabling\nefficient sampling with semantic consistency. Compared to existing methods, our\nframework enables the efficient generation of adversarial examples that exhibit\nminimal discernible semantic changes. Consequently, we for the first time\nintroduce Semantic-Consistent Adversarial Examples (SCAE). Extensive\nexperiments and visualizations have demonstrated the high efficiency of SCA,\nparticularly in being on average 12 times faster than the state-of-the-art\nattacks. Our code can be found at\nhttps://github.com/Pan-Zihao/SCA}{https://github.com/Pan-Zihao/SCA.\n","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.02240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02237v1","updated":"2024-10-03T06:16:50Z","published":"2024-10-03T06:16:50Z","title":"Key-Grid: Unsupervised 3D Keypoints Detection using Grid Heatmap\n  Features","summary":"  Detecting 3D keypoints with semantic consistency is widely used in many\nscenarios such as pose estimation, shape registration and robotics. Currently,\nmost unsupervised 3D keypoint detection methods focus on the rigid-body\nobjects. However, when faced with deformable objects, the keypoints they\nidentify do not preserve semantic consistency well. In this paper, we introduce\nan innovative unsupervised keypoint detector Key-Grid for both the rigid-body\nand deformable objects, which is an autoencoder framework. The encoder predicts\nkeypoints and the decoder utilizes the generated keypoints to reconstruct the\nobjects. Unlike previous work, we leverage the identified keypoint in formation\nto form a 3D grid feature heatmap called grid heatmap, which is used in the\ndecoder section. Grid heatmap is a novel concept that represents the latent\nvariables for grid points sampled uniformly in the 3D cubic space, where these\nvariables are the shortest distance between the grid points and the skeleton\nconnected by keypoint pairs. Meanwhile, we incorporate the information from\neach layer of the encoder into the decoder section. We conduct an extensive\nevaluation of Key-Grid on a list of benchmark datasets. Key-Grid achieves the\nstate-of-the-art performance on the semantic consistency and position accuracy\nof keypoints. Moreover, we demonstrate the robustness of Key-Grid to noise and\ndownsampling. In addition, we achieve SE-(3) invariance of keypoints though\ngeneralizing Key-Grid to a SE(3)-invariant backbone.\n","authors":["Chengkai Hou","Zhengrong Xue","Bingyang Zhou","Jinghan Ke","Lin Shao","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02224v1","updated":"2024-10-03T05:45:24Z","published":"2024-10-03T05:45:24Z","title":"Efficient Semantic Segmentation via Lightweight Multiple-Information\n  Interaction Network","summary":"  Recently, the integration of the local modeling capabilities of Convolutional\nNeural Networks (CNNs) with the global dependency strengths of Transformers has\ncreated a sensation in the semantic segmentation community. However,\nsubstantial computational workloads and high hardware memory demands remain\nmajor obstacles to their further application in real-time scenarios. In this\nwork, we propose a lightweight multiple-information interaction network for\nreal-time semantic segmentation, called LMIINet, which effectively combines\nCNNs and Transformers while reducing redundant computations and memory\nfootprint. It features Lightweight Feature Interaction Bottleneck (LFIB)\nmodules comprising efficient convolutions that enhance context integration.\nAdditionally, improvements are made to the Flatten Transformer by enhancing\nlocal and global feature interaction to capture detailed semantic information.\nThe incorporation of a combination coefficient learning scheme in both LFIB and\nTransformer blocks facilitates improved feature interaction. Extensive\nexperiments demonstrate that LMIINet excels in balancing accuracy and\nefficiency. With only 0.72M parameters and 11.74G FLOPs, LMIINet achieves 72.0%\nmIoU at 100 FPS on the Cityscapes test set and 69.94% mIoU at 160 FPS on the\nCamVid test dataset using a single RTX2080Ti GPU.\n","authors":["Yangyang Qiu","Guoan Xu","Guangwei Gao","Zhenhua Guo","Yi Yu","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2410.02224v1.pdf","comment":"10 pages, 6 figures, 9 tables"},{"id":"http://arxiv.org/abs/2410.02221v1","updated":"2024-10-03T05:32:16Z","published":"2024-10-03T05:32:16Z","title":"Capturing complex hand movements and object interactions using machine\n  learning-powered stretchable smart textile gloves","summary":"  Accurate real-time tracking of dexterous hand movements and interactions has\nnumerous applications in human-computer interaction, metaverse, robotics, and\ntele-health. Capturing realistic hand movements is challenging because of the\nlarge number of articulations and degrees of freedom. Here, we report accurate\nand dynamic tracking of articulated hand and finger movements using\nstretchable, washable smart gloves with embedded helical sensor yarns and\ninertial measurement units. The sensor yarns have a high dynamic range,\nresponding to low 0.005 % to high 155 % strains, and show stability during\nextensive use and washing cycles. We use multi-stage machine learning to report\naverage joint angle estimation root mean square errors of 1.21 and 1.45 degrees\nfor intra- and inter-subjects cross-validation, respectively, matching accuracy\nof costly motion capture cameras without occlusion or field of view\nlimitations. We report a data augmentation technique that enhances robustness\nto noise and variations of sensors. We demonstrate accurate tracking of\ndexterous hand movements during object interactions, opening new avenues of\napplications including accurate typing on a mock paper keyboard, recognition of\ncomplex dynamic and static gestures adapted from American Sign Language and\nobject identification.\n","authors":["Arvin Tashakori","Zenan Jiang","Amir Servati","Saeid Soltanian","Harishkumar Narayana","Katherine Le","Caroline Nakayama","Chieh-ling Yang","Z. Jane Wang","Janice J. Eng","Peyman Servati"],"pdf_url":"https://arxiv.org/pdf/2410.02221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02217v1","updated":"2024-10-03T05:18:28Z","published":"2024-10-03T05:18:28Z","title":"Stochastic Sampling from Deterministic Flow Models","summary":"  Deterministic flow models, such as rectified flows, offer a general framework\nfor learning a deterministic transport map between two distributions, realized\nas the vector field for an ordinary differential equation (ODE). However, they\nare sensitive to model estimation and discretization errors and do not permit\ndifferent samples conditioned on an intermediate state, limiting their\napplication. We present a general method to turn the underlying ODE of such\nflow models into a family of stochastic differential equations (SDEs) that have\nthe same marginal distributions. This method permits us to derive families of\n\\emph{stochastic samplers}, for fixed (e.g., previously trained)\n\\emph{deterministic} flow models, that continuously span the spectrum of\ndeterministic and stochastic sampling, given access to the flow field and the\nscore function. Our method provides additional degrees of freedom that help\nalleviate the issues with the deterministic samplers and empirically\noutperforms them. We empirically demonstrate advantages of our method on a toy\nGaussian setup and on the large scale ImageNet generation task. Further, our\nfamily of stochastic samplers provide an additional knob for controlling the\ndiversity of generation, which we qualitatively demonstrate in our experiments.\n","authors":["Saurabh Singh","Ian Fischer"],"pdf_url":"https://arxiv.org/pdf/2410.02217v1.pdf","comment":"Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.02212v1","updated":"2024-10-03T04:58:33Z","published":"2024-10-03T04:58:33Z","title":"Hard Negative Sample Mining for Whole Slide Image Classification","summary":"  Weakly supervised whole slide image (WSI) classification is challenging due\nto the lack of patch-level labels and high computational costs.\nState-of-the-art methods use self-supervised patch-wise feature representations\nfor multiple instance learning (MIL). Recently, methods have been proposed to\nfine-tune the feature representation on the downstream task using pseudo\nlabeling, but mostly focusing on selecting high-quality positive patches. In\nthis paper, we propose to mine hard negative samples during fine-tuning. This\nallows us to obtain better feature representations and reduce the training\ncost. Furthermore, we propose a novel patch-wise ranking loss in MIL to better\nexploit these hard negative samples. Experiments on two public datasets\ndemonstrate the efficacy of these proposed ideas. Our codes are available at\nhttps://github.com/winston52/HNM-WSI\n","authors":["Wentao Huang","Xiaoling Hu","Shahira Abousamra","Prateek Prasanna","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02212v1.pdf","comment":"13 pages, 4 figures, accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.02207v1","updated":"2024-10-03T04:40:18Z","published":"2024-10-03T04:40:18Z","title":"Adapting Segment Anything Model to Melanoma Segmentation in Microscopy\n  Slide Images","summary":"  Melanoma segmentation in Whole Slide Images (WSIs) is useful for prognosis\nand the measurement of crucial prognostic factors such as Breslow depth and\nprimary invasive tumor size. In this paper, we present a novel approach that\nuses the Segment Anything Model (SAM) for automatic melanoma segmentation in\nmicroscopy slide images. Our method employs an initial semantic segmentation\nmodel to generate preliminary segmentation masks that are then used to prompt\nSAM. We design a dynamic prompting strategy that uses a combination of centroid\nand grid prompts to achieve optimal coverage of the super high-resolution slide\nimages while maintaining the quality of generated prompts. To optimize for\ninvasive melanoma segmentation, we further refine the prompt generation process\nby implementing in-situ melanoma detection and low-confidence region filtering.\nWe select Segformer as the initial segmentation model and EfficientSAM as the\nsegment anything model for parameter-efficient fine-tuning. Our experimental\nresults demonstrate that this approach not only surpasses other\nstate-of-the-art melanoma segmentation methods but also significantly\noutperforms the baseline Segformer by 9.1% in terms of IoU.\n","authors":["Qingyuan Liu","Avideh Zakhor"],"pdf_url":"https://arxiv.org/pdf/2410.02207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02201v1","updated":"2024-10-03T04:32:21Z","published":"2024-10-03T04:32:21Z","title":"Remember and Recall: Associative-Memory-based Trajectory Prediction","summary":"  Trajectory prediction is a pivotal component of autonomous driving systems,\nenabling the application of accumulated movement experience to current\nscenarios. Although most existing methods concentrate on learning continuous\nrepresentations to gain valuable experience, they often suffer from\ncomputational inefficiencies and struggle with unfamiliar situations. To\naddress this issue, we propose the Fragmented-Memory-based Trajectory\nPrediction (FMTP) model, inspired by the remarkable learning capabilities of\nhumans, particularly their ability to leverage accumulated experience and\nrecall relevant memories in unfamiliar situations. The FMTP model employs\ndiscrete representations to enhance computational efficiency by reducing\ninformation redundancy while maintaining the flexibility to utilize past\nexperiences. Specifically, we design a learnable memory array by consolidating\ncontinuous trajectory representations from the training set using defined\nquantization operations during the training phase. This approach further\neliminates redundant information while preserving essential features in\ndiscrete form. Additionally, we develop an advanced reasoning engine based on\nlanguage models to deeply learn the associative rules among these discrete\nrepresentations. Our method has been evaluated on various public datasets,\nincluding ETH-UCY, inD, SDD, nuScenes, Waymo, and VTL-TP. The extensive\nexperimental results demonstrate that our approach achieves significant\nperformance and extracts more valuable experience from past trajectories to\ninform the current state.\n","authors":["Hang Guo","Yuzhen Zhang","Tianci Gao","Junning Su","Pei Lv","Mingliang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03717v2","updated":"2024-10-03T04:09:33Z","published":"2024-08-07T12:10:32Z","title":"Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss\n  Trade-Offs via Selective Rank-Aware Attention","summary":"  Infrared small target detection faces the inherent challenge of precisely\nlocalizing dim targets amidst complex background clutter. Traditional\napproaches struggle to balance detection precision and false alarm rates. To\nbreak this dilemma, we propose SeRankDet, a deep network that achieves high\naccuracy beyond the conventional hit-miss trade-off, by following the ``Pick of\nthe Bunch'' principle. At its core lies our Selective Rank-Aware Attention\n(SeRank) module, employing a non-linear Top-K selection process that preserves\nthe most salient responses, preventing target signal dilution while maintaining\nconstant complexity. Furthermore, we replace the static concatenation typical\nin U-Net structures with our Large Selective Feature Fusion (LSFF) module, a\ndynamic fusion strategy that empowers SeRankDet with adaptive feature\nintegration, enhancing its ability to discriminate true targets from false\nalarms. The network's discernment is further refined by our Dilated Difference\nConvolution (DDC) module, which merges differential convolution aimed at\namplifying subtle target characteristics with dilated convolution to expand the\nreceptive field, thereby substantially improving target-background separation.\nDespite its lightweight architecture, the proposed SeRankDet sets new\nbenchmarks in state-of-the-art performance across multiple public datasets. The\ncode is available at https://github.com/GrokCV/SeRankDet.\n","authors":["Yimian Dai","Peiwen Pan","Yulei Qian","Yuxuan Li","Xiang Li","Jian Yang","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03717v2.pdf","comment":"IEEE TGRS 2024"},{"id":"http://arxiv.org/abs/2309.04148v3","updated":"2024-10-03T03:55:04Z","published":"2023-09-08T06:24:44Z","title":"Representation Synthesis by Probabilistic Many-Valued Logic Operation in\n  Self-Supervised Learning","summary":"  In this paper, we propose a new self-supervised learning (SSL) method for\nrepresentations that enable logic operations. Representation learning has been\napplied to various tasks, such as image generation and retrieval. The logical\ncontrollability of representations is important for these tasks. Although some\nmethods have been shown to enable the intuitive control of representations\nusing natural languages as the inputs, representation control via logic\noperations between representations has not been demonstrated. Some SSL methods\nusing representation synthesis (e.g., elementwise mean and maximum operations)\nhave been proposed, but the operations performed in these methods do not\nincorporate logic operations. In this work, we propose a logic-operable\nself-supervised representation learning method by replacing the existing\nrepresentation synthesis with the OR operation on the probabilistic extension\nof many-valued logic. The representations comprise a set of feature-possession\ndegrees, which are truth values indicating the presence or absence of each\nfeature in the image, and realize the logic operations (e.g., OR and AND). Our\nmethod can generate a representation that has the features of both\nrepresentations or only those features common to both representations. In\naddition, the expression of the ambiguous presence of a feature is realized by\nindicating the feature-possession degree by the probability distribution of\ntruth values of the many-valued logic. We showed that our method performs\ncompetitively in single and multi-label classification tasks compared with\nprior SSL methods using synthetic representations. Moreover, experiments on\nimage retrieval using MNIST and PascalVOC showed that the representations of\nour method can be operated by OR and AND operations.\n","authors":["Hiroki Nakamura","Masashi Okada","Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2309.04148v3.pdf","comment":"Accepted to the IEEE Open Journal of Signal Processing (ICIP2024\n  track)"},{"id":"http://arxiv.org/abs/2410.02182v1","updated":"2024-10-03T03:51:53Z","published":"2024-10-03T03:51:53Z","title":"BadCM: Invisible Backdoor Attack Against Cross-Modal Learning","summary":"  Despite remarkable successes in unimodal learning tasks, backdoor attacks\nagainst cross-modal learning are still underexplored due to the limited\ngeneralization and inferior stealthiness when involving multiple modalities.\nNotably, since works in this area mainly inherit ideas from unimodal visual\nattacks, they struggle with dealing with diverse cross-modal attack\ncircumstances and manipulating imperceptible trigger samples, which hinders\ntheir practicability in real-world applications. In this paper, we introduce a\nnovel bilateral backdoor to fill in the missing pieces of the puzzle in the\ncross-modal backdoor and propose a generalized invisible backdoor framework\nagainst cross-modal learning (BadCM). Specifically, a cross-modal mining scheme\nis developed to capture the modality-invariant components as target poisoning\nareas, where well-designed trigger patterns injected into these regions can be\nefficiently recognized by the victim models. This strategy is adapted to\ndifferent image-text cross-modal models, making our framework available to\nvarious attack scenarios. Furthermore, for generating poisoned samples of high\nstealthiness, we conceive modality-specific generators for visual and\nlinguistic modalities that facilitate hiding explicit trigger patterns in\nmodality-invariant regions. To the best of our knowledge, BadCM is the first\ninvisible backdoor method deliberately designed for diverse cross-modal attacks\nwithin one unified framework. Comprehensive experimental evaluations on two\ntypical applications, i.e., cross-modal retrieval and VQA, demonstrate the\neffectiveness and generalization of our method under multiple kinds of attack\nscenarios. Moreover, we show that BadCM can robustly evade existing backdoor\ndefenses. Our code is available at https://github.com/xandery-geek/BadCM.\n","authors":["Zheng Zhang","Xu Yuan","Lei Zhu","Jingkuan Song","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.02182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14637v2","updated":"2024-10-03T03:51:22Z","published":"2023-10-23T07:21:40Z","title":"Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval","summary":"  Deep hashing has been intensively studied and successfully applied in\nlarge-scale image retrieval systems due to its efficiency and effectiveness.\nRecent studies have recognized that the existence of adversarial examples poses\na security threat to deep hashing models, that is, adversarial vulnerability.\nNotably, it is challenging to efficiently distill reliable semantic\nrepresentatives for deep hashing to guide adversarial learning, and thereby it\nhinders the enhancement of adversarial robustness of deep hashing-based\nretrieval models. Moreover, current researches on adversarial training for deep\nhashing are hard to be formalized into a unified minimax structure. In this\npaper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the\nadversarial robustness of deep hashing models. Specifically, we conceive a\ndiscriminative mainstay features learning (DMFL) scheme to construct semantic\nrepresentatives for guiding adversarial learning in deep hashing. Particularly,\nour DMFL with the strict theoretical guarantee is adaptively optimized in a\ndiscriminative learning manner, where both discriminative and semantic\nproperties are jointly considered. Moreover, adversarial examples are\nfabricated by maximizing the Hamming distance between the hash codes of\nadversarial samples and mainstay features, the efficacy of which is validated\nin the adversarial attack trials. Further, we, for the first time, formulate\nthe formalized adversarial training of deep hashing into a unified minimax\noptimization under the guidance of the generated mainstay codes. Extensive\nexperiments on benchmark datasets show superb attack performance against the\nstate-of-the-art algorithms, meanwhile, the proposed adversarial training can\neffectively eliminate adversarial perturbations for trustworthy deep\nhashing-based retrieval. Our code is available at\nhttps://github.com/xandery-geek/SAAT.\n","authors":["Xu Yuan","Zheng Zhang","Xunguang Wang","Lin Wu"],"pdf_url":"https://arxiv.org/pdf/2310.14637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03722v2","updated":"2024-10-03T03:46:05Z","published":"2023-09-07T13:58:31Z","title":"A boundary-aware point clustering approach in Euclidean and embedding\n  spaces for roof plane segmentation","summary":"  Roof plane segmentation from airborne LiDAR point clouds is an important\ntechnology for 3D building model reconstruction. One of the key issues of plane\nsegmentation is how to design powerful features that can exactly distinguish\nadjacent planar patches. The quality of point feature directly determines the\naccuracy of roof plane segmentation. Most of existing approaches use\nhandcrafted features to extract roof planes. However, the abilities of these\nfeatures are relatively low, especially in boundary area. To solve this\nproblem, we propose a boundary-aware point clustering approach in Euclidean and\nembedding spaces constructed by a multi-task deep network for roof plane\nsegmentation. We design a three-branch network to predict semantic labels,\npoint offsets and extract deep embedding features. In the first branch, we\nclassify the input data as non-roof, boundary and plane points. In the second\nbranch, we predict point offsets for shifting each point toward its respective\ninstance center. In the third branch, we constrain that points of the same\nplane instance should have the similar embeddings. We aim to ensure that points\nof the same plane instance are close as much as possible in both Euclidean and\nembedding spaces. However, although deep network has strong feature\nrepresentative ability, it is still hard to accurately distinguish points near\nplane instance boundary. Therefore, we first group plane points into many\nclusters in the two spaces, and then we assign the rest boundary points to\ntheir closest clusters to generate final complete roof planes. In this way, we\ncan effectively reduce the influence of unreliable boundary points. In\naddition, we prepare a synthetic dataset and two real datasets to train and\nevaluate our approach. The experiments results show that the proposed approach\nsignificantly outperforms the existing state-of-the-art approaches.\n","authors":["Li Li","Qingqing Li","Guozheng Xu","Pengwei Zhou","Jingmin Tu","Jie Li","Mingming Li","Jian Yao"],"pdf_url":"https://arxiv.org/pdf/2309.03722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02179v1","updated":"2024-10-03T03:43:29Z","published":"2024-10-03T03:43:29Z","title":"HATFormer: Historic Handwritten Arabic Text Recognition with\n  Transformers","summary":"  Arabic handwritten text recognition (HTR) is challenging, especially for\nhistorical texts, due to diverse writing styles and the intrinsic features of\nArabic script. Additionally, Arabic handwriting datasets are smaller compared\nto English ones, making it difficult to train generalizable Arabic HTR models.\nTo address these challenges, we propose HATFormer, a transformer-based\nencoder-decoder architecture that builds on a state-of-the-art English HTR\nmodel. By leveraging the transformer's attention mechanism, HATFormer captures\nspatial contextual information to address the intrinsic challenges of Arabic\nscript through differentiating cursive characters, decomposing visual\nrepresentations, and identifying diacritics. Our customization to historical\nhandwritten Arabic includes an image processor for effective ViT information\npreprocessing, a text tokenizer for compact Arabic text representation, and a\ntraining pipeline that accounts for a limited amount of historic Arabic\nhandwriting data. HATFormer achieves a character error rate (CER) of 8.6% on\nthe largest public historical handwritten Arabic dataset, with a 51%\nimprovement over the best baseline in the literature. HATFormer also attains a\ncomparable CER of 4.2% on the largest private non-historical dataset. Our work\ndemonstrates the feasibility of adapting an English HTR method to a\nlow-resource language with complex, language-specific challenges, contributing\nto advancements in document digitization, information retrieval, and cultural\npreservation.\n","authors":["Adrian Chan","Anupam Mijar","Mehreen Saeed","Chau-Wai Wong","Akram Khater"],"pdf_url":"https://arxiv.org/pdf/2410.02179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03757v2","updated":"2024-10-03T03:40:20Z","published":"2024-02-06T06:48:46Z","title":"The Instinctive Bias: Spurious Images lead to Illusion in MLLMs","summary":"  Large language models (LLMs) have recently experienced remarkable progress,\nwhere the advent of multi-modal large language models (MLLMs) has endowed LLMs\nwith visual capabilities, leading to impressive performances in various\nmulti-modal tasks. However, those powerful MLLMs such as GPT-4V still fail\nspectacularly when presented with certain image and text inputs. In this paper,\nwe identify a typical class of inputs that baffles MLLMs, which consist of\nimages that are highly relevant but inconsistent with answers, causing MLLMs to\nsuffer from visual illusion. To quantify the effect, we propose CorrelationQA,\nthe first benchmark that assesses the visual illusion level given spurious\nimages. This benchmark contains 7,308 text-image pairs across 13 categories.\nBased on the proposed CorrelationQA, we conduct a thorough analysis on 9\nmainstream MLLMs, illustrating that they universally suffer from this\ninstinctive bias to varying degrees. We hope that our curated benchmark and\nevaluation results aid in better assessments of the MLLMs' robustness in the\npresence of misleading images. The code and datasets are available at\nhttps://github.com/MasaiahHan/CorrelationQA.\n","authors":["Tianyang Han","Qing Lian","Rui Pan","Renjie Pi","Jipeng Zhang","Shizhe Diao","Yong Lin","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.03757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18944v4","updated":"2024-10-03T03:35:53Z","published":"2024-06-27T07:14:14Z","title":"Rethinking and Defending Protective Perturbation in Personalized\n  Diffusion Models","summary":"  Personalized diffusion models (PDMs) have become prominent for adapting\npretrained text-to-image models to generate images of specific subjects using\nminimal training data. However, PDMs are susceptible to minor adversarial\nperturbations, leading to significant degradation when fine-tuned on corrupted\ndatasets. These vulnerabilities are exploited to create protective\nperturbations that prevent unauthorized image generation. Existing purification\nmethods attempt to mitigate this issue but often over-purify images, resulting\nin information loss. In this work, we conduct an in-depth analysis of the\nfine-tuning process of PDMs through the lens of shortcut learning. We\nhypothesize and empirically demonstrate that adversarial perturbations induce a\nlatent-space misalignment between images and their text prompts in the CLIP\nembedding space. This misalignment causes the model to erroneously associate\nnoisy patterns with unique identifiers during fine-tuning, resulting in poor\ngeneralization. Based on these insights, we propose a systematic defense\nframework that includes data purification and contrastive decoupling learning.\nWe first employ off-the-shelf image restoration techniques to realign images\nwith their original semantic meanings in latent space. Then, we introduce\ncontrastive decoupling learning with noise tokens to decouple the learning of\npersonalized concepts from spurious noise patterns. Our study not only uncovers\nfundamental shortcut learning vulnerabilities in PDMs but also provides a\ncomprehensive evaluation framework for developing stronger protection. Our\nextensive evaluation demonstrates its superiority over existing purification\nmethods and stronger robustness against adaptive perturbation.\n","authors":["Yixin Liu","Ruoxi Chen","Xun Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18944v4.pdf","comment":"Our code is available at\n  https://github.com/liuyixin-louis/DiffShortcut"},{"id":"http://arxiv.org/abs/2410.02155v1","updated":"2024-10-03T02:34:31Z","published":"2024-10-03T02:34:31Z","title":"From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities","summary":"  Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Our method not only improves performance across various\nbenchmarks but also shows promising scalability, potentially paving the way for\nmore efficient and capable multimodal foundation models.\n","authors":["Wanpeng Zhang","Zilong Xie","Yicheng Feng","Yijiang Li","Xingrun Xing","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02152v1","updated":"2024-10-03T02:31:14Z","published":"2024-10-03T02:31:14Z","title":"An Evaluation of Large Pre-Trained Models for Gesture Recognition using\n  Synthetic Videos","summary":"  In this work, we explore the possibility of using synthetically generated\ndata for video-based gesture recognition with large pre-trained models. We\nconsider whether these models have sufficiently robust and expressive\nrepresentation spaces to enable \"training-free\" classification. Specifically,\nwe utilize various state-of-the-art video encoders to extract features for use\nin k-nearest neighbors classification, where the training data points are\nderived from synthetic videos only. We compare these results with another\ntraining-free approach -- zero-shot classification using text descriptions of\neach gesture. In our experiments with the RoCoG-v2 dataset, we find that using\nsynthetic training videos yields significantly lower classification accuracy on\nreal test videos compared to using a relatively small number of real training\nvideos. We also observe that video backbones that were fine-tuned on\nclassification tasks serve as superior feature extractors, and that the choice\nof fine-tuning data has a substantial impact on k-nearest neighbors\nperformance. Lastly, we find that zero-shot text-based classification performs\npoorly on the gesture recognition task, as gestures are not easily described\nthrough natural language.\n","authors":["Arun Reddy","Ketul Shah","Corban Rivera","William Paul","Celso M. De Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2410.02152v1.pdf","comment":"Synthetic Data for Artificial Intelligence and Machine Learning:\n  Tools, Techniques, and Applications II (SPIE Defense + Commercial Sensing,\n  2024)"},{"id":"http://arxiv.org/abs/2407.19340v4","updated":"2024-10-03T02:23:50Z","published":"2024-07-27T21:00:36Z","title":"Integrating Large Language Models into a Tri-Modal Architecture for\n  Automated Depression Classification","summary":"  Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.\n","authors":["Santosh V. Patapati"],"pdf_url":"https://arxiv.org/pdf/2407.19340v4.pdf","comment":"Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language\n  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ"},{"id":"http://arxiv.org/abs/2210.15889v5","updated":"2024-10-03T02:17:04Z","published":"2022-10-28T04:38:10Z","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on\n  Neuro-Symbolic Computing","summary":"  Neural-symbolic computing (NeSy), which pursues the integration of the\nsymbolic and statistical paradigms of cognition, has been an active research\narea of Artificial Intelligence (AI) for many years. As NeSy shows promise of\nreconciling the advantages of reasoning and interpretability of symbolic\nrepresentation and robust learning in neural networks, it may serve as a\ncatalyst for the next generation of AI. In the present paper, we provide a\nsystematic overview of the recent developments and important contributions of\nNeSy research. Firstly, we introduce study history of this area, covering early\nwork and foundations. We further discuss background concepts and identify key\ndriving factors behind the development of NeSy. Afterward, we categorize recent\nlandmark approaches along several main characteristics that underline this\nresearch paradigm, including neural-symbolic integration, knowledge\nrepresentation, knowledge embedding, and functionality. Next, we briefly\ndiscuss the successful application of modern NeSy approaches in several\ndomains. Then, we benchmark several NeSy methods on three representative\napplication tasks. Finally, we identify the open problems together with\npotential future research directions. This survey is expected to help new\nresearchers enter this rapidly evolving field and accelerate the progress\ntowards data-and knowledge-driven AI.\n","authors":["Wenguan Wang","Yi Yang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2210.15889v5.pdf","comment":"PAMI 2024"},{"id":"http://arxiv.org/abs/2406.09588v2","updated":"2024-10-03T01:48:47Z","published":"2024-06-13T21:02:03Z","title":"Color Equivariant Network","summary":"  Group equivariant convolutional neural networks have been designed for a\nvariety of geometric transformations from 2D and 3D rotation groups, to\nsemi-groups such as scale. Despite the improved interpretability, accuracy and\ngeneralizability afforded by these architectures, group equivariant networks\nhave seen limited application in the context of perceptual quantities such as\nhue and saturation, even though their variation can lead to significant\nreductions in classification performance. In this paper, we introduce\nconvolutional neural networks equivariant to variations in hue and saturation\nby design. To achieve this, we leverage the observation that hue and saturation\ntransformations can be identified with the 2D rotation and 1D translation\ngroups respectively. Our hue-, saturation-, and fully color-equivariant\nnetworks achieve equivariance to these perceptual transformations without an\nincrease in network parameters. We demonstrate the utility of our networks on\nsynthetic and real world datasets where color and lighting variations are\ncommonplace.\n","authors":["Felix O'Mahony","Yulong Yang","Christine Allen-Blanchette"],"pdf_url":"https://arxiv.org/pdf/2406.09588v2.pdf","comment":"Accepted at CVPR 2024 Equivariant Vision Workshop"},{"id":"http://arxiv.org/abs/2410.02130v1","updated":"2024-10-03T01:23:44Z","published":"2024-10-03T01:23:44Z","title":"MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers\n  for Open-Domain Sound Generation","summary":"  We introduce MDSGen, a novel framework for vision-guided open-domain sound\ngeneration optimized for model parameter size, memory consumption, and\ninference speed. This framework incorporates two key innovations: (1) a\nredundant video feature removal module that filters out unnecessary visual\ninformation, and (2) a temporal-aware masking strategy that leverages temporal\ncontext for enhanced audio generation accuracy. In contrast to existing\nresource-heavy Unet-based models, MDSGen employs denoising masked diffusion\ntransformers, facilitating efficient generation without reliance on pre-trained\ndiffusion models. Evaluated on the benchmark VGGSound dataset, our smallest\nmodel (5M parameters) achieves 97.9% alignment accuracy, using 172x fewer\nparameters, 371% less memory, and offering 36x faster inference than the\ncurrent 860M-parameter state-of-the-art model (93.9% accuracy). The larger\nmodel (131M parameters) reaches nearly 99% accuracy while requiring 6.5x fewer\nparameters. These results highlight the scalability and effectiveness of our\napproach.\n","authors":["Trung X. Pham","Tri Ton","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.02130v1.pdf","comment":"21 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.02129v1","updated":"2024-10-03T01:19:21Z","published":"2024-10-03T01:19:21Z","title":"DMC-Net: Lightweight Dynamic Multi-Scale and Multi-Resolution\n  Convolution Network for Pancreas Segmentation in CT Images","summary":"  Convolutional neural networks (CNNs) have shown great effectiveness in\nmedical image segmentation. However, they may be limited in modeling large\ninter-subject variations in organ shapes and sizes and exploiting global\nlong-range contextual information. This is because CNNs typically employ\nconvolutions with fixed-sized local receptive fields and lack the mechanisms to\nutilize global information. To address these limitations, we developed Dynamic\nMulti-Resolution Convolution (DMRC) and Dynamic Multi-Scale Convolution (DMSC)\nmodules. Both modules enhance the representation capabilities of single\nconvolutions to capture varying scaled features and global contextual\ninformation. This is achieved in the DMRC module by employing a convolutional\nfilter on images with different resolutions and subsequently utilizing dynamic\nmechanisms to model global inter-dependencies between features. In contrast,\nthe DMSC module extracts features at different scales by employing convolutions\nwith different kernel sizes and utilizing dynamic mechanisms to extract global\ncontextual information. The utilization of convolutions with different kernel\nsizes in the DMSC module may increase computational complexity. To lessen this\nburden, we propose to use a lightweight design for convolution layers with a\nlarge kernel size. Thus, DMSC and DMRC modules are designed as lightweight\ndrop-in replacements for single convolutions, and they can be easily integrated\ninto general CNN architectures for end-to-end training. The segmentation\nnetwork was proposed by incorporating our DMSC and DMRC modules into a standard\nU-Net architecture, termed Dynamic Multi-scale and Multi-resolution Convolution\nnetwork (DMC-Net). The results demonstrate that our proposed DMSC and DMRC can\nenhance the representation capabilities of single convolutions and improve\nsegmentation accuracy.\n","authors":["Jin Yang","Daniel S. Marcus","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2410.02129v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.13438v2","updated":"2024-10-03T00:51:50Z","published":"2024-08-24T02:26:42Z","title":"Explainable Concept Generation through Vision-Language Preference\n  Learning","summary":"  Concept-based explanations have become a popular choice for explaining deep\nneural networks post-hoc because, unlike most other explainable AI techniques,\nthey can be used to test high-level visual \"concepts\" that are not directly\nrelated to feature attributes. For instance, the concept of \"stripes\" is\nimportant to classify an image as a zebra. Concept-based explanation methods,\nhowever, require practitioners to guess and collect multiple candidate concept\nimage sets, which can often be imprecise and labor-intensive. Addressing this\nlimitation, in this paper, we frame concept image set creation as an image\ngeneration problem. However, since naively using a generative model does not\nresult in meaningful concepts, we devise a reinforcement learning-based\npreference optimization (RLPO) algorithm that fine-tunes the vision-language\ngenerative model from approximate textual descriptions of concepts. Through a\nseries of experiments, we demonstrate the capability of our method to\narticulate complex and abstract concepts which aligns with the test class that\nare otherwise challenging to craft manually. In addition to showing the\nefficacy and reliability of our method, we show how our method can be used as a\ndiagnostic tool for analyzing neural networks.\n","authors":["Aditya Taparia","Som Sagar","Ransalu Senanayake"],"pdf_url":"https://arxiv.org/pdf/2408.13438v2.pdf","comment":"25 pages, 27 figures"},{"id":"http://arxiv.org/abs/2409.06183v2","updated":"2024-10-03T00:48:28Z","published":"2024-09-10T03:25:24Z","title":"EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation","summary":"  Due to their text-to-image synthesis feature, diffusion models have recently\nseen a rise in visual perception tasks, such as depth estimation. The lack of\ngood-quality datasets makes the extraction of a fine-grain semantic context\nchallenging for the diffusion models. The semantic context with fewer details\nfurther worsens the process of creating effective text embeddings that will be\nused as input for diffusion models. In this paper, we propose a novel EDADepth,\nan enhanced data augmentation method to estimate monocular depth without using\nadditional training data. We use Swin2SR, a super-resolution model, to enhance\nthe quality of input images. We employ the BEiT pre-trained semantic\nsegmentation model for better extraction of text embeddings. We use BLIP-2\ntokenizer to generate tokens from these text embeddings. The novelty of our\napproach is the introduction of Swin2SR, the BEiT model, and the BLIP-2\ntokenizer in the diffusion-based pipeline for the monocular depth estimation.\nOur model achieves state-of-the-art results (SOTA) on the delta3 metric on\nNYUv2 and KITTI datasets. It also achieves results comparable to those of the\nSOTA models in the RMSE and REL metrics. Finally, we also show improvements in\nthe visualization of the estimated depth compared to the SOTA diffusion-based\nmonocular depth estimation models. Code:\nhttps://github.com/edadepthmde/EDADepth_ICMLA.\n","authors":["Nischal Khanal","Shivanand Venkanna Sheshappanavar"],"pdf_url":"https://arxiv.org/pdf/2409.06183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03039v1","updated":"2024-10-03T23:06:11Z","published":"2024-10-03T23:06:11Z","title":"Revealing the Unseen: Guiding Personalized Diffusion Models to Expose\n  Training Data","summary":"  Diffusion Models (DMs) have evolved into advanced image generation tools,\nespecially for few-shot fine-tuning where a pretrained DM is fine-tuned on a\nsmall set of images to capture specific styles or objects. Many people upload\nthese personalized checkpoints online, fostering communities such as Civitai\nand HuggingFace. However, model owners may overlook the potential risks of data\nleakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding\ncopyright violations arise when unauthorized data is used during fine-tuning.\nIn this paper, we ask: \"Can training data be extracted from these fine-tuned\nDMs shared online?\" A successful extraction would present not only data leakage\nthreats but also offer tangible evidence of copyright infringement. To answer\nthis, we propose FineXtract, a framework for extracting fine-tuning data. Our\nmethod approximates fine-tuning as a gradual shift in the model's learned\ndistribution -- from the original pretrained DM toward the fine-tuning data. By\nextrapolating the models before and after fine-tuning, we guide the generation\ntoward high-probability regions within the fine-tuned data distribution. We\nthen apply a clustering algorithm to extract the most probable images from\nthose generated using this extrapolated guidance. Experiments on DMs fine-tuned\nwith datasets such as WikiArt, DreamBooth, and real-world checkpoints posted\nonline validate the effectiveness of our method, extracting approximately 20%\nof fine-tuning data in most cases, significantly surpassing baseline\nperformance.\n","authors":["Xiaoyu Wu","Jiaru Zhang","Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2410.03039v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2402.11789v3","updated":"2024-10-03T22:59:49Z","published":"2024-02-19T02:32:45Z","title":"Statistical Test on Diffusion Model-based Anomaly Detection by Selective\n  Inference","summary":"  Advancements in AI image generation, particularly diffusion models, have\nprogressed rapidly. However, the absence of an established framework for\nquantifying the reliability of AI-generated images hinders their use in\ncritical decision-making tasks, such as medical image diagnosis. In this study,\nwe address the task of detecting anomalous regions in medical images using\ndiffusion models and propose a statistical method to quantify the reliability\nof the detected anomalies. The core concept of our method involves a selective\ninference framework, wherein statistical tests are conducted under the\ncondition that the images are produced by a diffusion model. With our approach,\nthe statistical significance of anomaly detection results can be quantified in\nthe form of a $p$-value, enabling decision-making with controlled error rates,\nas is standard in medical practice. We demonstrate the theoretical soundness\nand practical effectiveness of our statistical test through numerical\nexperiments on both synthetic and brain image datasets.\n","authors":["Teruyuki Katsuoka","Tomohiro Shiraishi","Daiki Miwa","Vo Nguyen Le Duy","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2402.11789v3.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.03038v1","updated":"2024-10-03T22:58:56Z","published":"2024-10-03T22:58:56Z","title":"CPFD: Confidence-aware Privileged Feature Distillation for Short Video\n  Classification","summary":"  Dense features, customized for different business scenarios, are essential in\nshort video classification. However, their complexity, specific adaptation\nrequirements, and high computational costs make them resource-intensive and\nless accessible during online inference. Consequently, these dense features are\ncategorized as `Privileged Dense Features'.Meanwhile, end-to-end multi-modal\nmodels have shown promising results in numerous computer vision tasks. In\nindustrial applications, prioritizing end-to-end multi-modal features, can\nenhance efficiency but often leads to the loss of valuable information from\nhistorical privileged dense features.To integrate both features while\nmaintaining efficiency and manageable resource costs, we present\nConfidence-aware Privileged Feature Distillation (CPFD), which empowers\nfeatures of an end-to-end multi-modal model by adaptively distilling privileged\nfeatures during training.Unlike existing privileged feature distillation (PFD)\nmethods, which apply uniform weights to all instances during distillation,\npotentially causing unstable performance across different business scenarios\nand a notable performance gap between teacher model (Dense Feature enhanced\nmultimodal-model DF-X-VLM) and student model (multimodal-model only X-VLM), our\nCPFD leverages confidence scores derived from the teacher model to adaptively\nmitigate the performance variance with the student model.We conducted extensive\noffline experiments on five diverse tasks demonstrating that CPFD improves the\nvideo classification F1 score by 6.76% compared with end-to-end\nmultimodal-model (X-VLM) and by 2.31% with vanilla PFD on-average. And it\nreduces the performance gap by 84.6% and achieves results comparable to teacher\nmodel DF-X-VLM. The effectiveness of CPFD is further substantiated by online\nexperiments, and our framework has been deployed in production systems for over\na dozen models.\n","authors":["Jinghao Shi","Xiang Shen","Kaili Zhao","Xuedong Wang","Vera Wen","Zixuan Wang","Yifan Wu","Zhixin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03038v1.pdf","comment":"Camera ready for CIKM 2024"},{"id":"http://arxiv.org/abs/2409.18260v2","updated":"2024-10-03T22:57:24Z","published":"2024-09-26T20:13:03Z","title":"PCEvE: Part Contribution Evaluation Based Model Explanation for Human\n  Figure Drawing Assessment and Beyond","summary":"  For automatic human figure drawing (HFD) assessment tasks, such as diagnosing\nautism spectrum disorder (ASD) using HFD images, the clarity and explainability\nof a model decision are crucial. Existing pixel-level attribution-based\nexplainable AI (XAI) approaches demand considerable effort from users to\ninterpret the semantic information of a region in an image, which can be often\ntime-consuming and impractical. To overcome this challenge, we propose a part\ncontribution evaluation based model explanation (PCEvE) framework. On top of\nthe part detection, we measure the Shapley Value of each individual part to\nevaluate the contribution to a model decision. Unlike existing\nattribution-based XAI approaches, the PCEvE provides a straightforward\nexplanation of a model decision, i.e., a part contribution histogram.\nFurthermore, the PCEvE expands the scope of explanations beyond the\nconventional sample-level to include class-level and task-level insights,\noffering a richer, more comprehensive understanding of model behavior. We\nrigorously validate the PCEvE via extensive experiments on multiple HFD\nassessment datasets. Also, we sanity-check the proposed method with a set of\ncontrolled experiments. Additionally, we demonstrate the versatility and\napplicability of our method to other domains by applying it to a\nphoto-realistic dataset, the Stanford Cars.\n","authors":["Jongseo Lee","Geo Ahn","Seong Tae Kim","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2409.18260v2.pdf","comment":"This papaer is under review"},{"id":"http://arxiv.org/abs/2409.17924v2","updated":"2024-10-03T22:57:06Z","published":"2024-09-26T15:05:29Z","title":"Neural Light Spheres for Implicit Image Stitching and View Synthesis","summary":"  Challenging to capture, and challenging to display on a cellphone screen, the\npanorama paradoxically remains both a staple and underused feature of modern\nmobile camera applications. In this work we address both of these challenges\nwith a spherical neural light field model for implicit panoramic image\nstitching and re-rendering; able to accommodate for depth parallax,\nview-dependent lighting, and local scene motion and color changes during\ncapture. Fit during test-time to an arbitrary path panoramic video capture --\nvertical, horizontal, random-walk -- these neural light spheres jointly\nestimate the camera path and a high-resolution scene reconstruction to produce\nnovel wide field-of-view projections of the environment. Our single-layer model\navoids expensive volumetric sampling, and decomposes the scene into compact\nview-dependent ray offset and color components, with a total model size of 80\nMB per scene, and real-time (50 FPS) rendering at 1080p resolution. We\ndemonstrate improved reconstruction quality over traditional image stitching\nand radiance field methods, with significantly higher tolerance to scene motion\nand non-ideal capture settings.\n","authors":["Ilya Chugunov","Amogh Joshi","Kiran Murthy","Francois Bleibel","Felix Heide"],"pdf_url":"https://arxiv.org/pdf/2409.17924v2.pdf","comment":"Project site: https://light.princeton.edu/publication/neuls/"},{"id":"http://arxiv.org/abs/2405.15668v2","updated":"2024-10-03T22:53:09Z","published":"2024-05-24T16:05:15Z","title":"What Do You See? Enhancing Zero-Shot Image Classification with\n  Multimodal Large Language Models","summary":"  Large language models (LLMs) have been effectively used for many computer\nvision tasks, including image classification. In this paper, we present a\nsimple yet effective approach for zero-shot image classification using\nmultimodal LLMs. By employing multimodal LLMs, we generate comprehensive\ntextual representations from input images. These textual representations are\nthen utilized to generate fixed-dimensional features in a cross-modal embedding\nspace. Subsequently, these features are fused together to perform zero-shot\nclassification using a linear classifier. Our method does not require prompt\nengineering for each dataset; instead, we use a single, straightforward, set of\nprompts across all datasets. We evaluated our method on several datasets, and\nour results demonstrate its remarkable effectiveness, surpassing benchmark\naccuracy on multiple datasets. On average, our method achieved an accuracy gain\nof 4.1 percentage points, with an increase of 6.8 percentage points on the\nImageNet dataset, compared to prior methods. Our findings highlight the\npotential of multimodal LLMs to enhance computer vision tasks such as zero-shot\nimage classification, offering a significant improvement over traditional\nmethods.\n","authors":["Abdelrahman Abdelhamed","Mahmoud Afifi","Alec Go"],"pdf_url":"https://arxiv.org/pdf/2405.15668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15272v3","updated":"2024-10-03T22:32:50Z","published":"2024-09-23T17:59:05Z","title":"OmniBench: Towards The Future of Universal Omni-Language Models","summary":"  Recent advancements in multimodal large language models (MLLMs) have aimed to\nintegrate and interpret data across diverse modalities. However, the capacity\nof these models to concurrently process and reason about multiple modalities\nremains inadequately explored, partly due to the lack of comprehensive\nmodality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to\nrigorously evaluate models' ability to recognize, interpret, and reason across\nvisual, acoustic, and textual inputs simultaneously. We define models capable\nof such tri-modal processing as omni-language models (OLMs). OmniBench is\ndistinguished by high-quality human annotations, ensuring that accurate\nresponses require integrated understanding and reasoning across all three\nmodalities. Our main findings reveal that: i) most OLMs exhibit critical\nlimitations in instruction-following and reasoning capabilities within\ntri-modal contexts; and ii) most baselines models perform poorly (below 50\\%\naccuracy) even when provided with alternative textual representations of images\nor/and audio. These results suggest that the ability to construct a consistent\ncontext from text, image, and audio is often overlooked in existing MLLM\ntraining paradigms. To address this gap, we curate an instruction tuning\ndataset of 84.5K training samples, OmniInstruct, for training OLMs to adapt to\nmultimodal contexts. We advocate for future research to focus on developing\nmore robust tri-modal integration techniques and training strategies to enhance\nOLM performance across diverse modalities. The codes and live leaderboard could\nbe found at https://m-a-p.ai/OmniBench.\n","authors":["Yizhi Li","Ge Zhang","Yinghao Ma","Ruibin Yuan","Kang Zhu","Hangyu Guo","Yiming Liang","Jiaheng Liu","Zekun Wang","Jian Yang","Siwei Wu","Xingwei Qu","Jinjie Shi","Xinyue Zhang","Zhenzhu Yang","Xiangzhou Wang","Zhaoxiang Zhang","Zachary Liu","Emmanouil Benetos","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.15272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v4","updated":"2024-10-03T22:29:14Z","published":"2024-03-24T14:21:06Z","title":"ESREAL: Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.08659v2","updated":"2024-10-03T22:25:54Z","published":"2024-07-11T16:46:04Z","title":"Controlling the Fidelity and Diversity of Deep Generative Models via\n  Pseudo Density","summary":"  We introduce an approach to bias deep generative models, such as GANs and\ndiffusion models, towards generating data with either enhanced fidelity or\nincreased diversity. Our approach involves manipulating the distribution of\ntraining and generated data through a novel metric for individual samples,\nnamed pseudo density, which is based on the nearest-neighbor information from\nreal samples. Our approach offers three distinct techniques to adjust the\nfidelity and diversity of deep generative models: 1) Per-sample perturbation,\nenabling precise adjustments for individual samples towards either more common\nor more unique characteristics; 2) Importance sampling during model inference\nto enhance either fidelity or diversity in the generated data; 3) Fine-tuning\nwith importance sampling, which guides the generative model to learn an\nadjusted distribution, thus controlling fidelity and diversity. Furthermore,\nour fine-tuning method demonstrates the ability to improve the Frechet\nInception Distance (FID) for pre-trained generative models with minimal\niterations.\n","authors":["Shuangqi Li","Chen Liu","Tong Zhang","Hieu Le","Sabine Ssstrunk","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2407.08659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03030v1","updated":"2024-10-03T22:24:54Z","published":"2024-10-03T22:24:54Z","title":"Dynamic Sparse Training versus Dense Training: The Unexpected Winner in\n  Image Corruption Robustness","summary":"  It is generally perceived that Dynamic Sparse Training opens the door to a\nnew era of scalability and efficiency for artificial neural networks at,\nperhaps, some costs in accuracy performance for the classification task. At the\nsame time, Dense Training is widely accepted as being the \"de facto\" approach\nto train artificial neural networks if one would like to maximize their\nrobustness against image corruption. In this paper, we question this general\npractice. Consequently, we claim that, contrary to what is commonly thought,\nthe Dynamic Sparse Training methods can consistently outperform Dense Training\nin terms of robustness accuracy, particularly if the efficiency aspect is not\nconsidered as a main objective (i.e., sparsity levels between 10% and up to\n50%), without adding (or even reducing) resource cost. We validate our claim on\ntwo types of data, images and videos, using several traditional and modern deep\nlearning architectures for computer vision and three widely studied Dynamic\nSparse Training algorithms. Our findings reveal a new yet-unknown benefit of\nDynamic Sparse Training and open new possibilities in improving deep learning\nrobustness beyond the current state of the art.\n","authors":["Boqian Wu","Qiao Xiao","Shunxin Wang","Nicola Strisciuglio","Mykola Pechenizkiy","Maurice van Keulen","Decebal Constantin Mocanu","Elena Mocanu"],"pdf_url":"https://arxiv.org/pdf/2410.03030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13248v3","updated":"2024-10-03T22:13:39Z","published":"2024-03-20T02:19:21Z","title":"Mora: Enabling Generalist Video Generation via A Multi-Agent Framework","summary":"  Text-to-video generation has made significant strides, but replicating the\ncapabilities of advanced systems like OpenAI Sora remains challenging due to\ntheir closed-source nature. Existing open-source methods struggle to achieve\ncomparable performance, often hindered by ineffective agent collaboration and\ninadequate training data quality. In this paper, we introduce Mora, a novel\nmulti-agent framework that leverages existing open-source modules to replicate\nSora functionalities. We address these fundamental limitations by proposing\nthree key techniques: (1) multi-agent fine-tuning with a self-modulation factor\nto enhance inter-agent coordination, (2) a data-free training strategy that\nuses large models to synthesize training data, and (3) a human-in-the-loop\nmechanism combined with multimodal large language models for data filtering to\nensure high-quality training datasets. Our comprehensive experiments on six\nvideo generation tasks demonstrate that Mora achieves performance comparable to\nSora on VBench, outperforming existing open-source methods across various\ntasks. Specifically, in the text-to-video generation task, Mora achieved a\nVideo Quality score of 0.800, surpassing Sora 0.797 and outperforming all other\nbaseline models across six key metrics. Additionally, in the image-to-video\ngeneration task, Mora achieved a perfect Dynamic Degree score of 1.00,\ndemonstrating exceptional capability in enhancing motion realism and achieving\nhigher Imaging Quality than Sora. These results highlight the potential of\ncollaborative multi-agent systems and human-in-the-loop mechanisms in advancing\ntext-to-video generation. Our code is available at\n\\url{https://github.com/lichao-sun/Mora}.\n","authors":["Zhengqing Yuan","Yixin Liu","Yihan Cao","Weixiang Sun","Haolong Jia","Ruoxi Chen","Zhaoxu Li","Bin Lin","Li Yuan","Lifang He","Chi Wang","Yanfang Ye","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.13248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15002v2","updated":"2024-10-03T22:11:21Z","published":"2024-02-22T22:31:39Z","title":"CommVQA: Situating Visual Question Answering in Communicative Contexts","summary":"  Current visual question answering (VQA) models tend to be trained and\nevaluated on image-question pairs in isolation. However, the questions people\nask are dependent on their informational needs and prior knowledge about the\nimage content. To evaluate how situating images within naturalistic contexts\nshapes visual questions, we introduce CommVQA, a VQA dataset consisting of\nimages, image descriptions, real-world communicative scenarios where the image\nmight appear (e.g., a travel website), and follow-up questions and answers\nconditioned on the scenario and description. CommVQA, which contains 1000\nimages and 8,949 question-answer pairs, poses a challenge for current models.\nError analyses and a human-subjects study suggest that generated answers still\ncontain high rates of hallucinations, fail to fittingly address unanswerable\nquestions, and don't suitably reflect contextual information. Overall, we show\nthat access to contextual information is essential for solving CommVQA, leading\nto the highest performing VQA model and highlighting the relevance of situating\nsystems within communicative scenarios.\n","authors":["Nandita Shankar Naik","Christopher Potts","Elisa Kreiss"],"pdf_url":"https://arxiv.org/pdf/2402.15002v2.pdf","comment":"EMNLP 2024 camera ready version"},{"id":"http://arxiv.org/abs/2403.15651v2","updated":"2024-10-03T22:11:19Z","published":"2024-03-22T23:47:19Z","title":"GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering","summary":"  In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.\n","authors":["Jiaye Wu","Saeed Hadadan","Geng Lin","Matthias Zwicker","David Jacobs","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2403.15651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09458v2","updated":"2024-10-03T22:10:20Z","published":"2024-06-12T20:24:51Z","title":"Updating CLIP to Prefer Descriptions Over Captions","summary":"  Although CLIPScore is a powerful generic metric that captures the similarity\nbetween a text and an image, it fails to distinguish between a caption that is\nmeant to complement the information in an image and a description that is meant\nto replace an image entirely, e.g., for accessibility. We address this\nshortcoming by updating the CLIP model with the Concadia dataset to assign\nhigher scores to descriptions than captions using parameter efficient\nfine-tuning and a loss objective derived from work on causal interpretability.\nThis model correlates with the judgements of blind and low-vision people while\npreserving transfer capabilities and has interpretable structure that sheds\nlight on the caption--description distinction.\n","authors":["Amir Zur","Elisa Kreiss","Karel D'Oosterlinck","Christopher Potts","Atticus Geiger"],"pdf_url":"https://arxiv.org/pdf/2406.09458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03021v1","updated":"2024-10-03T22:08:41Z","published":"2024-10-03T22:08:41Z","title":"PixelShuffler: A Simple Image Translation Through Pixel Rearrangement","summary":"  Image-to-image translation is a topic in computer vision that has a vast\nrange of use cases ranging from medical image translation, such as converting\nMRI scans to CT scans or to other MRI contrasts, to image colorization,\nsuper-resolution, domain adaptation, and generating photorealistic images from\nsketches or semantic maps. Image style transfer is also a widely researched\napplication of image-to-image translation, where the goal is to synthesize an\nimage that combines the content of one image with the style of another.\nExisting state-of-the-art methods often rely on complex neural networks,\nincluding diffusion models and language models, to achieve high-quality style\ntransfer, but these methods can be computationally expensive and intricate to\nimplement. In this paper, we propose a novel pixel shuffle method that\naddresses the image-to-image translation problem generally with a specific\ndemonstrative application in style transfer. The proposed method approaches\nstyle transfer by shuffling the pixels of the style image such that the mutual\ninformation between the shuffled image and the content image is maximized. This\napproach inherently preserves the colors of the style image while ensuring that\nthe structural details of the content image are retained in the stylized\noutput. We demonstrate that this simple and straightforward method produces\nresults that are comparable to state-of-the-art techniques, as measured by the\nLearned Perceptual Image Patch Similarity (LPIPS) loss for content preservation\nand the Fr\\'echet Inception Distance (FID) score for style similarity. Our\nexperiments validate that the proposed pixel shuffle method achieves\ncompetitive performance with significantly reduced complexity, offering a\npromising alternative for efficient image style transfer, as well as a promise\nin usability of the method in general image-to-image translation tasks.\n","authors":["Omar Zamzam"],"pdf_url":"https://arxiv.org/pdf/2410.03021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05762v2","updated":"2024-10-03T22:07:18Z","published":"2023-10-09T14:44:01Z","title":"MonoVisual3DFilter: 3D tomatoes' localisation with monocular cameras\n  using histogram filters","summary":"  Performing tasks in agriculture, such as fruit monitoring or harvesting,\nrequires perceiving the objects' spatial position. RGB-D cameras are limited\nunder open-field environments due to lightning interferences. So, in this\nstudy, we state to answer the research question: \"How can we use and control\nmonocular sensors to perceive objects' position in the 3D task space?\" Towards\nthis aim, we approached histogram filters (Bayesian discrete filters) to\nestimate the position of tomatoes in the tomato plant through the algorithm\nMonoVisual3DFilter. Two kernel filters were studied: the square kernel and the\nGaussian kernel. The implemented algorithm was essayed in simulation, with and\nwithout Gaussian noise and random noise, and in a testbed at laboratory\nconditions. The algorithm reported a mean absolute error lower than 10 mm in\nsimulation and 20 mm in the testbed at laboratory conditions with an assessing\ndistance of about 0.5 m. So, the results are viable for real environments and\nshould be improved at closer distances.\n","authors":["Sandro Costa Magalhes","Filipe Neves dos Santos","Antnio Paulo Moreira","Jorge Dias"],"pdf_url":"https://arxiv.org/pdf/2310.05762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13104v2","updated":"2024-10-03T22:06:25Z","published":"2024-09-19T22:11:08Z","title":"ERIC: Estimating Rainfall with Commodity Doorbell Camera for Precision\n  Residential Irrigation","summary":"  Current state-of-the-art residential irrigation systems, such as WaterMyYard,\nrely on rainfall data from nearby weather stations to adjust irrigation\namounts. However, the accuracy of rainfall data is compromised by the limited\nspatial resolution of rain gauges and the significant variability of hyperlocal\nrainfall, leading to substantial water waste. To improve irrigation efficiency,\nwe developed a cost-effective irrigation system, dubbed ERIC, which employs\nmachine learning models to estimate rainfall from commodity doorbell camera\nfootage and optimizes irrigation schedules without human intervention.\nSpecifically, we: a) designed novel visual and audio features with lightweight\nneural network models to infer rainfall from the camera at the edge, preserving\nuser privacy; b) built a complete end-to-end irrigation system on Raspberry Pi\n4, costing only \\$75. We deployed the system across five locations (collecting\nover 750 hours of video) with varying backgrounds and light conditions.\nComprehensive evaluation validates that ERIC achieves state-of-the-art rainfall\nestimation performance ($\\sim$ 5mm/day), saving 9,112 gallons/month of water,\ntranslating to \\$28.56/month in utility savings. Data and code are available at\nhttps://github.com/LENSS/ERIC-BuildSys2024.git\n","authors":["Tian Liu","Liuyi Jin","Radu Stoleru","Amran Haroon","Charles Swanson","Kexin Feng"],"pdf_url":"https://arxiv.org/pdf/2409.13104v2.pdf","comment":"BuildSys 2024"},{"id":"http://arxiv.org/abs/2404.06479v4","updated":"2024-10-03T21:59:32Z","published":"2024-04-09T17:30:18Z","title":"Visually Descriptive Language Model for Vector Graphics Reasoning","summary":"  Despite significant advancements, large multimodal models (LMMs) still\nstruggle to bridge the gap between low-level visual perception -- focusing on\nshapes, sizes, and layouts -- and high-level language reasoning, such as\nsemantics and logic. This limitation is evident in tasks that require precise\nvisual perception, like comparing geometric properties or solving visual\nreasoning problems. To study this failure mode, we focus on vector graphics --\nimages composed of 2D objects and shapes, prevalent in LMM-based tasks in web,\ndesign, and OS environments. We identify two key research questions: how can we\nenable precise visual perception, and how can we facilitate high-level\nreasoning based on such low-level perceptions? To capture fine visual details,\nwe use Scalable Vector Graphics (SVG) for accurate encoding of visual scenes.\nHowever, SVGs are not readily interpretable by LMMs in a zero-shot manner. To\ntackle this, we propose the Visually Descriptive Language Model (VDLM), which\nintroduces a Primal Visual Description (PVD) as an intermediate textual\nrepresentation. PVD translates SVGs into a text-based abstraction consisting of\nprimitive attributes (e.g., shape, position, measurement) and their\ncorresponding values. PVD can be learned using task-agnostic synthesized data\nand represents visual primitives that are universal across vector graphics.\nThis abstraction is more structured, allowing for direct interpretation by\nfoundation models for zero-shot generalization. Without human-annotated data,\nempirical results show that VDLM significantly improves state-of-the-art LMMs\nlike GPT-4o on various multimodal perception and reasoning tasks. Extensive\nanalyses of VDLM show improved interpretability due to its disentangled\nperception and reasoning. We also demonstrate a positive correlation between\nPVD quality and task performance. Project page:\nhttps://mikewangwzhl.github.io/VDLM/\n","authors":["Zhenhailong Wang","Joy Hsu","Xingyao Wang","Kuan-Hao Huang","Manling Li","Jiajun Wu","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2404.06479v4.pdf","comment":"Project page: https://mikewangwzhl.github.io/VDLM/"},{"id":"http://arxiv.org/abs/2410.03010v1","updated":"2024-10-03T21:41:12Z","published":"2024-10-03T21:41:12Z","title":"MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection","summary":"  Multimodal learning seeks to combine data from multiple input sources to\nenhance the performance of different downstream tasks. In real-world scenarios,\nperformance can degrade substantially if some input modalities are missing.\nExisting methods that can handle missing modalities involve custom training or\nadaptation steps for each input modality combination. These approaches are\neither tied to specific modalities or become computationally expensive as the\nnumber of input modalities increases. In this paper, we propose Masked Modality\nProjection (MMP), a method designed to train a single model that is robust to\nany missing modality scenario. We achieve this by randomly masking a subset of\nmodalities during training and learning to project available input modalities\nto estimate the tokens for the masked modalities. This approach enables the\nmodel to effectively learn to leverage the information from the available\nmodalities to compensate for the missing ones, enhancing missing modality\nrobustness. We conduct a series of experiments with various baseline models and\ndatasets to assess the effectiveness of this strategy. Experiments demonstrate\nthat our approach improves robustness to different missing modality scenarios,\noutperforming existing methods designed for missing modalities or specific\nmodality combinations.\n","authors":["Niki Nezakati","Md Kaykobad Reza","Ameya Patil","Mashhour Solh","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2410.03010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13874v4","updated":"2024-10-03T21:40:37Z","published":"2024-04-22T04:49:22Z","title":"VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) suffer from hallucination issues,\nwherein the models generate plausible-sounding but factually incorrect outputs,\nundermining their reliability. A comprehensive quantitative evaluation is\nnecessary to identify and understand the extent of hallucinations in these\nmodels. However, existing benchmarks are often limited in scope, focusing\nmainly on object hallucinations. Furthermore, current evaluation methods\nstruggle to effectively address the subtle semantic distinctions between model\noutputs and reference data, as well as the balance between hallucination and\ninformativeness. To address these issues, we introduce a multi-dimensional\nbenchmark covering objects, attributes, and relations, with challenging images\nselected based on associative biases. Moreover, we propose a large language\nmodel (LLM)-based two-stage evaluation framework that generalizes the popular\nCHAIR metric and incorporates both faithfulness and coverage into the\nevaluation. Experiments on 10 established LVLMs demonstrate that our evaluation\nmetric is more comprehensive and better correlated with humans than existing\nwork when evaluating on our challenging human-annotated benchmark dataset. Our\nwork also highlights the critical balance between faithfulness and coverage of\nmodel outputs, and encourages future works to address hallucinations in LVLMs\nwhile keeping their outputs informative.\n","authors":["Haoyi Qiu","Wenbo Hu","Zi-Yi Dou","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2404.13874v4.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2410.02988v1","updated":"2024-10-03T20:58:07Z","published":"2024-10-03T20:58:07Z","title":"Fully Automated CTC Detection, Segmentation and Classification for\n  Multi-Channel IF Imaging","summary":"  Liquid biopsies (eg., blood draws) offer a less invasive and non-localized\nalternative to tissue biopsies for monitoring the progression of metastatic\nbreast cancer (mBCa). Immunofluoresence (IF) microscopy is a tool to image and\nanalyze millions of blood cells in a patient sample. By detecting and\ngenetically sequencing circulating tumor cells (CTCs) in the blood,\npersonalized treatment plans are achievable for various cancer subtypes.\nHowever, CTCs are rare (about 1 in 2M), making manual CTC detection very\ndifficult. In addition, clinicians rely on quantitative cellular biomarkers to\nmanually classify CTCs. This requires prior tasks of cell detection,\nsegmentation and feature extraction. To assist clinicians, we have developed a\nfully automated machine learning-based production-level pipeline to efficiently\ndetect, segment and classify CTCs in multi-channel IF images. We achieve over\n99% sensitivity and 97% specificity on 9,533 cells from 15 mBCa patients. Our\npipeline has been successfully deployed on real mBCa patients, reducing a\npatient average of 14M detected cells to only 335 CTC candidates for manual\nreview.\n","authors":["Evan Schwab","Bharat Annaldas","Nisha Ramesh","Anna Lundberg","Vishal Shelke","Xinran Xu","Cole Gilbertson","Jiyun Byun","Ernest T. Lam"],"pdf_url":"https://arxiv.org/pdf/2410.02988v1.pdf","comment":"Published in MICCAI 2024 MOVI Workshop Conference Proceedings"},{"id":"http://arxiv.org/abs/2410.02981v1","updated":"2024-10-03T20:45:23Z","published":"2024-10-03T20:45:23Z","title":"GABIC: Graph-based Attention Block for Image Compression","summary":"  While standardized codecs like JPEG and HEVC-intra represent the industry\nstandard in image compression, neural Learned Image Compression (LIC) codecs\nrepresent a promising alternative. In detail, integrating attention mechanisms\nfrom Vision Transformers into LIC models has shown improved compression\nefficiency. However, extra efficiency often comes at the cost of aggregating\nredundant features. This work proposes a Graph-based Attention Block for Image\nCompression (GABIC), a method to reduce feature redundancy based on a k-Nearest\nNeighbors enhanced attention mechanism. Our experiments show that GABIC\noutperforms comparable methods, particularly at high bit rates, enhancing\ncompression performance.\n","authors":["Gabriele Spadaro","Alberto Presta","Enzo Tartaglione","Jhony H. Giraldo","Marco Grangetto","Attilio Fiandrotti"],"pdf_url":"https://arxiv.org/pdf/2410.02981v1.pdf","comment":"10 pages, 5 figures, accepted at ICIP 2024"},{"id":"http://arxiv.org/abs/2309.13042v2","updated":"2024-10-03T20:32:33Z","published":"2023-09-22T17:59:42Z","title":"MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary\n  Instance Segmentation","summary":"  We present MosaicFusion, a simple yet effective diffusion-based data\naugmentation approach for large vocabulary instance segmentation. Our method is\ntraining-free and does not rely on any label supervision. Two key designs\nenable us to employ an off-the-shelf text-to-image diffusion model as a useful\ndataset generator for object instances and mask annotations. First, we divide\nan image canvas into several regions and perform a single round of diffusion\nprocess to generate multiple instances simultaneously, conditioning on\ndifferent text prompts. Second, we obtain corresponding instance masks by\naggregating cross-attention maps associated with object prompts across layers\nand diffusion time steps, followed by simple thresholding and edge-aware\nrefinement processing. Without bells and whistles, our MosaicFusion can produce\na significant amount of synthetic labeled data for both rare and novel\ncategories. Experimental results on the challenging LVIS long-tailed and\nopen-vocabulary benchmarks demonstrate that MosaicFusion can significantly\nimprove the performance of existing instance segmentation models, especially\nfor rare and novel categories. Code: https://github.com/Jiahao000/MosaicFusion.\n","authors":["Jiahao Xie","Wei Li","Xiangtai Li","Ziwei Liu","Yew Soon Ong","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2309.13042v2.pdf","comment":"International Journal of Computer Vision (IJCV), 2024"},{"id":"http://arxiv.org/abs/2409.08520v2","updated":"2024-10-03T20:01:54Z","published":"2024-09-13T03:40:58Z","title":"GroundingBooth: Grounding Text-to-Image Customization","summary":"  Recent studies in text-to-image customization show great success in\ngenerating personalized object variants given several images of a subject.\nWhile existing methods focus more on preserving the identity of the subject,\nthey often fall short of controlling the spatial relationship between objects.\nIn this work, we introduce GroundingBooth, a framework that achieves zero-shot\ninstance-level spatial grounding on both foreground subjects and background\nobjects in the text-to-image customization task. Our proposed text-image\ngrounding module and masked cross-attention layer allow us to generate\npersonalized images with both accurate layout alignment and identity\npreservation while maintaining text-image coherence. With such layout control,\nour model inherently enables the customization of multiple subjects at once.\nOur model is evaluated on both layout-guided image synthesis and\nreference-based customization tasks, showing strong results compared to\nexisting methods. Our work is the first work to achieve a joint grounding on\nboth subject-driven foreground generation and text-driven background\ngeneration.\n","authors":["Zhexiao Xiong","Wei Xiong","Jing Shi","He Zhang","Yizhi Song","Nathan Jacobs"],"pdf_url":"https://arxiv.org/pdf/2409.08520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02942v1","updated":"2024-10-03T19:37:40Z","published":"2024-10-03T19:37:40Z","title":"SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric\n  Groups","summary":"  Finite symmetric groups $S_n$ are essential in fields such as combinatorics,\nphysics, and chemistry. However, learning a probability distribution over $S_n$\nposes significant challenges due to its intractable size and discrete nature.\nIn this paper, we introduce SymmetricDiffusers, a novel discrete diffusion\nmodel that simplifies the task of learning a complicated distribution over\n$S_n$ by decomposing it into learning simpler transitions of the reverse\ndiffusion using deep neural networks. We identify the riffle shuffle as an\neffective forward transition and provide empirical guidelines for selecting the\ndiffusion length based on the theory of random walks on finite groups.\nAdditionally, we propose a generalized Plackett-Luce (PL) distribution for the\nreverse transition, which is provably more expressive than the PL distribution.\nWe further introduce a theoretically grounded \"denoising schedule\" to improve\nsampling and learning efficiency. Extensive experiments show that our model\nachieves state-of-the-art or comparable performances on solving tasks including\nsorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.\nOur code is released at https://github.com/NickZhang53/SymmetricDiffusers.\n","authors":["Yongxing Zhang","Donglin Yang","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2410.02942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02924v1","updated":"2024-10-03T19:18:13Z","published":"2024-10-03T19:18:13Z","title":"RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through\n  Language Descriptions","summary":"  We propose a method for metric-scale monocular depth estimation. Inferring\ndepth from a single image is an ill-posed problem due to the loss of scale from\nperspective projection during the image formation process. Any scale chosen is\na bias, typically stemming from training on a dataset; hence, existing works\nhave instead opted to use relative (normalized, inverse) depth. Our goal is to\nrecover metric-scaled depth maps through a linear transformation. The crux of\nour method lies in the observation that certain objects (e.g., cars, trees,\nstreet signs) are typically found or associated with certain types of scenes\n(e.g., outdoor). We explore whether language descriptions can be used to\ntransform relative depth predictions to those in metric scale. Our method, RSA,\ntakes as input a text caption describing objects present in an image and\noutputs the parameters of a linear transformation which can be applied globally\nto a relative depth map to yield metric-scaled depth predictions. We\ndemonstrate our method on recent general-purpose monocular depth models on\nindoors (NYUv2) and outdoors (KITTI). When trained on multiple datasets, RSA\ncan serve as a general alignment module in zero-shot settings. Our method\nimproves over common practices in aligning relative to metric depth and results\nin predictions that are comparable to an upper bound of fitting relative depth\nto ground truth via a linear transformation.\n","authors":["Ziyao Zeng","Yangchao Wu","Hyoungseob Park","Daniel Wang","Fengyu Yang","Stefano Soatto","Dong Lao","Byung-Woo Hong","Alex Wong"],"pdf_url":"https://arxiv.org/pdf/2410.02924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02921v1","updated":"2024-10-03T19:13:28Z","published":"2024-10-03T19:13:28Z","title":"AirLetters: An Open Video Dataset of Characters Drawn in the Air","summary":"  We introduce AirLetters, a new video dataset consisting of real-world videos\nof human-generated, articulated motions. Specifically, our dataset requires a\nvision model to predict letters that humans draw in the air. Unlike existing\nvideo datasets, accurate classification predictions for AirLetters rely\ncritically on discerning motion patterns and on integrating long-range\ninformation in the video over time. An extensive evaluation of state-of-the-art\nimage and video understanding models on AirLetters shows that these methods\nperform poorly and fall far behind a human baseline. Our work shows that,\ndespite recent progress in end-to-end video understanding, accurate\nrepresentations of complex articulated motions -- a task that is trivial for\nhumans -- remains an open problem for end-to-end learning.\n","authors":["Rishit Dagli","Guillaume Berger","Joanna Materzynska","Ingo Bax","Roland Memisevic"],"pdf_url":"https://arxiv.org/pdf/2410.02921v1.pdf","comment":"ECCV'24, HANDS workshop"},{"id":"http://arxiv.org/abs/2410.02894v1","updated":"2024-10-03T18:32:33Z","published":"2024-10-03T18:32:33Z","title":"Task-Decoupled Image Inpainting Framework for Class-specific Object\n  Remover","summary":"  Object removal refers to the process of erasing designated objects from an\nimage while preserving the overall appearance. Existing works on object removal\nerase removal targets using image inpainting networks. However, image\ninpainting networks often generate unsatisfactory removal results. In this\nwork, we find that the current training approach which encourages a single\nimage inpainting model to handle both object removal and restoration tasks is\none of the reasons behind such unsatisfactory result. Based on this finding, we\npropose a task-decoupled image inpainting framework which generates two\nseparate inpainting models: an object restorer for object restoration tasks and\nan object remover for object removal tasks. We train the object restorer with\nthe masks that partially cover the removal targets. Then, the proposed\nframework makes an object restorer to generate a guidance for training the\nobject remover. Using the proposed framework, we obtain a class-specific object\nremover which focuses on removing objects of a target class, aiming to better\nerase target class objects than general object removers. We also introduce a\ndata curation method that encompasses the image selection and mask generation\napproaches used to produce training data for the proposed class-specific object\nremover. Using the proposed curation method, we can simulate the scenarios\nwhere an object remover is trained on the data with object removal ground truth\nimages. Experiments on multiple datasets show that the proposed class-specific\nobject remover can better remove target class objects than object removers\nbased on image inpainting networks.\n","authors":["Changsuk Oh","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2410.02894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10582v2","updated":"2024-10-03T18:26:42Z","published":"2024-09-16T04:16:52Z","title":"WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency","summary":"  Recent advancements in single image super-resolution have been predominantly\ndriven by token mixers and transformer architectures. WaveMixSR utilized the\nWaveMix architecture, employing a two-dimensional discrete wavelet transform\nfor spatial token mixing, achieving superior performance in super-resolution\ntasks with remarkable resource efficiency. In this work, we present an enhanced\nversion of the WaveMixSR architecture by (1) replacing the traditional\ntranspose convolution layer with a pixel shuffle operation and (2) implementing\na multistage design for higher resolution tasks ($4\\times$). Our experiments\ndemonstrate that our enhanced model -- WaveMixSR-V2 -- outperforms other\narchitectures in multiple super-resolution tasks, achieving state-of-the-art\nfor the BSD100 dataset, while also consuming fewer resources, exhibits higher\nparameter efficiency, lower latency and higher throughput. Our code is\navailable at https://github.com/pranavphoenix/WaveMixSR.\n","authors":["Pranav Jeevan","Neeraj Nixon","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2409.10582v2.pdf","comment":"10 pages. arXiv admin note: text overlap with arXiv:2307.00430"},{"id":"http://arxiv.org/abs/2305.07857v2","updated":"2024-10-03T18:25:58Z","published":"2023-05-13T07:51:35Z","title":"AURA : Automatic Mask Generator using Randomized Input Sampling for\n  Object Removal","summary":"  The objective of the image inpainting task is to fill missing regions of an\nimage in a visually plausible way. Recently, deep-learning-based image\ninpainting networks have generated outstanding results, and some utilize their\nmodels as object removers by masking unwanted objects in an image. However,\nwhile trying to better remove objects using their networks, the previous works\npay less attention to the importance of the input mask. In this paper, we focus\non generating the input mask to better remove objects using the off-the-shelf\nimage inpainting network. We propose an automatic mask generator inspired by\nthe explainable AI (XAI) method, whose output can better remove objects than a\nsemantic segmentation mask. The proposed method generates an importance map\nusing randomly sampled input masks and quantitatively estimated scores of the\ncompleted images obtained from the random masks. The output mask is selected by\na judge module among the candidate masks which are generated from the\nimportance map. We design the judge module to quantitatively estimate the\nquality of the object removal results. In addition, we empirically find that\nthe evaluation methods used in the previous works reporting object removal\nresults are not appropriate for estimating the performance of an object\nremover. Therefore, we propose new evaluation metrics (FID$^*$ and U-IDS$^*$)\nto properly evaluate the quality of object removers. Experiments confirm that\nour method shows better performance in removing target class objects than the\nmasks generated from the semantic segmentation maps, and the two proposed\nmetrics make judgments consistent with humans.\n","authors":["Changsuk Oh","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2305.07857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01804v2","updated":"2024-10-03T18:21:07Z","published":"2024-10-02T17:59:09Z","title":"EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis","summary":"  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n","authors":["Alexander Mai","Peter Hedman","George Kopanas","Dor Verbin","David Futschik","Qiangeng Xu","Falko Kuester","Jonathan T. Barron","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01804v2.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/ever"},{"id":"http://arxiv.org/abs/2403.10492v3","updated":"2024-10-03T18:08:57Z","published":"2024-03-15T17:27:12Z","title":"Mitigating Dialogue Hallucination for Large Vision Language Models via\n  Adversarial Instruction Tuning","summary":"  Mitigating hallucinations of Large Vision Language Models,(LVLMs) is crucial\nto enhance their reliability for general-purpose assistants. This paper shows\nthat such hallucinations of LVLMs can be significantly exacerbated by preceding\nuser-system dialogues. To precisely measure this, we first present an\nevaluation benchmark by extending popular multi-modal benchmark datasets with\nprepended hallucinatory dialogues powered by our novel Adversarial Question\nGenerator (AQG), which can automatically generate image-related yet adversarial\ndialogues by adopting adversarial attacks on LVLMs. On our benchmark, the\nzero-shot performance of state-of-the-art LVLMs drops significantly for both\nthe VQA and Captioning tasks. Next, we further reveal this hallucination is\nmainly due to the prediction bias toward preceding dialogues rather than visual\ncontent. To reduce this bias, we propose Adversarial Instruction Tuning (AIT)\nthat robustly fine-tunes LVLMs against hallucinatory dialogues. Extensive\nexperiments show our proposed approach successfully reduces dialogue\nhallucination while maintaining performance.\n","authors":["Dongmin Park","Zhaofang Qian","Guangxing Han","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2403.10492v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04559v3","updated":"2024-10-03T18:02:07Z","published":"2024-07-05T14:48:15Z","title":"Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition","summary":"  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n","authors":["Aditya K Surikuchi","Raquel Fernndez","Sandro Pezzelle"],"pdf_url":"https://arxiv.org/pdf/2407.04559v3.pdf","comment":"In proceedings of EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.02870v1","updated":"2024-10-03T18:01:41Z","published":"2024-10-03T18:01:41Z","title":"Individuation of 3D perceptual units from neurogeometry of binocular\n  cells","summary":"  We model the functional architecture of the early stages of three-dimensional\nvision by extending the neurogeometric sub-Riemannian model for stereo-vision\nintroduced in \\cite{BCSZ23}. A new framework for correspondence is introduced\nthat integrates a neural-based algorithm to achieve stereo correspondence\nlocally while, simultaneously, organizing the corresponding points into global\nperceptual units. The result is an effective scene segmentation. We achieve\nthis using harmonic analysis on the sub-Riemannian structure and show, in a\ncomparison against Riemannian distance, that the sub-Riemannian metric is\ncentral to the solution.\n","authors":["Maria Virginia Bolelli","Giovanna Citti","Alessandro Sarti","Steven W. Zucker"],"pdf_url":"https://arxiv.org/pdf/2410.02870v1.pdf","comment":"30 pages, 13 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.18511v3","updated":"2024-10-03T01:44:40Z","published":"2024-09-27T07:46:06Z","title":"Do We Need Domain-Specific Embedding Models? An Empirical Investigation","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advancements in Large\nLanguage Models (LLMs) have further enhanced the performance of embedding\nmodels, which are trained on massive amounts of text covering almost every\ndomain. These models are often benchmarked on general-purpose datasets like\nMassive Text Embedding Benchmark (MTEB), where they demonstrate superior\nperformance. However, a critical question arises: Is the development of\ndomain-specific embedding models necessary when general-purpose models are\ntrained on vast corpora that already include specialized domain texts? In this\npaper, we empirically investigate this question, choosing the finance domain as\nan example. We introduce the Finance Massive Text Embedding Benchmark\n(FinMTEB), a counterpart to MTEB that consists of financial domain-specific\ntext datasets. We evaluate the performance of seven state-of-the-art embedding\nmodels on FinMTEB and observe a significant performance drop compared to their\nperformance on MTEB. To account for the possibility that this drop is driven by\nFinMTEB's higher complexity, we propose four measures to quantify dataset\ncomplexity and control for this factor in our analysis. Our analysis provides\ncompelling evidence that state-of-the-art embedding models struggle to capture\ndomain-specific linguistic and semantic patterns. Moreover, we find that the\nperformance of general-purpose embedding models on MTEB is not correlated with\ntheir performance on FinMTEB, indicating the need for domain-specific embedding\nbenchmarks for domain-specific embedding models. This study sheds light on\ndeveloping domain-specific embedding models in the LLM era. FinMTEB comes with\nopen-source code at https://github.com/yixuantt/FinMTEB\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.18511v3.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2409.18957v2","updated":"2024-10-03T17:57:07Z","published":"2024-09-27T17:58:50Z","title":"LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction","summary":"  Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2409.18957v2.pdf","comment":"Updated title, abstract, and images"},{"id":"http://arxiv.org/abs/2409.15558v2","updated":"2024-10-03T10:40:23Z","published":"2024-09-23T21:29:03Z","title":"Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning\n  Systems","summary":"  Machine learning (ML) models trained on datasets owned by different\norganizations and physically located in remote databases offer benefits in many\nreal-world use cases. State regulations or business requirements often prevent\ndata transfer to a central location, making it difficult to utilize standard\nmachine learning algorithms. Federated Learning (FL) is a technique that\nenables models to learn from distributed datasets without revealing the\noriginal data. Vertical Federated learning (VFL) is a type of FL where data\nsamples are divided by features across several data owners. For instance, in a\nrecommendation task, a user can interact with various sets of items, and the\nlogs of these interactions are stored by different organizations. In this demo\npaper, we present \\emph{Stalactite} - an open-source framework for VFL that\nprovides the necessary functionality for building prototypes of VFL systems. It\nhas several advantages over the existing frameworks. In particular, it allows\nresearchers to focus on the algorithmic side rather than engineering and to\neasily deploy learning in a distributed environment. It implements several VFL\nalgorithms and has a built-in homomorphic encryption layer. We demonstrate its\nuse on a real-world recommendation datasets.\n","authors":["Anastasiia Zakharova","Dmitriy Alexandrov","Maria Khodorchenko","Nikolay Butakov","Alexey Vasilev","Maxim Savchenko","Alexander Grigorievskiy"],"pdf_url":"https://arxiv.org/pdf/2409.15558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02729v1","updated":"2024-10-03T17:49:09Z","published":"2024-10-03T17:49:09Z","title":"Unified Multi-Modal Interleaved Document Representation for Information\n  Retrieval","summary":"  Information Retrieval (IR) methods aim to identify relevant documents in\nresponse to a given query, which have gained remarkable attention due to their\nsuccessful application in various natural language tasks. However, existing\napproaches typically consider only the textual information within the\ndocuments, which overlooks the fact that documents can contain multiple\nmodalities, including texts, images, and tables. Further, they often segment\neach long document into multiple discrete passages for embedding, preventing\nthem from capturing the overall document context and interactions between\nparagraphs. We argue that these two limitations lead to suboptimal document\nrepresentations for retrieval. In this work, to address them, we aim to produce\nmore comprehensive and nuanced document representations by holistically\nembedding documents interleaved with different modalities. Specifically, we\nachieve this by leveraging the capability of recent vision-language models that\nenable the processing and integration of text, images, and tables into a\nunified format and representation. Moreover, to mitigate the information loss\nfrom segmenting documents into passages, instead of representing and retrieving\npassages individually, we further merge the representations of segmented\npassages into one single document representation, while we additionally\nintroduce a reranking strategy to decouple and identify the relevant passage\nwithin the document if necessary. Then, through extensive experiments on\ndiverse information retrieval scenarios considering both the textual and\nmultimodal queries, we show that our approach substantially outperforms\nrelevant baselines, thanks to the consideration of the multimodal information\ninterleaved within the documents in a unified way.\n","authors":["Jaewoo Lee","Joonho Ko","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.02729v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02721v1","updated":"2024-10-03T17:40:55Z","published":"2024-10-03T17:40:55Z","title":"Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization","summary":"  Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.\n","authors":["Ryan C. Barron","Ves Grantcharov","Selma Wanna","Maksim E. Eren","Manish Bhattarai","Nicholas Solovyev","George Tompkins","Charles Nicholas","Kim . Rasmussen","Cynthia Matuszek","Boian S. Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2410.02721v1.pdf","comment":"9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024"},{"id":"http://arxiv.org/abs/2410.02642v1","updated":"2024-10-03T16:25:37Z","published":"2024-10-03T16:25:37Z","title":"Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers","summary":"  Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.\n","authors":["Shijie Chen","Bernal Jimnez Gutirrez","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.02642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02604v1","updated":"2024-10-03T15:45:15Z","published":"2024-10-03T15:45:15Z","title":"Long-Sequence Recommendation Models Need Decoupled Embeddings","summary":"  Lifelong user behavior sequences, comprising up to tens of thousands of\nhistory behaviors, are crucial for capturing user interests and predicting user\nresponses in modern recommendation systems. A two-stage paradigm is typically\nadopted to handle these long sequences: a few relevant behaviors are first\nsearched from the original long sequences via an attention mechanism in the\nfirst stage and then aggregated with the target item to construct a\ndiscriminative representation for prediction in the second stage. In this work,\nwe identify and characterize, for the first time, a neglected deficiency in\nexisting long-sequence recommendation models: a single set of embeddings\nstruggles with learning both attention and representation, leading to\ninterference between these two processes. Initial attempts to address this\nissue using linear projections -- a technique borrowed from language processing\n-- proved ineffective, shedding light on the unique challenges of\nrecommendation models. To overcome this, we propose the Decoupled Attention and\nRepresentation Embeddings (DARE) model, where two distinct embedding tables are\ninitialized and learned separately to fully decouple attention and\nrepresentation. Extensive experiments and analysis demonstrate that DARE\nprovides more accurate search of correlated behaviors and outperforms baselines\nwith AUC gains up to 0.9% on public datasets and notable online system\nimprovements. Furthermore, decoupling embedding spaces allows us to reduce the\nattention embedding dimension and accelerate the search procedure by 50%\nwithout significant performance impact, enabling more efficient,\nhigh-performance online serving.\n","authors":["Ningya Feng","Junwei Pan","Jialong Wu","Baixu Chen","Ximei Wang","Qian Li","Xian Hu","Jie Jiang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2410.02604v1.pdf","comment":"First three authors contributed equally"},{"id":"http://arxiv.org/abs/2305.18952v4","updated":"2024-10-03T15:08:07Z","published":"2023-05-27T16:05:00Z","title":"Exploring the Practicality of Generative Retrieval on Dynamic Corpora","summary":"  Benchmarking the performance of information retrieval (IR) is mostly\nconducted with a fixed set of documents (static corpora). However, in realistic\nscenarios, this is rarely the case and the documents to be retrieved are\nconstantly updated and added. In this paper, we focus on Generative Retrievals\n(GR), which apply autoregressive language models to IR problems, and explore\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor real-world deployment of IR systems handling vast and ever-changing\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\nGR is more adaptable to evolving knowledge (4 -- 11%), robust in learning\nknowledge with temporal information, and efficient in terms of inference FLOPs\n(x 2), indexing time (x 6), and storage footprint (x 4) compared to Dual\nEncoders (DE), which are commonly used in retrieval systems. Our paper\nhighlights the potential of GR for future use in practical IR systems within\ndynamic environments.\n","authors":["Chaeeun Kim","Soyoung Yoon","Hyunji Lee","Joel Jang","Sohee Yang","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2305.18952v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07103v3","updated":"2024-10-03T13:55:08Z","published":"2024-04-10T15:41:53Z","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs","summary":"  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n","authors":["Bowen Jin","Chulin Xie","Jiawei Zhang","Kashob Kumar Roy","Yu Zhang","Zheng Li","Ruirui Li","Xianfeng Tang","Suhang Wang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2404.07103v3.pdf","comment":"21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT"},{"id":"http://arxiv.org/abs/2410.02453v1","updated":"2024-10-03T13:02:07Z","published":"2024-10-03T13:02:07Z","title":"Quantifying User Coherence: A Unified Framework for Cross-Domain\n  Recommendation Analysis","summary":"  The effectiveness of Recommender Systems (RS) is closely tied to the quality\nand distinctiveness of user profiles, yet despite many advancements in raw\nperformance, the sensitivity of RS to user profile quality remains\nunder-researched. This paper introduces novel information-theoretic measures\nfor understanding recommender systems: a \"surprise\" measure quantifying users'\ndeviations from popular choices, and a \"conditional surprise\" measure capturing\nuser interaction coherence. We evaluate 7 recommendation algorithms across 9\ndatasets, revealing the relationships between our measures and standard\nperformance metrics. Using a rigorous statistical framework, our analysis\nquantifies how much user profile density and information measures impact\nalgorithm performance across domains. By segmenting users based on these\nmeasures, we achieve improved performance with reduced data and show that\nsimpler algorithms can match complex ones for low-coherence users.\nAdditionally, we employ our measures to analyze how well different\nrecommendation algorithms maintain the coherence and diversity of user\npreferences in their predictions, providing insights into algorithm behavior.\nThis work advances the theoretical understanding of user behavior and practical\nheuristics for personalized recommendation systems, promoting more efficient\nand adaptive architectures.\n","authors":["Michal Soumm","Alexandre Fournier-Montgieux","Adrian Popescu","Bertrand Delezoide"],"pdf_url":"https://arxiv.org/pdf/2410.02453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02219v1","updated":"2024-10-03T05:23:39Z","published":"2024-10-03T05:23:39Z","title":"Multi-modal clothing recommendation model based on large model and VAE\n  enhancement","summary":"  Accurately recommending products has long been a subject requiring in-depth\nresearch. This study proposes a multimodal paradigm for clothing\nrecommendations. Specifically, it designs a multimodal analysis method that\nintegrates clothing description texts and images, utilizing a pre-trained large\nlanguage model to deeply explore the hidden meanings of users and products.\nAdditionally, a variational encoder is employed to learn the relationship\nbetween user information and products to address the cold start problem in\nrecommendation systems. This study also validates the significant performance\nadvantages of this method over various recommendation system methods through\nextensive ablation experiments, providing crucial practical guidance for the\ncomprehensive optimization of recommendation systems.\n","authors":["Bingjie Huang","Qingyu Lu","Shuaishuai Huang","Xue-she Wang","Haowei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02191v1","updated":"2024-10-03T04:11:42Z","published":"2024-10-03T04:11:42Z","title":"A Survey on Point-of-Interest Recommendation: Models, Architectures, and\n  Security","summary":"  The widespread adoption of smartphones and Location-Based Social Networks has\nled to a massive influx of spatio-temporal data, creating unparalleled\nopportunities for enhancing Point-of-Interest (POI) recommendation systems.\nThese advanced POI systems are crucial for enriching user experiences, enabling\npersonalized interactions, and optimizing decision-making processes in the\ndigital landscape. However, existing surveys tend to focus on traditional\napproaches and few of them delve into cutting-edge developments, emerging\narchitectures, as well as security considerations in POI recommendations. To\naddress this gap, our survey stands out by offering a comprehensive, up-to-date\nreview of POI recommendation systems, covering advancements in models,\narchitectures, and security aspects. We systematically examine the transition\nfrom traditional models to advanced techniques such as large language models.\nAdditionally, we explore the architectural evolution from centralized to\ndecentralized and federated learning systems, highlighting the improvements in\nscalability and privacy. Furthermore, we address the increasing importance of\nsecurity, examining potential vulnerabilities and privacy-preserving\napproaches. Our taxonomy provides a structured overview of the current state of\nPOI recommendation, while we also identify promising directions for future\nresearch in this rapidly advancing field.\n","authors":["Qianru Zhang","Peng Yang","Junliang Yu","Haixin Wang","Xingwei He","Siu-Ming Yiu","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.02191v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2407.05441v2","updated":"2024-10-03T03:41:56Z","published":"2024-07-07T17:05:24Z","title":"Language Representations Can be What Recommenders Need: Findings and\n  Potentials","summary":"  Recent studies empirically indicate that language models (LMs) encode rich\nworld knowledge beyond mere semantics, attracting significant attention across\nvarious fields. However, in the recommendation domain, it remains uncertain\nwhether LMs implicitly encode user preference information. Contrary to\nprevailing understanding that LMs and traditional recommenders learn two\ndistinct representation spaces due to the huge gap in language and behavior\nmodeling objectives, this work re-examines such understanding and explores\nextracting a recommendation space directly from the language representation\nspace. Surprisingly, our findings demonstrate that item representations, when\nlinearly mapped from advanced LM representations, yield superior recommendation\nperformance. This outcome suggests the possible homomorphism between the\nadvanced language representation space and an effective item representation\nspace for recommendation, implying that collaborative signals may be implicitly\nencoded within LMs. Motivated by these findings, we explore the possibility of\ndesigning advanced collaborative filtering (CF) models purely based on language\nrepresentations without ID-based embeddings. To be specific, we incorporate\nseveral crucial components to build a simple yet effective model, with item\ntitles as the input. Empirical results show that such a simple model can\noutperform leading ID-based CF models, which sheds light on using language\nrepresentations for better recommendation. Moreover, we systematically analyze\nthis simple model and find several key features for using advanced language\nrepresentations: a good initialization for item representations, zero-shot\nrecommendation abilities, and being aware of user intention. Our findings\nhighlight the connection between language modeling and behavior modeling, which\ncan inspire both natural language processing and recommender system\ncommunities.\n","authors":["Leheng Sheng","An Zhang","Yi Zhang","Yuxin Chen","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.05441v2.pdf","comment":"Codes are available at https://github.com/LehengTHU/AlphaRec"},{"id":"http://arxiv.org/abs/2407.17451v2","updated":"2024-10-03T01:51:55Z","published":"2024-07-24T17:31:48Z","title":"BlueTempNet: A Temporal Multi-network Dataset of Social Interactions in\n  Bluesky Social","summary":"  Decentralized social media platforms like Bluesky Social (Bluesky) have made\nit possible to publicly disclose some user behaviors with millisecond-level\nprecision. Embracing Bluesky's principles of open-source and open-data, we\npresent the first collection of the temporal dynamics of user-driven social\ninteractions. BlueTempNet integrates multiple types of networks into a single\nmulti-network, including user-to-user interactions (following and blocking\nusers) and user-to-community interactions (creating and joining communities).\nCommunities are user-formed groups in custom Feeds, where users subscribe to\nposts aligned with their interests. Following Bluesky's public data policy, we\ncollect existing Bluesky Feeds, including the users who liked and generated\nthese Feeds, and provide tools to gather users' social interactions within a\ndate range. This data-collection strategy captures past user behaviors and\nsupports the future data collection of user behavior.\n","authors":["Ujun Jeong","Bohan Jiang","Zhen Tan","H. Russell Bernard","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17451v2.pdf","comment":"accepted to IEEE Data Descriptions 24"},{"id":"http://arxiv.org/abs/2410.02126v1","updated":"2024-10-03T01:14:30Z","published":"2024-10-03T01:14:30Z","title":"BayesCNS: A Unified Bayesian Approach to Address Cold Start and\n  Non-Stationarity in Search Systems at Scale","summary":"  Information Retrieval (IR) systems used in search and recommendation\nplatforms frequently employ Learning-to-Rank (LTR) models to rank items in\nresponse to user queries. These models heavily rely on features derived from\nuser interactions, such as clicks and engagement data. This dependence\nintroduces cold start issues for items lacking user engagement and poses\nchallenges in adapting to non-stationary shifts in user behavior over time. We\naddress both challenges holistically as an online learning problem and propose\nBayesCNS, a Bayesian approach designed to handle cold start and non-stationary\ndistribution shifts in search systems at scale. BayesCNS achieves this by\nestimating prior distributions for user-item interactions, which are\ncontinuously updated with new user interactions gathered online. This online\nlearning procedure is guided by a ranker model, enabling efficient exploration\nof relevant items using contextual information provided by the ranker. We\nsuccessfully deployed BayesCNS in a large-scale search system and demonstrated\nits efficacy through comprehensive offline and online experiments. Notably, an\nonline A/B experiment showed a 10.60% increase in new item interactions and a\n1.05% improvement in overall success metrics over the existing production\nbaseline.\n","authors":["Randy Ardywibowo","Rakesh Sunki","Lucy Kuo","Sankalp Nayak"],"pdf_url":"https://arxiv.org/pdf/2410.02126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11035v3","updated":"2024-10-03T23:40:56Z","published":"2024-02-16T19:28:52Z","title":"Dense Passage Retrieval: Is it Retrieving?","summary":"  Dense passage retrieval (DPR) is the first step in the retrieval augmented\ngeneration (RAG) paradigm for improving the performance of large language\nmodels (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\nthe embeddings between queries and relevant textual data. A deeper\nunderstanding of DPR fine-tuning will be required to fundamentally unlock the\nfull potential of this approach. In this work, we explore DPR-trained models\nmechanistically by using a combination of probing, layer activation analysis,\nand model editing. Our experiments show that DPR training decentralizes how\nknowledge is stored in the network, creating multiple access pathways to the\nsame information. We also uncover a limitation in this training style: the\ninternal knowledge of the pre-trained model bounds what the retrieval model can\nretrieve. These findings suggest a few possible directions for dense retrieval:\n(1) expose the DPR training process to more knowledge so more can be\ndecentralized, (2) inject facts as decentralized representations, (3) model and\nincorporate knowledge uncertainty in the retrieval process, and (4) directly\nmap internal model knowledge to a knowledge base.\n","authors":["Benjamin Reichman","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2402.11035v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17890v2","updated":"2024-10-03T21:00:05Z","published":"2024-05-28T07:12:06Z","title":"SLMRec: Empowering Small Language Models for Sequential Recommendation","summary":"  Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR.\n","authors":["Wujiang Xu","Qitian Wu","Zujie Liang","Jiaojiao Han","Xuying Ning","Yunxiao Shi","Wenfang Lin","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.17890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18878v2","updated":"2024-10-03T20:49:55Z","published":"2024-09-27T16:13:38Z","title":"Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric\n  Hospital Using Multi-Label Classification with Pre-Trained Language Models","summary":"  Accurate identification and categorization of suicidal events can yield\nbetter suicide precautions, reducing operational burden, and improving care\nquality in high-acuity psychiatric settings. Pre-trained language models offer\npromise for identifying suicidality from unstructured clinical narratives. We\nevaluated the performance of four BERT-based models using two fine-tuning\nstrategies (multiple single-label and single multi-label) for detecting\ncoexisting suicidal events from 500 annotated psychiatric evaluation notes. The\nnotes were labeled for suicidal ideation (SI), suicide attempts (SA), exposure\nto suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformed\nother models using multiple single-label classification strategy (acc=0.86,\nF1=0.78). MentalBERT (acc=0.83, F1=0.74) also exceeded BioClinicalBERT\n(acc=0.82, F1=0.72) which outperformed BERT (acc=0.80, F1=0.70). RoBERTa\nfine-tuned with single multi-label classification further improved the model\nperformance (acc=0.88, F1=0.81). The findings highlight that the model\noptimization, pretraining with domain-relevant data, and the single multi-label\nclassification strategy enhance the model performance of suicide phenotyping.\nKeywords: EHR-based Phenotyping; Natural Language Processing; Secondary Use of\nEHR Data; Suicide Classification; BERT-based Model; Psychiatry; Mental Health\n","authors":["Zehan Li","Yan Hu","Scott Lane","Salih Selek","Lokesh Shahani","Rodrigo Machado-Vieira","Jair Soares","Hua Xu","Hongfang Liu","Ming Huang"],"pdf_url":"https://arxiv.org/pdf/2409.18878v2.pdf","comment":"submitted to AMIA Informatics Summit 2025 as a conference paper"},{"id":"http://arxiv.org/abs/2404.18304v2","updated":"2024-10-03T20:14:47Z","published":"2024-04-28T20:21:03Z","title":"Retrieval-Oriented Knowledge for Click-Through Rate Prediction","summary":"  Click-through rate (CTR) prediction is crucial for personalized online\nservices. Sample-level retrieval-based models, such as RIM, have demonstrated\nremarkable performance. However, they face challenges including inference\ninefficiency and high resource consumption due to the retrieval process, which\nhinder their practical application in industrial settings. To address this, we\npropose a universal plug-and-play \\underline{r}etrieval-\\underline{o}riented\n\\underline{k}nowledge (\\textbf{\\name}) framework that bypasses the real\nretrieval process. The framework features a knowledge base that preserves and\nimitates the retrieved \\& aggregated representations using a\ndecomposition-reconstruction paradigm. Knowledge distillation and contrastive\nlearning optimize the knowledge base, enabling the integration of\nretrieval-enhanced representations with various CTR models. Experiments on\nthree large-scale datasets demonstrate \\name's exceptional compatibility and\nperformance, with the neural knowledge base serving as an effective surrogate\nfor the retrieval pool. \\name surpasses the teacher model while maintaining\nsuperior inference efficiency and demonstrates the feasibility of distilling\nknowledge from non-parametric methods using a parametric approach. These\nresults highlight \\name's strong potential for real-world applications and its\nability to transform retrieval-based methods into practical solutions. Our\nimplementation code is available to support reproducibility in\n\\url{https://github.com/HSLiu-Initial/ROK.git}.\n","authors":["Huanshuo Liu","Bo Chen","Menghui Zhu","Jianghao Lin","Jiarui Qin","Yang Yang","Hao Zhang","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2404.18304v2.pdf","comment":"11 pages, 6 figures, 6 tables.Accepted by CIKM'24"},{"id":"http://arxiv.org/abs/2405.18727v2","updated":"2024-10-03T20:09:31Z","published":"2024-05-29T03:17:16Z","title":"CtrlA: Adaptive Retrieval-Augmented Generation via Inherent Control","summary":"  Retrieval-augmented generation (RAG) has emerged as a promising solution for\nmitigating hallucinations of large language models (LLMs) with retrieved\nexternal knowledge. Adaptive RAG enhances this approach by enabling dynamic\nretrieval during generation, activating retrieval only when the query exceeds\nLLM's internal knowledge. Existing methods primarily focus on detecting LLM's\nconfidence via statistical uncertainty. Instead, we present the first attempts\nto solve adaptive RAG from a representation perspective and develop an inherent\ncontrol-based framework, termed \\name. Specifically, we extract the features\nthat represent the honesty and confidence directions of LLM and adopt them to\ncontrol LLM behavior and guide retrieval timing decisions. We also design a\nsimple yet effective query formulation strategy to support adaptive retrieval.\nExperiments show that \\name is superior to existing adaptive RAG methods on a\ndiverse set of tasks, the honesty steering can effectively make LLMs more\nhonest and confidence monitoring is a promising indicator of retrieval\ntrigger.Our code is available at \\url{https://github.com/HSLiu-Initial/CtrlA}.\n","authors":["Huanshuo Liu","Hao Zhang","Zhijiang Guo","Jing Wang","Kuicai Dong","Xiangyang Li","Yi Quan Lee","Cong Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2405.18727v2.pdf","comment":"29 pages, 10 figures, 11 tables"},{"id":"http://arxiv.org/abs/2410.02939v1","updated":"2024-10-03T19:32:32Z","published":"2024-10-03T19:32:32Z","title":"Inductive Generative Recommendation via Retrieval-based Speculation","summary":"  Generative recommendation (GR) is an emerging paradigm that tokenizes items\ninto discrete tokens and learns to autoregressively generate the next tokens as\npredictions. Although effective, GR models operate in a transductive setting,\nmeaning they can only generate items seen during training without applying\nheuristic re-ranking strategies. In this paper, we propose SpecGR, a\nplug-and-play framework that enables GR models to recommend new items in an\ninductive setting. SpecGR uses a drafter model with inductive capability to\npropose candidate items, which may include both existing items and new items.\nThe GR model then acts as a verifier, accepting or rejecting candidates while\nretaining its strong ranking capabilities. We further introduce the guided\nre-drafting technique to make the proposed candidates more aligned with the\noutputs of generative recommendation models, improving the verification\nefficiency. We consider two variants for drafting: (1) using an auxiliary\ndrafter model for better flexibility, or (2) leveraging the GR model's own\nencoder for parameter-efficient self-drafting. Extensive experiments on three\nreal-world datasets demonstrate that SpecGR exhibits both strong inductive\nrecommendation ability and the best overall performance among the compared\nmethods. Our code is available at: https://github.com/Jamesding000/SpecGR.\n","authors":["Yijie Ding","Yupeng Hou","Jiacheng Li","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.02939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02914v1","updated":"2024-10-03T19:05:47Z","published":"2024-10-03T19:05:47Z","title":"Streamlining Conformal Information Retrieval via Score Refinement","summary":"  Information retrieval (IR) methods, like retrieval augmented generation, are\nfundamental to modern applications but often lack statistical guarantees.\nConformal prediction addresses this by retrieving sets guaranteed to include\nrelevant information, yet existing approaches produce large-sized sets,\nincurring high computational costs and slow response times. In this work, we\nintroduce a score refinement method that applies a simple monotone\ntransformation to retrieval scores, leading to significantly smaller conformal\nsets while maintaining their statistical guarantees. Experiments on various\nBEIR benchmarks validate the effectiveness of our approach in producing compact\nsets containing relevant information.\n","authors":["Yotam Intrator","Ori Kelner","Regev Cohen","Roman Goldenberg","Ehud Rivlin","Daniel Freedman"],"pdf_url":"https://arxiv.org/pdf/2410.02914v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.02897v1","updated":"2024-10-03T18:42:07Z","published":"2024-10-03T18:42:07Z","title":"Cognitive Biases in Large Language Models for News Recommendation","summary":"  Despite large language models (LLMs) increasingly becoming important\ncomponents of news recommender systems, employing LLMs in such systems\nintroduces new risks, such as the influence of cognitive biases in LLMs.\nCognitive biases refer to systematic patterns of deviation from norms or\nrationality in the judgment process, which can result in inaccurate outputs\nfrom LLMs, thus threatening the reliability of news recommender systems.\nSpecifically, LLM-based news recommender systems affected by cognitive biases\ncould lead to the propagation of misinformation, reinforcement of stereotypes,\nand the formation of echo chambers. In this paper, we explore the potential\nimpact of multiple cognitive biases on LLM-based news recommender systems,\nincluding anchoring bias, framing bias, status quo bias and group attribution\nbias. Furthermore, to facilitate future research at improving the reliability\nof LLM-based news recommender systems, we discuss strategies to mitigate these\nbiases through data augmentation, prompt engineering and learning algorithms\naspects.\n","authors":["Yougang Lyu","Xiaoyu Zhang","Zhaochun Ren","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2410.02897v1.pdf","comment":"Accepted at the ROGEN '24 workshop, co-located with ACM RecSys '24"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.01778v2","updated":"2024-10-03T01:58:26Z","published":"2024-10-02T17:31:33Z","title":"TopER: Topological Embeddings in Graph Representation Learning","summary":"  Graph embeddings play a critical role in graph representation learning,\nallowing machine learning models to explore and interpret graph-structured\ndata. However, existing methods often rely on opaque, high-dimensional\nembeddings, limiting interpretability and practical visualization.\n  In this work, we introduce Topological Evolution Rate (TopER), a novel,\nlow-dimensional embedding approach grounded in topological data analysis. TopER\nsimplifies a key topological approach, Persistent Homology, by calculating the\nevolution rate of graph substructures, resulting in intuitive and interpretable\nvisualizations of graph data. This approach not only enhances the exploration\nof graph datasets but also delivers competitive performance in graph clustering\nand classification tasks. Our TopER-based models achieve or surpass\nstate-of-the-art results across molecular, biological, and social network\ndatasets in tasks such as classification, clustering, and visualization.\n","authors":["Astrit Tola","Funmilola Mary Taiwo","Cuneyt Gurcan Akcora","Baris Coskunuzer"],"pdf_url":"https://arxiv.org/pdf/2410.01778v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.01697v2","updated":"2024-10-03T09:28:48Z","published":"2024-10-02T16:05:03Z","title":"MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning","summary":"  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n","authors":["Sedjro Salomon Hotegni","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2410.01697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01687v2","updated":"2024-10-03T02:21:38Z","published":"2024-10-02T15:57:18Z","title":"Uncertainty Quantification with Bayesian Higher Order ReLU KANs","summary":"  We introduce the first method of uncertainty quantification in the domain of\nKolmogorov-Arnold Networks, specifically focusing on (Higher Order) ReLUKANs to\nenhance computational efficiency given the computational demands of Bayesian\nmethods. The method we propose is general in nature, providing access to both\nepistemic and aleatoric uncertainties. It is also capable of generalization to\nother various basis functions. We validate our method through a series of\nclosure tests, including simple one-dimensional functions and application to\nthe domain of (Stochastic) Partial Differential Equations. Referring to the\nlatter, we demonstrate the method's ability to correctly identify functional\ndependencies introduced through the inclusion of a stochastic term. The code\nsupporting this work can be found at\nhttps://github.com/wmdataphys/Bayesian-HR-KAN\n","authors":["James Giroux","Cristiano Fanelli"],"pdf_url":"https://arxiv.org/pdf/2410.01687v2.pdf","comment":"13 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2410.01574v2","updated":"2024-10-03T10:11:53Z","published":"2024-10-02T14:11:29Z","title":"Fake It Until You Break It: On the Adversarial Robustness of\n  AI-generated Image Detectors","summary":"  While generative AI (GenAI) offers countless possibilities for creative and\nproductive tasks, artificially generated media can be misused for fraud,\nmanipulation, scams, misinformation campaigns, and more. To mitigate the risks\nassociated with maliciously generated media, forensic classifiers are employed\nto identify AI-generated content. However, current forensic classifiers are\noften not evaluated in practically relevant scenarios, such as the presence of\nan attacker or when real-world artifacts like social media degradations affect\nimages. In this paper, we evaluate state-of-the-art AI-generated image (AIGI)\ndetectors under different attack scenarios. We demonstrate that forensic\nclassifiers can be effectively attacked in realistic settings, even when the\nattacker does not have access to the target model and post-processing occurs\nafter the adversarial examples are created, which is standard on social media\nplatforms. These attacks can significantly reduce detection accuracy to the\nextent that the risks of relying on detectors outweigh their benefits. Finally,\nwe propose a simple defense mechanism to make CLIP-based detectors, which are\ncurrently the best-performing detectors, robust against these attacks.\n","authors":["Sina Mavali","Jonas Ricker","David Pape","Yash Sharma","Asja Fischer","Lea Schnherr"],"pdf_url":"https://arxiv.org/pdf/2410.01574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01556v2","updated":"2024-10-03T03:11:24Z","published":"2024-10-02T13:52:55Z","title":"Integrative Decoding: Improve Factuality via Implicit Self-consistency","summary":"  Self-consistency-based approaches, which involve repeatedly sampling multiple\noutputs and selecting the most consistent one as the final response, prove to\nbe remarkably effective in improving the factual accuracy of large language\nmodels. Nonetheless, existing methods usually have strict constraints on the\ntask format, largely limiting their applicability. In this paper, we present\nIntegrative Decoding (ID), to unlock the potential of self-consistency in\nopen-ended generation tasks. ID operates by constructing a set of inputs, each\nprepended with a previously sampled response, and then processes them\nconcurrently, with the next token being selected by aggregating of all their\ncorresponding predictions at each decoding step. In essence, this simple\napproach implicitly incorporates self-consistency in the decoding objective.\nExtensive evaluation shows that ID consistently enhances factuality over a wide\nrange of language models, with substantial improvements on the TruthfulQA\n(+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance\ngains amplify progressively as the number of sampled responses increases,\nindicating the potential of ID to scale up with repeated sampling.\n","authors":["Yi Cheng","Xiao Liang","Yeyun Gong","Wen Xiao","Song Wang","Yuji Zhang","Wenjun Hou","Kaishuai Xu","Wenge Liu","Wenjie Li","Jian Jiao","Qi Chen","Peng Cheng","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.01556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01506v2","updated":"2024-10-03T05:50:09Z","published":"2024-10-02T12:58:55Z","title":"LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature\n  Fusion","summary":"  In computer vision tasks, features often come from diverse representations,\ndomains, and modalities, such as text, images, and videos. Effectively fusing\nthese features is essential for robust performance, especially with the\navailability of powerful pre-trained models like vision-language models.\nHowever, common fusion methods, such as concatenation, element-wise operations,\nand non-linear techniques, often fail to capture structural relationships, deep\nfeature interactions, and suffer from inefficiency or misalignment of features\nacross domains. In this paper, we shift from high-dimensional feature space to\na lower-dimensional, interpretable graph space by constructing similarity\ngraphs that encode feature relationships at different levels, e.g., clip,\nframe, patch, token, etc. To capture deeper interactions, we use graph power\nexpansions and introduce a learnable graph fusion operator to combine these\ngraph powers for more effective fusion. Our approach is relationship-centric,\noperates in a homogeneous space, and is mathematically principled, resembling\nelement-wise similarity score aggregation via multilinear polynomials. We\ndemonstrate the effectiveness of our graph-based fusion method on video anomaly\ndetection, showing strong performance across multi-representational,\nmulti-modal, and multi-domain feature fusion tasks.\n","authors":["Dexuan Ding","Lei Wang","Liyun Zhu","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2410.01506v2.pdf","comment":"Research paper"},{"id":"http://arxiv.org/abs/2410.02764v1","updated":"2024-10-03T17:59:59Z","published":"2024-10-03T17:59:59Z","title":"Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats","summary":"  We introduce a simple yet effective approach for separating transmitted and\nreflected light. Our key insight is that the powerful novel view synthesis\ncapabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian\nsplatting) allow one to perform flash/no-flash reflection separation using\nunpaired measurements -- this relaxation dramatically simplifies image\nacquisition over conventional paired flash/no-flash reflection separation\nmethods. Through extensive real-world experiments, we demonstrate our method,\nFlash-Splat, accurately reconstructs both transmitted and reflected scenes in\n3D. Our method outperforms existing 3D reflection separation methods, which do\nnot leverage illumination control, by a large margin. Our project webpage is at\nhttps://flash-splat.github.io/.\n","authors":["Mingyang Xie","Haoming Cai","Sachin Shah","Yiran Xu","Brandon Y. Feng","Jia-Bin Huang","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2410.02764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02763v1","updated":"2024-10-03T17:59:58Z","published":"2024-10-03T17:59:58Z","title":"Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos","summary":"  There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.\n","authors":["Jianrui Zhang","Mu Cai","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.02763v1.pdf","comment":"Project Page: https://vinoground.github.io"},{"id":"http://arxiv.org/abs/2410.02762v1","updated":"2024-10-03T17:59:57Z","published":"2024-10-03T17:59:57Z","title":"Interpreting and Editing Vision-Language Representations to Mitigate\n  Hallucinations","summary":"  We investigate the internal representations of vision-language models (VLMs)\nto address hallucinations, a persistent challenge despite advances in model\nsize and training. We project VLMs' internal image representations to their\nlanguage vocabulary and observe more confident output probabilities on real\nobjects than hallucinated objects. We additionally use these output\nprobabilities to spatially localize real objects. Building on this approach, we\nintroduce a knowledge erasure algorithm that removes hallucinations by linearly\northogonalizing image features with respect to hallucinated object features. We\nshow that targeted edits to a model's latent representations can reduce\nhallucinations by up to 25.7% on the COCO2014 dataset while preserving\nperformance. Our findings demonstrate how a deeper understanding of VLMs'\nlatent representations can enhance reliability and enable novel capabilities,\nsuch as zero-shot segmentation.\n","authors":["Nick Jiang","Anish Kachinthaya","Suzie Petryk","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2410.02762v1.pdf","comment":"Project page and code: http://anishk23733.github.io/vl-interp/"},{"id":"http://arxiv.org/abs/2410.02760v1","updated":"2024-10-03T17:59:30Z","published":"2024-10-03T17:59:30Z","title":"Erasing Conceptual Knowledge from Language Models","summary":"  Concept erasure in language models has traditionally lacked a comprehensive\nevaluation framework, leading to incomplete assessments of effectiveness of\nerasure methods. We propose an evaluation paradigm centered on three critical\ncriteria: innocence (complete knowledge removal), seamlessness (maintaining\nconditional fluent generation), and specificity (preserving unrelated task\nperformance). Our evaluation metrics naturally motivate the development of\nErasure of Language Memory (ELM), a new method designed to address all three\ndimensions. ELM employs targeted low-rank updates to alter output distributions\nfor erased concepts while preserving overall model capabilities including\nfluency when prompted for an erased concept. We demonstrate ELM's efficacy on\nbiosecurity, cybersecurity, and literary domain erasure tasks. Comparative\nanalysis shows that ELM achieves superior performance across our proposed\nmetrics, including near-random scores on erased topic assessments, generation\nfluency, maintained accuracy on unrelated benchmarks, and robustness under\nadversarial attacks. Our code, data, and trained models are available at\nhttps://elm.baulab.info\n","authors":["Rohit Gandikota","Sheridan Feucht","Samuel Marks","David Bau"],"pdf_url":"https://arxiv.org/pdf/2410.02760v1.pdf","comment":"Project Page: https://elm.baulab.info"},{"id":"http://arxiv.org/abs/2403.17916v2","updated":"2024-10-03T17:59:25Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as model input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies on the OPV2V and V2V4Real datasets, we demonstrate the effectiveness of\nour method in cooperative perception, tracking, and motion prediction. In\nparticular, CMP reduces the average prediction error by 16.4\\% with fewer\nmissing detections compared with the no cooperation setting and by 12.3\\%\ncompared with the strongest baseline. Our work marks a significant step forward\nin the cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios. The code can be found on the project website:\nhttps://cmp-cooperative-prediction.github.io/.\n","authors":["Zehao Wang","Yuping Wang","Zhuoyuan Wu","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v2.pdf","comment":"Project website: https://cmp-cooperative-prediction.github.io/"},{"id":"http://arxiv.org/abs/2410.02759v1","updated":"2024-10-03T17:59:13Z","published":"2024-10-03T17:59:13Z","title":"Forecasting Smog Clouds With Deep Learning","summary":"  In this proof-of-concept study, we conduct multivariate timeseries\nforecasting for the concentrations of nitrogen dioxide (NO2), ozone (O3), and\n(fine) particulate matter (PM10 & PM2.5) with meteorological covariates between\ntwo locations using various deep learning models, with a focus on long\nshort-term memory (LSTM) and gated recurrent unit (GRU) architectures. In\nparticular, we propose an integrated, hierarchical model architecture inspired\nby air pollution dynamics and atmospheric science that employs multi-task\nlearning and is benchmarked by unidirectional and fully-connected models.\nResults demonstrate that, above all, the hierarchical GRU proves itself as a\ncompetitive and efficient method for forecasting the concentration of\nsmog-related pollutants.\n","authors":["Valentijn Oldenburg","Juan Cardenas-Cartagena","Matias Valdenegro-Toro"],"pdf_url":"https://arxiv.org/pdf/2410.02759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02755v1","updated":"2024-10-03T17:58:29Z","published":"2024-10-03T17:58:29Z","title":"SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost","summary":"  Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.\n","authors":["Jifan Zhang","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2410.02755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02751v1","updated":"2024-10-03T17:58:11Z","published":"2024-10-03T17:58:11Z","title":"ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI","summary":"  Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic\n","authors":["Ahmad Elawady","Gunjan Chhablani","Ram Ramrakhya","Karmesh Yadav","Dhruv Batra","Zsolt Kira","Andrew Szot"],"pdf_url":"https://arxiv.org/pdf/2410.02751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04434v2","updated":"2024-10-03T17:57:59Z","published":"2024-09-06T17:55:49Z","title":"Accelerating Training with Neuron Interaction and Nowcasting Networks","summary":"  Neural network training can be accelerated when a learnable update rule is\nused in lieu of classic adaptive optimizers (e.g. Adam). However, learnable\nupdate rules can be costly and unstable to train and use. Recently, Jang et al.\n(2023) proposed a simpler approach to accelerate training based on weight\nnowcaster networks (WNNs). In their approach, Adam is used for most of the\noptimization steps and periodically, only every few steps, a WNN nowcasts\n(predicts near future) parameters. We improve WNNs by proposing neuron\ninteraction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leverages\nneuron connectivity and graph neural networks to more accurately nowcast\nparameters. We further show that in some networks, such as Transformers,\nmodeling neuron connectivity accurately is challenging. We address this and\nother limitations, which allows NiNo to accelerate Adam training by up to 50%\nin vision and language tasks.\n","authors":["Boris Knyazev","Abhinav Moudgil","Guillaume Lajoie","Eugene Belilovsky","Simon Lacoste-Julien"],"pdf_url":"https://arxiv.org/pdf/2409.04434v2.pdf","comment":"added Llama3-based results and other updates, code is\n  https://github.com/SamsungSAILMontreal/nino"},{"id":"http://arxiv.org/abs/2410.02750v1","updated":"2024-10-03T17:57:50Z","published":"2024-10-03T17:57:50Z","title":"An Online Automatic Modulation Classification Scheme Based on Isolation\n  Distributional Kernel","summary":"  Automatic Modulation Classification (AMC), as a crucial technique in modern\nnon-cooperative communication networks, plays a key role in various civil and\nmilitary applications. However, existing AMC methods usually are complicated\nand can work in batch mode only due to their high computational complexity.\nThis paper introduces a new online AMC scheme based on Isolation Distributional\nKernel. Our method stands out in two aspects. Firstly, it is the first proposal\nto represent baseband signals using a distributional kernel. Secondly, it\nintroduces a pioneering AMC technique that works well in online settings under\nrealistic time-varying channel conditions. Through extensive experiments in\nonline settings, we demonstrate the effectiveness of the proposed classifier.\nOur results indicate that the proposed approach outperforms existing baseline\nmodels, including two state-of-the-art deep learning classifiers. Moreover, it\ndistinguishes itself as the first online classifier for AMC with linear time\ncomplexity, which marks a significant efficiency boost for real-time\napplications.\n","authors":["Xinpeng Li","Zile Jiang","Kai Ming Ting","Ye Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.02750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v1","updated":"2024-10-03T17:57:22Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18957v2","updated":"2024-10-03T17:57:07Z","published":"2024-09-27T17:58:50Z","title":"LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction","summary":"  Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2409.18957v2.pdf","comment":"Updated title, abstract, and images"},{"id":"http://arxiv.org/abs/2410.02748v1","updated":"2024-10-03T17:57:01Z","published":"2024-10-03T17:57:01Z","title":"CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Han He","Qianchu Liu","Lei Xu","Chaitanya Shivade","Yi Zhang","Sundararajan Srinivasan","Katrin Kirchhoff"],"pdf_url":"https://arxiv.org/pdf/2410.02748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07840v3","updated":"2024-10-03T17:56:12Z","published":"2024-04-11T15:27:56Z","title":"On Training Data Influence of GPT Models","summary":"  Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We make our code and data publicly available at\nhttps://github.com/ernie-research/gptfluence.\n","authors":["Yekun Chai","Qingyi Liu","Shuohuan Wang","Yu Sun","Qiwei Peng","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2404.07840v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02746v1","updated":"2024-10-03T17:56:09Z","published":"2024-10-03T17:56:09Z","title":"Contrastive Localized Language-Image Pre-Training","summary":"  Contrastive Language-Image Pre-training (CLIP) has been a celebrated method\nfor training vision encoders to generate image/text representations\nfacilitating various applications. Recently, CLIP has been widely adopted as\nthe vision backbone of multimodal large language models (MLLMs) to connect\nimage inputs for language interactions. The success of CLIP as a\nvision-language foundation model relies on aligning web-crawled noisy text\nannotations at image levels. Nevertheless, such criteria may become\ninsufficient for downstream tasks in need of fine-grained vision\nrepresentations, especially when region-level understanding is demanding for\nMLLMs. In this paper, we improve the localization capability of CLIP with\nseveral advances. We propose a pre-training method called Contrastive Localized\nLanguage-Image Pre-training (CLOC) by complementing CLIP with region-text\ncontrastive loss and modules. We formulate a new concept, promptable\nembeddings, of which the encoder produces image embeddings easy to transform\ninto region representations given spatial hints. To support large-scale\npre-training, we design a visually-enriched and spatially-localized captioning\nframework to effectively generate region-text pseudo-labels at scale. By\nscaling up to billions of annotated images, CLOC enables high-quality regional\nembeddings for image region recognition and retrieval tasks, and can be a\ndrop-in replacement of CLIP to enhance MLLMs, especially on referring and\ngrounding tasks.\n","authors":["Hong-You Chen","Zhengfeng Lai","Haotian Zhang","Xinze Wang","Marcin Eichner","Keen You","Meng Cao","Bowen Zhang","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2410.02746v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02744v1","updated":"2024-10-03T17:55:17Z","published":"2024-10-03T17:55:17Z","title":"Neutral residues: revisiting adapters for model extension","summary":"  We address the problem of extending a pretrained large language model to a\nnew domain that was not seen at training time, like adding a language for which\nthe original model has seen no or little training data. Popular solutions like\nfine-tuning or low-rank adaptation are successful at domain adaptation, but\nformally they do not add any extra capacity and degrade the performance in the\noriginal domain.\n  Our paper analyzes this extension problem under three angles: data,\narchitecture and training procedure, which are advantageously considered\njointly. In particular, we improve adapters and make it possible to learn an\nentire new language while ensuring that the output of the neural network is\nalmost unchanged in the original domain. For this purpose, we modify the new\nresidual blocks in a way that leads each new residual block to output\nnear-zeros in the original domain.\n  This solution of neutral residues, which borrows architectural components\nfrom mixture of experts, is effective: with only 20% extra learnable weights\ncompared to an original model trained on English, we get results that are\nsignificantly better than concurrent approaches (fine-tuning, low-rank or\nvanilla adapters) in terms of the trade-off between learning a new language and\nnot forgetting English.\n","authors":["Franck Signe Talla","Herve Jegou","Edouard Grave"],"pdf_url":"https://arxiv.org/pdf/2410.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02742v1","updated":"2024-10-03T17:55:09Z","published":"2024-10-03T17:55:09Z","title":"Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models","summary":"  Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.\n","authors":["Haolan Liu","Jishen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.02742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02741v1","updated":"2024-10-03T17:54:56Z","published":"2024-10-03T17:54:56Z","title":"Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization","summary":"  Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.\n","authors":["Lei Xu","Mohammed Asad Karim","Saket Dingliwal","Aparna Elangovan"],"pdf_url":"https://arxiv.org/pdf/2410.02741v1.pdf","comment":"Accepted to EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.02740v1","updated":"2024-10-03T17:54:52Z","published":"2024-10-03T17:54:52Z","title":"Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models","summary":"  Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.\n","authors":["Zhengfeng Lai","Vasileios Saveris","Chen Chen","Hong-You Chen","Haotian Zhang","Bowen Zhang","Juan Lao Tebar","Wenze Hu","Zhe Gan","Peter Grasch","Meng Cao","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02740v1.pdf","comment":"CV/ML"},{"id":"http://arxiv.org/abs/2410.02735v1","updated":"2024-10-03T17:52:42Z","published":"2024-10-03T17:52:42Z","title":"OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?","summary":"  Out-of-distribution (OOD) generalization is challenging because distribution\nshifts come in many forms. A multitude of learning algorithms exist and each\ncan improve performance in specific OOD situations. We posit that much of the\nchallenge of OOD generalization lies in choosing the right algorithm for the\nright dataset. However, such algorithm selection is often elusive under complex\nreal-world shifts. In this work, we formalize the task of algorithm selection\nfor OOD generalization and investigate whether it could be approached by\nlearning. We propose a solution, dubbed OOD-Chameleon that treats the task as a\nsupervised classification over candidate algorithms. We construct a dataset of\ndatasets to learn from, which represents diverse types, magnitudes and\ncombinations of shifts (covariate shift, label shift, spurious correlations).\nWe train the model to predict the relative performance of algorithms given a\ndataset's characteristics. This enables a priori selection of the best learning\nstrategy, i.e. without training various models as needed with traditional model\nselection. Our experiments show that the adaptive selection outperforms any\nindividual algorithm and simple selection heuristics, on unseen datasets of\ncontrollable and realistic image data. Inspecting the model shows that it\nlearns non-trivial data/algorithms interactions, and reveals the conditions for\nany one algorithm to surpass another. This opens new avenues for (1) enhancing\nOOD generalization with existing algorithms instead of designing new ones, and\n(2) gaining insights into the applicability of existing algorithms with respect\nto datasets' properties.\n","authors":["Liangze Jiang","Damien Teney"],"pdf_url":"https://arxiv.org/pdf/2410.02735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02733v1","updated":"2024-10-03T17:51:21Z","published":"2024-10-03T17:51:21Z","title":"Data Similarity-Based One-Shot Clustering for Multi-Task Hierarchical\n  Federated Learning","summary":"  We address the problem of cluster identity estimation in a hierarchical\nfederated learning setting in which users work toward learning different tasks.\nTo overcome the challenge of task heterogeneity, users need to be grouped in a\nway such that users with the same task are in the same group, conducting\ntraining together, while sharing the weights of feature extraction layers with\nthe other groups. Toward that end, we propose a one-shot clustering algorithm\nthat can effectively identify and group users based on their data similarity.\nThis enables more efficient collaboration and sharing of a common layer\nrepresentation within the federated learning system. Our proposed algorithm not\nonly enhances the clustering process, but also overcomes challenges related to\nprivacy concerns, communication overhead, and the need for prior knowledge\nabout learning models or loss function behaviors. We validate our proposed\nalgorithm using various datasets such as CIFAR-10 and Fashion MNIST, and show\nthat it outperforms the baseline in terms of accuracy and variance reduction.\n","authors":["Abdulmoneam Ali","Ahmed Arafa"],"pdf_url":"https://arxiv.org/pdf/2410.02733v1.pdf","comment":"To appear in Asilomar 2024"},{"id":"http://arxiv.org/abs/2407.00023v2","updated":"2024-10-03T17:50:33Z","published":"2024-05-08T06:30:58Z","title":"Preble: Efficient Distributed Prompt Scheduling for LLM Serving","summary":"  Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.\n","authors":["Vikranth Srivatsa","Zijian He","Reyna Abhyankar","Dongming Li","Yiying Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.00023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02725v1","updated":"2024-10-03T17:47:29Z","published":"2024-10-03T17:47:29Z","title":"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation","summary":"  Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.\n","authors":["Rohin Manvi","Anikait Singh","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.02725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02724v1","updated":"2024-10-03T17:45:31Z","published":"2024-10-03T17:45:31Z","title":"Large Language Models as Markov Chains","summary":"  Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.\n","authors":["Oussama Zekri","Ambroise Odonnat","Abdelhakim Benechehab","Linus Bleistein","Nicolas Boull","Ievgen Redko"],"pdf_url":"https://arxiv.org/pdf/2410.02724v1.pdf","comment":"49 pages, 17 figures"},{"id":"http://arxiv.org/abs/2405.15429v4","updated":"2024-10-03T17:44:27Z","published":"2024-05-24T10:55:38Z","title":"E(n) Equivariant Topological Neural Networks","summary":"  Graph neural networks excel at modeling pairwise interactions, but they\ncannot flexibly accommodate higher-order interactions and features. Topological\ndeep learning (TDL) has emerged recently as a promising tool for addressing\nthis issue. TDL enables the principled modeling of arbitrary multi-way,\nhierarchical higher-order interactions by operating on combinatorial\ntopological spaces, such as simplicial or cell complexes, instead of graphs.\nHowever, little is known about how to leverage geometric features such as\npositions and velocities for TDL. This paper introduces E(n)-Equivariant\nTopological Neural Networks (ETNNs), which are E(n)-equivariant message-passing\nnetworks operating on combinatorial complexes, formal objects unifying graphs,\nhypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric\nnode features while respecting rotation, reflection, and translation\nequivariance. Moreover, ETNNs are natively ready for settings with\nheterogeneous interactions. We provide a theoretical analysis to show the\nimproved expressiveness of ETNNs over architectures for geometric graphs. We\nalso show how E(n)-equivariant variants of TDL models can be directly derived\nfrom our framework. The broad applicability of ETNNs is demonstrated through\ntwo tasks of vastly different scales: i) molecular property prediction on the\nQM9 benchmark and ii) land-use regression for hyper-local estimation of air\npollution with multi-resolution irregular geospatial data. The results indicate\nthat ETNNs are an effective tool for learning from diverse types of richly\nstructured data, as they match or surpass SotA equivariant TDL models with a\nsignificantly smaller computational burden, thus highlighting the benefits of a\nprincipled geometric inductive bias.\n","authors":["Claudio Battiloro","Ege Karaismailolu","Mauricio Tec","George Dasoulas","Michelle Audirac","Francesca Dominici"],"pdf_url":"https://arxiv.org/pdf/2405.15429v4.pdf","comment":"41 pages, 11 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.02718v1","updated":"2024-10-03T17:38:46Z","published":"2024-10-03T17:38:46Z","title":"SynthFormer: Equivariant Pharmacophore-based Generation of Molecules for\n  Ligand-Based Drug Design","summary":"  Drug discovery is a complex and resource-intensive process, with significant\ntime and cost investments required to bring new medicines to patients. Recent\nadvancements in generative machine learning (ML) methods offer promising\navenues to accelerate early-stage drug discovery by efficiently exploring\nchemical space. This paper addresses the gap between in silico generative\napproaches and practical in vitro methodologies, highlighting the need for\ntheir integration to optimize molecule discovery. We introduce SynthFormer, a\nnovel ML model that utilizes a 3D equivariant encoder for pharmacophores to\ngenerate fully synthesizable molecules, constructed as synthetic trees. Unlike\nprevious methods, SynthFormer incorporates 3D information and provides\nsynthetic paths, enhancing its ability to produce molecules with good docking\nscores across various proteins. Our contributions include a new methodology for\nefficient chemical space exploration using 3D information, a novel architecture\ncalled Synthformer for translating 3D pharmacophore representations into\nmolecules, and a meaningful embedding space that organizes reagents for drug\ndiscovery optimization. Synthformer generates molecules that dock well and\nenables effective late-stage optimization restricted by synthesis paths.\n","authors":["Zygimantas Jocys","Henriette M. G. Willems","Katayoun Farrahi"],"pdf_url":"https://arxiv.org/pdf/2410.02718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02717v1","updated":"2024-10-03T17:38:43Z","published":"2024-10-03T17:38:43Z","title":"Measurements with Noise: Bayesian Optimization for Co-optimizing Noise\n  and Property Discovery in Automated Experiments","summary":"  We have developed a Bayesian optimization (BO) workflow that integrates\nintra-step noise optimization into automated experimental cycles. Traditional\nBO approaches in automated experiments focus on optimizing experimental\ntrajectories but often overlook the impact of measurement noise on data quality\nand cost. Our proposed framework simultaneously optimizes both the target\nproperty and the associated measurement noise by introducing time as an\nadditional input parameter, thereby balancing the signal-to-noise ratio and\nexperimental duration. Two approaches are explored: a reward-driven noise\noptimization and a double-optimization acquisition function, both enhancing the\nefficiency of automated workflows by considering noise and cost within the\noptimization process. We validate our method through simulations and real-world\nexperiments using Piezoresponse Force Microscopy (PFM), demonstrating the\nsuccessful optimization of measurement duration and property exploration. Our\napproach offers a scalable solution for optimizing multiple variables in\nautomated experimental workflows, improving data quality, and reducing resource\nexpenditure in materials science and beyond.\n","authors":["Boris N. Slautin","Yu Liu","Jan Dec","Vladimir V. Shvartsman","Doru C. Lupascu","Maxim Ziatdinov","Sergei V. Kalinin"],"pdf_url":"https://arxiv.org/pdf/2410.02717v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.05689v3","updated":"2024-10-03T17:37:33Z","published":"2024-02-08T14:07:20Z","title":"Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of\n  Average-Reward Restless Bandits","summary":"  We consider the infinite-horizon, average-reward restless bandit problem in\ndiscrete time. We propose a new class of policies that are designed to drive a\nprogressively larger subset of arms toward the optimal distribution. We show\nthat our policies are asymptotically optimal with an $O(1/\\sqrt{N})$ optimality\ngap for an $N$-armed problem, assuming only a unichain and aperiodicity\nassumption. Our approach departs from most existing work that focuses on index\nor priority policies, which rely on the Global Attractor Property (GAP) to\nguarantee convergence to the optimum, or a recently developed simulation-based\npolicy, which requires a Synchronization Assumption (SA).\n","authors":["Yige Hong","Qiaomin Xie","Yudong Chen","Weina Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05689v3.pdf","comment":"58 pages, 14 figures. This version includes a restructured main\n  result section and new experiments"},{"id":"http://arxiv.org/abs/2410.02714v1","updated":"2024-10-03T17:37:18Z","published":"2024-10-03T17:37:18Z","title":"AlzhiNet: Traversing from 2DCNN to 3DCNN, Towards Early Detection and\n  Diagnosis of Alzheimer's Disease","summary":"  Alzheimer's disease (AD) is a progressive neurodegenerative disorder with\nincreasing prevalence among the aging population, necessitating early and\naccurate diagnosis for effective disease management. In this study, we present\na novel hybrid deep learning framework that integrates both 2D Convolutional\nNeural Networks (2D-CNN) and 3D Convolutional Neural Networks (3D-CNN), along\nwith a custom loss function and volumetric data augmentation, to enhance\nfeature extraction and improve classification performance in AD diagnosis.\nAccording to extensive experiments, AlzhiNet outperforms standalone 2D and 3D\nmodels, highlighting the importance of combining these complementary\nrepresentations of data. The depth and quality of 3D volumes derived from the\naugmented 2D slices also significantly influence the model's performance. The\nresults indicate that carefully selecting weighting factors in hybrid\npredictions is imperative for achieving optimal results. Our framework has been\nvalidated on the Magnetic Resonance Imaging (MRI) from Kaggle and MIRIAD\ndatasets, obtaining accuracies of 98.9% and 99.99%, respectively, with an AUC\nof 100%. Furthermore, AlzhiNet was studied under a variety of perturbation\nscenarios on the Alzheimer's Kaggle dataset, including Gaussian noise,\nbrightness, contrast, salt and pepper noise, color jitter, and occlusion. The\nresults obtained show that AlzhiNet is more robust to perturbations than\nResNet-18, making it an excellent choice for real-world applications. This\napproach represents a promising advancement in the early diagnosis and\ntreatment planning for Alzheimer's disease.\n","authors":["Romoke Grace Akindele","Samuel Adebayo","Paul Shekonya Kanda","Ming Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02711v1","updated":"2024-10-03T17:35:38Z","published":"2024-10-03T17:35:38Z","title":"NETS: A Non-Equilibrium Transport Sampler","summary":"  We propose an algorithm, termed the Non-Equilibrium Transport Sampler (NETS),\nto sample from unnormalized probability distributions. NETS can be viewed as a\nvariant of annealed importance sampling (AIS) based on Jarzynski's equality, in\nwhich the stochastic differential equation used to perform the non-equilibrium\nsampling is augmented with an additional learned drift term that lowers the\nimpact of the unbiasing weights used in AIS. We show that this drift is the\nminimizer of a variety of objective functions, which can all be estimated in an\nunbiased fashion without backpropagating through solutions of the stochastic\ndifferential equations governing the sampling. We also prove that some these\nobjectives control the Kullback-Leibler divergence of the estimated\ndistribution from its target. NETS is shown to be unbiased and, in addition,\nhas a tunable diffusion coefficient which can be adjusted post-training to\nmaximize the effective sample size. We demonstrate the efficacy of the method\non standard benchmarks, high-dimensional Gaussian mixture distributions, and a\nmodel from statistical lattice field theory, for which it surpasses the\nperformances of related work and existing baselines.\n","authors":["Michael S. Albergo","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2410.02711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02703v1","updated":"2024-10-03T17:27:30Z","published":"2024-10-03T17:27:30Z","title":"Selective Attention Improves Transformer","summary":"  Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.\n","authors":["Yaniv Leviathan","Matan Kalman","Yossi Matias"],"pdf_url":"https://arxiv.org/pdf/2410.02703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07071v2","updated":"2024-10-03T17:26:48Z","published":"2024-07-09T17:44:34Z","title":"Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps","summary":"  When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.\n","authors":["Yung-Sung Chuang","Linlu Qiu","Cheng-Yu Hsieh","Ranjay Krishna","Yoon Kim","James Glass"],"pdf_url":"https://arxiv.org/pdf/2407.07071v2.pdf","comment":"EMNLP 2024 main conference long paper. The source code is available\n  at https://github.com/voidism/Lookback-Lens"},{"id":"http://arxiv.org/abs/2406.03520v2","updated":"2024-10-03T17:24:40Z","published":"2024-06-05T17:53:55Z","title":"VideoPhy: Evaluating Physical Commonsense for Video Generation","summary":"  Recent advances in internet-scale video data pretraining have led to the\ndevelopment of text-to-video generative models that can create high-quality\nvideos across a broad range of visual concepts, synthesize realistic motions\nand render complex objects. Hence, these generative models have the potential\nto become general-purpose simulators of the physical world. However, it is\nunclear how far we are from this goal with the existing text-to-video\ngenerative models. To this end, we present VideoPhy, a benchmark designed to\nassess whether the generated videos follow physical commonsense for real-world\nactivities (e.g. marbles will roll down when placed on a slanted surface).\nSpecifically, we curate diverse prompts that involve interactions between\nvarious material types in the physical world (e.g., solid-solid, solid-fluid,\nfluid-fluid). We then generate videos conditioned on these captions from\ndiverse state-of-the-art text-to-video generative models, including open models\n(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human\nevaluation reveals that the existing models severely lack the ability to\ngenerate videos adhering to the given text prompts, while also lack physical\ncommonsense. Specifically, the best performing model, CogVideoX-5B, generates\nvideos that adhere to the caption and physical laws for 39.6% of the instances.\nVideoPhy thus highlights that the video generative models are far from\naccurately simulating the physical world. Finally, we propose an\nauto-evaluator, VideoCon-Physics, to assess the performance reliably for the\nnewly released models.\n","authors":["Hritik Bansal","Zongyu Lin","Tianyi Xie","Zeshun Zong","Michal Yarom","Yonatan Bitton","Chenfanfu Jiang","Yizhou Sun","Kai-Wei Chang","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.03520v2.pdf","comment":"43 pages, 29 figures, 12 tables. Added CogVideo and Dream Machine in\n  v2"},{"id":"http://arxiv.org/abs/2410.02698v1","updated":"2024-10-03T17:21:30Z","published":"2024-10-03T17:21:30Z","title":"Lie Algebra Canonicalization: Equivariant Neural Operators under\n  arbitrary Lie Groups","summary":"  The quest for robust and generalizable machine learning models has driven\nrecent interest in exploiting symmetries through equivariant neural networks.\nIn the context of PDE solvers, recent works have shown that Lie point\nsymmetries can be a useful inductive bias for Physics-Informed Neural Networks\n(PINNs) through data and loss augmentation. Despite this, directly enforcing\nequivariance within the model architecture for these problems remains elusive.\nThis is because many PDEs admit non-compact symmetry groups, oftentimes not\nstudied beyond their infinitesimal generators, making them incompatible with\nmost existing equivariant architectures. In this work, we propose Lie aLgebrA\nCanonicalization (LieLAC), a novel approach that exploits only the action of\ninfinitesimal generators of the symmetry group, circumventing the need for\nknowledge of the full group structure. To achieve this, we address existing\ntheoretical issues in the canonicalization literature, establishing connections\nwith frame averaging in the case of continuous non-compact groups. Operating\nwithin the framework of canonicalization, LieLAC can easily be integrated with\nunconstrained pre-trained models, transforming inputs to a canonical form\nbefore feeding them into the existing model, effectively aligning the input for\nmodel inference according to allowed symmetries. LieLAC utilizes standard Lie\ngroup descent schemes, achieving equivariance in pre-trained models. Finally,\nwe showcase LieLAC's efficacy on tasks of invariant image classification and\nLie point symmetry equivariant neural PDE solvers using pre-trained models.\n","authors":["Zakhar Shumaylov","Peter Zaika","James Rowbottom","Ferdia Sherry","Melanie Weber","Carola-Bibiane Schnlieb"],"pdf_url":"https://arxiv.org/pdf/2410.02698v1.pdf","comment":"40 pages; preprint"},{"id":"http://arxiv.org/abs/2402.17917v2","updated":"2024-10-03T17:18:53Z","published":"2024-02-27T22:10:51Z","title":"Collaborative learning of common latent representations in routinely\n  collected multivariate ICU physiological signals","summary":"  In Intensive Care Units (ICU), the abundance of multivariate time series\npresents an opportunity for machine learning (ML) to enhance patient\nphenotyping. In contrast to previous research focused on electronic health\nrecords (EHR), here we propose an ML approach for phenotyping using routinely\ncollected physiological time series data. Our new algorithm integrates Long\nShort-Term Memory (LSTM) networks with collaborative filtering concepts to\nidentify common physiological states across patients. Tested on real-world ICU\nclinical data for intracranial hypertension (IH) detection in patients with\nbrain injury, our method achieved an area under the curve (AUC) of 0.889 and\naverage precision (AP) of 0.725. Moreover, our algorithm outperforms\nautoencoders in learning more structured latent representations of the\nphysiological signals. These findings highlight the promise of our methodology\nfor patient phenotyping, leveraging routinely collected multivariate time\nseries to improve clinical care practices.\n","authors":["Hollan Haule","Ian Piper","Patricia Jones","Tsz-Yan Milly Lo","Javier Escudero"],"pdf_url":"https://arxiv.org/pdf/2402.17917v2.pdf","comment":"Published in 2024 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing Workshops (ICASSPW)"},{"id":"http://arxiv.org/abs/2410.02693v1","updated":"2024-10-03T17:18:37Z","published":"2024-10-03T17:18:37Z","title":"Discovering Clues of Spoofed LM Watermarks","summary":"  LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. While recent works have\ndemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,\nthey lack deeper qualitative analysis of the texts produced by spoofing\nmethods. In this work, we for the first time reveal that there are observable\ndifferences between genuine and spoofed watermark texts. Namely, we show that\nregardless of their underlying approach, all current spoofing methods\nconsistently leave observable artifacts in spoofed texts, indicative of\nwatermark forgery. We build upon these findings to propose rigorous statistical\ntests that reliably reveal the presence of such artifacts, effectively\ndiscovering that a watermark was spoofed. Our experimental evaluation shows\nhigh test power across all current spoofing methods, providing insights into\ntheir fundamental limitations, and suggesting a way to mitigate this threat.\n","authors":["Thibaud Gloaguen","Nikola Jovanovi","Robin Staab","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.02693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10224v4","updated":"2024-10-03T17:13:41Z","published":"2023-10-16T09:34:06Z","title":"Generalizing Medical Image Representations via Quaternion Wavelet\n  Networks","summary":"  Neural network generalizability is becoming a broad research field due to the\nincreasing availability of datasets from different sources and for various\ntasks. This issue is even wider when processing medical data, where a lack of\nmethodological standards causes large variations being provided by different\nimaging centers or acquired with various devices and cofactors. To overcome\nthese limitations, we introduce a novel, generalizable, data- and task-agnostic\nframework able to extract salient features from medical images. The proposed\nquaternion wavelet network (QUAVE) can be easily integrated with any\npre-existing medical image analysis or synthesis task, and it can be involved\nwith real, quaternion, or hypercomplex-valued models, generalizing their\nadoption to single-channel data. QUAVE first extracts different sub-bands\nthrough the quaternion wavelet transform, resulting in both\nlow-frequency/approximation bands and high-frequency/fine-grained features.\nThen, it weighs the most representative set of sub-bands to be involved as\ninput to any other neural model for image processing, replacing standard data\nsamples. We conduct an extensive experimental evaluation comprising different\ndatasets, diverse image analysis, and synthesis tasks including reconstruction,\nsegmentation, and modality translation. We also evaluate QUAVE in combination\nwith both real and quaternion-valued models. Results demonstrate the\neffectiveness and the generalizability of the proposed framework that improves\nnetwork performance while being flexible to be adopted in manifold scenarios\nand robust to domain shifts. The full code is available at:\nhttps://github.com/ispamm/QWT.\n","authors":["Luigi Sigillo","Eleonora Grassucci","Aurelio Uncini","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.10224v4.pdf","comment":"This paper is currently under review"},{"id":"http://arxiv.org/abs/2409.03650v2","updated":"2024-10-03T17:13:04Z","published":"2024-09-05T16:08:19Z","title":"On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.\n","authors":["Yong Lin","Skyler Seto","Maartje ter Hoeve","Katherine Metcalf","Barry-John Theobald","Xuan Wang","Yizhe Zhang","Chen Huang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.03650v2.pdf","comment":"12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.18725v2","updated":"2024-10-03T17:10:09Z","published":"2024-06-26T19:48:48Z","title":"Jailbreaking LLMs with Arabic Transliteration and Arabizi","summary":"  This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.\n","authors":["Mansour Al Ghanim","Saleh Almohaimeed","Mengxin Zheng","Yan Solihin","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2406.18725v2.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02683v1","updated":"2024-10-03T17:08:52Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2410.02681v1","updated":"2024-10-03T17:06:21Z","published":"2024-10-03T17:06:21Z","title":"Understanding and Mitigating Miscalibration in Prompt Tuning for\n  Vision-Language Models","summary":"  Confidence calibration is critical for the safe deployment of machine\nlearning models in the real world. However, such issue in vision-language\nmodels like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead\nto a trade-off of calibration between base and new classes: the cross-entropy\nloss in CoOp causes overconfidence in new classes by increasing textual label\ndivergence, whereas the regularization of KgCoOp maintains the confidence level\nbut results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR)\nto ensure the confidence calibration on both base and new classes after\nfine-tuning. In particular, we propose to minimize the feature deviation of\nnovel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while\neasing restrictions on base classes. Extensive experiments demonstrate that DOR\ncan enhance the calibration performance of current fine-tuning methods on base\nand new classes.\n","authors":["Shuoyuan Wang","Yixuan Li","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2410.02681v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02680v1","updated":"2024-10-03T17:06:06Z","published":"2024-10-03T17:06:06Z","title":"Highly Adaptive Ridge","summary":"  In this paper we propose the Highly Adaptive Ridge (HAR): a regression method\nthat achieves a $n^{-1/3}$ dimension-free L2 convergence rate in the class of\nright-continuous functions with square-integrable sectional derivatives. This\nis a large nonparametric function class that is particularly appropriate for\ntabular data. HAR is exactly kernel ridge regression with a specific\ndata-adaptive kernel based on a saturated zero-order tensor-product spline\nbasis expansion. We use simulation and real data to confirm our theory. We\ndemonstrate empirical performance better than state-of-the-art algorithms for\nsmall datasets in particular.\n","authors":["Alejandro Schuler","Alexander Hagemeister","Mark van der Laan"],"pdf_url":"https://arxiv.org/pdf/2410.02680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00147v3","updated":"2024-10-03T17:05:51Z","published":"2024-05-31T19:26:05Z","title":"Fair Allocation in Dynamic Mechanism Design","summary":"  We consider a dynamic mechanism design problem where an auctioneer sells an\nindivisible good to groups of buyers in every round, for a total of $T$ rounds.\nThe auctioneer aims to maximize their discounted overall revenue while adhering\nto a fairness constraint that guarantees a minimum average allocation for each\ngroup. We begin by studying the static case ($T=1$) and establish that the\noptimal mechanism involves two types of subsidization: one that increases the\noverall probability of allocation to all buyers, and another that favors the\ngroups which otherwise have a lower probability of winning the item. We then\nextend our results to the dynamic case by characterizing a set of recursive\nfunctions that determine the optimal allocation and payments in each round.\nNotably, our results establish that in the dynamic case, the seller, on the one\nhand, commits to a participation bonus to incentivize truth-telling, and on the\nother hand, charges an entry fee for every round. Moreover, the optimal\nallocation once more involves subsidization, which its extent depends on the\ndifference in future utilities for both the seller and buyers when allocating\nthe item to one group versus the others. Finally, we present an approximation\nscheme to solve the recursive equations and determine an approximately optimal\nand fair allocation efficiently.\n","authors":["Alireza Fallah","Michael I. Jordan","Annie Ulichney"],"pdf_url":"https://arxiv.org/pdf/2406.00147v3.pdf","comment":"A shorter conference version has been accepted at the Advances in\n  Neural Information Processing Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2403.04405v2","updated":"2024-10-03T17:05:49Z","published":"2024-03-07T11:00:35Z","title":"Signature Isolation Forest","summary":"  Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly\nDetection (AD) algorithm designed for functional data. It relies on a tree\npartition procedure where an abnormality score is computed by projecting each\ncurve observation on a drawn dictionary through a linear inner product. Such\nlinear inner product and the dictionary are a priori choices that highly\ninfluence the algorithm's performances and might lead to unreliable results,\nparticularly with complex datasets. This work addresses these challenges by\nintroducing \\textit{Signature Isolation Forest}, a novel AD algorithm class\nleveraging the rough path theory's signature transform. Our objective is to\nremove the constraints imposed by FIF through the proposition of two algorithms\nwhich specifically target the linearity of the FIF inner product and the choice\nof the dictionary. We provide several numerical experiments, including a\nreal-world applications benchmark showing the relevance of our methods.\n","authors":["Marta Campi","Guillaume Staerman","Gareth W. Peters","Tomoko Matsui"],"pdf_url":"https://arxiv.org/pdf/2403.04405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02677v1","updated":"2024-10-03T17:04:31Z","published":"2024-10-03T17:04:31Z","title":"CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs","summary":"  To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.\n","authors":["Yu Ying Chiu","Liwei Jiang","Bill Yuchen Lin","Chan Young Park","Shuyue Stella Li","Sahithya Ravi","Mehar Bhatia","Maria Antoniak","Yulia Tsvetkov","Vered Shwartz","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02677v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2410.02675v1","updated":"2024-10-03T17:02:21Z","published":"2024-10-03T17:02:21Z","title":"FAN: Fourier Analysis Networks","summary":"  Despite the remarkable success achieved by neural networks, particularly\nthose represented by MLP and Transformer, we reveal that they exhibit potential\nflaws in the modeling and reasoning of periodicity, i.e., they tend to memorize\nthe periodic data rather than genuinely understanding the underlying principles\nof periodicity. However, periodicity is a crucial trait in various forms of\nreasoning and generalization, underpinning predictability across natural and\nengineered systems through recurring patterns in observations. In this paper,\nwe propose FAN, a novel network architecture based on Fourier Analysis, which\nempowers the ability to efficiently model and reason about periodic phenomena.\nBy introducing Fourier Series, the periodicity is naturally integrated into the\nstructure and computational processes of the neural network, thus achieving a\nmore accurate expression and prediction of periodic patterns. As a promising\nsubstitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in\nvarious models with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the effectiveness of FAN in modeling and reasoning about\nperiodic functions, and the superiority and generalizability of FAN across a\nrange of real-world tasks, including symbolic formula representation, time\nseries forecasting, and language modeling.\n","authors":["Yihong Dong","Ge Li","Yongding Tao","Xue Jiang","Kechi Zhang","Jia Li","Jing Su","Jun Zhang","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13937v6","updated":"2024-10-03T16:59:18Z","published":"2024-05-22T19:10:24Z","title":"DyGPrompt: Learning Feature and Time Prompts on Dynamic Graphs","summary":"  Dynamic graphs capture evolving interactions between entities, such as in\nsocial networks, online learning platforms, and crowdsourcing projects. For\ndynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a\nmainstream technique. However, they are generally pre-trained on the link\nprediction task, leaving a significant gap from the objectives of downstream\ntasks such as node classification. To bridge the gap, prompt-based learning has\ngained traction on graphs, but most existing efforts focus on static graphs,\nneglecting the evolution of dynamic graphs. In this paper, we propose\nDYGPROMPT, a novel pre-training and prompt learning framework for dynamic graph\nmodeling. First, we design dual prompts to address the gap in both task\nobjectives and temporal variations across pre-training and downstream tasks.\nSecond, we recognize that node and time features mutually characterize each\nother, and propose dual condition-nets to model the evolving node-time patterns\nin downstream tasks. Finally, we thoroughly evaluate and analyze DYGPROMPT\nthrough extensive experiments on four public datasets.\n","authors":["Xingtong Yu","Zhenghao Liu","Yuan Fang","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.13937v6.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.02667v1","updated":"2024-10-03T16:51:14Z","published":"2024-10-03T16:51:14Z","title":"GUD: Generation with Unified Diffusion","summary":"  Diffusion generative models transform noise into data by inverting a process\nthat progressively adds noise to data samples. Inspired by concepts from the\nrenormalization group in physics, which analyzes systems across different\nscales, we revisit diffusion models by exploring three key design aspects: 1)\nthe choice of representation in which the diffusion process operates (e.g.\npixel-, PCA-, Fourier-, or wavelet-basis), 2) the prior distribution that data\nis transformed into during diffusion (e.g. Gaussian with covariance $\\Sigma$),\nand 3) the scheduling of noise levels applied separately to different parts of\nthe data, captured by a component-wise noise schedule. Incorporating the\nflexibility in these choices, we develop a unified framework for diffusion\ngenerative models with greatly enhanced design freedom. In particular, we\nintroduce soft-conditioning models that smoothly interpolate between standard\ndiffusion models and autoregressive models (in any basis), conceptually\nbridging these two approaches. Our framework opens up a wide design space which\nmay lead to more efficient training and data generation, and paves the way to\nnovel architectures integrating different generative approaches and generation\ntasks.\n","authors":["Mathis Gerdes","Max Welling","Miranda C. N. Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.02667v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.02666v1","updated":"2024-10-03T16:50:30Z","published":"2024-10-03T16:50:30Z","title":"AlphaIntegrator: Transformer Action Search for Symbolic Integration\n  Proofs","summary":"  We present the first correct-by-construction learning-based system for\nstep-by-step mathematical integration. The key idea is to learn a policy,\nrepresented by a GPT transformer model, which guides the search for the right\nmathematical integration rule, to be carried out by a symbolic solver.\nConcretely, we introduce a symbolic engine with axiomatically correct actions\non mathematical expressions, as well as the first dataset for step-by-step\nintegration. Our GPT-style transformer model, trained on this synthetic data,\ndemonstrates strong generalization by surpassing its own data generator in\naccuracy and efficiency, using 50% fewer search steps. Our experimental results\nwith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs\non a set of question-answer pairs is insufficient for solving this mathematical\ntask. This motivates the importance of discovering creative methods for\ncombining LLMs with symbolic reasoning engines, of which our work is an\ninstance.\n","authors":["Mert nsal","Timon Gehr","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.02666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02660v1","updated":"2024-10-03T16:46:52Z","published":"2024-10-03T16:46:52Z","title":"How to Train Long-Context Language Models (Effectively)","summary":"  We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- Instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context tasks, and we evaluate models after SFT with instruction data\nas this better reveals long-context abilities. Supported by our robust\nevaluations, we run thorough experiments to decide the data mix for continued\npre-training, the instruction tuning dataset, and many other design choices. We\nfind that (1) code repositories and books are excellent sources of long data,\nbut it is crucial to combine them with high-quality short data; (2) training\nwith a sequence length beyond the evaluation length boosts long-context\nperformance; (3) for SFT, using only short instruction datasets yields strong\nperformance on long-context tasks. Our final model, ProLong-8B, which is\ninitialized from Llama-3 and trained on 40B tokens, demonstrates\nstate-of-the-art long-context performance among similarly sized models at a\nlength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of\nlong-context tasks despite having seen only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.\n","authors":["Tianyu Gao","Alexander Wettig","Howard Yen","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02660v1.pdf","comment":"Our code, data, and models are available at\n  https://github.com/princeton-nlp/ProLong"},{"id":"http://arxiv.org/abs/2407.11969v3","updated":"2024-10-03T16:46:09Z","published":"2024-07-16T17:59:55Z","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","summary":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.\n","authors":["Maksym Andriushchenko","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2407.11969v3.pdf","comment":"Update in v3: o1-mini and o1-preview results (on top of GPT-4o and\n  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at\n  https://github.com/tml-epfl/llm-past-tense"},{"id":"http://arxiv.org/abs/2410.02656v1","updated":"2024-10-03T16:43:00Z","published":"2024-10-03T16:43:00Z","title":"Scalable Simulation-free Entropic Unbalanced Optimal Transport","summary":"  The Optimal Transport (OT) problem investigates a transport map that connects\ntwo distributions while minimizing a given cost function. Finding such a\ntransport map has diverse applications in machine learning, such as generative\nmodeling and image-to-image translation. In this paper, we introduce a scalable\nand simulation-free approach for solving the Entropic Unbalanced Optimal\nTransport (EUOT) problem. We derive the dynamical form of this EUOT problem,\nwhich is a generalization of the Schr\\\"odinger bridges (SB) problem. Based on\nthis, we derive dual formulation and optimality conditions of the EUOT problem\nfrom the stochastic optimal control interpretation. By leveraging these\nproperties, we propose a simulation-free algorithm to solve EUOT, called\nSimulation-free EUOT (SF-EUOT). While existing SB models require expensive\nsimulation costs during training and evaluation, our model achieves\nsimulation-free training and one-step generation by utilizing the reciprocal\nproperty. Our model demonstrates significantly improved scalability in\ngenerative modeling and image-to-image translation tasks compared to previous\nSB methods.\n","authors":["Jaemoo Choi","Jaewoong Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02656v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2311.16556v2","updated":"2024-10-03T16:42:08Z","published":"2023-11-28T06:52:53Z","title":"Scalable Label Distribution Learning for Multi-Label Classification","summary":"  Multi-label classification (MLC) refers to the problem of tagging a given\ninstance with a set of relevant labels. Most existing MLC methods are based on\nthe assumption that the correlation of two labels in each label pair is\nsymmetric, which is violated in many real-world scenarios. Moreover, most\nexisting methods design learning processes associated with the number of\nlabels, which makes their computational complexity a bottleneck when scaling up\nto large-scale output space. To tackle these issues, we propose a novel method\nnamed Scalable Label Distribution Learning (SLDL) for multi-label\nclassification which can describe different labels as distributions in a latent\nspace, where the label correlation is asymmetric and the dimension is\nindependent of the number of labels. Specifically, SLDL first converts labels\ninto continuous distributions within a low-dimensional latent space and\nleverages the asymmetric metric to establish the correlation between different\nlabels. Then, it learns the mapping from the feature space to the latent space,\nresulting in the computational complexity is no longer related to the number of\nlabels. Finally, SLDL leverages a nearest-neighbor-based strategy to decode the\nlatent representations and obtain the final predictions. Extensive experiments\nillustrate that SLDL achieves very competitive classification performances with\nlittle computational consumption.\n","authors":["Xingyu Zhao","Yuexuan An","Lei Qi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2311.16556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02654v1","updated":"2024-10-03T16:41:51Z","published":"2024-10-03T16:41:51Z","title":"Deconstructing Recurrence, Attention, and Gating: Investigating the\n  transferability of Transformers and Gated Recurrent Neural Networks in\n  forecasting of dynamical systems","summary":"  Machine learning architectures, including transformers and recurrent neural\nnetworks (RNNs) have revolutionized forecasting in applications ranging from\ntext processing to extreme weather. Notably, advanced network architectures,\ntuned for applications such as natural language processing, are transferable to\nother tasks such as spatiotemporal forecasting tasks. However, there is a\nscarcity of ablation studies to illustrate the key components that enable this\nforecasting accuracy. The absence of such studies, although explainable due to\nthe associated computational cost, intensifies the belief that these models\nought to be considered as black boxes. In this work, we decompose the key\narchitectural components of the most powerful neural architectures, namely\ngating and recurrence in RNNs, and attention mechanisms in transformers. Then,\nwe synthesize and build novel hybrid architectures from the standard blocks,\nperforming ablation studies to identify which mechanisms are effective for each\ntask. The importance of considering these components as hyper-parameters that\ncan augment the standard architectures is exhibited on various forecasting\ndatasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96\nsystem, the Kuramoto-Sivashinsky equation, as well as standard real world\ntime-series benchmarks. A key finding is that neural gating and attention\nimproves the performance of all standard RNNs in most tasks, while the addition\nof a notion of recurrence in transformers is detrimental. Furthermore, our\nstudy reveals that a novel, sparsely used, architecture which integrates\nRecurrent Highway Networks with neural gating and attention mechanisms, emerges\nas the best performing architecture in high-dimensional spatiotemporal\nforecasting of dynamical systems.\n","authors":["Hunter S. Heidenreich","Pantelis R. Vlachas","Petros Koumoutsakos"],"pdf_url":"https://arxiv.org/pdf/2410.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02651v1","updated":"2024-10-03T16:36:05Z","published":"2024-10-03T16:36:05Z","title":"CAX: Cellular Automata Accelerated in JAX","summary":"  Cellular automata have become a cornerstone for investigating emergence and\nself-organization across diverse scientific disciplines, spanning neuroscience,\nartificial life, and theoretical physics. However, the absence of a\nhardware-accelerated cellular automata library limits the exploration of new\nresearch directions, hinders collaboration, and impedes reproducibility. In\nthis work, we introduce CAX (Cellular Automata Accelerated in JAX), a\nhigh-performance and flexible open-source library designed to accelerate\ncellular automata research. CAX offers cutting-edge performance and a modular\ndesign through a user-friendly interface, and can support both discrete and\ncontinuous cellular automata with any number of dimensions. We demonstrate\nCAX's performance and flexibility through a wide range of benchmarks and\napplications. From classic models like elementary cellular automata and\nConway's Game of Life to advanced applications such as growing neural cellular\nautomata and self-classifying MNIST digits, CAX speeds up simulations up to\n2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate\nresearch by presenting a collection of three novel cellular automata\nexperiments, each implemented in just a few lines of code thanks to the\nlibrary's modular architecture. Notably, we show that a simple one-dimensional\ncellular automaton can outperform GPT-4 on the 1D-ARC challenge.\n","authors":["Maxence Faldor","Antoine Cully"],"pdf_url":"https://arxiv.org/pdf/2410.02651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02647v1","updated":"2024-10-03T16:33:35Z","published":"2024-10-03T16:33:35Z","title":"Immunogenicity Prediction with Dual Attention Enables Vaccine Target\n  Selection","summary":"  Immunogenicity prediction is a central topic in reverse vaccinology for\nfinding candidate vaccines that can trigger protective immune responses.\nExisting approaches typically rely on highly compressed features and simple\nmodel architectures, leading to limited prediction accuracy and poor\ngeneralizability. To address these challenges, we introduce ProVaccine, a novel\ndeep learning solution with a dual attention mechanism that integrates\npre-trained latent vector representations of protein sequences and structures.\nWe also compile the most comprehensive immunogenicity dataset to date,\nencompassing over 9,500 antigen sequences, structures, and immunogenicity\nlabels from bacteria, viruses, and tumors. Extensive experiments demonstrate\nthat ProVaccine outperforms existing methods across a wide range of evaluation\nmetrics. Furthermore, we establish a post-hoc validation protocol to assess the\npractical significance of deep learning models in tackling vaccine design\nchallenges. Our work provides an effective tool for vaccine design and sets\nvaluable benchmarks for future research.\n","authors":["Song Li","Yang Tan","Song Ke","Liang Hong","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.02647v1.pdf","comment":"18 pages, 11 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.02026v2","updated":"2024-10-03T16:31:59Z","published":"2024-09-03T16:20:22Z","title":"Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization","summary":"  In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.\n","authors":["Sean I. Young"],"pdf_url":"https://arxiv.org/pdf/2409.02026v2.pdf","comment":"Preprint. 17 pages, 4 figures, 5 appendices"},{"id":"http://arxiv.org/abs/2410.00712v2","updated":"2024-10-03T16:31:23Z","published":"2024-10-01T14:05:30Z","title":"NECOMIMI: Neural-Cognitive Multimodal EEG-informed Image Generation with\n  Diffusion Models","summary":"  NECOMIMI (NEural-COgnitive MultImodal EEG-Informed Image Generation with\nDiffusion Models) introduces a novel framework for generating images directly\nfrom EEG signals using advanced diffusion models. Unlike previous works that\nfocused solely on EEG-image classification through contrastive learning,\nNECOMIMI extends this task to image generation. The proposed NERV EEG encoder\ndemonstrates state-of-the-art (SoTA) performance across multiple zero-shot\nclassification tasks, including 2-way, 4-way, and 200-way, and achieves top\nresults in our newly proposed Category-based Assessment Table (CAT) Score,\nwhich evaluates the quality of EEG-generated images based on semantic concepts.\nA key discovery of this work is that the model tends to generate abstract or\ngeneralized images, such as landscapes, rather than specific objects,\nhighlighting the inherent challenges of translating noisy and low-resolution\nEEG data into detailed visual outputs. Additionally, we introduce the CAT Score\nas a new metric tailored for EEG-to-image evaluation and establish a benchmark\non the ThingsEEG dataset. This study underscores the potential of EEG-to-image\ngeneration while revealing the complexities and challenges that remain in\nbridging neural activity with visual representation.\n","authors":["Chi-Sheng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.00712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11295v2","updated":"2024-10-03T16:30:43Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2401.13858v3","updated":"2024-10-03T16:29:02Z","published":"2024-01-24T23:45:31Z","title":"Graph Diffusion Transformers for Multi-Conditional Molecular Generation","summary":"  Inverse molecular design with diffusion models holds great potential for\nadvancements in material and drug discovery. Despite success in unconditional\nmolecular generation, integrating multiple properties such as synthetic score\nand gas permeability as condition constraints into diffusion models remains\nunexplored. We present the Graph Diffusion Transformer (Graph DiT) for\nmulti-conditional molecular generation. Graph DiT integrates an encoder to\nlearn numerical and categorical property representations with the\nTransformer-based denoiser. Unlike previous graph diffusion models that add\nnoise separately on the atoms and bonds in the forward diffusion process, Graph\nDiT is trained with a novel graph-dependent noise model for accurate estimation\nof graph-related noise in molecules. We extensively validate Graph DiT for\nmulti-conditional polymer and small molecule generation. Results demonstrate\nthe superiority of Graph DiT across nine metrics from distribution learning to\ncondition control for molecular properties. A polymer inverse design task for\ngas separation with feedback from domain experts further demonstrates its\npractical utility.\n","authors":["Gang Liu","Jiaxin Xu","Tengfei Luo","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.13858v3.pdf","comment":"Accepted by NeurIPS 2024 (Oral). 21 pages, 11 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.02639v1","updated":"2024-10-03T16:24:14Z","published":"2024-10-03T16:24:14Z","title":"Labor Migration Modeling through Large-scale Job Query Data","summary":"  Accurate and timely modeling of labor migration is crucial for various urban\ngovernance and commercial tasks, such as local policy-making and business site\nselection. However, existing studies on labor migration largely rely on limited\nsurvey data with statistical methods, which fail to deliver timely and\nfine-grained insights for time-varying regional trends. To this end, we propose\na deep learning-based spatial-temporal labor migration analysis framework,\nDHG-SIL, by leveraging large-scale job query data. Specifically, we first\nacquire labor migration intention as a proxy of labor migration via job queries\nfrom one of the world's largest search engines. Then, a Disprepant Homophily\nco-preserved Graph Convolutional Network (DH-GCN) and an interpretable temporal\nmodule are respectively proposed to capture cross-city and sequential labor\nmigration dependencies. Besides, we introduce four interpretable variables to\nquantify city migration properties, which are co-optimized with city\nrepresentations via tailor-designed contrastive losses. Extensive experiments\non three real-world datasets demonstrate the superiority of our DHG-SIL.\nNotably, DHG-SIL has been deployed as a core component of a cooperative\npartner's intelligent human resource system, and the system supported a series\nof city talent attraction reports.\n","authors":["Zhuoning Guo","Le Zhang","Hengshu Zhu","Weijia Zhang","Hui Xiong","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04840v2","updated":"2024-10-03T16:23:07Z","published":"2024-09-07T14:38:05Z","title":"Sample and Oracle Efficient Reinforcement Learning for MDPs with\n  Linearly-Realizable Value Functions","summary":"  Designing sample-efficient and computationally feasible reinforcement\nlearning (RL) algorithms is particularly challenging in environments with large\nor infinite state and action spaces. In this paper, we advance this effort by\npresenting an efficient algorithm for Markov Decision Processes (MDPs) where\nthe state-action value function of any policy is linear in a given feature map.\nThis challenging setting can model environments with infinite states and\nactions, strictly generalizes classic linear MDPs, and currently lacks a\ncomputationally efficient algorithm under online access to the MDP.\nSpecifically, we introduce a new RL algorithm that efficiently finds a\nnear-optimal policy in this setting, using a number of episodes and calls to a\ncost-sensitive classification (CSC) oracle that are both polynomial in the\nproblem parameters. Notably, our CSC oracle can be efficiently implemented when\nthe feature dimension is constant, representing a clear improvement over\nstate-of-the-art methods, which require solving non-convex problems with\nhorizon-many variables and can incur computational costs that are exponential\nin the horizon.\n","authors":["Zakaria Mhammedi"],"pdf_url":"https://arxiv.org/pdf/2409.04840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.04901v3","updated":"2024-10-03T16:17:50Z","published":"2022-05-10T13:50:10Z","title":"Adjusted Expected Improvement for Cumulative Regret Minimization in\n  Noisy Bayesian Optimization","summary":"  The expected improvement (EI) is one of the most popular acquisition\nfunctions for Bayesian optimization (BO) and has demonstrated good empirical\nperformances in many applications for the minimization of simple regret.\nHowever, under the evaluation metric of cumulative regret, the performance of\nEI may not be competitive, and its existing theoretical regret upper bound\nstill has room for improvement. To adapt the EI for better performance under\ncumulative regret, we introduce a novel quantity called the evaluation cost\nwhich is compared against the acquisition function, and with this, develop the\nexpected improvement-cost (EIC) algorithm. In each iteration of EIC, a new\npoint with the largest acquisition function value is sampled, only if that\nvalue exceeds its evaluation cost. If none meets this criteria, the current\nbest point is resampled. This evaluation cost quantifies the potential downside\nof sampling a point, which is important under the cumulative regret metric as\nthe objective function value in every iteration affects the performance\nmeasure. We establish in theory a high-probability regret upper bound of EIC\nbased on the maximum information gain, which is tighter than the bound of\nexisting EI-based algorithms. It is also comparable to the regret bound of\nother popular BO algorithms such as Thompson sampling (GP-TS) and upper\nconfidence bound (GP-UCB). We further perform experiments to illustrate the\nimprovement of EIC over several popular BO algorithms.\n","authors":["Shouri Hu","Haowei Wang","Zhongxiang Dai","Bryan Kian Hsiang Low","Szu Hui Ng"],"pdf_url":"https://arxiv.org/pdf/2205.04901v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02629v1","updated":"2024-10-03T16:13:42Z","published":"2024-10-03T16:13:42Z","title":"Estimating Generalization Performance Along the Trajectory of Proximal\n  SGD in Robust Regression","summary":"  This paper studies the generalization performance of iterates obtained by\nGradient Descent (GD), Stochastic Gradient Descent (SGD) and their proximal\nvariants in high-dimensional robust regression problems. The number of features\nis comparable to the sample size and errors may be heavy-tailed. We introduce\nestimators that precisely track the generalization error of the iterates along\nthe trajectory of the iterative algorithm. These estimators are provably\nconsistent under suitable conditions. The results are illustrated through\nseveral examples, including Huber regression, pseudo-Huber regression, and\ntheir penalized variants with non-smooth regularizer. We provide explicit\ngeneralization error estimates for iterates generated from GD and SGD, or from\nproximal SGD in the presence of a non-smooth regularizer. The proposed risk\nestimates serve as effective proxies for the actual generalization error,\nallowing us to determine the optimal stopping iteration that minimizes the\ngeneralization error. Extensive simulations confirm the effectiveness of the\nproposed generalization error estimates.\n","authors":["Kai Tan","Pierre C. Bellec"],"pdf_url":"https://arxiv.org/pdf/2410.02629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02628v1","updated":"2024-10-03T16:12:59Z","published":"2024-10-03T16:12:59Z","title":"Inverse Entropic Optimal Transport Solves Semi-supervised Learning via\n  Data Likelihood Maximization","summary":"  Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in\nmachine learning, which is typically approached via supervised methods with\npaired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often\nchallenging, especially in problems such as domain translation. This\nnecessitates the development of $\\textit{semi-supervised}$ models that utilize\nboth limited paired data and additional unpaired i.i.d. samples $x \\sim\n\\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of\nsuch combined data is complex and often relies on heuristic approaches. To\ntackle this issue, we propose a new learning paradigm that integrates both\npaired and unpaired data $\\textbf{seamlessly}$ through the data likelihood\nmaximization techniques. We demonstrate that our approach also connects\nintriguingly with inverse entropic optimal transport (OT). This finding allows\nus to apply recent advances in computational OT to establish a $\\textbf{light}$\nlearning algorithm to get $\\pi^*(\\cdot|x)$. Furthermore, we demonstrate through\nempirical tests that our method effectively learns conditional distributions\nusing paired and unpaired data simultaneously.\n","authors":["Mikhail Persiianov","Arip Asadulaev","Nikita Andreev","Nikita Starodubcev","Dmitry Baranchuk","Anastasis Kratsios","Evgeny Burnaev","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2410.02628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06121v2","updated":"2024-10-03T16:10:43Z","published":"2024-08-12T13:03:34Z","title":"A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs","summary":"  In this paper, we explore different approaches to anomaly detection on\ndynamic knowledge graphs, specifically in a microservices environment for\nKubernetes applications. Our approach explores three dynamic knowledge graph\nrepresentations: sequential data, one-hop graph structure, and two-hop graph\nstructure, with each representation incorporating increasingly complex\nstructural information. Each phase includes different machine learning and deep\nlearning models. We empirically analyse their performance and propose an\napproach based on ensemble learning of these models. Our approach significantly\noutperforms the baseline on the ISWC 2024 Dynamic Knowledge Graph Anomaly\nDetection dataset, providing a robust solution for anomaly detection in dynamic\ncomplex data.\n","authors":["Xiaohua Lu","Leshanshui Yang"],"pdf_url":"https://arxiv.org/pdf/2408.06121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03731v2","updated":"2024-10-03T16:09:10Z","published":"2024-09-05T17:42:19Z","title":"A Deep Generative Learning Approach for Two-stage Adaptive Robust\n  Optimization","summary":"  Two-stage adaptive robust optimization (ARO) is a powerful approach for\nplanning under uncertainty, balancing first-stage decisions with recourse\ndecisions made after uncertainty is realized. To account for uncertainty,\nmodelers typically define a simple uncertainty set over which potential\noutcomes are considered. However, classical methods for defining these sets\nunintentionally capture a wide range of unrealistic outcomes, resulting in\noverly-conservative and costly planning in anticipation of unlikely\ncontingencies. In this work, we introduce AGRO, a solution algorithm that\nperforms adversarial generation for two-stage adaptive robust optimization\nusing a variational autoencoder. AGRO generates high-dimensional contingencies\nthat are simultaneously adversarial and realistic, improving the robustness of\nfirst-stage decisions at a lower planning cost than standard methods. To ensure\ngenerated contingencies lie in high-density regions of the uncertainty\ndistribution, AGRO defines a tight uncertainty set as the image of \"latent\"\nuncertainty sets under the VAE decoding transformation. Projected gradient\nascent is then used to maximize recourse costs over the latent uncertainty sets\nby leveraging differentiable optimization methods. We demonstrate the\ncost-efficiency of AGRO by applying it to both a synthetic\nproduction-distribution problem and a real-world power system expansion\nsetting. We show that AGRO outperforms the standard column-and-constraint\nalgorithm by up to 1.8% in production-distribution planning and up to 11.6% in\npower system expansion.\n","authors":["Aron Brenner","Rahman Khorramfar","Jennifer Sun","Saurabh Amin"],"pdf_url":"https://arxiv.org/pdf/2409.03731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02626v1","updated":"2024-10-03T16:08:16Z","published":"2024-10-03T16:08:16Z","title":"Online Learning Guided Quasi-Newton Methods with Global Non-Asymptotic\n  Convergence","summary":"  In this paper, we propose a quasi-Newton method for solving smooth and\nmonotone nonlinear equations, including unconstrained minimization and minimax\noptimization as special cases. For the strongly monotone setting, we establish\ntwo global convergence bounds: (i) a linear convergence rate that matches the\nrate of the celebrated extragradient method, and (ii) an explicit global\nsuperlinear convergence rate that provably surpasses the linear convergence\nrate after at most ${O}(d)$ iterations, where $d$ is the problem's dimension.\nIn addition, for the case where the operator is only monotone, we prove a\nglobal convergence rate of ${O}(\\min\\{{1}/{k},{\\sqrt{d}}/{k^{1.25}}\\})$ in\nterms of the duality gap. This matches the rate of the extragradient method\nwhen $k = {O}(d^2)$ and is faster when $k = \\Omega(d^2)$. These results are the\nfirst global convergence results to demonstrate a provable advantage of a\nquasi-Newton method over the extragradient method, without querying the\nJacobian of the operator. Unlike classical quasi-Newton methods, we achieve\nthis by using the hybrid proximal extragradient framework and a novel online\nlearning approach for updating the Jacobian approximation matrices.\nSpecifically, guided by the convergence analysis, we formulate the Jacobian\napproximation update as an online convex optimization problem over\nnon-symmetric matrices, relating the regret of the online problem to the\nconvergence rate of our method. To facilitate efficient implementation, we\nfurther develop a tailored online learning algorithm based on an approximate\nseparation oracle, which preserves structures such as symmetry and sparsity in\nthe Jacobian matrices.\n","authors":["Ruichen Jiang","Aryan Mokhtari"],"pdf_url":"https://arxiv.org/pdf/2410.02626v1.pdf","comment":"54 pages"},{"id":"http://arxiv.org/abs/2410.02622v1","updated":"2024-10-03T16:02:02Z","published":"2024-10-03T16:02:02Z","title":"Diss-l-ECT: Dissecting Graph Data with local Euler Characteristic\n  Transforms","summary":"  The Euler Characteristic Transform (ECT) is an efficiently-computable\ngeometrical-topological invariant that characterizes the global shape of data.\nIn this paper, we introduce the Local Euler Characteristic Transform\n($\\ell$-ECT), a novel extension of the ECT particularly designed to enhance\nexpressivity and interpretability in graph representation learning. Unlike\ntraditional Graph Neural Networks (GNNs), which may lose critical local details\nthrough aggregation, the $\\ell$-ECT provides a lossless representation of local\nneighborhoods. This approach addresses key limitations in GNNs by preserving\nnuanced local structures while maintaining global interpretability. Moreover,\nwe construct a rotation-invariant metric based on $\\ell$-ECTs for spatial\nalignment of data spaces. Our method exhibits superior performance than\nstandard GNNs on a variety of node classification tasks, particularly in graphs\nwith high heterophily.\n","authors":["Julius von Rohrscheidt","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.02622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13681v2","updated":"2024-10-03T16:01:01Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?","summary":"  In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02618v1","updated":"2024-10-03T15:56:03Z","published":"2024-10-03T15:56:03Z","title":"Achieving Fairness in Predictive Process Analytics via Adversarial\n  Learning (Extended Version)","summary":"  Predictive business process analytics has become important for organizations,\noffering real-time operational support for their processes. However, these\nalgorithms often perform unfair predictions because they are based on biased\nvariables (e.g., gender or nationality), namely variables embodying\ndiscrimination. This paper addresses the challenge of integrating a debiasing\nphase into predictive business process analytics to ensure that predictions are\nnot influenced by biased variables. Our framework leverages on adversial\ndebiasing is evaluated on four case studies, showing a significant reduction in\nthe contribution of biased variables to the predicted value. The proposed\ntechnique is also compared with the state of the art in fairness in process\nmining, illustrating that our framework allows for a more enhanced level of\nfairness, while retaining a better prediction quality.\n","authors":["Massimiliano de Leoni","Alessandro Padella"],"pdf_url":"https://arxiv.org/pdf/2410.02618v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.14662v2","updated":"2024-10-03T15:52:17Z","published":"2024-06-20T18:30:09Z","title":"Advantage Alignment Algorithms","summary":"  Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation.\n","authors":["Juan Agustin Duque","Milad Aghajohari","Tim Cooijmans","Razvan Ciuca","Tianyu Zhang","Gauthier Gidel","Aaron Courville"],"pdf_url":"https://arxiv.org/pdf/2406.14662v2.pdf","comment":"25 Pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.02615v1","updated":"2024-10-03T15:52:03Z","published":"2024-10-03T15:52:03Z","title":"LoGra-Med: Long Context Multi-Graph Alignment for Medical\n  Vision-Language Model","summary":"  State-of-the-art medical multi-modal large language models (med-MLLM), like\nLLaVA-Med or BioMedGPT, leverage instruction-following data in pre-training.\nHowever, those models primarily focus on scaling the model size and data volume\nto boost performance while mainly relying on the autoregressive learning\nobjectives. Surprisingly, we reveal that such learning schemes might result in\na weak alignment between vision and language modalities, making these models\nhighly reliant on extensive pre-training datasets - a significant challenge in\nmedical domains due to the expensive and time-consuming nature of curating\nhigh-quality instruction-following instances. We address this with LoGra-Med, a\nnew multi-graph alignment algorithm that enforces triplet correlations across\nimage modalities, conversation-based descriptions, and extended captions. This\nhelps the model capture contextual meaning, handle linguistic variability, and\nbuild cross-modal associations between visuals and text. To scale our approach,\nwe designed an efficient end-to-end learning scheme using black-box gradient\nestimation, enabling faster LLaMa 7B training. Our results show LoGra-Med\nmatches LLAVA-Med performance on 600K image-text pairs for Medical VQA and\nsignificantly outperforms it when trained on 10% of the data. For example, on\nVQA-RAD, we exceed LLAVA-Med by 20.13% and nearly match the 100% pre-training\nscore (72.52% vs. 72.64%). We also surpass SOTA methods like BiomedGPT on\nvisual chatbots and RadFM on zero-shot image classification with VQA,\nhighlighting the effectiveness of multi-graph alignment.\n","authors":["Duy M. H. Nguyen","Nghiem T. Diep","Trung Q. Nguyen","Hoang-Bao Le","Tai Nguyen","Tien Nguyen","TrungTin Nguyen","Nhat Ho","Pengtao Xie","Roger Wattenhofer","James Zhou","Daniel Sonntag","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2410.02615v1.pdf","comment":"First version"},{"id":"http://arxiv.org/abs/2410.02611v1","updated":"2024-10-03T15:50:08Z","published":"2024-10-03T15:50:08Z","title":"IndicSentEval: How Effectively do Multilingual Transformer Models encode\n  Linguistic Properties for Indic Languages?","summary":"  Transformer-based models have revolutionized the field of natural language\nprocessing. To understand why they perform so well and to assess their\nreliability, several studies have focused on questions such as: Which\nlinguistic properties are encoded by these models, and to what extent? How\nrobust are these models in encoding linguistic properties when faced with\nperturbations in the input text? However, these studies have mainly focused on\nBERT and the English language. In this paper, we investigate similar questions\nregarding encoding capability and robustness for 8 linguistic properties across\n13 different perturbations in 6 Indic languages, using 9 multilingual\nTransformer models (7 universal and 2 Indic-specific). To conduct this study,\nwe introduce a novel multilingual benchmark dataset, IndicSentEval, containing\napproximately $\\sim$47K sentences. Surprisingly, our probing analysis of\nsurface, syntactic, and semantic properties reveals that while almost all\nmultilingual models demonstrate consistent encoding performance for English,\nthey show mixed results for Indic languages. As expected, Indic-specific\nmultilingual models capture linguistic properties in Indic languages better\nthan universal models. Intriguingly, universal models broadly exhibit better\nrobustness compared to Indic-specific models, particularly under perturbations\nsuch as dropping both nouns and verbs, dropping only verbs, or keeping only\nnouns. Overall, this study provides valuable insights into probing and\nperturbation-specific strengths and weaknesses of popular multilingual\nTransformer-based models for different Indic languages. We make our code and\ndataset publicly available [https://tinyurl.com/IndicSentEval}].\n","authors":["Akhilesh Aravapalli","Mounika Marreddy","Subba Reddy Oota","Radhika Mamidi","Manish Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.02611v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.10960v3","updated":"2024-10-03T15:48:45Z","published":"2024-07-15T17:55:42Z","title":"Fast Matrix Multiplications for Lookup Table-Quantized LLMs","summary":"  The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.\n","authors":["Han Guo","William Brandon","Radostin Cholakov","Jonathan Ragan-Kelley","Eric P. Xing","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.10960v3.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.18164v3","updated":"2024-10-03T15:46:16Z","published":"2024-06-26T08:24:44Z","title":"Nebula: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2304.08460v3","updated":"2024-10-03T15:46:13Z","published":"2023-04-17T17:36:35Z","title":"LongForm: Effective Instruction Tuning with Reverse Instructions","summary":"  Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.\n","authors":["Abdullatif Kksal","Timo Schick","Anna Korhonen","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2304.08460v3.pdf","comment":"EMNLP 2024 Findings. This version extends the training with recent\n  LLMs, evaluation with new metrics, and NLU tasks"},{"id":"http://arxiv.org/abs/2410.02605v1","updated":"2024-10-03T15:45:39Z","published":"2024-10-03T15:45:39Z","title":"Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative\n  Prospect Theoretic Reinforcement Learning","summary":"  The widely used expected utility theory has been shown to be empirically\ninconsistent with human preferences in the psychology and behavioral economy\nliteratures. Cumulative Prospect Theory (CPT) has been developed to fill in\nthis gap and provide a better model for human-based decision-making supported\nby empirical evidence. It allows to express a wide range of attitudes and\nperceptions towards risk, gains and losses. A few years ago, CPT has been\ncombined with Reinforcement Learning (RL) to formulate a CPT policy\noptimization problem where the goal of the agent is to search for a policy\ngenerating long-term returns which are aligned with their preferences. In this\nwork, we revisit this policy optimization problem and provide new insights on\noptimal policies and their nature depending on the utility function under\nconsideration. We further derive a novel policy gradient theorem for the CPT\npolicy optimization objective generalizing the seminal corresponding result in\nstandard RL. This result enables us to design a model-free policy gradient\nalgorithm to solve the CPT-RL problem. We illustrate the performance of our\nalgorithm in simple examples motivated by traffic control and electricity\nmanagement applications. We also demonstrate that our policy gradient algorithm\nscales better to larger state spaces compared to the existing zeroth order\nalgorithm for solving the same problem.\n","authors":["Olivier Lepel","Anas Barakat"],"pdf_url":"https://arxiv.org/pdf/2410.02605v1.pdf","comment":"33 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.02604v1","updated":"2024-10-03T15:45:15Z","published":"2024-10-03T15:45:15Z","title":"Long-Sequence Recommendation Models Need Decoupled Embeddings","summary":"  Lifelong user behavior sequences, comprising up to tens of thousands of\nhistory behaviors, are crucial for capturing user interests and predicting user\nresponses in modern recommendation systems. A two-stage paradigm is typically\nadopted to handle these long sequences: a few relevant behaviors are first\nsearched from the original long sequences via an attention mechanism in the\nfirst stage and then aggregated with the target item to construct a\ndiscriminative representation for prediction in the second stage. In this work,\nwe identify and characterize, for the first time, a neglected deficiency in\nexisting long-sequence recommendation models: a single set of embeddings\nstruggles with learning both attention and representation, leading to\ninterference between these two processes. Initial attempts to address this\nissue using linear projections -- a technique borrowed from language processing\n-- proved ineffective, shedding light on the unique challenges of\nrecommendation models. To overcome this, we propose the Decoupled Attention and\nRepresentation Embeddings (DARE) model, where two distinct embedding tables are\ninitialized and learned separately to fully decouple attention and\nrepresentation. Extensive experiments and analysis demonstrate that DARE\nprovides more accurate search of correlated behaviors and outperforms baselines\nwith AUC gains up to 0.9% on public datasets and notable online system\nimprovements. Furthermore, decoupling embedding spaces allows us to reduce the\nattention embedding dimension and accelerate the search procedure by 50%\nwithout significant performance impact, enabling more efficient,\nhigh-performance online serving.\n","authors":["Ningya Feng","Junwei Pan","Jialong Wu","Baixu Chen","Ximei Wang","Qian Li","Xian Hu","Jie Jiang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2410.02604v1.pdf","comment":"First three authors contributed equally"},{"id":"http://arxiv.org/abs/2410.02603v1","updated":"2024-10-03T15:44:42Z","published":"2024-10-03T15:44:42Z","title":"Agents' Room: Narrative Generation through Multi-step Collaboration","summary":"  Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.\n","authors":["Fantine Huot","Reinald Kim Amplayo","Jennimaria Palomaki","Alice Shoshana Jakobovits","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.02603v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.02601v1","updated":"2024-10-03T15:43:17Z","published":"2024-10-03T15:43:17Z","title":"Diffusion & Adversarial Schrdinger Bridges via Iterative Proportional\n  Markovian Fitting","summary":"  The Iterative Markovian Fitting (IMF) procedure based on iterative reciprocal\nand Markovian projections has recently been proposed as a powerful method for\nsolving the Schr\\\"odinger Bridge problem. However, it has been observed that\nfor the practical implementation of this procedure, it is crucial to alternate\nbetween fitting a forward and backward time diffusion at each iteration. Such\nimplementation is thought to be a practical heuristic, which is required to\nstabilize training and obtain good results in applications such as unpaired\ndomain translation. In our work, we show that this heuristic closely connects\nwith the pioneer approaches for the Schr\\\"odinger Bridge based on the Iterative\nProportional Fitting (IPF) procedure. Namely, we find that the practical\nimplementation of IMF is, in fact, a combination of IMF and IPF procedures, and\nwe call this combination the Iterative Proportional Markovian Fitting (IPMF)\nprocedure. We show both theoretically and practically that this combined IPMF\nprocedure can converge under more general settings, thus, showing that the IPMF\nprocedure opens a door towards developing a unified framework for solving\nSchr\\\"odinger Bridge problems.\n","authors":["Sergei Kholkin","Grigoriy Ksenofontov","David Li","Nikita Kornilov","Nikita Gushchin","Evgeny Burnaev","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2410.02601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02597v1","updated":"2024-10-03T15:38:20Z","published":"2024-10-03T15:38:20Z","title":"Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR","summary":"  We present \\textbf{H}ybrid-\\textbf{A}utoregressive \\textbf{IN}ference\nTr\\textbf{AN}sducers (HAINAN), a novel architecture for speech recognition that\nextends the Token-and-Duration Transducer (TDT) model. Trained with randomly\nmasked predictor network outputs, HAINAN supports both autoregressive inference\nwith all network components and non-autoregressive inference without the\npredictor. Additionally, we propose a novel semi-autoregressive inference\nparadigm that first generates an initial hypothesis using non-autoregressive\ninference, followed by refinement steps where each token prediction is\nregenerated using parallelized autoregression on the initial hypothesis.\nExperiments on multiple datasets across different languages demonstrate that\nHAINAN achieves efficiency parity with CTC in non-autoregressive mode and with\nTDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN\noutperforms TDT and RNN-T, while non-autoregressive HAINAN significantly\noutperforms CTC. Semi-autoregressive inference further enhances the model's\naccuracy with minimal computational overhead, and even outperforms TDT results\nin some cases. These results highlight HAINAN's flexibility in balancing\naccuracy and speed, positioning it as a strong candidate for real-world speech\nrecognition applications.\n","authors":["Hainan Xu","Travis M. Bartley","Vladimir Bataev","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2410.02597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02596v1","updated":"2024-10-03T15:37:22Z","published":"2024-10-03T15:37:22Z","title":"Beyond Squared Error: Exploring Loss Design for Enhanced Training of\n  Generative Flow Networks","summary":"  Generative Flow Networks (GFlowNets) are a novel class of generative models\ndesigned to sample from unnormalized distributions and have found applications\nin various important tasks, attracting great research interest in their\ntraining algorithms. In general, GFlowNets are trained by fitting the forward\nflow to the backward flow on sampled training objects. Prior work focused on\nthe choice of training objects, parameterizations, sampling and resampling\nstrategies, and backward policies, aiming to enhance credit assignment,\nexploration, or exploitation of the training process. However, the choice of\nregression loss, which can highly influence the exploration and exploitation\nbehavior of the under-training policy, has been overlooked. Due to the lack of\ntheoretical understanding for choosing an appropriate regression loss, most\nexisting algorithms train the flow network by minimizing the squared error of\nthe forward and backward flows in log-space, i.e., using the quadratic\nregression loss. In this work, we rigorously prove that distinct regression\nlosses correspond to specific divergence measures, enabling us to design and\nanalyze regression losses according to the desired properties of the\ncorresponding divergence measures. Specifically, we examine two key properties:\nzero-forcing and zero-avoiding, where the former promotes exploitation and\nhigher rewards, and the latter encourages exploration and enhances diversity.\nBased on our theoretical framework, we propose three novel regression losses,\nnamely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three\nbenchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our\nproposed losses are compatible with most existing training algorithms, and\nsignificantly improve the performances of the algorithms concerning convergence\nspeed, sample diversity, and robustness.\n","authors":["Rui Hu","Yifan Zhang","Zhuoran Li","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02592v1","updated":"2024-10-03T15:34:41Z","published":"2024-10-03T15:34:41Z","title":"IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of\n  Both Driver and Passengers","summary":"  Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.\n","authors":["Zihan Fang","Zheng Lin","Senkang Hu","Hangcheng Cao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.02592v1.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2405.06443v2","updated":"2024-10-03T15:34:02Z","published":"2024-05-10T12:48:57Z","title":"Residual-based Attention Physics-informed Neural Networks for\n  Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power\n  Plants","summary":"  Transformers are crucial for reliable and efficient power system operations,\nparticularly in supporting the integration of renewable energy. Effective\nmonitoring of transformer health is critical to maintain grid stability and\nperformance. Thermal insulation ageing is a key transformer failure mode, which\nis generally tracked by monitoring the hotspot temperature (HST). However, HST\nmeasurement is complex, costly, and often estimated from indirect measurements.\nExisting HST models focus on space-agnostic thermal models, providing\nworst-case HST estimates. This article introduces a spatio-temporal model for\ntransformer winding temperature and ageing estimation, which leverages\nphysics-based partial differential equations (PDEs) with data-driven Neural\nNetworks (NN) in a Physics Informed Neural Networks (PINNs) configuration to\nimprove prediction accuracy and acquire spatio-temporal resolution. The\ncomputational accuracy of the PINN model is improved through the implementation\nof the Residual-Based Attention (PINN-RBA) scheme that accelerates the PINN\nmodel convergence. The PINN-RBA model is benchmarked against self-adaptive\nattention schemes and classical vanilla PINN configurations. For the first\ntime, PINN based oil temperature predictions are used to estimate\nspatio-temporal transformer winding temperature values, validated through PDE\nnumerical solution and fiber optic sensor measurements. Furthermore, the\nspatio-temporal transformer ageing model is inferred, which supports\ntransformer health management decision-making. Results are validated with a\ndistribution transformer operating on a floating photovoltaic power plant.\n","authors":["Ibai Ramirez","Joel Pino","David Pardo","Mikel Sanz","Luis del Rio","Alvaro Ortiz","Kateryna Morozovska","Jose I. Aizpurua"],"pdf_url":"https://arxiv.org/pdf/2405.06443v2.pdf","comment":"23 pages, 18 figures"},{"id":"http://arxiv.org/abs/2410.02590v1","updated":"2024-10-03T15:32:08Z","published":"2024-10-03T15:32:08Z","title":"Generalization emerges from local optimization in a self-organized\n  learning network","summary":"  We design and analyze a new paradigm for building supervised learning\nnetworks, driven only by local optimization rules without relying on a global\nerror function. Traditional neural networks with a fixed topology are made up\nof identical nodes and derive their expressiveness from an appropriate\nadjustment of connection weights. In contrast, our network stores new knowledge\nin the nodes accurately and instantaneously, in the form of a lookup table.\nOnly then is some of this information structured and incorporated into the\nnetwork geometry. The training error is initially zero by construction and\nremains so throughout the network topology transformation phase. The latter\ninvolves a small number of local topological transformations, such as splitting\nor merging of nodes and adding binary connections between them. The choice of\noperations to be carried out is only driven by optimization of expressivity at\nthe local scale. What we are primarily looking for in a learning network is its\nability to generalize, i.e. its capacity to correctly answer questions for\nwhich it has never learned the answers. We show on numerous examples of\nclassification tasks that the networks generated by our algorithm\nsystematically reach such a state of perfect generalization when the number of\nlearned examples becomes sufficiently large. We report on the dynamics of the\nchange of state and show that it is abrupt and has the distinctive\ncharacteristics of a first order phase transition, a phenomenon already\nobserved for traditional learning networks and known as grokking. In addition\nto proposing a non-potential approach for the construction of learning\nnetworks, our algorithm makes it possible to rethink the grokking transition in\na new light, under which acquisition of training data and topological\nstructuring of data are completely decoupled phenomena.\n","authors":["S. Barland","L. Gil"],"pdf_url":"https://arxiv.org/pdf/2410.02590v1.pdf","comment":"This paper is submitted to Phys. Rev. X. It's a physicist's study\n  that focus on a new paradigm for deep learning networks. We would have liked\n  to choose other keywords for arXiv to reach a wider community, but don't have\n  the rights to do so"},{"id":"http://arxiv.org/abs/2410.02581v1","updated":"2024-10-03T15:25:37Z","published":"2024-10-03T15:25:37Z","title":"Boosting Sample Efficiency and Generalization in Multi-agent\n  Reinforcement Learning via Equivariance","summary":"  Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency\nand poor generalization [1]. These challenges are partially due to a lack of\nstructure or inductive bias in the neural networks typically used in learning\nthe policy. One such form of structure that is commonly observed in multi-agent\nscenarios is symmetry. The field of Geometric Deep Learning has developed\nEquivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to\nrotations, translations, and reflections of nodes. Incorporating equivariance\nhas been shown to improve learning efficiency and decrease error [ 2 ]. In this\npaper, we demonstrate that EGNNs improve the sample efficiency and\ngeneralization in MARL. However, we also show that a naive application of EGNNs\nto MARL results in poor early exploration due to a bias in the EGNN structure.\nTo mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural\nNetworks or E2GN2. We compare E2GN2 to other common function approximators\nusing common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant\nimprovement in sample efficiency, greater final reward convergence, and a 2x-5x\ngain in over standard GNNs in our generalization tests. These results pave the\nway for more reliable and effective solutions in complex multi-agent systems.\n","authors":["Joshua McClellan","Naveed Haghani","John Winder","Furong Huang","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2410.02581v1.pdf","comment":"accepted as a poster at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.18313v3","updated":"2024-10-03T15:17:22Z","published":"2024-09-26T21:44:11Z","title":"Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and\n  Generation","summary":"  There is no limit to how much a robot might explore and learn, but all of\nthat knowledge needs to be searchable and actionable. Within language research,\nretrieval augmented generation (RAG) has become the workhouse of large-scale\nnon-parametric knowledge, however existing techniques do not directly transfer\nto the embodied domain, which is multimodal, data is highly correlated, and\nperception requires abstraction.\n  To address these challenges, we introduce Embodied-RAG, a framework that\nenhances the foundational model of an embodied agent with a non-parametric\nmemory system capable of autonomously constructing hierarchical knowledge for\nboth navigation and language generation. Embodied-RAG handles a full range of\nspatial and semantic resolutions across diverse environments and query types,\nwhether for a specific object or a holistic description of ambiance. At its\ncore, Embodied-RAG's memory is structured as a semantic forest, storing\nlanguage descriptions at varying levels of detail. This hierarchical\norganization allows the system to efficiently generate context-sensitive\noutputs across different robotic platforms. We demonstrate that Embodied-RAG\neffectively bridges RAG to the robotics domain, successfully handling over 200\nexplanation and navigation queries across 19 environments, highlighting its\npromise for general-purpose non-parametric system for embodied agents.\n","authors":["Quanting Xie","So Yeon Min","Tianyi Zhang","Aarav Bajaj","Ruslan Salakhutdinov","Matthew Johnson-Roberson","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2409.18313v3.pdf","comment":"Web: https://quanting-xie.github.io/Embodied-RAG-web/"},{"id":"http://arxiv.org/abs/2405.17829v2","updated":"2024-10-03T15:14:29Z","published":"2024-05-28T04:59:13Z","title":"LDMol: Text-to-Molecule Diffusion Model with Structurally Informative\n  Latent Space","summary":"  With the emergence of diffusion models as the frontline of generative models,\nmany researchers have proposed molecule generation techniques with conditional\ndiffusion models. However, the unavoidable discreteness of a molecule makes it\ndifficult for a diffusion model to connect raw data with highly complex\nconditions like natural language. To address this, we present a novel latent\ndiffusion model dubbed LDMol for text-conditioned molecule generation. LDMol\ncomprises a molecule autoencoder that produces a learnable and structurally\ninformative feature space, and a natural language-conditioned latent diffusion\nmodel. In particular, recognizing that multiple SMILES notations can represent\nthe same molecule, we employ a contrastive learning strategy to extract feature\nspace that is aware of the unique characteristics of the molecule structure.\nLDMol outperforms the existing baselines on the text-to-molecule generation\nbenchmark, suggesting a potential for diffusion models can outperform\nautoregressive models in text data generation with a better choice of the\nlatent domain. Furthermore, we show that LDMol can be applied to downstream\ntasks such as molecule-to-text retrieval and text-guided molecule editing,\ndemonstrating its versatility as a diffusion model.\n","authors":["Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2405.17829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02566v1","updated":"2024-10-03T15:10:02Z","published":"2024-10-03T15:10:02Z","title":"Deep Learning-Based Prediction of Suspension Dynamics Performance in\n  Multi-Axle Vehicles","summary":"  This paper presents a deep learning-based framework for predicting the\ndynamic performance of suspension systems in multi-axle vehicles, emphasizing\nthe integration of machine learning with traditional vehicle dynamics modeling.\nA Multi-Task Deep Belief Network Deep Neural Network (MTL-DBN-DNN) was\ndeveloped to capture the relationships between key vehicle parameters and\nsuspension performance metrics. The model was trained on data generated from\nnumerical simulations and demonstrated superior prediction accuracy compared to\nconventional DNN models. A comprehensive sensitivity analysis was conducted to\nassess the impact of various vehicle and suspension parameters on dynamic\nsuspension performance. Additionally, the Suspension Dynamic Performance Index\n(SDPI) was introduced as a holistic measure to quantify overall suspension\nperformance, accounting for the combined effects of multiple parameters. The\nfindings highlight the effectiveness of multitask learning in improving\npredictive models for complex vehicle systems.\n","authors":["Kai Chun Lin","Bo-Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2410.02566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14407v2","updated":"2024-10-03T15:07:52Z","published":"2024-02-22T09:48:47Z","title":"Learning an Actionable Discrete Diffusion Policy via Large-Scale\n  Actionless Video Pre-Training","summary":"  Learning a generalist embodied agent capable of completing multiple tasks\nposes challenges, primarily stemming from the scarcity of action-labeled\nrobotic datasets. In contrast, a vast amount of human videos exist, capturing\nintricate tasks and interactions with the physical world. Promising prospects\narise for utilizing actionless human videos for pre-training and transferring\nthe knowledge to facilitate robot policy learning through limited robot\ndemonstrations. However, it remains a challenge due to the domain gap between\nhumans and robots. Moreover, it is difficult to extract useful information\nrepresenting the dynamic world from human videos, because of its noisy and\nmultimodal data structure. In this paper, we introduce a novel framework to\ntackle these challenges, which leverages a unified discrete diffusion to\ncombine generative pre-training on human videos and policy fine-tuning on a\nsmall number of action-labeled robot videos. We start by compressing both human\nand robot videos into unified video tokens. In the pre-training stage, we\nemploy a discrete diffusion model with a mask-and-replace diffusion strategy to\npredict future video tokens in the latent space. In the fine-tuning stage, we\nharness the imagined future videos to guide low-level action learning with a\nlimited set of robot data. Experiments demonstrate that our method generates\nhigh-fidelity future videos for planning and enhances the fine-tuned policies\ncompared to previous state-of-the-art approaches with superior performance. Our\nproject website is available at https://video-diff.github.io/.\n","authors":["Haoran He","Chenjia Bai","Ling Pan","Weinan Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2402.14407v2.pdf","comment":"Accepted by NeurIPS 2024. 24 pages"},{"id":"http://arxiv.org/abs/2410.02561v1","updated":"2024-10-03T15:04:47Z","published":"2024-10-03T15:04:47Z","title":"The Benefit of Being Bayesian in Online Conformal Prediction","summary":"  Based on the framework of Conformal Prediction (CP), we study the online\nconstruction of valid confidence sets given a black-box machine learning model.\nBy converting the target confidence levels into quantile levels, the problem\ncan be reduced to predicting the quantiles (in hindsight) of a sequentially\nrevealed data sequence. Two very different approaches have been studied\npreviously. (i) Direct approach: Assuming the data sequence is iid or\nexchangeable, one could maintain the empirical distribution of the observed\ndata as an algorithmic belief, and directly predict its quantiles. (ii)\nIndirect approach: As statistical assumptions often do not hold in practice, a\nrecent trend is to consider the adversarial setting and apply first-order\nonline optimization to moving quantile losses (Gibbs & Cand\\`es, 2021). It\nrequires knowing the target quantile level beforehand, and suffers from certain\nvalidity issues on the obtained confidence sets, due to the associated loss\nlinearization.\n  This paper presents a novel Bayesian CP framework that combines their\nstrengths. Without any statistical assumption, it is able to both: (i) answer\nmultiple arbitrary confidence level queries online, with provably low regret;\nand (ii) overcome the validity issues suffered by first-order optimization\nbaselines, due to being \"data-centric\" rather than \"iterate-centric\".\n  From a technical perspective, our key idea is to regularize the algorithmic\nbelief of the above direct approach by a Bayesian prior, which \"robustifies\" it\nby simulating a non-linearized Follow the Regularized Leader (FTRL) algorithm\non the output. For statisticians, this can be regarded as an online adversarial\nview of Bayesian inference. Importantly, the proposed belief update backbone is\nshared by prediction heads targeting different confidence levels, bringing\npractical benefits analogous to U-calibration (Kleinberg et al., 2023).\n","authors":["Zhiyu Zhang","Zhou Lu","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02559v1","updated":"2024-10-03T15:04:01Z","published":"2024-10-03T15:04:01Z","title":"Obtaining Lower Query Complexities through Lightweight Zeroth-Order\n  Proximal Gradient Algorithms","summary":"  Zeroth-order (ZO) optimization is one key technique for machine learning\nproblems where gradient calculation is expensive or impossible. Several\nvariance reduced ZO proximal algorithms have been proposed to speed up ZO\noptimization for non-smooth problems, and all of them opted for the coordinated\nZO estimator against the random ZO estimator when approximating the true\ngradient, since the former is more accurate. While the random ZO estimator\nintroduces bigger error and makes convergence analysis more challenging\ncompared to coordinated ZO estimator, it requires only $\\mathcal{O}(1)$\ncomputation, which is significantly less than $\\mathcal{O}(d)$ computation of\nthe coordinated ZO estimator, with $d$ being dimension of the problem space. To\ntake advantage of the computationally efficient nature of the random ZO\nestimator, we first propose a ZO objective decrease (ZOOD) property which can\nincorporate two different types of errors in the upper bound of convergence\nrate. Next, we propose two generic reduction frameworks for ZO optimization\nwhich can automatically derive the convergence results for convex and\nnon-convex problems respectively, as long as the convergence rate for the inner\nsolver satisfies the ZOOD property. With the application of two reduction\nframeworks on our proposed ZOR-ProxSVRG and ZOR-ProxSAGA, two variance reduced\nZO proximal algorithms with fully random ZO estimators, we improve the\nstate-of-the-art function query complexities from\n$\\mathcal{O}\\left(\\min\\{\\frac{dn^{1/2}}{\\epsilon^2},\n\\frac{d}{\\epsilon^3}\\}\\right)$ to\n$\\tilde{\\mathcal{O}}\\left(\\frac{n+d}{\\epsilon^2}\\right)$ under $d >\nn^{\\frac{1}{2}}$ for non-convex problems, and from\n$\\mathcal{O}\\left(\\frac{d}{\\epsilon^2}\\right)$ to\n$\\tilde{\\mathcal{O}}\\left(n\\log\\frac{1}{\\epsilon}+\\frac{d}{\\epsilon}\\right)$\nfor convex problems.\n","authors":["Bin Gu","Xiyuan Wei","Hualin Zhang","Yi Chang","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02559v1.pdf","comment":"Neural Computation 36 (5), 897-935"},{"id":"http://arxiv.org/abs/2402.12817v2","updated":"2024-10-03T14:56:24Z","published":"2024-02-20T08:38:19Z","title":"On Sensitivity of Learning with Limited Labelled Data to the Effects of\n  Randomness: Impact of Interactions and Systematic Choices","summary":"  While learning with limited labelled data can improve performance when the\nlabels are lacking, it is also sensitive to the effects of uncontrolled\nrandomness introduced by so-called randomness factors (e.g., varying order of\ndata). We propose a method to systematically investigate the effects of\nrandomness factors while taking the interactions between them into\nconsideration. To measure the true effects of an individual randomness factor,\nour method mitigates the effects of other factors and observes how the\nperformance varies across multiple runs. Applying our method to multiple\nrandomness factors across in-context learning and fine-tuning approaches on 7\nrepresentative text classification tasks and meta-learning on 3 tasks, we show\nthat: 1) disregarding interactions between randomness factors in existing works\ncaused inconsistent findings due to incorrect attribution of the effects of\nrandomness factors, such as disproving the consistent sensitivity of in-context\nlearning to sample order even with random sample selection; and 2) besides\nmutual interactions, the effects of randomness factors, especially sample\norder, are also dependent on more systematic choices unexplored in existing\nworks, such as number of classes, samples per class or choice of prompt format.\n","authors":["Branislav Pecher","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2402.12817v2.pdf","comment":"Accepted to the EMNLP'24 Main Conference"},{"id":"http://arxiv.org/abs/2309.16519v3","updated":"2024-10-03T14:55:41Z","published":"2023-09-28T15:25:17Z","title":"AtomSurf : Surface Representation for Learning on Protein Structures","summary":"  While there has been significant progress in evaluating and comparing\ndifferent representations for learning on protein data, the role of\nsurface-based learning approaches remains not well-understood. In particular,\nthere is a lack of direct and fair benchmark comparison between the best\navailable surface-based learning methods against alternative representations\nsuch as graphs. Moreover, the few existing surface-based approaches either use\nsurface information in isolation or, at best, perform global pooling between\nsurface and graph-based architectures.\n  In this work, we fill this gap by first adapting a state-of-the-art surface\nencoder for protein learning tasks. We then perform a direct and fair\ncomparison of the resulting method against alternative approaches within the\nAtom3D benchmark, highlighting the limitations of pure surface-based learning.\nFinally, we propose an integrated approach, which allows learned feature\nsharing between graphs and surface representations on the level of nodes and\nvertices $\\textit{across all layers}$.\n  We demonstrate that the resulting architecture achieves state-of-the-art\nresults on all tasks in the Atom3D benchmark, while adhering to the strict\nbenchmark protocol, as well as more broadly on binding site identification and\nbinding pocket classification. Furthermore, we use coarsened surfaces and\noptimize our approach for efficiency, making our tool competitive in training\nand inference time with existing techniques. Our code and data can be found\nonline: $\\texttt{github.com/Vincentx15/atomsurf}$\n","authors":["Vincent Mallet","Souhaib Attaiki","Yangyang Miao","Bruno Correia","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2309.16519v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.02551v1","updated":"2024-10-03T14:55:22Z","published":"2024-10-03T14:55:22Z","title":"ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration","summary":"  We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app.\n","authors":["Zixiang Wang","Yinghao Zhu","Huiya Zhao","Xiaochen Zheng","Tianlong Wang","Wen Tang","Yasha Wang","Chengwei Pan","Ewen M. Harrison","Junyi Gao","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02548v1","updated":"2024-10-03T14:53:10Z","published":"2024-10-03T14:53:10Z","title":"Local Flow Matching Generative Models","summary":"  Flow Matching (FM) is a simulation-free method for learning a continuous and\ninvertible flow to interpolate between two distributions, and in particular to\ngenerate data from noise in generative modeling. In this paper, we introduce\nLocal Flow Matching (LFM), which learns a sequence of FM sub-models and each\nmatches a diffusion process up to the time of the step size in the\ndata-to-noise direction. In each step, the two distributions to be interpolated\nby the sub-model are closer to each other than data vs. noise, and this enables\nthe use of smaller models with faster training. The stepwise structure of LFM\nis natural to be distilled and different distillation techniques can be adopted\nto speed up generation. Theoretically, we prove a generation guarantee of the\nproposed flow model in terms of the $\\chi^2$-divergence between the generated\nand true data distributions. In experiments, we demonstrate the improved\ntraining efficiency and competitive generative performance of LFM compared to\nFM on the unconditional generation of tabular data and image datasets, and also\non the conditional generation of robotic manipulation policies.\n","authors":["Chen Xu","Xiuyuan Cheng","Yao Xie"],"pdf_url":"https://arxiv.org/pdf/2410.02548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02543v1","updated":"2024-10-03T14:47:46Z","published":"2024-10-03T14:47:46Z","title":"Diffusion Models are Evolutionary Algorithms","summary":"  In a convergence of machine learning and biology, we reveal that diffusion\nmodels are evolutionary algorithms. By considering evolution as a denoising\nprocess and reversed evolution as diffusion, we mathematically demonstrate that\ndiffusion models inherently perform evolutionary algorithms, naturally\nencompassing selection, mutation, and reproductive isolation. Building on this\nequivalence, we propose the Diffusion Evolution method: an evolutionary\nalgorithm utilizing iterative denoising -- as originally introduced in the\ncontext of diffusion models -- to heuristically refine solutions in parameter\nspaces. Unlike traditional approaches, Diffusion Evolution efficiently\nidentifies multiple optimal solutions and outperforms prominent mainstream\nevolutionary algorithms. Furthermore, leveraging advanced concepts from\ndiffusion models, namely latent space diffusion and accelerated sampling, we\nintroduce Latent Space Diffusion Evolution, which finds solutions for\nevolutionary tasks in high-dimensional complex parameter space while\nsignificantly reducing computational steps. This parallel between diffusion and\nevolution not only bridges two different fields but also opens new avenues for\nmutual enhancement, raising questions about open-ended evolution and\npotentially utilizing non-Gaussian or discrete diffusion models in the context\nof Diffusion Evolution.\n","authors":["Yanbo Zhang","Benedikt Hartl","Hananel Hazan","Michael Levin"],"pdf_url":"https://arxiv.org/pdf/2410.02543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02541v1","updated":"2024-10-03T14:45:23Z","published":"2024-10-03T14:45:23Z","title":"Fair Decentralized Learning","summary":"  Decentralized learning (DL) is an emerging approach that enables nodes to\ncollaboratively train a machine learning model without sharing raw data. In\nmany application domains, such as healthcare, this approach faces challenges\ndue to the high level of heterogeneity in the training data's feature space.\nSuch feature heterogeneity lowers model utility and negatively impacts\nfairness, particularly for nodes with under-represented training data. In this\npaper, we introduce \\textsc{Facade}, a clustering-based DL algorithm\nspecifically designed for fair model training when the training data exhibits\nseveral distinct features. The challenge of \\textsc{Facade} is to assign nodes\nto clusters, one for each feature, based on the similarity in the features of\ntheir local data, without requiring individual nodes to know apriori which\ncluster they belong to. \\textsc{Facade} (1) dynamically assigns nodes to their\nappropriate clusters over time, and (2) enables nodes to collaboratively train\na specialized model for each cluster in a fully decentralized manner. We\ntheoretically prove the convergence of \\textsc{Facade}, implement our\nalgorithm, and compare it against three state-of-the-art baselines. Our\nexperimental results on three datasets demonstrate the superiority of our\napproach in terms of model accuracy and fairness compared to all three\ncompetitors. Compared to the best-performing baseline, \\textsc{Facade} on the\nCIFAR-10 dataset also reduces communication costs by 32.3\\% to reach a target\naccuracy when cluster sizes are imbalanced.\n","authors":["Sayan Biswas","Anne-Marie Kermarrec","Rishi Sharma","Thibaud Trinca","Martijn de Vos"],"pdf_url":"https://arxiv.org/pdf/2410.02541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04057v3","updated":"2024-10-03T14:40:12Z","published":"2024-08-07T19:39:37Z","title":"PowerPM: Foundation Model for Power Systems","summary":"  The emergence of abundant electricity time series (ETS) data provides ample\nopportunities for various applications in the power systems, including\ndemand-side management, grid stability, and consumer behavior analysis. Deep\nlearning models have advanced ETS modeling by effectively capturing sequence\ndependence. Nevertheless, learning a generic representation of ETS data for\nvarious applications remains challenging due to the inherently complex\nhierarchical structure of ETS data. Moreover, ETS data exhibits intricate\ntemporal dependencies and is suscepti ble to the influence of exogenous\nvariables. Furthermore, different instances exhibit diverse electricity\nconsumption behavior. In this paper, we propose a foundation model PowerPM to\nmodel ETS data, providing a large-scale, off-the-shelf model for power systems.\nPowerPM consists of a temporal encoder and a hierarchical encoder. The temporal\nencoder captures both temporal dependencies in ETS data, considering exogenous\nvariables. The hierarchical encoder models the correlation between hierarchy.\nFurthermore, PowerPM leverages a novel self-supervised pretraining framework\nconsisting of masked ETS modeling and dual-view contrastive learning, which\nenable PowerPM to capture temporal dependency within ETS windows and aware the\ndiscrepancy across ETS windows, providing two different perspectives to learn\ngeneric representation. Our experiments involve five real world scenario\ndatasets, comprising private and public data. Through pre-training on massive\nETS data, PowerPM achieves SOTA performance on diverse downstream tasks within\nthe private dataset. Impressively, when transferred to the public datasets,\nPowerPM maintains its superiority, showcasing its remarkable generalization\nability across various tasks and domains. Moreover, ablation studies, few-shot\nexperiments provide additional evidence of the effectiveness of our model.\n","authors":["Shihao Tu","Yupeng Zhang","Jing Zhang","Zhendong Fu","Yin Zhang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04057v3.pdf","comment":"23 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.07961v4","updated":"2024-10-03T14:35:40Z","published":"2024-01-15T20:57:50Z","title":"Solution of the Probabilistic Lambert Problem: Connections with Optimal\n  Mass Transport, Schrdinger Bridge and Reaction-Diffusion PDEs","summary":"  The Lambert problem originated in orbital mechanics. It concerns with\ndetermining the initial velocity for a boundary value problem involving the\ndynamical constraint due to gravitational potential with additional time\nhorizon and endpoint position constraints. Its solution has application in\ntransferring a spacecraft from a given initial to a given terminal position\nwithin prescribed flight time via velocity control. We consider a probabilistic\nvariant of the Lambert problem where the knowledge of the endpoint constraints\nin position vectors are replaced by the knowledge of their respective joint\nprobability density functions. We show that the Lambert problem with endpoint\njoint probability density constraints is a generalized optimal mass transport\n(OMT) problem, thereby connecting this classical astrodynamics problem with a\nburgeoning area of research in modern stochastic control and stochastic machine\nlearning. This newfound connection allows us to rigorously establish the\nexistence and uniqueness of solution for the probabilistic Lambert problem. The\nsame connection also helps to numerically solve the probabilistic Lambert\nproblem via diffusion regularization, i.e., by leveraging further connection of\nthe OMT with the Schr\\\"odinger bridge problem (SBP). This also shows that the\nprobabilistic Lambert problem with additive dynamic process noise is a\ngeneralized SBP, and can be solved numerically using the so-called\nSchr\\\"odinger factors, as we do in this work. Our analysis leads to solving a\nsystem of reaction-diffusion PDEs where the gravitational potential appears as\nthe reaction rate.\n","authors":["Alexis M. H. Teter","Iman Nodozi","Abhishek Halder"],"pdf_url":"https://arxiv.org/pdf/2401.07961v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02519v1","updated":"2024-10-03T14:28:05Z","published":"2024-10-03T14:28:05Z","title":"Semantic-Guided RL for Interpretable Feature Engineering","summary":"  The quality of Machine Learning (ML) models strongly depends on the input\ndata, as such generating high-quality features is often required to improve the\npredictive accuracy. This process is referred to as Feature Engineering (FE).\nHowever, since manual feature engineering is time-consuming and requires\ncase-by-case domain knowledge, Automated Feature Engineering (AutoFE) is\ncrucial. A major challenge that remains is to generate interpretable features.\nTo tackle this problem, we introduce SMART, a hybrid approach that uses\nsemantic technologies to guide the generation of interpretable features through\na two-step process: Exploitation and Exploration. The former uses Description\nLogics (DL) to reason on the semantics embedded in Knowledge Graphs (KG) to\ninfer domain-specific features, while the latter exploits the knowledge graph\nto conduct a guided exploration of the search space through Deep Reinforcement\nLearning (DRL). Our experiments on public datasets demonstrate that SMART\nsignificantly improves prediction accuracy while ensuring a high level of\ninterpretability.\n","authors":["Mohamed Bouadi","Arta Alavi","Salima Benbernou","Mourad Ouziri"],"pdf_url":"https://arxiv.org/pdf/2410.02519v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.00544"},{"id":"http://arxiv.org/abs/2405.02954v3","updated":"2024-10-03T14:25:07Z","published":"2024-05-05T14:48:13Z","title":"Source-Free Domain Adaptation Guided by Vision and Vision-Language\n  Pre-Training","summary":"  Source-free domain adaptation (SFDA) aims to adapt a source model trained on\na fully-labeled source domain to a related but unlabeled target domain. While\nthe source model is a key avenue for acquiring target pseudolabels, the\ngenerated pseudolabels may exhibit source bias. In the conventional SFDA\npipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to\ninitialize the source model at the start of source training, and subsequently\ndiscarded. Despite having diverse features important for generalization, the\npre-trained feature extractor can overfit to the source data distribution\nduring source training and forget relevant target domain knowledge. Rather than\ndiscarding this valuable knowledge, we introduce an integrated framework to\nincorporate pre-trained networks into the target adaptation process. The\nproposed framework is flexible and allows us to plug modern pre-trained\nnetworks into the adaptation process to leverage their stronger representation\nlearning capabilities. For adaptation, we propose the Co-learn algorithm to\nimprove target pseudolabel quality collaboratively through the source model and\na pre-trained feature extractor. Building on the recent success of the\nvision-language model CLIP in zero-shot image recognition, we present an\nextension Co-learn++ to further incorporate CLIP's zero-shot classification\ndecisions. We evaluate on 4 benchmark datasets and include more challenging\nscenarios such as open-set, partial-set and open-partial SFDA. Experimental\nresults demonstrate that our proposed strategy improves adaptation performance\nand can be successfully integrated with existing SFDA methods. Project code is\navailable at https://github.com/zwenyu/colearn-plus.\n","authors":["Wenyu Zhang","Li Shen","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2405.02954v3.pdf","comment":"Extension of ICCV paper arXiv:2212.07585; Published at IJCV"},{"id":"http://arxiv.org/abs/2410.02516v1","updated":"2024-10-03T14:25:02Z","published":"2024-10-03T14:25:02Z","title":"Learning Emergence of Interaction Patterns across Independent RL Agents\n  in Multi-Agent Environments","summary":"  Many real-world problems, such as controlling swarms of drones and urban\ntraffic, naturally lend themselves to modeling as multi-agent reinforcement\nlearning (RL) problems. However, existing multi-agent RL methods often suffer\nfrom scalability challenges, primarily due to the introduction of communication\namong agents. Consequently, a key challenge lies in adapting the success of\ndeep learning in single-agent RL to the multi-agent setting. In response to\nthis challenge, we propose an approach that fundamentally reimagines\nmulti-agent environments. Unlike conventional methods that model each agent\nindividually with separate networks, our approach, the Bottom Up Network (BUN),\nadopts a unique perspective. BUN treats the collective of multi-agents as a\nunified entity while employing a specialized weight initialization strategy\nthat promotes independent learning. Furthermore, we dynamically establish\nconnections among agents using gradient information, enabling coordination when\nnecessary while maintaining these connections as limited and sparse to\neffectively manage the computational budget. Our extensive empirical\nevaluations across a variety of cooperative multi-agent scenarios, including\ntasks such as cooperative navigation and traffic control, consistently\ndemonstrate BUN's superiority over baseline methods with substantially reduced\ncomputational costs.\n","authors":["Vasanth Reddy Baddam","Suat Gumussoy","Almuatazbellah Boker","Hoda Eldardiry"],"pdf_url":"https://arxiv.org/pdf/2410.02516v1.pdf","comment":"13 pages, 24 figures"},{"id":"http://arxiv.org/abs/2410.02513v1","updated":"2024-10-03T14:22:55Z","published":"2024-10-03T14:22:55Z","title":"Minimax Group Fairness in Strategic Classification","summary":"  In strategic classification, agents manipulate their features, at a cost, to\nreceive a positive classification outcome from the learner's classifier. The\ngoal of the learner in such settings is to learn a classifier that is robust to\nstrategic manipulations. While the majority of works in this domain consider\naccuracy as the primary objective of the learner, in this work, we consider\nlearning objectives that have group fairness guarantees in addition to accuracy\nguarantees. We work with the minimax group fairness notion that asks for\nminimizing the maximal group error rate across population groups.\n  We formalize a fairness-aware Stackelberg game between a population of agents\nconsisting of several groups, with each group having its own cost function, and\na learner in the agnostic PAC setting in which the learner is working with a\nhypothesis class H. When the cost functions of the agents are separable, we\nshow the existence of an efficient algorithm that finds an approximately\noptimal deterministic classifier for the learner when the number of groups is\nsmall. This algorithm remains efficient, both statistically and\ncomputationally, even when H is the set of all classifiers. We then consider\ncost functions that are not necessarily separable and show the existence of\noracle-efficient algorithms that find approximately optimal randomized\nclassifiers for the learner when H has finite strategic VC dimension. These\nalgorithms work under the assumption that the learner is fully transparent: the\nlearner draws a classifier from its distribution (randomized classifier) before\nthe agents respond by manipulating their feature vectors. We highlight the\neffectiveness of such transparency in developing oracle-efficient algorithms.\nWe conclude with verifying the efficacy of our algorithms on real data by\nconducting an experimental analysis.\n","authors":["Emily Diana","Saeed Sharifi-Malvajerdi","Ali Vakilian"],"pdf_url":"https://arxiv.org/pdf/2410.02513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16791v2","updated":"2024-10-03T14:22:02Z","published":"2024-09-25T10:09:47Z","title":"Symbolic State Partitioning for Reinforcement Learning","summary":"  Tabular reinforcement learning methods cannot operate directly on continuous\nstate spaces. One solution for this problem is to partition the state space. A\ngood partitioning enables generalization during learning and more efficient\nexploitation of prior experiences. Consequently, the learning process becomes\nfaster and produces more reliable policies. However, partitioning introduces\napproximation, which is particularly harmful in the presence of nonlinear\nrelations between state components. An ideal partition should be as coarse as\npossible, while capturing the key structure of the state space for the given\nproblem. This work extracts partitions from the environment dynamics by\nsymbolic execution. We show that symbolic partitioning improves state space\ncoverage with respect to environmental behavior and allows reinforcement\nlearning to perform better for sparse rewards. We evaluate symbolic state space\npartitioning with respect to precision, scalability, learning agent performance\nand state space coverage for the learnt policies.\n","authors":["Mohsen Ghaffari","Mahsa Varshosaz","Einar Broch Johnsen","Andrzej Wsowski"],"pdf_url":"https://arxiv.org/pdf/2409.16791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02512v1","updated":"2024-10-03T14:21:49Z","published":"2024-10-03T14:21:49Z","title":"SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation","summary":"  Data augmentation, a cornerstone technique in deep learning, is crucial in\nenhancing model performance, especially with scarce labeled data. While\ntraditional techniques are effective, their reliance on hand-crafted methods\nlimits their applicability across diverse data types and tasks. Although modern\nlearnable augmentation methods offer increased adaptability, they are\ncomputationally expensive and challenging to incorporate within prevalent\naugmentation workflows. In this work, we present a novel, efficient method for\ndata augmentation, effectively bridging the gap between existing augmentation\nstrategies and emerging datasets and learning tasks. We introduce SAFLEX\n(Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns the\nsample weights and soft labels of augmented samples provided by any given\nupstream augmentation pipeline, using a specifically designed efficient bilevel\noptimization algorithm. Remarkably, SAFLEX effectively reduces the noise and\nlabel errors of the upstream augmentation pipeline with a marginal\ncomputational cost. As a versatile module, SAFLEX excels across diverse\ndatasets, including natural and medical images and tabular data, showcasing its\nprowess in few-shot learning and out-of-distribution generalization. SAFLEX\nseamlessly integrates with common augmentation strategies like RandAug, CutMix,\nand those from large pre-trained generative models like stable diffusion and is\nalso compatible with frameworks such as CLIP's fine-tuning. Our findings\nhighlight the potential to adapt existing augmentation pipelines for new data\ntypes and tasks, signaling a move towards more adaptable and resilient training\nframeworks.\n","authors":["Mucong Ding","Bang An","Yuancheng Xu","Anirudh Satheesh","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02512v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2408.03350v2","updated":"2024-10-03T14:20:40Z","published":"2024-08-05T20:19:18Z","title":"miniCTX: Neural Theorem Proving with (Long-)Contexts","summary":"  Real-world formal theorem proving often depends on a wealth of context,\nincluding definitions, lemmas, comments, file structure, and other information.\nWe introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new context that is not seen during\ntraining. miniCTX contains theorems sourced from real Lean projects and\ntextbooks, each associated with a context that can span tens of thousands of\ntokens. Models are tasked with proving a theorem given access to code from the\ntheorem's repository, which contains context that is needed for the proof. As a\nbaseline for miniCTX, we tested fine-tuning and prompting methods that\ncondition theorem proving on preceding context. Both approaches substantially\noutperform traditional methods that rely solely on state information. We found\nthat this ability to use context is not captured by previous benchmarks such as\nminiF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting\nand annotating theorem proving data, making it easy to add new projects into\nminiCTX to ensure that contexts are not seen during training. miniCTX offers a\nchallenging and realistic evaluation of neural theorem provers.\n","authors":["Jiewen Hu","Thomas Zhu","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2408.03350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14488v2","updated":"2024-10-03T14:16:47Z","published":"2024-03-21T15:36:26Z","title":"A Causal Bayesian Network and Probabilistic Programming Based Reasoning\n  Framework for Robot Manipulation Under Uncertainty","summary":"  Robot object manipulation in real-world environments is challenging because\nrobot operation must be robust to a range of sensing, estimation, and actuation\nuncertainties to avoid potentially unsafe and costly mistakes that are a\nbarrier to their adoption. In this paper, we propose a flexible and\ngeneralisable physics-informed causal Bayesian network (CBN) based framework\nfor a robot to probabilistically reason about candidate manipulation actions,\nto enable robot decision-making robust to arbitrary robot system uncertainties\n-- the first of its kind to use a probabilistic programming language\nimplementation. Using experiments in high-fidelity Gazebo simulation of an\nexemplar block stacking task, we demonstrate our framework's ability to: (1)\npredict manipulation outcomes with high accuracy (Pred Acc: 88.6%); and, (2)\nperform greedy next-best action selection with 94.2% task success rate. We also\ndemonstrate our framework's suitability for real-world robot systems with a\ndomestic robot. Thus, we show that by combining probabilistic causal modelling\nwith physics simulations, we can make robot manipulation more robust to system\nuncertainties and hence more feasible for real-world applications. Further, our\ngeneralised reasoning framework can be used and extended for future robotics\nand causality research.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v2.pdf","comment":"7 pages, 7 figures, submitted to the 2025 IEEE Conference on Robotics\n  and Automation (ICRA 2025)"},{"id":"http://arxiv.org/abs/2410.02506v1","updated":"2024-10-03T14:14:31Z","published":"2024-10-03T14:14:31Z","title":"Cut the Crap: An Economical Communication Pipeline for LLM-based\n  Multi-Agent Systems","summary":"  Recent advancements in large language model (LLM)-powered agents have shown\nthat collective intelligence can significantly outperform individual\ncapabilities, largely attributed to the meticulously designed inter-agent\ncommunication topologies. Though impressive in performance, existing\nmulti-agent pipelines inherently introduce substantial token overhead, as well\nas increased economic costs, which pose challenges for their large-scale\ndeployments. In response to this challenge, we propose an economical, simple,\nand robust multi-agent communication framework, termed $\\texttt{AgentPrune}$,\nwhich can seamlessly integrate into mainstream multi-agent systems and prunes\nredundant or even malicious communication messages. Technically,\n$\\texttt{AgentPrune}$ is the first to identify and formally define the\n\\textit{communication redundancy} issue present in current LLM-based\nmulti-agent pipelines, and efficiently performs one-shot pruning on the\nspatial-temporal message-passing graph, yielding a token-economic and\nhigh-performing communication topology. Extensive experiments across six\nbenchmarks demonstrate that $\\texttt{AgentPrune}$ \\textbf{(I)} achieves\ncomparable results as state-of-the-art topologies at merely $\\$5.6$ cost\ncompared to their $\\$43.7$, \\textbf{(II)} integrates seamlessly into existing\nmulti-agent frameworks with $28.1\\%\\sim72.8\\%\\downarrow$ token reduction, and\n\\textbf{(III)} successfully defend against two types of agent-based adversarial\nattacks with $3.5\\%\\sim10.8\\%\\uparrow$ performance boost.\n","authors":["Guibin Zhang","Yanwei Yue","Zhixun Li","Sukwon Yun","Guancheng Wan","Kun Wang","Dawei Cheng","Jeffrey Xu Yu","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02504v1","updated":"2024-10-03T14:09:58Z","published":"2024-10-03T14:09:58Z","title":"Dual Active Learning for Reinforcement Learning from Human Feedback","summary":"  Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts.\n","authors":["Pangpang Liu","Chengchun Shi","Will Wei Sun"],"pdf_url":"https://arxiv.org/pdf/2410.02504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04557v2","updated":"2024-10-03T14:08:12Z","published":"2023-09-08T19:17:03Z","title":"Regret-Optimal Federated Transfer Learning for Kernel Regression with\n  Applications in American Option Pricing","summary":"  We propose an optimal iterative scheme for federated transfer learning, where\na central planner has access to datasets ${\\cal D}_1,\\dots,{\\cal D}_N$ for the\nsame learning model $f_{\\theta}$. Our objective is to minimize the cumulative\ndeviation of the generated parameters $\\{\\theta_i(t)\\}_{t=0}^T$ across all $T$\niterations from the specialized parameters\n$\\theta^\\star_{1},\\ldots,\\theta^\\star_N$ obtained for each dataset, while\nrespecting the loss function for the model $f_{\\theta(T)}$ produced by the\nalgorithm upon halting. We only allow for continual communication between each\nof the specialized models (nodes/agents) and the central planner (server), at\neach iteration (round). For the case where the model $f_{\\theta}$ is a\nfinite-rank kernel regression, we derive explicit updates for the\nregret-optimal algorithm. By leveraging symmetries within the regret-optimal\nalgorithm, we further develop a nearly regret-optimal heuristic that runs with\n$\\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of\nthe parameter space. Additionally, we investigate the adversarial robustness of\nthe regret-optimal algorithm showing that an adversary which perturbs $q$\ntraining pairs by at-most $\\varepsilon>0$, across all training sets, cannot\nreduce the regret-optimal algorithm's regret by more than\n$\\mathcal{O}(\\varepsilon q \\bar{N}^{1/2})$, where $\\bar{N}$ is the aggregate\nnumber of training pairs. To validate our theoretical findings, we conduct\nnumerical experiments in the context of American option pricing, utilizing a\nrandomly generated finite-rank kernel.\n","authors":["Xuwei Yang","Anastasis Kratsios","Florian Krach","Matheus Grasselli","Aurelien Lucchi"],"pdf_url":"https://arxiv.org/pdf/2309.04557v2.pdf","comment":"51 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.02498v1","updated":"2024-10-03T14:00:44Z","published":"2024-10-03T14:00:44Z","title":"Dynamic Gradient Alignment for Online Data Mixing","summary":"  The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.\n","authors":["Simin Fan","David Grangier","Pierre Ablin"],"pdf_url":"https://arxiv.org/pdf/2410.02498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02496v1","updated":"2024-10-03T13:59:38Z","published":"2024-10-03T13:59:38Z","title":"Efficient learning of differential network in multi-source\n  non-paranormal graphical models","summary":"  This paper addresses learning of sparse structural changes or differential\nnetwork between two classes of non-paranormal graphical models. We assume a\nmulti-source and heterogeneous dataset is available for each class, where the\ncovariance matrices are identical for all non-paranormal graphical models. The\ndifferential network, which are encoded by the difference precision matrix, can\nthen be decoded by optimizing a lasso penalized D-trace loss function. To this\naim, an efficient approach is proposed that outputs the exact solution path,\noutperforming the previous methods that only sample from the solution path in\npre-selected regularization parameters. Notably, our proposed method has low\ncomputational complexity, especially when the differential network are sparse.\nOur simulations on synthetic data demonstrate a superior performance for our\nstrategy in terms of speed and accuracy compared to an existing method.\nMoreover, our strategy in combining datasets from multiple sources is shown to\nbe very effective in inferring differential network in real-world problems.\nThis is backed by our experimental results on drug resistance in tumor cancers.\nIn the latter case, our strategy outputs important genes for drug resistance\nwhich are already confirmed by various independent studies.\n","authors":["Mojtaba Nikahd","Seyed Abolfazl Motahari"],"pdf_url":"https://arxiv.org/pdf/2410.02496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02490v1","updated":"2024-10-03T13:55:49Z","published":"2024-10-03T13:55:49Z","title":"Stochastic variance-reduced Gaussian variational inference on the\n  Bures-Wasserstein manifold","summary":"  Optimization in the Bures-Wasserstein space has been gaining popularity in\nthe machine learning community since it draws connections between variational\ninference and Wasserstein gradient flows. The variational inference objective\nfunction of Kullback-Leibler divergence can be written as the sum of the\nnegative entropy and the potential energy, making forward-backward Euler the\nmethod of choice. Notably, the backward step admits a closed-form solution in\nthis case, facilitating the practicality of the scheme. However, the forward\nstep is no longer exact since the Bures-Wasserstein gradient of the potential\nenergy involves \"intractable\" expectations. Recent approaches propose using the\nMonte Carlo method -- in practice a single-sample estimator -- to approximate\nthese terms, resulting in high variance and poor performance. We propose a\nnovel variance-reduced estimator based on the principle of control variates. We\ntheoretically show that this estimator has a smaller variance than the\nMonte-Carlo estimator in scenarios of interest. We also prove that variance\nreduction helps improve the optimization bounds of the current analysis. We\ndemonstrate that the proposed estimator gains order-of-magnitude improvements\nover the previous Bures-Wasserstein methods.\n","authors":["Hoang Phuc Hau Luu","Hanlin Yu","Bernardo Williams","Marcelo Hartmann","Arto Klami"],"pdf_url":"https://arxiv.org/pdf/2410.02490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07103v3","updated":"2024-10-03T13:55:08Z","published":"2024-04-10T15:41:53Z","title":"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs","summary":"  Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.\n","authors":["Bowen Jin","Chulin Xie","Jiawei Zhang","Kashob Kumar Roy","Yu Zhang","Zheng Li","Ruirui Li","Xianfeng Tang","Suhang Wang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2404.07103v3.pdf","comment":"21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT"},{"id":"http://arxiv.org/abs/2407.04069v2","updated":"2024-10-03T13:51:53Z","published":"2024-07-04T17:15:37Z","title":"A Systematic Survey and Critical Review on Evaluating Large Language\n  Models: Challenges, Limitations, and Recommendations","summary":"  Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust.\n","authors":["Md Tahmid Rahman Laskar","Sawsan Alqahtani","M Saiful Bari","Mizanur Rahman","Mohammad Abdullah Matin Khan","Haidar Khan","Israt Jahan","Amran Bhuiyan","Chee Wei Tan","Md Rizwan Parvez","Enamul Hoque","Shafiq Joty","Jimmy Huang"],"pdf_url":"https://arxiv.org/pdf/2407.04069v2.pdf","comment":"Accepted at EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2409.20195v2","updated":"2024-10-03T13:50:29Z","published":"2024-09-30T11:11:35Z","title":"Forecasting Disease Progression with Parallel Hyperplanes in\n  Longitudinal Retinal OCT","summary":"  Predicting future disease progression risk from medical images is challenging\ndue to patient heterogeneity, and subtle or unknown imaging biomarkers.\nMoreover, deep learning (DL) methods for survival analysis are susceptible to\nimage domain shifts across scanners. We tackle these issues in the task of\npredicting late dry Age-related Macular Degeneration (dAMD) onset from retinal\nOCT scans. We propose a novel DL method for survival prediction to jointly\npredict from the current scan a risk score, inversely related to\ntime-to-conversion, and the probability of conversion within a time interval\n$t$. It uses a family of parallel hyperplanes generated by parameterizing the\nbias term as a function of $t$. In addition, we develop unsupervised losses\nbased on intra-subject image pairs to ensure that risk scores increase over\ntime and that future conversion predictions are consistent with AMD stage\nprediction using actual scans of future visits. Such losses enable\ndata-efficient fine-tuning of the trained model on new unlabeled datasets\nacquired with a different scanner. Extensive evaluation on two large datasets\nacquired with different scanners resulted in a mean AUROCs of 0.82 for\nDataset-1 and 0.83 for Dataset-2, across prediction intervals of 6,12 and 24\nmonths.\n","authors":["Arunava Chakravarty","Taha Emre","Dmitrii Lachinov","Antoine Rivail","Hendrik Scholl","Lars Fritsche","Sobha Sivaprasad","Daniel Rueckert","Andrew Lotery","Ursula Schmidt-Erfurth","Hrvoje Bogunovi"],"pdf_url":"https://arxiv.org/pdf/2409.20195v2.pdf","comment":"accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.02486v1","updated":"2024-10-03T13:48:35Z","published":"2024-10-03T13:48:35Z","title":"Encryption-Friendly LLM Architecture","summary":"  Large language models (LLMs) offer personalized responses based on user\ninteractions, but this use case raises serious privacy concerns. Homomorphic\nencryption (HE) is a cryptographic protocol supporting arithmetic computations\nin encrypted states and provides a potential solution for privacy-preserving\nmachine learning (PPML). However, the computational intensity of transformers\nposes challenges for applying HE to LLMs. In this work, we propose a modified\nHE-friendly transformer architecture with an emphasis on inference following\npersonalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian\nkernels, we achieve significant computational speedups -- 6.94x for fine-tuning\nand 2.3x for inference -- while maintaining performance comparable to plaintext\nmodels. Our findings provide a viable proof of concept for offering\nprivacy-preserving LLM services in areas where data protection is crucial.\n","authors":["Donghwan Rho","Taeseong Kim","Minje Park","Jung Woo Kim","Hyunsik Chae","Jung Hee Cheon","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2410.02486v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2312.02783v3","updated":"2024-10-03T13:47:02Z","published":"2023-12-05T14:14:27Z","title":"Large Language Models on Graphs: A Comprehensive Survey","summary":"  Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.\n","authors":["Bowen Jin","Gang Liu","Chi Han","Meng Jiang","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2312.02783v3.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2410.02479v1","updated":"2024-10-03T13:36:02Z","published":"2024-10-03T13:36:02Z","title":"Cross-Embodiment Dexterous Grasping with Reinforcement Learning","summary":"  Dexterous hands exhibit significant potential for complex real-world grasping\ntasks. While recent studies have primarily focused on learning policies for\nspecific robotic hands, the development of a universal policy that controls\ndiverse dexterous hands remains largely unexplored. In this work, we study the\nlearning of cross-embodiment dexterous grasping policies using reinforcement\nlearning (RL). Inspired by the capability of human hands to control various\ndexterous hands through teleoperation, we propose a universal action space\nbased on the human hand's eigengrasps. The policy outputs eigengrasp actions\nthat are then converted into specific joint actions for each robot hand through\na retargeting mapping. We simplify the robot hand's proprioception to include\nonly the positions of fingertips and the palm, offering a unified observation\nspace across different robot hands. Our approach demonstrates an 80% success\nrate in grasping objects from the YCB dataset across four distinct embodiments\nusing a single vision-based policy. Additionally, our policy exhibits zero-shot\ngeneralization to two previously unseen embodiments and significant improvement\nin efficient finetuning. For further details and videos, visit our project page\nhttps://sites.google.com/view/crossdex.\n","authors":["Haoqi Yuan","Bohan Zhou","Yuhui Fu","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02478v1","updated":"2024-10-03T13:35:28Z","published":"2024-10-03T13:35:28Z","title":"Temporal Predictive Coding for Gradient Compression in Distributed\n  Learning","summary":"  This paper proposes a prediction-based gradient compression method for\ndistributed learning with event-triggered communication. Our goal is to reduce\nthe amount of information transmitted from the distributed agents to the\nparameter server by exploiting temporal correlation in the local gradients. We\nuse a linear predictor that \\textit{combines past gradients to form a\nprediction of the current gradient}, with coefficients that are optimized by\nsolving a least-square problem. In each iteration, every agent transmits the\npredictor coefficients to the server such that the predicted local gradient can\nbe computed. The difference between the true local gradient and the predicted\none, termed the \\textit{prediction residual, is only transmitted when its norm\nis above some threshold.} When this additional communication step is omitted,\nthe server uses the prediction as the estimated gradient. This proposed design\nshows notable performance gains compared to existing methods in the literature,\nachieving convergence with reduced communication costs.\n","authors":["Adrian Edin","Zheng Chen","Michel Kieffer","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2410.02478v1.pdf","comment":"8 pages, 3 figures, presented at the 60th Allerton conference on\n  Communication, Control, and Computing"},{"id":"http://arxiv.org/abs/2410.02477v1","updated":"2024-10-03T13:35:15Z","published":"2024-10-03T13:35:15Z","title":"Learning Diverse Bimanual Dexterous Manipulation Skills from Human\n  Demonstrations","summary":"  Bimanual dexterous manipulation is a critical yet underexplored area in\nrobotics. Its high-dimensional action space and inherent task complexity\npresent significant challenges for policy learning, and the limited task\ndiversity in existing benchmarks hinders general-purpose skill development.\nExisting approaches largely depend on reinforcement learning, often constrained\nby intricately designed reward functions tailored to a narrow set of tasks. In\nthis work, we present a novel approach for efficiently learning diverse\nbimanual dexterous skills from abundant human demonstrations. Specifically, we\nintroduce BiDexHD, a framework that unifies task construction from existing\nbimanual datasets and employs teacher-student policy learning to address all\ntasks. The teacher learns state-based policies using a general two-stage reward\nfunction across tasks with shared behaviors, while the student distills the\nlearned multi-task policies into a vision-based policy. With BiDexHD, scalable\nlearning of numerous bimanual dexterous skills from auto-constructed tasks\nbecomes feasible, offering promising advances toward universal bimanual\ndexterous manipulation. Our empirical evaluation on the TACO dataset, spanning\n141 tasks across six categories, demonstrates a task fulfillment rate of 74.59%\non trained tasks and 51.07% on unseen tasks, showcasing the effectiveness and\ncompetitive zero-shot generalization capabilities of BiDexHD. For videos and\nmore information, visit our project page https://sites.google.com/view/bidexhd.\n","authors":["Bohan Zhou","Haoqi Yuan","Yuhui Fu","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02476v1","updated":"2024-10-03T13:35:08Z","published":"2024-10-03T13:35:08Z","title":"Online Convex Optimization with a Separation Oracle","summary":"  In this paper, we introduce a new projection-free algorithm for Online Convex\nOptimization (OCO) with a state-of-the-art regret guarantee among\nseparation-based algorithms. Existing projection-free methods based on the\nclassical Frank-Wolfe algorithm achieve a suboptimal regret bound of\n$O(T^{3/4})$, while more recent separation-based approaches guarantee a regret\nbound of $O(\\kappa \\sqrt{T})$, where $\\kappa$ denotes the asphericity of the\nfeasible set, defined as the ratio of the radii of the containing and contained\nballs. However, for ill-conditioned sets, $\\kappa$ can be arbitrarily large,\npotentially leading to poor performance. Our algorithm achieves a regret bound\nof $\\tilde{O}(\\sqrt{dT} + \\kappa d)$, while requiring only $\\tilde{O}(1)$ calls\nto a separation oracle per round. Crucially, the main term in the bound,\n$\\tilde{O}(\\sqrt{d T})$, is independent of $\\kappa$, addressing the limitations\nof previous methods. Additionally, as a by-product of our analysis, we recover\nthe $O(\\kappa \\sqrt{T})$ regret bound of existing OCO algorithms with a more\nstraightforward analysis and improve the regret bound for projection-free\nonline exp-concave optimization. Finally, for constrained stochastic convex\noptimization, we achieve a state-of-the-art convergence rate of\n$\\tilde{O}(\\sigma/\\sqrt{T} + \\kappa d/T)$, where $\\sigma$ represents the noise\nin the stochastic gradients, while requiring only $\\tilde{O}(1)$ calls to a\nseparation oracle per iteration.\n","authors":["Zakaria Mhammedi"],"pdf_url":"https://arxiv.org/pdf/2410.02476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02475v1","updated":"2024-10-03T13:33:02Z","published":"2024-10-03T13:33:02Z","title":"Efficient Residual Learning with Mixture-of-Experts for Universal\n  Dexterous Grasping","summary":"  Universal dexterous grasping across diverse objects presents a fundamental\nyet formidable challenge in robot learning. Existing approaches using\nreinforcement learning (RL) to develop policies on extensive object datasets\nface critical limitations, including complex curriculum design for multi-task\nlearning and limited generalization to unseen objects. To overcome these\nchallenges, we introduce ResDex, a novel approach that integrates residual\npolicy learning with a mixture-of-experts (MoE) framework. ResDex is\ndistinguished by its use of geometry-unaware base policies that are efficiently\nacquired on individual objects and capable of generalizing across a wide range\nof unseen objects. Our MoE framework incorporates several base policies to\nfacilitate diverse grasping styles suitable for various objects. By learning\nresidual actions alongside weights that combine these base policies, ResDex\nenables efficient multi-task RL for universal dexterous grasping. ResDex\nachieves state-of-the-art performance on the DexGraspNet dataset comprising\n3,200 objects with an 88.8% success rate. It exhibits no generalization gap\nwith unseen objects and demonstrates superior training efficiency, mastering\nall tasks within only 12 hours on a single GPU.\n","authors":["Ziye Huang","Haoqi Yuan","Yuhui Fu","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02472v1","updated":"2024-10-03T13:25:15Z","published":"2024-10-03T13:25:15Z","title":"Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language","summary":"  As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area.\n","authors":["Anthony Costarelli","Mat Allen","Severin Field","Joshua Clymer"],"pdf_url":"https://arxiv.org/pdf/2410.02472v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.02467v1","updated":"2024-10-03T13:17:06Z","published":"2024-10-03T13:17:06Z","title":"Towards a Theoretical Understanding of Memorization in Diffusion Models","summary":"  As diffusion probabilistic models (DPMs) are being employed as mainstream\nmodels for Generative Artificial Intelligence (GenAI), the study of their\nmemorization of training data has attracted growing attention. Existing works\nin this direction aim to establish an understanding of whether or to what\nextent DPMs learn via memorization. Such an understanding is crucial for\nidentifying potential risks of data leakage and copyright infringement in\ndiffusion models and, more importantly, for trustworthy application of GenAI.\nExisting works revealed that conditional DPMs are more prone to training data\nmemorization than unconditional DPMs, and the motivated data extraction methods\nare mostly for conditional DPMs. However, these understandings are primarily\nempirical, and extracting training data from unconditional models has been\nfound to be extremely challenging. In this work, we provide a theoretical\nunderstanding of memorization in both conditional and unconditional DPMs under\nthe assumption of model convergence. Our theoretical analysis indicates that\nextracting data from unconditional models can also be effective by constructing\na proper surrogate condition. Based on this result, we propose a novel data\nextraction method named \\textbf{Surrogate condItional Data Extraction (SIDE)}\nthat leverages a time-dependent classifier trained on the generated data as a\nsurrogate condition to extract training data from unconditional DPMs. Empirical\nresults demonstrate that our SIDE can extract training data in challenging\nscenarios where previous methods fail, and it is, on average, over 50\\% more\neffective across different scales of the CelebA dataset.\n","authors":["Yunhao Chen","Xingjun Ma","Difan Zou","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.02467v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.12752"},{"id":"http://arxiv.org/abs/2409.07431v2","updated":"2024-10-03T13:07:25Z","published":"2024-09-11T17:21:59Z","title":"Synthetic continued pretraining","summary":"  Pretraining on large-scale, unstructured internet text enables language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient--to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining with EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If, instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.\n","authors":["Zitong Yang","Neil Band","Shuangping Li","Emmanuel Cands","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2409.07431v2.pdf","comment":"Updated organization of experimental results and methods\n  introduction. Released the dataset and model weights artifact"},{"id":"http://arxiv.org/abs/2410.02453v1","updated":"2024-10-03T13:02:07Z","published":"2024-10-03T13:02:07Z","title":"Quantifying User Coherence: A Unified Framework for Cross-Domain\n  Recommendation Analysis","summary":"  The effectiveness of Recommender Systems (RS) is closely tied to the quality\nand distinctiveness of user profiles, yet despite many advancements in raw\nperformance, the sensitivity of RS to user profile quality remains\nunder-researched. This paper introduces novel information-theoretic measures\nfor understanding recommender systems: a \"surprise\" measure quantifying users'\ndeviations from popular choices, and a \"conditional surprise\" measure capturing\nuser interaction coherence. We evaluate 7 recommendation algorithms across 9\ndatasets, revealing the relationships between our measures and standard\nperformance metrics. Using a rigorous statistical framework, our analysis\nquantifies how much user profile density and information measures impact\nalgorithm performance across domains. By segmenting users based on these\nmeasures, we achieve improved performance with reduced data and show that\nsimpler algorithms can match complex ones for low-coherence users.\nAdditionally, we employ our measures to analyze how well different\nrecommendation algorithms maintain the coherence and diversity of user\npreferences in their predictions, providing insights into algorithm behavior.\nThis work advances the theoretical understanding of user behavior and practical\nheuristics for personalized recommendation systems, promoting more efficient\nand adaptive architectures.\n","authors":["Michal Soumm","Alexandre Fournier-Montgieux","Adrian Popescu","Bertrand Delezoide"],"pdf_url":"https://arxiv.org/pdf/2410.02453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02450v1","updated":"2024-10-03T12:52:36Z","published":"2024-10-03T12:52:36Z","title":"Personalized Federated Learning for Generative AI-Assisted Semantic\n  Communications","summary":"  Semantic Communication (SC) focuses on transmitting only the semantic\ninformation rather than the raw data. This approach offers an efficient\nsolution to the issue of spectrum resource utilization caused by the various\nintelligent applications on Mobile Users (MUs). Generative Artificial\nIntelligence (GAI) models have recently exhibited remarkable content generation\nand signal processing capabilities, presenting new opportunities for enhancing\nSC. Therefore, we propose a GAI-assisted SC (GSC) model deployed between MUs\nand the Base Station (BS). Then, to train the GSC model using the local data of\nMUs while ensuring privacy and accommodating heterogeneous requirements of MUs,\nwe introduce Personalized Semantic Federated Learning (PSFL). This approach\nincorporates a novel Personalized Local Distillation (PLD) and Adaptive Global\nPruning (AGP). In PLD, each MU selects a personalized GSC model as a mentor\ntailored to its local resources and a unified Convolutional Neural Networks\n(CNN)-based SC (CSC) model as a student. This mentor model is then distilled\ninto the student model for global aggregation. In AGP, we perform network\npruning on the aggregated global model according to real-time communication\nenvironments, reducing communication energy. Finally, numerical results\ndemonstrate the feasibility and efficiency of the proposed PSFL scheme.\n","authors":["Yubo Peng","Feibo Jiang","Li Dong","Kezhi Wang","Kun Yang"],"pdf_url":"https://arxiv.org/pdf/2410.02450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03086v2","updated":"2024-10-03T12:45:48Z","published":"2024-07-03T13:15:12Z","title":"Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation","summary":"  While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that \\system enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86x compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.\n","authors":["Yujin Shin","Kichang Lee","Sungmin Lee","You Rim Choi","Hyung-Sin Kim","JeongGil Ko"],"pdf_url":"https://arxiv.org/pdf/2407.03086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14260v2","updated":"2024-10-03T12:42:40Z","published":"2024-05-23T07:40:21Z","title":"Graph Sparsification via Mixture of Graphs","summary":"  Graph Neural Networks (GNNs) have demonstrated superior performance across\nvarious graph learning tasks but face significant computational challenges when\napplied to large-scale graphs. One effective approach to mitigate these\nchallenges is graph sparsification, which involves removing non-essential edges\nto reduce computational overhead. However, previous graph sparsification\nmethods often rely on a single global sparsity setting and uniform pruning\ncriteria, failing to provide customized sparsification schemes for each node's\ncomplex local context. In this paper, we introduce Mixture-of-Graphs (MoG),\nleveraging the concept of Mixture-of-Experts (MoE), to dynamically select\ntailored pruning solutions for each node. Specifically, MoG incorporates\nmultiple sparsifier experts, each characterized by unique sparsity levels and\npruning criteria, and selects the appropriate experts for each node.\nSubsequently, MoG performs a mixture of the sparse graphs produced by different\nexperts on the Grassmann manifold to derive an optimal sparse graph. One\nnotable property of MoG is its entirely local nature, as it depends on the\nspecific circumstances of each individual node. Extensive experiments on four\nlarge-scale OGB datasets and two superpixel datasets, equipped with five GNN\nbackbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity\nlevels ($8.67\\%\\sim 50.85\\%$), with performance equal to or better than the\ndense graph, (II) achieves $1.47-2.62\\times$ speedup in GNN inference with\nnegligible performance drop, and (III) boosts ``top-student'' GNN performance\n($1.02\\%\\uparrow$ on RevGNN+\\textsc{ogbn-proteins} and $1.74\\%\\uparrow$ on\nDeeperGCN+\\textsc{ogbg-ppa}).\n","authors":["Guibin Zhang","Xiangguo Sun","Yanwei Yue","Chonghe Jiang","Kun Wang","Tianlong Chen","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2405.14260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02443v1","updated":"2024-10-03T12:40:52Z","published":"2024-10-03T12:40:52Z","title":"Clinnova Federated Learning Proof of Concept: Key Takeaways from a\n  Cross-border Collaboration","summary":"  Clinnova, a collaborative initiative involving France, Germany, Switzerland,\nand Luxembourg, is dedicated to unlocking the power of precision medicine\nthrough data federation, standardization, and interoperability. This European\nGreater Region initiative seeks to create an interoperable European standard\nusing artificial intelligence (AI) and data science to enhance healthcare\noutcomes and efficiency. Key components include multidisciplinary research\ncenters, a federated biobanking strategy, a digital health innovation platform,\nand a federated AI strategy. It targets inflammatory bowel disease, rheumatoid\ndiseases, and multiple sclerosis (MS), emphasizing data quality to develop AI\nalgorithms for personalized treatment and translational research.\n  The IHU Strasbourg (Institute of Minimal-invasive Surgery) has the lead in\nthis initiative to develop the federated learning (FL) proof of concept (POC)\nthat will serve as a foundation for advancing AI in healthcare. At its core,\nClinnova-MS aims to enhance MS patient care by using FL to develop more\naccurate models that detect disease progression, guide interventions, and\nvalidate digital biomarkers across multiple sites. This technical report\npresents insights and key takeaways from the first cross-border federated POC\non MS segmentation of MRI images within the Clinnova framework. While our work\nmarks a significant milestone in advancing MS segmentation through cross-border\ncollaboration, it also underscores the importance of addressing technical,\nlogistical, and ethical considerations to realize the full potential of FL in\nhealthcare settings.\n","authors":["Julia Alekseenko","Bram Stieltjes","Michael Bach","Melanie Boerries","Oliver Opitz","Alexandros Karargyris","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2410.02443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02438v1","updated":"2024-10-03T12:35:17Z","published":"2024-10-03T12:35:17Z","title":"Learning K-U-Net with constant complexity: An Application to time series\n  forecasting","summary":"  Training deep models for time series forecasting is a critical task with an\ninherent challenge of time complexity. While current methods generally ensure\nlinear time complexity, our observations on temporal redundancy show that\nhigh-level features are learned 98.44\\% slower than low-level features. To\naddress this issue, we introduce a new exponentially weighted stochastic\ngradient descent algorithm designed to achieve constant time complexity in deep\nlearning models. We prove that the theoretical complexity of this learning\nmethod is constant. Evaluation of this method on Kernel U-Net (K-U-Net) on\nsynthetic datasets shows a significant reduction in complexity while improving\nthe accuracy of the test set.\n","authors":["Jiang You","Arben Cela","Ren Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2410.02438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02433v1","updated":"2024-10-03T12:28:13Z","published":"2024-10-03T12:28:13Z","title":"Better Call SAUL: Fluent and Consistent Language Model Editing with\n  Generation Regularization","summary":"  To ensure large language models contain up-to-date knowledge, they need to be\nupdated regularly. However, model editing is challenging as it might also\naffect knowledge that is unrelated to the new data. State-of-the-art methods\nidentify parameters associated with specific knowledge and then modify them via\ndirect weight updates. However, these locate-and-edit methods suffer from heavy\ncomputational overhead and lack theoretical validation. In contrast, directly\nfine-tuning the model on requested edits affects the model's behavior on\nunrelated knowledge, and significantly damages the model's generation fluency\nand consistency. To address these challenges, we propose SAUL, a streamlined\nmodel editing method that uses sentence concatenation with augmented random\nfacts for generation regularization. Evaluations on three model editing\nbenchmarks show that SAUL is a practical and reliable solution for model\nediting outperforming state-of-the-art methods while maintaining generation\nquality and reducing computational overhead.\n","authors":["Mingyang Wang","Lukas Lange","Heike Adel","Jannik Strtgen","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2410.02433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02430v1","updated":"2024-10-03T12:25:01Z","published":"2024-10-03T12:25:01Z","title":"Predictive Attractor Models","summary":"  Sequential memory, the ability to form and accurately recall a sequence of\nevents or stimuli in the correct order, is a fundamental prerequisite for\nbiological and artificial intelligence as it underpins numerous cognitive\nfunctions (e.g., language comprehension, planning, episodic memory formation,\netc.) However, existing methods of sequential memory suffer from catastrophic\nforgetting, limited capacity, slow iterative learning procedures, low-order\nMarkov memory, and, most importantly, the inability to represent and generate\nmultiple valid future possibilities stemming from the same context. Inspired by\nbiologically plausible neuroscience theories of cognition, we propose\n\\textit{Predictive Attractor Models (PAM)}, a novel sequence memory\narchitecture with desirable generative properties. PAM is a streaming model\nthat learns a sequence in an online, continuous manner by observing each input\n\\textit{only once}. Additionally, we find that PAM avoids catastrophic\nforgetting by uniquely representing past context through lateral inhibition in\ncortical minicolumns, which prevents new memories from overwriting previously\nlearned knowledge. PAM generates future predictions by sampling from a union\nset of predicted possibilities; this generative ability is realized through an\nattractor model trained alongside the predictor. We show that PAM is trained\nwith local computations through Hebbian plasticity rules in a biologically\nplausible framework. Other desirable traits (e.g., noise tolerance, CPU-based\nlearning, capacity scaling) are discussed throughout the paper. Our findings\nsuggest that PAM represents a significant step forward in the pursuit of\nbiologically plausible and computationally efficient sequential memory models,\nwith broad implications for cognitive science and artificial intelligence\nresearch.\n","authors":["Ramy Mounir","Sudeep Sarkar"],"pdf_url":"https://arxiv.org/pdf/2410.02430v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.02425v1","updated":"2024-10-03T12:19:06Z","published":"2024-10-03T12:19:06Z","title":"LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services","summary":"  As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.\n","authors":["Magorzata azuka","Andreea Anghel","Thomas Parnell"],"pdf_url":"https://arxiv.org/pdf/2410.02425v1.pdf","comment":"Accepted to the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC '24)"},{"id":"http://arxiv.org/abs/2405.03582v2","updated":"2024-10-03T12:18:09Z","published":"2024-05-06T15:53:55Z","title":"Functional Latent Dynamics for Irregularly Sampled Time Series\n  Forecasting","summary":"  Irregularly sampled time series with missing values are often observed in\nmultiple real-world applications such as healthcare, climate and astronomy.\nThey pose a significant challenge to standard deep learning models that operate\nonly on fully observed and regularly sampled time series. In order to capture\nthe continuous dynamics of the irregular time series, many models rely on\nsolving an Ordinary Differential Equation (ODE) in the hidden state. These\nODE-based models tend to perform slow and require large memory due to\nsequential operations and a complex ODE solver. As an alternative to complex\nODE-based models, we propose a family of models called Functional Latent\nDynamics (FLD). Instead of solving the ODE, we use simple curves which exist at\nall time points to specify the continuous latent state in the model. The\ncoefficients of these curves are learned only from the observed values in the\ntime series ignoring the missing values. Through extensive experiments, we\ndemonstrate that FLD achieves better performance compared to the best ODE-based\nmodel while reducing the runtime and memory overhead. Specifically, FLD\nrequires an order of magnitude less time to infer the forecasts compared to the\nbest performing forecasting model.\n","authors":["Christian Kltergens","Vijaya Krishna Yalavarthi","Maximilian Stubbemann","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2405.03582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02423v1","updated":"2024-10-03T12:13:56Z","published":"2024-10-03T12:13:56Z","title":"PnP-Flow: Plug-and-Play Image Restoration with Flow Matching","summary":"  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm\nfor solving imaging inverse problems. PnP methods leverage the strength of\npre-trained denoisers, often deep neural networks, by integrating them in\noptimization schemes. While they achieve state-of-the-art performance on\nvarious inverse problems in imaging, PnP approaches face inherent limitations\non more generative tasks like inpainting. On the other hand, generative models\nsuch as Flow Matching pushed the boundary in image sampling yet lack a clear\nmethod for efficient use in image restoration. We propose to combine the PnP\nframework with Flow Matching (FM) by defining a time-dependent denoiser using a\npre-trained FM model. Our algorithm alternates between gradient descent steps\non the data-fidelity term, reprojections onto the learned FM path, and\ndenoising. Notably, our method is computationally efficient and\nmemory-friendly, as it avoids backpropagation through ODEs and trace\ncomputations. We evaluate its performance on denoising, super-resolution,\ndeblurring, and inpainting tasks, demonstrating superior results compared to\nexisting PnP algorithms and Flow Matching based state-of-the-art methods.\n","authors":["Sgolne Martin","Anne Gagneux","Paul Hagemann","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2410.02423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02417v1","updated":"2024-10-03T12:07:34Z","published":"2024-10-03T12:07:34Z","title":"MenakBERT -- Hebrew Diacriticizer","summary":"  Diacritical marks in the Hebrew language give words their vocalized form. The\ntask of adding diacritical marks to plain Hebrew text is still dominated by a\nsystem that relies heavily on human-curated resources. Recent models trained on\ndiacritized Hebrew texts still present a gap in performance. We use a recently\ndeveloped char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a\ncharacter level transformer pretrained on Hebrew text and fine-tuned to produce\ndiacritical marks for Hebrew sentences. We continue to show how finetuning a\nmodel for diacritizing transfers to a task such as part of speech tagging.\n","authors":["Ido Cohen","Jacob Gidron","Idan Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.02417v1.pdf","comment":"Published at ISCOL2022 as a poster"},{"id":"http://arxiv.org/abs/2410.02416v1","updated":"2024-10-03T12:06:29Z","published":"2024-10-03T12:06:29Z","title":"Eliminating Oversaturation and Artifacts of High Guidance Scales in\n  Diffusion Models","summary":"  Classifier-free guidance (CFG) is crucial for improving both generation\nquality and alignment between the input condition and final output in diffusion\nmodels. While a high guidance scale is generally required to enhance these\naspects, it also causes oversaturation and unrealistic artifacts. In this\npaper, we revisit the CFG update rule and introduce modifications to address\nthis issue. We first decompose the update term in CFG into parallel and\northogonal components with respect to the conditional model prediction and\nobserve that the parallel component primarily causes oversaturation, while the\northogonal component enhances image quality. Accordingly, we propose\ndown-weighting the parallel component to achieve high-quality generations\nwithout oversaturation. Additionally, we draw a connection between CFG and\ngradient ascent and introduce a new rescaling and momentum method for the CFG\nupdate rule based on this insight. Our approach, termed adaptive projected\nguidance (APG), retains the quality-boosting advantages of CFG while enabling\nthe use of higher guidance scales without oversaturation. APG is easy to\nimplement and introduces practically no additional computational overhead to\nthe sampling process. Through extensive experiments, we demonstrate that APG is\ncompatible with various conditional diffusion models and samplers, leading to\nimproved FID, recall, and saturation scores while maintaining precision\ncomparable to CFG, making our method a superior plug-and-play alternative to\nstandard classifier-free guidance.\n","authors":["Seyedmorteza Sadat","Otmar Hilliges","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2410.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05754v3","updated":"2024-10-03T11:58:22Z","published":"2024-03-09T01:34:26Z","title":"Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition","summary":"  In this paper, we propose two hybrid quantum-inspired neural networks with\nresidual and dense connections respectively for pattern recognition. We explain\nthe concrete frameworks and illustrate the potential superiority to prevent\ngradient explosion of our hybrid models. A group of numerical experiments about\ngeneralization power shows that our hybrid models possess the same\ngeneralization power as the pure classical models with different noisy datasets\nutilized. More importantly, another group of numerical experiments of\nrobustness demonstrates that our hybrid models outperform pure classical models\nnotably in resistance to parameter attacks with various asymmetric noises.\nAlso, an ablation study indicate that the recognition accuracy of our hybrid\nmodels is 2\\%-3\\% higher than that of the quantum neural network without\nresidual or dense connection. Eventually, we discuss the application scenarios\nof our hybrid models by analyzing their computational complexities.\n","authors":["Andi Chen","Hua-Lei Yin","Zeng-Bing Chen","Shengjun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05754v3.pdf","comment":"12 pages for main paper with a hyperlink of a 18-page supplementary\n  material in the last page of the main paper"},{"id":"http://arxiv.org/abs/2402.04051v4","updated":"2024-10-03T11:36:28Z","published":"2024-02-06T14:53:28Z","title":"Analysis of Linear Mode Connectivity via Permutation-Based Weight\n  Matching","summary":"  Recently, Ainsworth et al. showed that using weight matching (WM) to minimize\nthe $L_2$ distance in a permutation search of model parameters effectively\nidentifies permutations that satisfy linear mode connectivity (LMC), where the\nloss along a linear path between two independently trained models with\ndifferent seeds remains nearly constant. This paper analyzes LMC using WM,\nwhich is useful for understanding stochastic gradient descent's effectiveness\nand its application in areas like model merging. We first empirically show that\npermutations found by WM do not significantly reduce the $L_2$ distance between\ntwo models, and the occurrence of LMC is not merely due to distance reduction\nby WM itself. We then demonstrate that permutations can change the directions\nof the singular vectors, but not the singular values, of the weight matrices in\neach layer. This finding shows that permutations found by WM primarily align\nthe directions of singular vectors associated with large singular values across\nmodels. This alignment brings the singular vectors with large singular values,\nwhich determine the model's functionality, closer between the original and\nmerged models, allowing the merged model to retain functionality similar to the\noriginal models, thereby satisfying LMC. This paper also analyzes activation\nmatching (AM) in terms of singular vectors and finds that the principle of AM\nis the same as that of WM. Finally, we analyze the difference between WM and\nthe straight-through estimator (STE), a dataset-dependent permutation search\nmethod, and show that WM can be more advantageous than STE in achieving LMC\namong three or more models.\n","authors":["Akira Ito","Masanori Yamada","Atsutoshi Kumagai"],"pdf_url":"https://arxiv.org/pdf/2402.04051v4.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.02400v1","updated":"2024-10-03T11:27:55Z","published":"2024-10-03T11:27:55Z","title":"An Online Feasible Point Method for Benign Generalized Nash Equilibrium\n  Problems","summary":"  We consider a repeatedly played generalized Nash equilibrium game. This\ninduces a multi-agent online learning problem with joint constraints. An\nimportant challenge in this setting is that the feasible set for each agent\ndepends on the simultaneous moves of the other agents and, therefore, varies\nover time. As a consequence, the agents face time-varying constraints, which\nare not adversarial but rather endogenous to the system. Prior work in this\nsetting focused on convergence to a feasible solution in the limit via\nintegrating the constraints in the objective as a penalty function. However, no\nexisting work can guarantee that the constraints are satisfied for all\niterations while simultaneously guaranteeing convergence to a generalized Nash\nequilibrium. This is a problem of fundamental theoretical interest and\npractical relevance. In this work, we introduce a new online feasible point\nmethod. Under the assumption that limited communication between the agents is\nallowed, this method guarantees feasibility. We identify the class of benign\ngeneralized Nash equilibrium problems, for which the convergence of our method\nto the equilibrium is guaranteed. We set this class of benign generalized Nash\nequilibrium games in context with existing definitions and illustrate our\nmethod with examples.\n","authors":["Sarah Sachs","Hedi Hadiji","Tim van Erven","Mathias Staudigl"],"pdf_url":"https://arxiv.org/pdf/2410.02400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02396v1","updated":"2024-10-03T11:17:58Z","published":"2024-10-03T11:17:58Z","title":"Parameter Competition Balancing for Model Merging","summary":"  While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.\n","authors":["Guodong Du","Junlin Lee","Jing Li","Runhua Jiang","Yifei Guo","Shuyang Yu","Hanting Liu","Sim Kuan Goh","Ho-Kin Tang","Daojing He","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02396v1.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.02394v1","updated":"2024-10-03T11:16:43Z","published":"2024-10-03T11:16:43Z","title":"Online Multi-Label Classification under Noisy and Changing Label\n  Distribution","summary":"  Multi-label data stream usually contains noisy labels in the real-world\napplications, namely occuring in both relevant and irrelevant labels. However,\nexisting online multi-label classification methods are mostly limited in terms\nof label quality and fail to deal with the case of noisy labels. On the other\nhand, the ground-truth label distribution may vary with the time changing,\nwhich is hidden in the observed noisy label distribution and difficult to\ntrack, posing a major challenge for concept drift adaptation. Motivated by\nthis, we propose an online multi-label classification algorithm under Noisy and\nChanging Label Distribution (NCLD). The convex objective is designed to\nsimultaneously model the label scoring and the label ranking for high accuracy,\nwhose robustness to NCLD benefits from three novel works: 1) The local feature\ngraph is used to reconstruct the label scores jointly with the observed labels,\nand an unbiased ranking loss is derived and applied to learn reliable ranking\ninformation. 2) By detecting the difference between two adjacent chunks with\nthe unbiased label cardinality, we identify the change in the ground-truth\nlabel distribution and reset the ranking or all information learned from the\npast to match the new distribution. 3) Efficient and accurate updating is\nachieved based on the updating rule derived from the closed-form optimal model\nsolution. Finally, empirical experimental results validate the effectiveness of\nour method in classifying instances under NCLD.\n","authors":["Yizhang Zou","Xuegang Hu","Peipei Li","Jun Hu","You Wu"],"pdf_url":"https://arxiv.org/pdf/2410.02394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02392v1","updated":"2024-10-03T11:13:55Z","published":"2024-10-03T11:13:55Z","title":"MANTRA: The Manifold Triangulations Assemblage","summary":"  The rising interest in leveraging higher-order interactions present in\ncomplex systems has led to a surge in more expressive models exploiting\nhigh-order structures in the data, especially in topological deep learning\n(TDL), which designs neural networks on high-order domains such as simplicial\ncomplexes. However, progress in this field is hindered by the scarcity of\ndatasets for benchmarking these architectures. To address this gap, we\nintroduce MANTRA, the first large-scale, diverse, and intrinsically high order\ndataset for benchmarking high-order models, comprising over 43,000 and 249,000\ntriangulations of surfaces and three-dimensional manifolds, respectively. With\nMANTRA, we assess several graph- and simplicial complex-based models on three\ntopological classification tasks. We demonstrate that while simplicial\ncomplex-based neural networks generally outperform their graph-based\ncounterparts in capturing simple topological invariants, they also struggle,\nsuggesting a rethink of TDL. Thus, MANTRA serves as a benchmark for assessing\nand advancing topological methods, leading the way for more effective\nhigh-order models.\n","authors":["Rubn Ballester","Ernst Rell","Daniel Bin Schmid","Mathieu Alain","Sergio Escalera","Carles Casacuberta","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.02392v1.pdf","comment":"26 pages, 2 figures, 22 tables"},{"id":"http://arxiv.org/abs/2410.02389v1","updated":"2024-10-03T11:10:37Z","published":"2024-10-03T11:10:37Z","title":"Diffusion Meets Options: Hierarchical Generative Skill Composition for\n  Temporally-Extended Tasks","summary":"  Safe and successful deployment of robots requires not only the ability to\ngenerate complex plans but also the capacity to frequently replan and correct\nexecution errors. This paper addresses the challenge of long-horizon trajectory\nplanning under temporally extended objectives in a receding horizon manner. To\nthis end, we propose DOPPLER, a data-driven hierarchical framework that\ngenerates and updates plans based on instruction specified by linear temporal\nlogic (LTL). Our method decomposes temporal tasks into chain of options with\nhierarchical reinforcement learning from offline non-expert datasets. It\nleverages diffusion models to generate options with low-level actions. We\ndevise a determinantal-guided posterior sampling technique during batch\ngeneration, which improves the speed and diversity of diffusion generated\noptions, leading to more efficient querying. Experiments on robot navigation\nand manipulation tasks demonstrate that DOPPLER can generate sequences of\ntrajectories that progressively satisfy the specified formulae for obstacle\navoidance and sequential visitation. Demonstration videos are available online\nat: https://philiptheother.github.io/doppler/.\n","authors":["Zeyu Feng","Hao Luan","Kevin Yuchen Ma","Harold Soh"],"pdf_url":"https://arxiv.org/pdf/2410.02389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02387v1","updated":"2024-10-03T11:07:43Z","published":"2024-10-03T11:07:43Z","title":"BiSSL: Bilevel Optimization for Self-Supervised Pre-Training and\n  Fine-Tuning","summary":"  In this work, we present BiSSL, a first-of-its-kind training framework that\nintroduces bilevel optimization to enhance the alignment between the pretext\npre-training and downstream fine-tuning stages in self-supervised learning.\nBiSSL formulates the pretext and downstream task objectives as the lower- and\nupper-level objectives in a bilevel optimization problem and serves as an\nintermediate training stage within the self-supervised learning pipeline. By\nmore explicitly modeling the interdependence of these training stages, BiSSL\nfacilitates enhanced information sharing between them, ultimately leading to a\nbackbone parameter initialization that is better suited for the downstream\ntask. We propose a training algorithm that alternates between optimizing the\ntwo objectives defined in BiSSL. Using a ResNet-18 backbone pre-trained with\nSimCLR on the STL10 dataset, we demonstrate that our proposed framework\nconsistently achieves improved or competitive classification accuracies across\nvarious downstream image classification datasets compared to the conventional\nself-supervised learning pipeline. Qualitative analyses of the backbone\nfeatures further suggest that BiSSL enhances the alignment of downstream\nfeatures in the backbone prior to fine-tuning.\n","authors":["Gustav Wagner Zakarias","Lars Kai Hansen","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2410.02387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13663v3","updated":"2024-10-03T11:03:22Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernndez","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v3.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.01654v2","updated":"2024-10-03T12:43:14Z","published":"2024-10-02T15:19:31Z","title":"Releasing the Parameter Latency of Neural Representation for\n  High-Efficiency Video Compression","summary":"  For decades, video compression technology has been a prominent research area.\nTraditional hybrid video compression framework and end-to-end frameworks\ncontinue to explore various intra- and inter-frame reference and prediction\nstrategies based on discrete transforms and deep learning techniques. However,\nthe emerging implicit neural representation (INR) technique models entire\nvideos as basic units, automatically capturing intra-frame and inter-frame\ncorrelations and obtaining promising performance. INR uses a compact neural\nnetwork to store video information in network parameters, effectively\neliminating spatial and temporal redundancy in the original video. However, in\nthis paper, our exploration and verification reveal that current INR video\ncompression methods do not fully exploit their potential to preserve\ninformation. We investigate the potential of enhancing network parameter\nstorage through parameter reuse. By deepening the network, we designed a\nfeasible INR parameter reuse scheme to further improve compression performance.\nExtensive experimental results show that our method significantly enhances the\nrate-distortion performance of INR video compression.\n","authors":["Gai Zhang","Xinfeng Zhang","Lv Tang","Yue Li","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19340v4","updated":"2024-10-03T02:23:50Z","published":"2024-07-27T21:00:36Z","title":"Integrating Large Language Models into a Tri-Modal Architecture for\n  Automated Depression Classification","summary":"  Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.\n","authors":["Santosh V. Patapati"],"pdf_url":"https://arxiv.org/pdf/2407.19340v4.pdf","comment":"Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language\n  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ"},{"id":"http://arxiv.org/abs/2406.17932v2","updated":"2024-10-03T14:34:52Z","published":"2024-06-25T20:47:10Z","title":"SonicSense: Object Perception from In-Hand Acoustic Vibration","summary":"  We introduce SonicSense, a holistic design of hardware and software to enable\nrich robot object perception through in-hand acoustic vibration sensing. While\nprevious studies have shown promising results with acoustic sensing for object\nperception, current solutions are constrained to a handful of objects with\nsimple geometries and homogeneous materials, single-finger sensing, and mixing\ntraining and testing on the same objects. SonicSense enables container\ninventory status differentiation, heterogeneous material prediction, 3D shape\nreconstruction, and object re-identification from a diverse set of 83\nreal-world objects. Our system employs a simple but effective heuristic\nexploration policy to interact with the objects as well as end-to-end\nlearning-based algorithms to fuse vibration signals to infer object properties.\nOur framework underscores the significance of in-hand acoustic vibration\nsensing in advancing robot tactile perception.\n","authors":["Jiaxun Liu","Boyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17932v2.pdf","comment":"Our project website is at: http://generalroboticslab.com/SonicSense"},{"id":"http://arxiv.org/abs/2409.02889v2","updated":"2024-10-03T11:01:14Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v2.pdf","comment":"20 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2406.00093v2","updated":"2024-10-03T08:20:17Z","published":"2024-05-31T17:59:56Z","title":"Bootstrap3D: Improving Multi-view Diffusion Model with Synthetic Data","summary":"  Recent years have witnessed remarkable progress in multi-view diffusion\nmodels for 3D content creation. However, there remains a significant gap in\nimage quality and prompt-following ability compared to 2D diffusion models. A\ncritical bottleneck is the scarcity of high-quality 3D objects with detailed\ncaptions. To address this challenge, we propose Bootstrap3D, a novel framework\nthat automatically generates an arbitrary quantity of multi-view images to\nassist in training multi-view diffusion models. Specifically, we introduce a\ndata generation pipeline that employs (1) 2D and video diffusion models to\ngenerate multi-view images based on constructed text prompts, and (2) our\nfine-tuned 3D-aware MV-LLaVA for filtering high-quality data and rewriting\ninaccurate captions. Leveraging this pipeline, we have generated 1 million\nhigh-quality synthetic multi-view images with dense descriptive captions to\naddress the shortage of high-quality 3D data. Furthermore, we present a\nTraining Timestep Reschedule (TTR) strategy that leverages the denoising\nprocess to learn multi-view consistency while maintaining the original 2D\ndiffusion prior. Extensive experiments demonstrate that Bootstrap3D can\ngenerate high-quality multi-view images with superior aesthetic quality,\nimage-text alignment, and maintained view consistency.\n","authors":["Zeyi Sun","Tong Wu","Pan Zhang","Yuhang Zang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.00093v2.pdf","comment":"Project Page: https://sunzey.github.io/Bootstrap3D/"},{"id":"http://arxiv.org/abs/2410.02182v1","updated":"2024-10-03T03:51:53Z","published":"2024-10-03T03:51:53Z","title":"BadCM: Invisible Backdoor Attack Against Cross-Modal Learning","summary":"  Despite remarkable successes in unimodal learning tasks, backdoor attacks\nagainst cross-modal learning are still underexplored due to the limited\ngeneralization and inferior stealthiness when involving multiple modalities.\nNotably, since works in this area mainly inherit ideas from unimodal visual\nattacks, they struggle with dealing with diverse cross-modal attack\ncircumstances and manipulating imperceptible trigger samples, which hinders\ntheir practicability in real-world applications. In this paper, we introduce a\nnovel bilateral backdoor to fill in the missing pieces of the puzzle in the\ncross-modal backdoor and propose a generalized invisible backdoor framework\nagainst cross-modal learning (BadCM). Specifically, a cross-modal mining scheme\nis developed to capture the modality-invariant components as target poisoning\nareas, where well-designed trigger patterns injected into these regions can be\nefficiently recognized by the victim models. This strategy is adapted to\ndifferent image-text cross-modal models, making our framework available to\nvarious attack scenarios. Furthermore, for generating poisoned samples of high\nstealthiness, we conceive modality-specific generators for visual and\nlinguistic modalities that facilitate hiding explicit trigger patterns in\nmodality-invariant regions. To the best of our knowledge, BadCM is the first\ninvisible backdoor method deliberately designed for diverse cross-modal attacks\nwithin one unified framework. Comprehensive experimental evaluations on two\ntypical applications, i.e., cross-modal retrieval and VQA, demonstrate the\neffectiveness and generalization of our method under multiple kinds of attack\nscenarios. Moreover, we show that BadCM can robustly evade existing backdoor\ndefenses. Our code is available at https://github.com/xandery-geek/BadCM.\n","authors":["Zheng Zhang","Xu Yuan","Lei Zhu","Jingkuan Song","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.02182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14637v2","updated":"2024-10-03T03:51:22Z","published":"2023-10-23T07:21:40Z","title":"Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval","summary":"  Deep hashing has been intensively studied and successfully applied in\nlarge-scale image retrieval systems due to its efficiency and effectiveness.\nRecent studies have recognized that the existence of adversarial examples poses\na security threat to deep hashing models, that is, adversarial vulnerability.\nNotably, it is challenging to efficiently distill reliable semantic\nrepresentatives for deep hashing to guide adversarial learning, and thereby it\nhinders the enhancement of adversarial robustness of deep hashing-based\nretrieval models. Moreover, current researches on adversarial training for deep\nhashing are hard to be formalized into a unified minimax structure. In this\npaper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the\nadversarial robustness of deep hashing models. Specifically, we conceive a\ndiscriminative mainstay features learning (DMFL) scheme to construct semantic\nrepresentatives for guiding adversarial learning in deep hashing. Particularly,\nour DMFL with the strict theoretical guarantee is adaptively optimized in a\ndiscriminative learning manner, where both discriminative and semantic\nproperties are jointly considered. Moreover, adversarial examples are\nfabricated by maximizing the Hamming distance between the hash codes of\nadversarial samples and mainstay features, the efficacy of which is validated\nin the adversarial attack trials. Further, we, for the first time, formulate\nthe formalized adversarial training of deep hashing into a unified minimax\noptimization under the guidance of the generated mainstay codes. Extensive\nexperiments on benchmark datasets show superb attack performance against the\nstate-of-the-art algorithms, meanwhile, the proposed adversarial training can\neffectively eliminate adversarial perturbations for trustworthy deep\nhashing-based retrieval. Our code is available at\nhttps://github.com/xandery-geek/SAAT.\n","authors":["Xu Yuan","Zheng Zhang","Xunguang Wang","Lin Wu"],"pdf_url":"https://arxiv.org/pdf/2310.14637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03787v1","updated":"2024-10-03T20:26:54Z","published":"2024-10-03T20:26:54Z","title":"CalliffusionV2: Personalized Natural Calligraphy Generation with\n  Flexible Multi-modal Control","summary":"  In this paper, we introduce CalliffusionV2, a novel system designed to\nproduce natural Chinese calligraphy with flexible multi-modal control. Unlike\nprevious approaches that rely solely on image or text inputs and lack\nfine-grained control, our system leverages both images to guide generations at\nfine-grained levels and natural language texts to describe the features of\ngenerations. CalliffusionV2 excels at creating a broad range of characters and\ncan quickly learn new styles through a few-shot learning approach. It is also\ncapable of generating non-Chinese characters without prior training.\nComprehensive tests confirm that our system produces calligraphy that is both\nstylistically accurate and recognizable by neural network classifiers and human\nevaluators.\n","authors":["Qisheng Liao","Liang Li","Yulang Fei","Gus Xia"],"pdf_url":"https://arxiv.org/pdf/2410.03787v1.pdf","comment":"11 pages, 7 figures"}]},"2024-10-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.03663v1","updated":"2024-10-04T17:59:41Z","published":"2024-10-04T17:59:41Z","title":"Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge\n  Distillation from Multiple Large Language Models","summary":"  Large language models (LLMs) have exhibited complex reasoning abilities by\ngenerating question rationales and demonstrated exceptional performance in\nnatural language processing (NLP) tasks. However, these reasoning capabilities\ngenerally emerge in models with tens of billions of parameters, creating\nsignificant computational challenges for real-world deployment. Recent research\nhas concentrated on improving open-source smaller models through knowledge\ndistillation (KD) from commercial LLMs. Nevertheless, most of these studies\nrely solely on the responses from one single LLM as the gold rationale for\ntraining. In this paper, we introduce a novel Mistake-Aware Peer-Review\nDistillation (MAPD) approach: 1) Instead of merely obtaining gold rationales\nfrom teachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.\n","authors":["Zhuochun Li","Yuelyu Ji","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.03663v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03659v1","updated":"2024-10-04T17:59:28Z","published":"2024-10-04T17:59:28Z","title":"Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language\n  Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.\n","authors":["Tinghui Zhu","Qin Liu","Fei Wang","Zhengzhong Tu","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.03659v1.pdf","comment":"Website:\n  https://darthzhu.github.io/cross-modality-knowledge-conflict/"},{"id":"http://arxiv.org/abs/2410.03658v1","updated":"2024-10-04T17:59:00Z","published":"2024-10-04T17:59:00Z","title":"RAFT: Realistic Attacks to Fool Text Detectors","summary":"  Large language models (LLMs) have exhibited remarkable fluency across various\ntasks. However, their unethical applications, such as disseminating\ndisinformation, have become a growing concern. Although recent works have\nproposed a number of LLM detection methods, their robustness and reliability\nremain unclear. In this paper, we present RAFT: a grammar error-free black-box\nattack against existing LLM detectors. In contrast to previous attacks for\nlanguage models, our method exploits the transferability of LLM embeddings at\nthe word-level while preserving the original text quality. We leverage an\nauxiliary embedding to greedily select candidate words to perturb against the\ntarget detector. Experiments reveal that our attack effectively compromises all\ndetectors in the study across various domains by up to 99%, and are\ntransferable across source models. Manual human evaluation studies show our\nattacks are realistic and indistinguishable from original human-written text.\nWe also show that examples generated by RAFT can be used to train adversarially\nrobust detectors. Our work shows that current LLM detectors are not\nadversarially robust, underscoring the urgent need for more resilient detection\nmechanisms.\n","authors":["James Wang","Ran Li","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2410.03658v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2408.06520v2","updated":"2024-10-04T17:50:34Z","published":"2024-08-12T22:40:01Z","title":"Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and\n  Hindsight Modular Reflections for Task Planning with LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable abilities in\nvarious language tasks, making them promising candidates for decision-making in\nrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose\nRetrieval-Augmented in-context reinforcement Learning (RAHL), a novel framework\nthat decomposes complex tasks into sub-tasks using an LLM-based high-level\npolicy, in which a complex task is decomposed into sub-tasks by a high-level\npolicy on-the-fly. The sub-tasks, defined by goals, are assigned to the\nlow-level policy to complete. To improve the agent's performance in\nmulti-episode execution, we propose Hindsight Modular Reflection (HMR), where,\ninstead of reflecting on the full trajectory, we let the agent reflect on\nshorter sub-trajectories to improve reflection efficiency. We evaluated the\ndecision-making ability of the proposed RAHL in three benchmark\nenvironments--ALFWorld, Webshop, and HotpotQA. The results show that RAHL can\nachieve an improvement in performance in 9%, 42%, and 10% in 5 episodes of\nexecution in strong baselines. Furthermore, we also implemented RAHL on the\nBoston Dynamics SPOT robot. The experiment shows that the robot can scan the\nenvironment, find entrances, and navigate to new rooms controlled by the LLM\npolicy.\n","authors":["Chuanneng Sun","Songjun Huang","Dario Pompili"],"pdf_url":"https://arxiv.org/pdf/2408.06520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03642v1","updated":"2024-10-04T17:48:29Z","published":"2024-10-04T17:48:29Z","title":"Aligning LLMs with Individual Preferences via Interaction","summary":"  As large language models (LLMs) demonstrate increasingly advanced\ncapabilities, aligning their behaviors with human values and preferences\nbecomes crucial for their wide adoption. While previous research focuses on\ngeneral alignment to principles such as helpfulness, harmlessness, and honesty,\nthe need to account for individual and diverse preferences has been largely\noverlooked, potentially undermining customized human experiences. To address\nthis gap, we train LLMs that can ''interact to align'', essentially cultivating\nthe meta-skill of LLMs to implicitly infer the unspoken personalized\npreferences of the current user through multi-turn conversations, and then\ndynamically align their following behaviors and responses to these inferred\npreferences. Our approach involves establishing a diverse pool of 3,310\ndistinct user personas by initially creating seed examples, which are then\nexpanded through iterative self-generation and filtering. Guided by distinct\nuser personas, we leverage multi-LLM collaboration to develop a multi-turn\npreference dataset containing 3K+ multi-turn conversations in tree structures.\nFinally, we apply supervised fine-tuning and reinforcement learning to enhance\nLLMs using this dataset. For evaluation, we establish the ALOE (ALign With\nCustOmized PrEferences) benchmark, consisting of 100 carefully selected\nexamples and well-designed metrics to measure the customized alignment\nperformance during conversations. Experimental results demonstrate the\neffectiveness of our method in enabling dynamic, personalized alignment via\ninteraction.\n","authors":["Shujin Wu","May Fung","Cheng Qian","Jeonghwan Kim","Dilek Hakkani-Tur","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2410.03642v1.pdf","comment":"The code and dataset are made public at\n  https://github.com/ShujinWu-0814/ALOE"},{"id":"http://arxiv.org/abs/2405.20974v3","updated":"2024-10-04T17:23:48Z","published":"2024-05-31T16:21:16Z","title":"SaySelf: Teaching LLMs to Express Confidence with Self-Reflective\n  Rationales","summary":"  Large language models (LLMs) often generate inaccurate or fabricated\ninformation and generally fail to indicate their confidence, which limits their\nbroader applications. Previous work elicits confidence from LLMs by direct or\nself-consistency prompting, or constructing specific datasets for supervised\nfinetuning. The prompting-based approaches have inferior performance, and the\ntraining-based approaches are limited to binary or inaccurate group-level\nconfidence estimates. In this work, we present the advanced SaySelf, a training\nframework that teaches LLMs to express more accurate fine-grained confidence\nestimates. In addition, beyond the confidence scores, SaySelf initiates the\nprocess of directing LLMs to produce self-reflective rationales that clearly\nidentify gaps in their parametric knowledge and explain their uncertainty. This\nis achieved by using an LLM to automatically summarize the uncertainties in\nspecific knowledge via natural language. The summarization is based on the\nanalysis of the inconsistency in multiple sampled reasoning chains, and the\nresulting data is utilized for supervised fine-tuning. Moreover, we utilize\nreinforcement learning with a meticulously crafted reward function to calibrate\nthe confidence estimates, motivating LLMs to deliver accurate, high-confidence\npredictions and to penalize overconfidence in erroneous outputs. Experimental\nresults in both in-distribution and out-of-distribution datasets demonstrate\nthe effectiveness of SaySelf in reducing the confidence calibration error and\nmaintaining the task performance. We show that the generated self-reflective\nrationales are reasonable and can further contribute to the calibration. The\ncode is made public at https://github.com/xu1868/SaySelf.\n","authors":["Tianyang Xu","Shujin Wu","Shizhe Diao","Xiaoze Liu","Xingyao Wang","Yangyi Chen","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2405.20974v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03617v1","updated":"2024-10-04T17:17:19Z","published":"2024-10-04T17:17:19Z","title":"What Matters for Model Merging at Scale?","summary":"  Model merging aims to combine multiple expert models into a more capable\nsingle model, offering benefits such as reduced storage and serving costs,\nimproved generalization, and support for decentralized model development.\nDespite its promise, previous studies have primarily focused on merging a few\nsmall models. This leaves many unanswered questions about the effect of scaling\nmodel size and how it interplays with other key factors -- like the base model\nquality and number of expert models -- , to affect the merged model's\nperformance. This work systematically evaluates the utility of model merging at\nscale, examining the impact of these different factors. We experiment with\nmerging fully fine-tuned models using 4 popular merging methods -- Averaging,\nTask~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B\nparameters and merging up to 8 different expert models. We evaluate the merged\nmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shot\ngeneralization to unseen held-out tasks. Our experiments provide several new\ninsights about model merging at scale and the interplay between different\nfactors. First, we find that merging is more effective when experts are created\nfrom strong base models, i.e., models with good zero-shot performance. Second,\nlarger models facilitate easier merging. Third merging consistently improves\ngeneralization capabilities. Notably, when merging 8 large expert models, the\nmerged models often generalize better compared to the multitask trained models.\nFourth, we can better merge more expert models when working with larger models.\nFifth, different merging methods behave very similarly at larger scales.\nOverall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.\n","authors":["Prateek Yadav","Tu Vu","Jonathan Lai","Alexandra Chronopoulou","Manaal Faruqui","Mohit Bansal","Tsendsuren Munkhdalai"],"pdf_url":"https://arxiv.org/pdf/2410.03617v1.pdf","comment":"20 Pages, 7 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2410.03608v1","updated":"2024-10-04T17:09:08Z","published":"2024-10-04T17:09:08Z","title":"TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and\n  Generation","summary":"  Given the widespread adoption and usage of Large Language Models (LLMs), it\nis crucial to have flexible and interpretable evaluations of their\ninstruction-following ability. Preference judgments between model outputs have\nbecome the de facto evaluation standard, despite distilling complex,\nmulti-faceted preferences into a single ranking. Furthermore, as human\nannotation is slow and costly, LLMs are increasingly used to make these\njudgments, at the expense of reliability and interpretability. In this work, we\npropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,\ninterpretable evaluation protocol that structures evaluations with\nLLM-generated, instruction-specific checklists. We first show that, given an\ninstruction, LLMs can reliably produce high-quality, tailored evaluation\nchecklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements\nand human preferences, as compared to having an LLM directly score an output.\nWe then show that STICK (Self-TICK) can be used to improve generation quality\nacross multiple benchmarks via self-refinement and Best-of-N selection. STICK\nself-refinement on LiveBench reasoning tasks leads to an absolute gain of\n$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute\nimprovement on the real-world instruction dataset, WildBench. In light of this,\nstructured, multi-faceted self-improvement is shown to be a promising way to\nfurther advance LLM capabilities. Finally, by providing LLM-generated\nchecklists to human evaluators tasked with directly scoring LLM responses to\nWildBench instructions, we notably increase inter-annotator agreement (0.194\n$\\to$ 0.256).\n","authors":["Jonathan Cook","Tim Rocktschel","Jakob Foerster","Dennis Aumiller","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14505v2","updated":"2024-10-04T17:08:17Z","published":"2024-08-24T07:59:36Z","title":"Language Model Empowered Spatio-Temporal Forecasting via Physics-Aware\n  Reprogramming","summary":"  Spatio-temporal forecasting is pivotal in numerous real-world applications,\nincluding transportation planning, energy management, and climate monitoring.\nIn this work, we aim to harness the reasoning and generalization abilities of\nPre-trained Language Models (PLMs) for more effective spatio-temporal\nforecasting, particularly in data-scarce scenarios. However, recent studies\nuncover that PLMs, which are primarily trained on textual data, often falter\nwhen tasked with modeling the intricate correlations in numerical time series,\nthereby limiting their effectiveness in comprehending spatio-temporal data. To\nbridge the gap, we propose RePST, a physics-aware PLM reprogramming framework\ntailored for spatio-temporal forecasting. Specifically, we first propose a\nphysics-aware decomposer that adaptively disentangles spatially correlated time\nseries into interpretable sub-components, which facilitates PLM to understand\nsophisticated spatio-temporal dynamics via a divide-and-conquer strategy.\nMoreover, we propose a selective discrete reprogramming scheme, which\nintroduces an expanded spatio-temporal vocabulary space to project\nspatio-temporal series into discrete representations. This scheme minimizes the\ninformation loss during reprogramming and enriches the representations derived\nby PLMs. Extensive experiments on real-world datasets show that the proposed\nRePST outperforms twelve state-of-the-art baseline methods, particularly in\ndata-scarce scenarios, highlighting the effectiveness and superior\ngeneralization capabilities of PLMs for spatio-temporal forecasting.\n","authors":["Hao Wang","Jindong Han","Wei Fan","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03600v1","updated":"2024-10-04T16:58:41Z","published":"2024-10-04T16:58:41Z","title":"Efficiently Identifying Watermarked Segments in Mixed-Source Texts","summary":"  Text watermarks in large language models (LLMs) are increasingly used to\ndetect synthetic text, mitigating misuse cases like fake news and academic\ndishonesty. While existing watermarking detection techniques primarily focus on\nclassifying entire documents as watermarked or not, they often neglect the\ncommon scenario of identifying individual watermark segments within longer,\nmixed-source documents. Drawing inspiration from plagiarism detection systems,\nwe propose two novel methods for partial watermark detection. First, we develop\na geometry cover detection framework aimed at determining whether there is a\nwatermark segment in long text. Second, we introduce an adaptive online\nlearning algorithm to pinpoint the precise location of watermark segments\nwithin the text. Evaluated on three popular watermarking techniques\n(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves\nhigh accuracy, significantly outperforming baseline methods. Moreover, our\nframework is adaptable to other watermarking techniques, offering new insights\nfor precise watermark detection.\n","authors":["Xuandong Zhao","Chenwen Liao","Yu-Xiang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.03600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09598v2","updated":"2024-10-04T16:57:08Z","published":"2024-09-15T03:25:55Z","title":"Improving Statistical Significance in Human Evaluation of Automatic\n  Metrics via Soft Pairwise Accuracy","summary":"  Selecting an automatic metric that best emulates human annotators is often\nnon-trivial, because there is no clear definition of \"best emulates.\" A\nmeta-metric is required to compare the human judgments to the automatic metric\nscores, and metric rankings depend on the choice of meta-metric. We propose\nSoft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise\nAccuracy (PA) but incorporates the statistical significance of both the human\njudgments and the metric scores. We show that SPA is more stable than PA with\nrespect to changes in the number of systems/segments used for evaluation. We\nalso show that PA can only assign a small set of distinct output values to\nmetrics, and this results in many metrics being artificially assigned the exact\nsame PA score. We demonstrate that SPA fixes this issue. Finally, we show that\nSPA is more discriminative than PA, producing more statistically significant\ncomparisons between metrics. SPA was selected as the official system-level\nmetric for the 2024 WMT Metrics Shared Task.\n","authors":["Brian Thompson","Nitika Mathur","Daniel Deutsch","Huda Khayrallah"],"pdf_url":"https://arxiv.org/pdf/2409.09598v2.pdf","comment":"Accepted at WMT 2024"},{"id":"http://arxiv.org/abs/2404.15155v2","updated":"2024-10-04T16:56:06Z","published":"2024-04-22T06:30:05Z","title":"MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making","summary":"  Foundation models are becoming valuable tools in medicine. Yet despite their\npromise, the best way to leverage Large Language Models (LLMs) in complex\nmedical tasks remains an open question. We introduce a novel multi-agent\nframework, named Medical Decision-making Agents (MDAgents) that helps address\nthis gap by automatically assigning a collaboration structure to a team of\nLLMs. The assigned solo or group collaboration structure is tailored to the\nmedical task at hand, emulating real-world medical decision-making processes\nadapted to tasks of varying complexities. We evaluate our framework and\nbaseline methods using state-of-the-art LLMs across a suite of real-world\nmedical knowledge and medical diagnosis benchmarks. MDAgents achieved the best\nperformance in seven out of ten benchmarks on tasks requiring an understanding\nof medical knowledge and multi-modal reasoning, showing a significant\nimprovement of up to 6.5% (p < 0.05) compared to previous methods' best\nperformances. Ablation studies reveal that MDAgents effectively determines\nmedical complexity to optimize for efficiency and accuracy across diverse\nmedical tasks. Notably, the combination of moderator review and external\nmedical knowledge in group collaboration resulted in an average accuracy\nimprovement of 11.8%. Our code can be found at\nhttps://github.com/mitmedialab/MDAgents.\n","authors":["Yubin Kim","Chanwoo Park","Hyewon Jeong","Yik Siu Chan","Xuhai Xu","Daniel McDuff","Hyeonhoon Lee","Marzyeh Ghassemi","Cynthia Breazeal","Hae Won Park"],"pdf_url":"https://arxiv.org/pdf/2404.15155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03595v1","updated":"2024-10-04T16:55:30Z","published":"2024-10-04T16:55:30Z","title":"Understanding Reasoning in Chain-of-Thought from the Hopfieldian View","summary":"  Large Language Models have demonstrated remarkable abilities across various\ntasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to\nenhance reasoning capabilities. However, existing research primarily focuses on\nimproving performance, lacking a comprehensive framework to explain and\nunderstand the fundamental factors behind CoT's success. To bridge this gap, we\nintroduce a novel perspective grounded in the Hopfieldian view of cognition in\ncognitive neuroscience. We establish a connection between CoT reasoning and key\ncognitive elements such as stimuli, actions, neural populations, and\nrepresentation spaces. From our view, we can understand the reasoning process\nas the movement between these representation spaces. Building on this insight,\nwe develop a method for localizing reasoning errors in the response of CoTs.\nMoreover, we propose the Representation-of-Thought (RoT) framework, which\nleverages the robustness of low-dimensional representation spaces to enhance\nthe robustness of the reasoning process in CoTs. Experimental results\ndemonstrate that RoT improves the robustness and interpretability of CoT\nreasoning while offering fine-grained control over the reasoning process.\n","authors":["Lijie Hu","Liang Liu","Shu Yang","Xin Chen","Zhen Tan","Muhammad Asif Ali","Mengdi Li","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03595v1.pdf","comment":"28 pages, a new version of \"A Hopfieldian View-based Interpretation\n  for Chain-of-Thought Reasoning\""},{"id":"http://arxiv.org/abs/2410.03594v1","updated":"2024-10-04T16:54:30Z","published":"2024-10-04T16:54:30Z","title":"Explicit, Implicit, and Scattered: Revisiting Event Extraction to\n  Capture Complex Arguments","summary":"  Prior works formulate the extraction of event-specific arguments as a span\nextraction problem, where event arguments are explicit -- i.e. assumed to be\ncontiguous spans of text in a document. In this study, we revisit this\ndefinition of Event Extraction (EE) by introducing two key argument types that\ncannot be modeled by existing EE frameworks. First, implicit arguments are\nevent arguments which are not explicitly mentioned in the text, but can be\ninferred through context. Second, scattered arguments are event arguments that\nare composed of information scattered throughout the text. These two argument\ntypes are crucial to elicit the full breadth of information required for proper\nevent modeling.\n  To support the extraction of explicit, implicit, and scattered arguments, we\ndevelop a novel dataset, DiscourseEE, which includes 7,464 argument annotations\nfrom online health discourse. Notably, 51.2% of the arguments are implicit, and\n17.4% are scattered, making DiscourseEE a unique corpus for complex event\nextraction. Additionally, we formulate argument extraction as a text generation\nproblem to facilitate the extraction of complex argument types. We provide a\ncomprehensive evaluation of state-of-the-art models and highlight critical open\nchallenges in generative event extraction. Our data and codebase are available\nat https://omar-sharif03.github.io/DiscourseEE.\n","authors":["Omar Sharif","Joseph Gatto","Madhusudan Basak","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2410.03594v1.pdf","comment":"Accepted in EMNLP-2024 (Main). 21 pages, 8 figures, and 11 tables"},{"id":"http://arxiv.org/abs/2407.11229v2","updated":"2024-10-04T16:52:57Z","published":"2024-07-15T20:29:24Z","title":"Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into\n  Consistency and Robustness","summary":"  Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.\n","authors":["Srija Mukhopadhyay","Adnan Qidwai","Aparna Garimella","Pritika Ramu","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.11229v2.pdf","comment":"22 pages, 9 Tables, 5 figures, 22 examples"},{"id":"http://arxiv.org/abs/2312.06149v4","updated":"2024-10-04T16:48:51Z","published":"2023-12-11T06:35:33Z","title":"Unlocking Anticipatory Text Generation: A Constrained Approach for Large\n  Language Models Decoding","summary":"  Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).\n","authors":["Lifu Tu","Semih Yavuz","Jin Qu","Jiacheng Xu","Rui Meng","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.06149v4.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2406.02018v2","updated":"2024-10-04T16:46:00Z","published":"2024-06-04T06:57:47Z","title":"Why Would You Suggest That? Human Trust in Language Model Responses","summary":"  The emergence of Large Language Models (LLMs) has revealed a growing need for\nhuman-AI collaboration, especially in creative decision-making scenarios where\ntrust and reliance are paramount. Through human studies and model evaluations\non the open-ended News Headline Generation task from the LaMP benchmark, we\nanalyze how the framing and presence of explanations affect user trust and\nmodel performance. Overall, we provide evidence that adding an explanation in\nthe model response to justify its reasoning significantly increases\nself-reported user trust in the model when the user has the opportunity to\ncompare various responses. Position and faithfulness of these explanations are\nalso important factors. However, these gains disappear when users are shown\nresponses independently, suggesting that humans trust all model responses,\nincluding deceptive ones, equitably when they are shown in isolation. Our\nfindings urge future research to delve deeper into the nuanced evaluation of\ntrust in human-machine teaming systems.\n","authors":["Manasi Sharma","Ho Chit Siu","Rohan Paleja","Jaime D. Pea"],"pdf_url":"https://arxiv.org/pdf/2406.02018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13069v3","updated":"2024-10-04T16:42:20Z","published":"2024-06-18T21:31:19Z","title":"Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG","summary":"  How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a\nnovel search tool inspired by indexing of genomic data. We compare the novelty\nof LM-generated text to human-written text and explore factors that affect\ngeneration novelty, focusing on the Pythia models. We find that, for $n > 4$,\nLM-generated text is less novel than human-written text, though it is more\nnovel for smaller $n$. Larger LMs and more constrained decoding strategies both\ndecrease novelty. Finally, we show that LMs complete $n$-grams with lower loss\nif they are more frequent in the training data. Overall, our results reveal\nfactors influencing the novelty of LM-generated text, and we release Rusty-DAWG\nto facilitate further pretraining data research.\n","authors":["William Merrill","Noah A. Smith","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2406.13069v3.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.13213v2","updated":"2024-10-04T16:29:58Z","published":"2024-02-20T18:24:47Z","title":"Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A","summary":"  We study 14 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigororous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.\n","authors":["Benjamin Plaut","Nguyen X. Khanh","Tu Trinh"],"pdf_url":"https://arxiv.org/pdf/2402.13213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03576v1","updated":"2024-10-04T16:26:12Z","published":"2024-10-04T16:26:12Z","title":"Table Question Answering for Low-resourced Indic Languages","summary":"  TableQA is the task of answering questions over tables of structured\ninformation, returning individual cells or tables as output. TableQA research\nhas focused primarily on high-resource languages, leaving medium- and\nlow-resource languages with little progress due to scarcity of annotated data\nand neural models. We address this gap by introducing a fully automatic\nlarge-scale tableQA data generation process for low-resource languages with\nlimited budget. We incorporate our data generation method on two Indic\nlanguages, Bengali and Hindi, which have no tableQA datasets or models. TableQA\nmodels trained on our large-scale datasets outperform state-of-the-art LLMs. We\nfurther study the trained models on different aspects, including mathematical\nreasoning capabilities and zero-shot cross-lingual transfer. Our work is the\nfirst on low-resource tableQA focusing on scalable data generation and\nevaluation procedures. Our proposed data generation method can be applied to\nany low-resource language with a web presence. We release datasets, models, and\ncode (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).\n","authors":["Vaishali Pal","Evangelos Kanoulas","Andrew Yates","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2410.03576v1.pdf","comment":"Accepted at EMNLP,2024"},{"id":"http://arxiv.org/abs/2410.03568v1","updated":"2024-10-04T16:18:29Z","published":"2024-10-04T16:18:29Z","title":"Towards Linguistically-Aware and Language-Independent Tokenization for\n  Large Language Models (LLMs)","summary":"  This paper presents a comprehensive study on the tokenization techniques\nemployed by state-of-the-art large language models (LLMs) and their\nimplications on the cost and availability of services across different\nlanguages, especially low resource languages. The analysis considers multiple\nLLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base\nembeddings), and DaVinci (employing r50k_base embeddings), as well as the\nwidely used BERT base tokenizer. The study evaluates the tokenization\nvariability observed across these models and investigates the challenges of\nlinguistic representation in subword tokenization. The research underscores the\nimportance of fostering linguistically-aware development practices, especially\nfor languages that are traditionally under-resourced. Moreover, this paper\nintroduces case studies that highlight the real-world implications of\ntokenization choices, particularly in the context of electronic health record\n(EHR) systems. This research aims to promote generalizable Internationalization\n(I18N) practices in the development of AI services in this domain and beyond,\nwith a strong emphasis on inclusivity, particularly for languages traditionally\nunderrepresented in AI applications.\n","authors":["Abrar Rahman","Garry Bowlin","Binit Mohanty","Sean McGunigal"],"pdf_url":"https://arxiv.org/pdf/2410.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00191v2","updated":"2024-10-04T16:11:20Z","published":"2024-06-28T19:02:59Z","title":"MetaKP: On-Demand Keyphrase Generation","summary":"  Traditional keyphrase prediction methods predict a single set of keyphrases\nper document, failing to cater to the diverse needs of users and downstream\napplications. To bridge the gap, we introduce on-demand keyphrase generation, a\nnovel paradigm that requires keyphrases that conform to specific high-level\ngoals or intents. For this task, we present MetaKP, a large-scale benchmark\ncomprising four datasets, 7500 documents, and 3760 goals across news and\nbiomedical domains with human-annotated keyphrases. Leveraging MetaKP, we\ndesign both supervised and unsupervised methods, including a multi-task\nfine-tuning approach and a self-consistency prompting method with large\nlanguage models. The results highlight the challenges of supervised\nfine-tuning, whose performance is not robust to distribution shifts. By\ncontrast, the proposed self-consistency prompting approach greatly improves the\nperformance of large language models, enabling GPT-4o to achieve 0.548 SemF1,\nsurpassing the performance of a fully fine-tuned BART-base model. Finally, we\ndemonstrate the potential of our method to serve as a general NLP\ninfrastructure, exemplified by its application in epidemic event detection from\nsocial media.\n","authors":["Di Wu","Xiaoxian Shen","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2407.00191v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.00741v2","updated":"2024-10-04T16:10:38Z","published":"2024-10-01T14:33:22Z","title":"VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP\n  Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has been widely studied and\napplied in numerous applications. However, the emphasis on brief summary texts\nduring pre-training prevents CLIP from understanding long descriptions. This\nissue is particularly acute regarding videos given that videos often contain\nabundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra\nLength) model, which aims to unleash the long-description understanding\ncapability of video CLIP models. Firstly, we establish an automatic data\ncollection system and gather a large-scale VILD pre-training dataset with VIdeo\nand Long-Description pairs. Then, we propose Text-similarity-guided Primary\nComponent Matching (TPCM) to better learn the distribution of feature space\nwhile expanding the long description capability. We also introduce two new\ntasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware\nDescription Ranking (HDR) for further understanding improvement. Finally, we\nconstruct a Long Video Description Ranking (LVDR) benchmark for evaluating the\nlong-description capability more comprehensively. Extensive experimental\nresults on widely-used text-video retrieval benchmarks with both short and long\ndescriptions and our LVDR benchmark can fully demonstrate the effectiveness of\nour method.\n","authors":["Jiapeng Wang","Chengyu Wang","Kunzhe Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2410.00741v2.pdf","comment":"EMNLP 2024 Main conference"},{"id":"http://arxiv.org/abs/2402.12821v3","updated":"2024-10-04T16:07:29Z","published":"2024-02-20T08:41:23Z","title":"Identifying Factual Inconsistencies in Summaries: Grounding LLM\n  Inference via Task Taxonomy","summary":"  Factual inconsistencies pose a significant hurdle for the faithful\nsummarization by generative models. While a major direction to enhance\ninconsistency detection is to derive stronger Natural Language Inference (NLI)\nmodels, we propose an orthogonal aspect that underscores the importance of\nincorporating task-specific taxonomy into the inference. To this end, we\nconsolidate key error types of inconsistent facts in summaries, and incorporate\nthem to facilitate both the zero-shot and supervised paradigms of LLMs.\nExtensive experiments on ten datasets of five distinct domains suggest that,\nzero-shot LLM inference could benefit from the explicit solution space depicted\nby the error type taxonomy, and achieves state-of-the-art performance overall,\nsurpassing specialized non-LLM baselines, as well as recent LLM baselines. We\nfurther distill models that fuse the taxonomy into parameters through our\ndesigned prompt completions and supervised training strategies, efficiently\nsubstituting state-of-the-art zero-shot inference with much larger LLMs.\n","authors":["Liyan Xu","Zhenlin Su","Mo Yu","Jin Xu","Jinho D. Choi","Jie Zhou","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2402.12821v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03553v1","updated":"2024-10-04T16:02:50Z","published":"2024-10-04T16:02:50Z","title":"Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding","summary":"  Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.\n","authors":["Wei Wu","Chao Wang","Liyi Chen","Mingze Yin","Yiheng Zhu","Kun Fu","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03545v1","updated":"2024-10-04T15:58:15Z","published":"2024-10-04T15:58:15Z","title":"Enhancing Data Quality through Simple De-duplication: Navigating\n  Responsible Computational Social Science Research","summary":"  Research in natural language processing (NLP) for Computational Social\nScience (CSS) heavily relies on data from social media platforms. This data\nplays a crucial role in the development of models for analysing\nsocio-linguistic phenomena within online communities. In this work, we conduct\nan in-depth examination of 20 datasets extensively used in NLP for CSS to\ncomprehensively examine data quality. Our analysis reveals that social media\ndatasets exhibit varying levels of data duplication. Consequently, this gives\nrise to challenges like label inconsistencies and data leakage, compromising\nthe reliability of models. Our findings also suggest that data duplication has\nan impact on the current claims of state-of-the-art performance, potentially\nleading to an overestimation of model effectiveness in real-world scenarios.\nFinally, we propose new protocols and best practices for improving dataset\ndevelopment from social media data and its usage.\n","authors":["Yida Mu","Mali Jin","Xingyi Song","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2410.03545v1.pdf","comment":"Accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03543v1","updated":"2024-10-04T15:57:58Z","published":"2024-10-04T15:57:58Z","title":"Re-examining Sexism and Misogyny Classification with Annotator Attitudes","summary":"  Gender-Based Violence (GBV) is an increasing problem online, but existing\ndatasets fail to capture the plurality of possible annotator perspectives or\nensure the representation of affected groups. We revisit two important stages\nin the moderation pipeline for GBV: (1) manual data labelling; and (2)\nautomated classification. For (1), we examine two datasets to investigate the\nrelationship between annotator identities and attitudes and the responses they\ngive to two GBV labelling tasks. To this end, we collect demographic and\nattitudinal information from crowd-sourced annotators using three validated\nsurveys from Social Psychology. We find that higher Right Wing Authoritarianism\nscores are associated with a higher propensity to label text as sexist, while\nfor Social Dominance Orientation and Neosexist Attitudes, higher scores are\nassociated with a negative tendency to do so. For (2), we conduct\nclassification experiments using Large Language Models and five prompting\nstrategies, including infusing prompts with annotator information. We find: (i)\nannotator attitudes affect the ability of classifiers to predict their labels;\n(ii) including attitudinal information can boost performance when we use\nwell-structured brief annotator descriptions; and (iii) models struggle to\nreflect the increased complexity and imbalanced classes of the new label sets.\n","authors":["Aiqi Jiang","Nikolas Vitsakis","Tanvi Dinkar","Gavin Abercrombie","Ioannis Konstas"],"pdf_url":"https://arxiv.org/pdf/2410.03543v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03531v1","updated":"2024-10-04T15:52:29Z","published":"2024-10-04T15:52:29Z","title":"MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale\n  Extraction","summary":"  Unsupervised rationale extraction aims to extract text snippets to support\nmodel predictions without explicit rationale annotation. Researchers have made\nmany efforts to solve this task. Previous works often encode each aspect\nindependently, which may limit their ability to capture meaningful internal\ncorrelations between aspects. While there has been significant work on\nmitigating spurious correlations, our approach focuses on leveraging the\nbeneficial internal correlations to improve multi-aspect rationale extraction.\nIn this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain\nand predict multiple aspects simultaneously. Concretely, we propose a\nMulti-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to\nencode multiple text chunks simultaneously. Furthermore, multiple special\ntokens are prepended in front of the text with each corresponding to one\ncertain aspect. Finally, multi-task training is deployed to reduce the training\noverhead. Experimental results on two unsupervised rationale extraction\nbenchmarks show that MARE achieves state-of-the-art performance. Ablation\nstudies further demonstrate the effectiveness of our method. Our codes have\nbeen available at https://github.com/CSU-NLP-Group/MARE.\n","authors":["Han Jiang","Junwen Duan","Zhe Qu","Jianxin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03531v1.pdf","comment":"Accepted in EMNLP2024(Main) conference"},{"id":"http://arxiv.org/abs/2410.03529v1","updated":"2024-10-04T15:50:10Z","published":"2024-10-04T15:50:10Z","title":"No Need to Talk: Asynchronous Mixture of Language Models","summary":"  We introduce SmallTalk LM, an innovative method for training a mixture of\nlanguage models in an almost asynchronous manner. Each model of the mixture\nspecializes in distinct parts of the data distribution, without the need of\nhigh-bandwidth communication between the nodes training each model. At\ninference, a lightweight router directs a given sequence to a single expert,\naccording to a short prefix. This inference scheme naturally uses a fraction of\nthe parameters from the overall mixture model. Our experiments on language\nmodeling demonstrate tha SmallTalk LM achieves significantly lower perplexity\nthan dense model baselines for the same total training FLOPs and an almost\nidentical inference cost. Finally, in our downstream evaluations we outperform\nthe dense baseline on $75\\%$ of the tasks.\n","authors":["Anastasiia Filippova","Angelos Katharopoulos","David Grangier","Ronan Collobert"],"pdf_url":"https://arxiv.org/pdf/2410.03529v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.03524v1","updated":"2024-10-04T15:44:47Z","published":"2024-10-04T15:44:47Z","title":"Steering Large Language Models between Code Execution and Textual\n  Reasoning","summary":"  While a lot of recent research focuses on enhancing the textual reasoning\ncapabilities of Large Language Models (LLMs) by optimizing the multi-agent\nframework or reasoning chains, several benchmark tasks can be solved with 100%\nsuccess through direct coding, which is more scalable and avoids the\ncomputational overhead associated with textual iterating and searching. Textual\nreasoning has inherent limitations in solving tasks with challenges in math,\nlogics, optimization, and searching, which is unlikely to be solved by simply\nscaling up the model and data size. The recently released OpenAI GPT Code\nInterpreter and multi-agent frameworks such as AutoGen have demonstrated\nremarkable proficiency of integrating code generation and execution to solve\ncomplex tasks using LLMs. However, based on our experiments on 7 existing\npopular methods for steering code/text generation in both single- and\nmulti-turn settings with 14 tasks and 6 types of LLMs (including the new\nO1-preview), currently there is no optimal method to correctly steer LLMs to\nwrite code when needed. We discover some interesting patterns on when models\nuse code vs. textual reasoning with the evolution to task complexity and model\nsizes, which even result in an astonishingly inverse scaling law. We also\ndiscover that results from LLM written code are not always better than using\ntextual reasoning, even if the task could be solved through code. To mitigate\nthe above issues, we propose three methods to better steer LLM code/text\ngeneration and achieve a notable improvement. The costs of token lengths and\nruntime are thoroughly discussed for all the methods. We believe the problem of\nsteering LLM code/text generation is critical for future research and has much\nspace for further improvement. Project Page, Datasets, and Codes are available\nat https://yongchao98.github.io/CodeSteer/.\n","authors":["Yongchao Chen","Harsh Jhamtani","Srinagesh Sharma","Chuchu Fan","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03524v1.pdf","comment":"32 pages, 12 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.05493v2","updated":"2024-10-04T15:34:48Z","published":"2024-03-08T18:04:03Z","title":"To Err Is Human, but Llamas Can Learn It Too","summary":"  This study explores enhancing grammatical error correction (GEC) through\nartificial error generation (AEG) using language models (LMs). Specifically, we\nfine-tune Llama 2-based LMs for error generation and find that this approach\nyields synthetic errors akin to human errors. Next, we train GEC Llama models\nwith the help of these artificial errors and outperform previous\nstate-of-the-art error correction models, with gains ranging between 0.8 and 6\nF0.5 points across all tested languages (German, Ukrainian, and Estonian).\nMoreover, we demonstrate that generating errors by fine-tuning smaller\nsequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and\nGPT-4) also results in synthetic errors beneficially affecting error generation\nmodels.\n","authors":["Agnes Luhtaru","Taido Purason","Martin Vainikko","Maksym Del","Mark Fishel"],"pdf_url":"https://arxiv.org/pdf/2403.05493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17600v2","updated":"2024-10-04T15:22:29Z","published":"2024-06-25T14:42:17Z","title":"\"Seeing the Big through the Small\": Can LLMs Approximate Human Judgment\n  Distributions on NLI from a Few Explanations?","summary":"  Human label variation (HLV) is a valuable source of information that arises\nwhen multiple human annotators provide different labels for valid reasons. In\nNatural Language Inference (NLI) earlier approaches to capturing HLV involve\neither collecting annotations from many crowd workers to represent human\njudgment distribution (HJD) or use expert linguists to provide detailed\nexplanations for their chosen labels. While the former method provides denser\nHJD information, obtaining it is resource-intensive. In contrast, the latter\noffers richer textual information but it is challenging to scale up to many\nhuman judges. Besides, large language models (LLMs) are increasingly used as\nevaluators (\"LLM judges\") but with mixed results, and few works aim to study\nHJDs. This study proposes to exploit LLMs to approximate HJDs using a small\nnumber of expert labels and explanations. Our experiments show that a few\nexplanations significantly improve LLMs' ability to approximate HJDs with and\nwithout explicit labels, thereby providing a solution to scale up annotations\nfor HJD. However, fine-tuning smaller soft-label aware models with the\nLLM-generated model judgment distributions (MJDs) presents partially\ninconsistent results: while similar in distance, their resulting fine-tuned\nmodels and visualized distributions differ substantially. We show the\nimportance of complementing instance-level distance measures with a\nglobal-level shape metric and visualization to more effectively evaluate MJDs\nagainst human judgment distributions.\n","authors":["Beiduo Chen","Xinpeng Wang","Siyao Peng","Robert Litschko","Anna Korhonen","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2406.17600v2.pdf","comment":"Accepted by EMNLP 2024 Findings, 24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.03502v1","updated":"2024-10-04T15:15:36Z","published":"2024-10-04T15:15:36Z","title":"CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical\n  Large Language Models in Clinical Scenarios","summary":"  With the proliferation of Large Language Models (LLMs) in diverse domains,\nthere is a particular need for unified evaluation standards in clinical medical\nscenarios, where models need to be examined very thoroughly. We present\nCliMedBench, a comprehensive benchmark with 14 expert-guided core clinical\nscenarios specifically designed to assess the medical ability of LLMs across 7\npivot dimensions. It comprises 33,735 questions derived from real-world medical\nreports of top-tier tertiary hospitals and authentic examination exercises. The\nreliability of this benchmark has been confirmed in several ways. Subsequent\nexperiments with existing LLMs have led to the following findings: (i) Chinese\nmedical LLMs underperform on this benchmark, especially where medical reasoning\nand factual consistency are vital, underscoring the need for advances in\nclinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs\ndemonstrate substantial potential in medical clinics, while the limited input\ncapacity of many medical LLMs hinders their practical use. These findings\nreveal both the strengths and limitations of LLMs in clinical scenarios and\noffer critical insights for medical research.\n","authors":["Zetian Ouyang","Yishuai Qiu","Linlin Wang","Gerard de Melo","Ya Zhang","Yanfeng Wang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2410.03502v1.pdf","comment":"accepted by ENMLP-2024"},{"id":"http://arxiv.org/abs/2406.15352v2","updated":"2024-10-04T15:15:26Z","published":"2024-06-21T17:59:51Z","title":"A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student\n  Feedback to Make Mnemonic Learning Stick","summary":"  Keyword mnemonics are memorable explanations that link new terms to simpler\nkeywords. Prior work generates mnemonics for students, but they do not train\nmodels using mnemonics students prefer and aid learning. We build SMART, a\nmnemonic generator trained on feedback from real students learning new terms.\nTo train SMART, we first fine-tune LLaMA-2 on a curated set of user-written\nmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics\ngenerated by SMART in a flashcard app to find preferences on mnemonics students\nfavor. We gather 2684 preferences from 45 students across two types: expressed\n(inferred from ratings) and observed (inferred from student learning), yielding\nthree key findings. First, expressed and observed preferences disagree; what\nstudents think is helpful does not always capture what is truly helpful.\nSecond, Bayesian models can synthesize complementary data from multiple\npreference types into a single effectiveness signal. SMART is tuned via Direct\nPreference Optimization on this signal, which resolves ties and missing labels\nin the typical method of pairwise comparisons, augmenting data for LLM output\nquality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much\nlower deployment costs, showing the utility of capturing diverse student\nfeedback to align LLMs in education.\n","authors":["Nishant Balepur","Matthew Shu","Alexander Hoyle","Alison Robey","Shi Feng","Seraphina Goldfarb-Tarrant","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2406.15352v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.05673v3","updated":"2024-10-04T15:14:55Z","published":"2024-06-09T07:06:58Z","title":"Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples","summary":"  The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reinforcement learning aims to find limited highest-reward\nsolutions while neglecting the solution diversity. To fill this gap, we propose\nFlow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning method\naimed at improving reasoning quality and diversity with minimal data. FoR\nformulates multi-step LLM reasoning as a Markovian flow on a DAG-structured\nreasoning graph. This formulation allows us to incorporate and adapt principled\nGFlowNet approaches, for finetuning LLMs to sample diverse reasoning paths with\nprobabilities proportional to the (unnormalized) reward of target problems.\nExtensive experiments show that, with limited training examples (e.g., 15\nexamples), FoR enables the discovery of diverse, creative, high-quality\nsolutions, greatly outperforming a wide range of existing inference and\ntraining methods across five challenging puzzle-solving tasks, including\nBlocksWorld (embodied reasoning), Game24 (math puzzle solving), Rubik's Cube\n(spatial reasoning), 1D-ARC (abstraction reasoning), and PrOntoQA (logical\nreasoning). Code is available at https://github.com/Yu-Fangxu/FoR.\n","authors":["Fangxu Yu","Lai Jiang","Haoqiang Kang","Shibo Hao","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2406.05673v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14393v3","updated":"2024-10-04T15:10:36Z","published":"2024-06-20T15:12:27Z","title":"Jailbreaking as a Reward Misspecification Problem","summary":"  The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.\n","authors":["Zhihui Xie","Jiahui Gao","Lei Li","Zhenguo Li","Qi Liu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2406.14393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12291v2","updated":"2024-10-04T15:08:14Z","published":"2024-02-19T17:05:29Z","title":"KARL: Knowledge-Aware Retrieval and Representations aid Retention and\n  Learning in Students","summary":"  Flashcard schedulers rely on 1) student models to predict the flashcards a\nstudent knows; and 2) teaching policies to pick which cards to show next via\nthese predictions. Prior student models, however, just use study data like the\nstudent's past responses, ignoring the text on cards. We propose content-aware\nscheduling, the first schedulers exploiting flashcard content. To give the\nfirst evidence that such schedulers enhance student learning, we build KARL, a\nsimple but effective content-aware student model employing deep knowledge\ntracing (DKT), retrieval, and BERT to predict student recall. We train KARL by\ncollecting a new dataset of 123,143 study logs on diverse trivia questions.\nKARL bests existing student models in AUC and calibration error. To ensure our\nimproved predictions lead to better student learning, we create a novel\ndelta-based teaching policy to deploy KARL online. Based on 32 study paths from\n27 users, KARL improves learning efficiency over SOTA, showing KARL's strength\nand encouraging researchers to look beyond historical study data to fully\ncapture student abilities.\n","authors":["Matthew Shu","Nishant Balepur","Shi Feng","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2402.12291v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03492v1","updated":"2024-10-04T15:04:28Z","published":"2024-10-04T15:04:28Z","title":"Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM\n  Benchmark Scores","summary":"  Large language models (LLMs) are stochastic, and not all models give\ndeterministic answers, even when setting temperature to zero with a fixed\nrandom seed. However, few benchmark studies attempt to quantify uncertainty,\npartly due to the time and cost of repeated experiments. We use benchmarks\ndesigned for testing LLMs' capacity to reason about cardinal directions to\nexplore the impact of experimental repeats on mean score and prediction\ninterval. We suggest a simple method for cost-effectively quantifying the\nuncertainty of a benchmark score and make recommendations concerning\nreproducible LLM evaluation.\n","authors":["Robert E. Blackwell","Jon Barry","Anthony G. Cohn"],"pdf_url":"https://arxiv.org/pdf/2410.03492v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.02338v2","updated":"2024-10-04T14:59:04Z","published":"2024-10-03T09:48:09Z","title":"How Much Can RAG Help the Reasoning of LLM?","summary":"  Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.\n","authors":["Jingyu Liu","Jiaen Lin","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16741v2","updated":"2024-10-04T14:54:08Z","published":"2024-07-23T17:50:43Z","title":"OpenHands: An Open Platform for AI Software Developers as Generalist\n  Agents","summary":"  Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the\ndevelopment of powerful and flexible AI agents that interact with the world in\nsimilar ways to those of a human developer: by writing code, interacting with a\ncommand line, and browsing the web. We describe how the platform allows for the\nimplementation of new agents, safe interaction with sandboxed environments for\ncode execution, coordination between multiple agents, and incorporation of\nevaluation benchmarks. Based on our currently incorporated benchmarks, we\nperform an evaluation of agents over 15 challenging tasks, including software\nengineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others.\nReleased under the permissive MIT license, OpenHands is a community project\nspanning academia and industry with more than 2.1K contributions from over 188\ncontributors.\n","authors":["Xingyao Wang","Boxuan Li","Yufan Song","Frank F. Xu","Xiangru Tang","Mingchen Zhuge","Jiayi Pan","Yueqi Song","Bowen Li","Jaskirat Singh","Hoang H. Tran","Fuqiang Li","Ren Ma","Mingzhang Zheng","Bill Qian","Yanjun Shao","Niklas Muennighoff","Yizhe Zhang","Binyuan Hui","Junyang Lin","Robert Brennan","Hao Peng","Heng Ji","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2407.16741v2.pdf","comment":"Code: https://github.com/All-Hands-AI/OpenHands"},{"id":"http://arxiv.org/abs/2407.17125v3","updated":"2024-10-04T14:36:36Z","published":"2024-07-24T09:48:48Z","title":"To Know or Not To Know? Analyzing Self-Consistency of Large Language\n  Models under Ambiguity","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. This paper focuses\non entity type ambiguity, analyzing the proficiency and consistency of\nstate-of-the-art LLMs in applying factual knowledge when prompted with\nambiguous entities. To do so, we propose an evaluation protocol that\ndisentangles knowing from applying knowledge, and test state-of-the-art LLMs on\n49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing\nthe correct entity reading, achieving an average accuracy of only 85%, and as\nlow as 75% with underspecified prompts. The results also reveal systematic\ndiscrepancies in LLM behavior, showing that while the models may possess\nknowledge, they struggle to apply it consistently, exhibit biases toward\npreferred readings, and display self-inconsistencies. This highlights the need\nto address entity ambiguity in the future for more trustworthy LLMs.\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2408.10902v3","updated":"2024-10-04T14:32:01Z","published":"2024-08-20T14:45:23Z","title":"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs","summary":"  Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.\n","authors":["John Mendona","Isabel Trancoso","Alon Lavie"],"pdf_url":"https://arxiv.org/pdf/2408.10902v3.pdf","comment":"Accepted to EMNLP2024 (findings)"},{"id":"http://arxiv.org/abs/2410.03466v1","updated":"2024-10-04T14:31:37Z","published":"2024-10-04T14:31:37Z","title":"Is Safer Better? The Impact of Guardrails on the Argumentative Strength\n  of LLMs in Hate Speech Countering","summary":"  The potential effectiveness of counterspeech as a hate speech mitigation\nstrategy is attracting increasing interest in the NLG research community,\nparticularly towards the task of automatically producing it. However,\nautomatically generated responses often lack the argumentative richness which\ncharacterises expert-produced counterspeech. In this work, we focus on two\naspects of counterspeech generation to produce more cogent responses. First, by\ninvestigating the tension between helpfulness and harmlessness of LLMs, we test\nwhether the presence of safety guardrails hinders the quality of the\ngenerations. Secondly, we assess whether attacking a specific component of the\nhate speech results in a more effective argumentative strategy to fight online\nhate. By conducting an extensive human and automatic evaluation, we show how\nthe presence of safety guardrails can be detrimental also to a task that\ninherently aims at fostering positive social interactions. Moreover, our\nresults show that attacking a specific component of the hate speech, and in\nparticular its implicit negative stereotype and its hateful parts, leads to\nhigher-quality generations.\n","authors":["Helena Bonaldi","Greta Damo","Nicols Benjamn Ocampo","Elena Cabrio","Serena Villata","Marco Guerini"],"pdf_url":"https://arxiv.org/pdf/2410.03466v1.pdf","comment":"To appear in Proceedings of the 2024 Conference on Empirical Methods\n  in Natural Language Processing (long paper)"},{"id":"http://arxiv.org/abs/2410.03461v1","updated":"2024-10-04T14:21:27Z","published":"2024-10-04T14:21:27Z","title":"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval Augmented Generation","summary":"  While retrieval augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. One common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10 % of their computational cost.\n","authors":["Tobias Leemann","Periklis Petridis","Giuseppe Vietri","Dionysis Manousakas","Aaron Roth","Sergul Aydore"],"pdf_url":"https://arxiv.org/pdf/2410.03461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17512v4","updated":"2024-10-04T14:19:27Z","published":"2024-02-27T13:54:48Z","title":"Latte: Latent Attention for Linear Time Transformers","summary":"  The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.\n","authors":["Rares Dolga","Lucas Maystre","Marius Cobzarenco","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.17512v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03458v1","updated":"2024-10-04T14:17:56Z","published":"2024-10-04T14:17:56Z","title":"Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges","summary":"  Vietnamese, a low-resource language, is typically categorized into three\nprimary dialect groups that belong to Northern, Central, and Southern Vietnam.\nHowever, each province within these regions exhibits its own distinct\npronunciation variations. Despite the existence of various speech recognition\ndatasets, none of them has provided a fine-grained classification of the 63\ndialects specific to individual provinces of Vietnam. To address this gap, we\nintroduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive\ndataset capturing the rich diversity of 63 provincial dialects spoken across\nVietnam. Our dataset comprises 102.56 hours of audio, consisting of\napproximately 19,000 utterances, and the associated transcripts contain over\n1.2 million words. To provide benchmarks and simultaneously demonstrate the\nchallenges of our dataset, we fine-tune state-of-the-art pre-trained models for\ntwo downstream tasks: (1) Dialect identification and (2) Speech recognition.\nThe empirical results suggest two implications including the influence of\ngeographical factors on dialects, and the constraints of current approaches in\nspeech recognition tasks involving multi-dialect speech data. Our dataset is\navailable for research purposes.\n","authors":["Nguyen Van Dinh","Thanh Chi Dang","Luan Thanh Nguyen","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.03458v1.pdf","comment":"Main EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03457v1","updated":"2024-10-04T14:15:56Z","published":"2024-10-04T14:15:56Z","title":"CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies\n  Written by LLM-Assisted Crowds","summary":"  Detecting logical fallacies in texts can help users spot argument flaws, but\nautomating this detection is not easy. Manually annotating fallacies in\nlarge-scale, real-world text data to create datasets for developing and\nvalidating detection models is costly. This paper introduces CoCoLoFa, the\nlargest known logical fallacy dataset, containing 7,706 comments for 648 news\narticles, with each comment labeled for fallacy presence and type. We recruited\n143 crowd workers to write comments embodying specific fallacy types (e.g.,\nslippery slope) in response to news articles. Recognizing the complexity of\nthis writing task, we built an LLM-powered assistant into the workers'\ninterface to aid in drafting and refining their comments. Experts rated the\nwriting quality and labeling validity of CoCoLoFa as high and reliable.\nBERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy\ndetection (F1=0.86) and classification (F1=0.87) performance on its test set,\noutperforming the state-of-the-art LLMs. Our work shows that combining\ncrowdsourcing and LLMs enables us to more effectively construct datasets for\ncomplex linguistic phenomena that crowd workers find challenging to produce on\ntheir own.\n","authors":["Min-Hsuan Yeh","Ruyuan Wan","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03457v1.pdf","comment":"In Proceedings of the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2024)"},{"id":"http://arxiv.org/abs/2410.03447v1","updated":"2024-10-04T14:09:05Z","published":"2024-10-04T14:09:05Z","title":"How Language Models Prioritize Contextual Grammatical Cues?","summary":"  Transformer-based language models have shown an excellent ability to\neffectively capture and utilize contextual information. Although various\nanalysis techniques have been used to quantify and trace the contribution of\nsingle contextual cues to a target task such as subject-verb agreement or\ncoreference resolution, scenarios in which multiple relevant cues are available\nin the context remain underexplored. In this paper, we investigate how language\nmodels handle gender agreement when multiple gender cue words are present, each\ncapable of independently disambiguating a target gender pronoun. We analyze two\nwidely used Transformer-based models: BERT, an encoder-based, and GPT-2, a\ndecoder-based model. Our analysis employs two complementary approaches: context\nmixing analysis, which tracks information flow within the model, and a variant\nof activation patching, which measures the impact of cues on the model's\nprediction. We find that BERT tends to prioritize the first cue in the context\nto form both the target word representations and the model's prediction, while\nGPT-2 relies more on the final cue. Our findings reveal striking differences in\nhow encoder-based and decoder-based models prioritize and use contextual\ninformation for their predictions.\n","authors":["Hamidreza Amirzadeh","Afra Alishahi","Hosein Mohebbi"],"pdf_url":"https://arxiv.org/pdf/2410.03447v1.pdf","comment":"Accepted to BlackboxNLP 2024"},{"id":"http://arxiv.org/abs/2410.03446v1","updated":"2024-10-04T14:08:02Z","published":"2024-10-04T14:08:02Z","title":"On Uncertainty In Natural Language Processing","summary":"  The last decade in deep learning has brought on increasingly capable systems\nthat are deployed on a wide variety of applications. In natural language\nprocessing, the field has been transformed by a number of breakthroughs\nincluding large language models, which are used in increasingly many\nuser-facing applications. In order to reap the benefits of this technology and\nreduce potential harms, it is important to quantify the reliability of model\npredictions and the uncertainties that shroud their development.\n  This thesis studies how uncertainty in natural language processing can be\ncharacterized from a linguistic, statistical and neural perspective, and how it\ncan be reduced and quantified through the design of the experimental pipeline.\nWe further explore uncertainty quantification in modeling by theoretically and\nempirically investigating the effect of inductive model biases in text\nclassification tasks. The corresponding experiments include data for three\ndifferent languages (Danish, English and Finnish) and tasks as well as a large\nset of different uncertainty quantification approaches. Additionally, we\npropose a method for calibrated sampling in natural language generation based\non non-exchangeable conformal prediction, which provides tighter token sets\nwith better coverage of the actual continuation. Lastly, we develop an approach\nto quantify confidence in large black-box language models using auxiliary\npredictors, where the confidence is predicted from the input to and generated\noutput text of the target model alone.\n","authors":["Dennis Ulmer"],"pdf_url":"https://arxiv.org/pdf/2410.03446v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2410.03440v1","updated":"2024-10-04T13:53:33Z","published":"2024-10-04T13:53:33Z","title":"Exploring the Benefit of Activation Sparsity in Pre-training","summary":"  Pre-trained Transformers inherently possess the characteristic of sparse\nactivation, where only a small fraction of the neurons are activated for each\ntoken. While sparse activation has been explored through post-training methods,\nits potential in pre-training remains untapped. In this work, we first study\nhow activation properties change during pre-training. Our examination reveals\nthat Transformers exhibit sparse activation throughout the majority of the\npre-training process while the activation correlation keeps evolving as\ntraining progresses. Leveraging this observation, we propose Switchable\nSparse-Dense Learning (SSD). SSD adaptively switches between the\nMixtures-of-Experts (MoE) based sparse training and the conventional dense\ntraining during the pre-training process, leveraging the efficiency of sparse\ntraining and avoiding the static activation correlation of sparse training.\nCompared to dense training, SSD achieves comparable performance with identical\nmodel size and reduces pre-training costs. Moreover, the models trained with\nSSD can be directly used as MoE models for sparse inference and achieve the\nsame performance as dense models with up to $2\\times$ faster inference speed.\nCodes are available at https://github.com/thunlp/moefication.\n","authors":["Zhengyan Zhang","Chaojun Xiao","Qiujieli Qin","Yankai Lin","Zhiyuan Zeng","Xu Han","Zhiyuan Liu","Ruobing Xie","Maosong Sun","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.03440v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2410.03439v1","updated":"2024-10-04T13:52:32Z","published":"2024-10-04T13:52:32Z","title":"ToolGen: Unified Tool Retrieval and Calling via Generation","summary":"  As large language models (LLMs) advance, their inability to autonomously\nexecute tasks by directly interacting with external tools remains a critical\nlimitation. Traditional methods rely on inputting tool descriptions as context,\nwhich is constrained by context length and requires separate, often\ninefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that\nintegrates tool knowledge directly into the LLM's parameters by representing\neach tool as a unique token. This enables the LLM to generate tool calls and\narguments as part of its next token prediction capabilities, seamlessly\nblending tool invocation with language generation. Our framework allows the LLM\nto access and utilize a vast amount of tools with no additional retrieval step,\nsignificantly enhancing both performance and scalability. Experimental results\nwith over 47,000 tools show that ToolGen not only achieves superior results in\nboth tool retrieval and autonomous task completion but also sets the stage for\na new era of AI agents that can adapt to tools across diverse domains. By\nfundamentally transforming tool retrieval into a generative process, ToolGen\npaves the way for more versatile, efficient, and autonomous AI systems. ToolGen\nenables end-to-end tool learning and opens opportunities for integration with\nother advanced techniques such as chain-of-thought and reinforcement learning,\nthereby expanding the practical capabilities of LLMs.\n","authors":["Renxi Wang","Xudong Han","Lei Ji","Shu Wang","Timothy Baldwin","Haonan Li"],"pdf_url":"https://arxiv.org/pdf/2410.03439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03435v1","updated":"2024-10-04T13:51:19Z","published":"2024-10-04T13:51:19Z","title":"A General Framework for Producing Interpretable Semantic Text Embeddings","summary":"  Semantic text embedding is essential to many tasks in Natural Language\nProcessing (NLP). While black-box models are capable of generating high-quality\nembeddings, their lack of interpretability limits their use in tasks that\ndemand transparency. Recent approaches have improved interpretability by\nleveraging domain-expert-crafted or LLM-generated questions, but these methods\nrely heavily on expert input or well-prompt design, which restricts their\ngeneralizability and ability to generate discriminative questions across a wide\nrange of tasks. To address these challenges, we introduce \\algo{CQG-MBQA}\n(Contrastive Question Generation - Multi-task Binary Question Answering), a\ngeneral framework for producing interpretable semantic text embeddings across\ndiverse tasks. Our framework systematically generates highly discriminative,\nlow cognitive load yes/no questions through the \\algo{CQG} method and answers\nthem efficiently with the \\algo{MBQA} model, resulting in interpretable\nembeddings in a cost-effective manner. We validate the effectiveness and\ninterpretability of \\algo{CQG-MBQA} through extensive experiments and ablation\nstudies, demonstrating that it delivers embedding quality comparable to many\nadvanced black-box models while maintaining inherently interpretability.\nAdditionally, \\algo{CQG-MBQA} outperforms other interpretable text embedding\nmethods across various downstream tasks.\n","authors":["Yiqun Sun","Qiang Huang","Yixuan Tang","Anthony K. H. Tung","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.03435v1.pdf","comment":"19 pages, 5 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2410.03430v1","updated":"2024-10-04T13:40:15Z","published":"2024-10-04T13:40:15Z","title":"Images Speak Volumes: User-Centric Assessment of Image Generation for\n  Accessible Communication","summary":"  Explanatory images play a pivotal role in accessible and easy-to-read (E2R)\ntexts. However, the images available in online databases are not tailored\ntoward the respective texts, and the creation of customized images is\nexpensive. In this large-scale study, we investigated whether text-to-image\ngeneration models can close this gap by providing customizable images quickly\nand easily. We benchmarked seven, four open- and three closed-source, image\ngeneration models and provide an extensive evaluation of the resulting images.\nIn addition, we performed a user study with people from the E2R target group to\nexamine whether the images met their requirements. We find that some of the\nmodels show remarkable performance, but none of the models are ready to be used\nat a larger scale without human supervision. Our research is an important step\ntoward facilitating the creation of accessible information for E2R creators and\ntailoring accessible images to the target group's needs.\n","authors":["Miriam Anschtz","Tringa Sylaj","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2410.03430v1.pdf","comment":"To be published at TSAR workshop 2024\n  (https://tsar-workshop.github.io/)"},{"id":"http://arxiv.org/abs/2410.03429v1","updated":"2024-10-04T13:39:21Z","published":"2024-10-04T13:39:21Z","title":"How Hard is this Test Set? NLI Characterization by Exploiting Training\n  Dynamics","summary":"  Natural Language Inference (NLI) evaluation is crucial for assessing language\nunderstanding models; however, popular datasets suffer from systematic spurious\ncorrelations that artificially inflate actual model performance. To address\nthis, we propose a method for the automated creation of a challenging test set\nwithout relying on the manual construction of artificial and unrealistic\nexamples. We categorize the test set of popular NLI datasets into three\ndifficulty levels by leveraging methods that exploit training dynamics. This\ncategorization significantly reduces spurious correlation measures, with\nexamples labeled as having the highest difficulty showing markedly decreased\nperformance and encompassing more realistic and diverse linguistic phenomena.\nWhen our characterization method is applied to the training set, models trained\nwith only a fraction of the data achieve comparable performance to those\ntrained on the full dataset, surpassing other dataset characterization\ntechniques. Our research addresses limitations in NLI dataset construction,\nproviding a more authentic evaluation of model performance with implications\nfor diverse NLU applications.\n","authors":["Adrian Cosma","Stefan Ruseti","Mihai Dascalu","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2410.03429v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.15925v2","updated":"2024-10-04T13:37:12Z","published":"2024-02-24T23:01:21Z","title":"MultiContrievers: Analysis of Dense Retrieval Representations","summary":"  Dense retrievers compress source documents into (possibly lossy) vector\nrepresentations, yet there is little analysis of what information is lost\nversus preserved, and how it affects downstream tasks. We conduct the first\nanalysis of the information captured by dense retrievers compared to the\nlanguage models they are based on (e.g., BERT versus Contriever). We use 25\nMultiBert checkpoints as randomized initialisations to train MultiContrievers,\na set of 25 contriever models. We test whether specific pieces of information\n-- such as gender and occupation -- can be extracted from contriever vectors of\nwikipedia-like documents. We measure this extractability via information\ntheoretic probing. We then examine the relationship of extractability to\nperformance and gender bias, as well as the sensitivity of these results to\nmany random initialisations and data shuffles. We find that (1) contriever\nmodels have significantly increased extractability, but extractability usually\ncorrelates poorly with benchmark performance 2) gender bias is present, but is\nnot caused by the contriever representations 3) there is high sensitivity to\nboth random initialisation and to data shuffle, suggesting that future\nretrieval research should test across a wider spread of both.\n","authors":["Seraphina Goldfarb-Tarrant","Pedro Rodriguez","Jane Dwivedi-Yu","Patrick Lewis"],"pdf_url":"https://arxiv.org/pdf/2402.15925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03421v1","updated":"2024-10-04T13:31:09Z","published":"2024-10-04T13:31:09Z","title":"One2set + Large Language Model: Best Partners for Keyphrase Generation","summary":"  Keyphrase generation (KPG) aims to automatically generate a collection of\nphrases representing the core concepts of a given document. The dominant\nparadigms in KPG include one2seq and one2set. Recently, there has been\nincreasing interest in applying large language models (LLMs) to KPG. Our\npreliminary experiments reveal that it is challenging for a single model to\nexcel in both recall and precision. Further analysis shows that: 1) the one2set\nparadigm owns the advantage of high recall, but suffers from improper\nassignments of supervision signals during training; 2) LLMs are powerful in\nkeyphrase selection, but existing selection methods often make redundant\nselections. Given these observations, we introduce a generate-then-select\nframework decomposing KPG into two steps, where we adopt a one2set-based model\nas generator to produce candidates and then use an LLM as selector to select\nkeyphrases from these candidates. Particularly, we make two important\nimprovements on our generator and selector: 1) we design an Optimal\nTransport-based assignment strategy to address the above improper assignments;\n2) we model the keyphrase selection as a sequence labeling task to alleviate\nredundant selections. Experimental results on multiple benchmark datasets show\nthat our framework significantly surpasses state-of-the-art models, especially\nin absent keyphrase prediction.\n","authors":["Liangying Shao","Liang Zhang","Minlong Peng","Guoqi Ma","Hao Yue","Mingming Sun","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2410.03421v1.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.02713v2","updated":"2024-10-04T13:29:09Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v2.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02560v2","updated":"2024-10-04T13:25:38Z","published":"2024-10-03T15:04:27Z","title":"Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition","summary":"  For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.\n","authors":["Olga Iakovenko","Ivan Bondarenko"],"pdf_url":"https://arxiv.org/pdf/2410.02560v2.pdf","comment":"Theory and Practice of Natural Computing 9th International\n  Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9"},{"id":"http://arxiv.org/abs/2410.03415v1","updated":"2024-10-04T13:25:32Z","published":"2024-10-04T13:25:32Z","title":"Surgical, Cheap, and Flexible: Mitigating False Refusal in Language\n  Models via Single Vector Ablation","summary":"  Training a language model to be both helpful and harmless requires careful\ncalibration of refusal behaviours: Models should refuse to follow malicious\ninstructions or give harmful advice (e.g. \"how do I kill someone?\"), but they\nshould not refuse safe requests, even if they superficially resemble unsafe\nones (e.g. \"how do I kill a Python process?\"). Avoiding such false refusal, as\nprior work has shown, is challenging even for highly-capable language models.\nIn this paper, we propose a simple and surgical method for mitigating false\nrefusal in language models via single vector ablation. For a given model, we\nextract a false refusal vector and show that ablating this vector reduces false\nrefusal rate without negatively impacting model safety and general model\ncapabilities. We also show that our approach can be used for fine-grained\ncalibration of model safety. Our approach is training-free and model-agnostic,\nmaking it useful for mitigating the problem of false refusal in current and\nfuture language models.\n","authors":["Xinpeng Wang","Chengzhi Hu","Paul Rttger","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.03415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03414v2","updated":"2024-10-04T13:24:59Z","published":"2024-08-06T19:23:42Z","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","summary":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","authors":["Marcus Buckmann","Edward Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03414v2.pdf","comment":"48 pages, 24 figures"},{"id":"http://arxiv.org/abs/2410.03412v1","updated":"2024-10-04T13:23:50Z","published":"2024-10-04T13:23:50Z","title":"Team MTS @ AutoMin 2021: An Overview of Existing Summarization\n  Approaches and Comparison to Unsupervised Summarization Techniques","summary":"  Remote communication through video or audio conferences has become more\npopular than ever because of the worldwide pandemic. These events, therefore,\nhave provoked the development of systems for automatic minuting of spoken\nlanguage leading to AutoMin 2021 challenge. The following paper illustrates the\nresults of the research that team MTS has carried out while participating in\nthe Automatic Minutes challenge. In particular, in this paper we analyze\nexisting approaches to text and speech summarization, propose an unsupervised\nsummarization technique based on clustering and provide a pipeline that\nincludes an adapted automatic speech recognition block able to run on real-life\nrecordings. The proposed unsupervised technique outperforms pre-trained\nsummarization models on the automatic minuting task with Rouge 1, Rouge 2 and\nRouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2,\nRouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035,\n0.098, 1.857, 2.304, 1.911 on the test set accordingly\n","authors":["Olga Iakovenko","Anna Andreeva","Anna Lapidus","Liana Mikaelyan"],"pdf_url":"https://arxiv.org/pdf/2410.03412v1.pdf","comment":"First Shared Task on Automatic Minuting at Interspeech 2021"},{"id":"http://arxiv.org/abs/2407.13297v2","updated":"2024-10-04T13:20:11Z","published":"2024-07-18T08:56:02Z","title":"SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning","summary":"  Specialized lexicons are collections of words with associated constraints\nsuch as special definitions, specific roles, and intended target audiences.\nThese constraints are necessary for content generation and documentation tasks\n(e.g., writing technical manuals or children's reading materials), where the\ngoal is to reduce the ambiguity of text content and increase its overall\nreadability for a specific group of audience. Understanding how large language\nmodels can capture these constraints can help researchers build better, more\nimpactful tools for wider use beyond the NLP community. Towards this end, we\nintroduce SpeciaLex, a benchmark for evaluating a language model's ability to\nfollow specialized lexicon-based constraints across 18 diverse subtasks with\n1,785 test instances covering core tasks of Checking, Identification,\nRewriting, and Open Generation. We present an empirical evaluation of 15 open\nand closed-source LLMs and discuss insights on how factors such as model scale,\nopenness, setup, and recency affect performance upon evaluating with the\nbenchmark.\n","authors":["Joseph Marvin Imperial","Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2407.13297v2.pdf","comment":"Camera-ready for EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.15227v2","updated":"2024-10-04T13:15:14Z","published":"2024-06-21T15:11:33Z","title":"A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation","summary":"  This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns.\n","authors":["Irune Zubiaga","Aitor Soroa","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2406.15227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11687v3","updated":"2024-10-04T13:06:24Z","published":"2024-06-17T16:05:32Z","title":"Tokenization Falling Short: On Subword Robustness in Large Language\n  Models","summary":"  Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.\n","authors":["Yekun Chai","Yewei Fang","Qiwei Peng","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2406.11687v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2401.12585v5","updated":"2024-10-04T13:03:49Z","published":"2024-01-23T09:33:31Z","title":"SLANG: New Concept Comprehension of Large Language Models","summary":"  The dynamic nature of language, particularly evident in the realm of slang\nand memes on the Internet, poses serious challenges to the adaptability of\nlarge language models (LLMs). Traditionally anchored to static datasets, these\nmodels often struggle to keep up with the rapid linguistic evolution\ncharacteristic of online communities. This research aims to bridge this gap by\nenhancing LLMs' comprehension of the evolving new concepts on the Internet,\nwithout the high cost of continual retraining. In pursuit of this goal, we\nintroduce $\\textbf{SLANG}$, a benchmark designed to autonomously integrate\nnovel data and assess LLMs' ability to comprehend emerging concepts, alongside\n$\\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to\nunderstand new phrases and their colloquial context. Our benchmark and approach\ninvolves understanding real-world instances of linguistic shifts, serving as\ncontextual beacons, to form more precise and contextually relevant connections\nbetween newly emerging expressions and their meanings. The empirical analysis\nshows that our causal inference-based approach outperforms the baseline methods\nin terms of precision and relevance in the comprehension of Internet slang and\nmemes.\n","authors":["Lingrui Mei","Shenghua Liu","Yiwei Wang","Baolong Bi","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.12585v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03394v1","updated":"2024-10-04T12:57:00Z","published":"2024-10-04T12:57:00Z","title":"Killing Two Flies with One Stone: An Attempt to Break LLMs Using\n  English->Icelandic Idioms and Proper Names","summary":"  This paper presents the submission of the \\'Arni Magn\\'usson Institute's team\nto the WMT24 test suite subtask, focusing on idiomatic expressions and proper\nnames for the English->Icelandic translation direction.\n  Intuitively and empirically, idioms and proper names are known to be a\nsignificant challenge for modern translation models. We create two different\ntest suites. The first evaluates the competency of MT systems in translating\ncommon English idiomatic expressions, as well as testing whether systems can\ndistinguish between those expressions and the same phrases when used in a\nliteral context. The second test suite consists of place names that should be\ntranslated into their Icelandic exonyms (and correctly inflected) and pairs of\nIcelandic names that share a surface form between the male and female variants,\nso that incorrect translations impact meaning as well as readability.\n  The scores reported are relatively low, especially for idiomatic expressions\nand place names, and indicate considerable room for improvement.\n","authors":["Bjarki rmannsson","Hinrik Hafsteinsson","Atli Jasonarson","Steinr Steingrmsson"],"pdf_url":"https://arxiv.org/pdf/2410.03394v1.pdf","comment":"WMT24 MT Test Suites subtask. 8 pages, 5 tables"},{"id":"http://arxiv.org/abs/2406.09206v2","updated":"2024-10-04T12:55:45Z","published":"2024-06-13T15:06:11Z","title":"Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models","summary":"  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. In\nthis work, we investigate how self-training, a semi-supervised approach that\nuses a model to obtain pseudo-labels for unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Building on a\ncomprehensive reproduction of four previous self-training approaches, some of\nwhich are evaluated for the first time in the context of active learning or\nnatural language processing, we introduce HAST, a new and effective\nself-training strategy, which is evaluated on four text classification\nbenchmarks. Our results show that it outperforms the reproduced self-training\napproaches and reaches classification results comparable to previous\nexperiments for three out of four datasets, using as little as 25% of the data.\nThe code is publicly available at\nhttps://github.com/chschroeder/self-training-for-sample-efficient-active-learning .\n","authors":["Christopher Schrder","Gerhard Heyer"],"pdf_url":"https://arxiv.org/pdf/2406.09206v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.02115v2","updated":"2024-10-04T12:52:28Z","published":"2024-10-03T00:38:12Z","title":"L-CiteEval: Do Long-Context Models Truly Leverage Context for\n  Responding?","summary":"  Long-context models (LCMs) have made remarkable strides in recent years,\noffering users great convenience for handling tasks that involve long context,\nsuch as document summarization. As the community increasingly prioritizes the\nfaithfulness of generated results, merely ensuring the accuracy of LCM outputs\nis insufficient, as it is quite challenging for humans to verify the results\nfrom the extremely lengthy context. Yet, although some efforts have been made\nto assess whether LCMs respond truly based on the context, these works either\nare limited to specific tasks or heavily rely on external evaluation resources\nlike GPT4.In this work, we introduce L-CiteEval, a comprehensive multi-task\nbenchmark for long-context understanding with citations, aiming to evaluate\nboth the understanding capability and faithfulness of LCMs. L-CiteEval covers\n11 tasks from diverse domains, spanning context lengths from 8K to 48K, and\nprovides a fully automated evaluation suite. Through testing with 11\ncutting-edge closed-source and open-source LCMs, we find that although these\nmodels show minor differences in their generated results, open-source models\nsubstantially trail behind their closed-source counterparts in terms of\ncitation accuracy and recall. This suggests that current open-source LCMs are\nprone to responding based on their inherent knowledge rather than the given\ncontext, posing a significant risk to the user experience in practical\napplications. We also evaluate the RAG approach and observe that RAG can\nsignificantly improve the faithfulness of LCMs, albeit with a slight decrease\nin the generation quality. Furthermore, we discover a correlation between the\nattention mechanisms of LCMs and the citation generation process.\n","authors":["Zecheng Tang","Keyan Zhou","Juntao Li","Baibei Ji","Jianye Hou","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14122v2","updated":"2024-10-04T12:50:46Z","published":"2024-04-22T12:21:12Z","title":"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy\n  Data in Misaligned Languages Suffice?","summary":"  Traditionally, success in multilingual machine translation can be attributed\nto three key factors in training data: large volume, diverse translation\ndirections, and high quality. In the current practice of fine-tuning large\nlanguage models (LLMs) for translation, we revisit the importance of these\nfactors. We find that LLMs display strong translation capability after being\nfine-tuned on as few as 32 parallel sentences and that fine-tuning on a single\ntranslation direction enables translation in multiple directions. However, the\nchoice of direction is critical: fine-tuning LLMs with only English on the\ntarget side can lead to task misinterpretation, which hinders translation into\nnon-English languages. Problems also arise when noisy synthetic data is placed\non the target side, especially when the target language is well-represented in\nLLM pre-training. Yet interestingly, synthesized data in an under-represented\nlanguage has a less pronounced effect. Our findings suggest that when adapting\nLLMs to translation, the requirement on data quantity can be eased but careful\nconsiderations are still crucial to prevent an LLM from exploiting unintended\ndata biases.\n","authors":["Dawei Zhu","Pinzhen Chen","Miaoran Zhang","Barry Haddow","Xiaoyu Shen","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2404.14122v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03381v1","updated":"2024-10-04T12:48:32Z","published":"2024-10-04T12:48:32Z","title":"Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission\n  to the WMT24 General Translation Task","summary":"  This paper presents the submission of the \\'Arni Magnusson Institute's team\nto the WMT24 General translation task. We work on the English->Icelandic\ntranslation direction. Our system comprises four translation models and a\ngrammar correction model. For training our models we carefully curate our\ndatasets, aggressively filtering out sentence pairs that may detrimentally\naffect the quality of our system's output. Some of our data are collected from\nhuman translations and some are synthetically generated. A part of the\nsynthetic data is generated using an LLM, and we find that it increases the\ntranslation capability of our system significantly.\n","authors":["Atli Jasonarson","Hinrik Hafsteinsson","Bjarki rmannsson","Steinr Steingrmsson"],"pdf_url":"https://arxiv.org/pdf/2410.03381v1.pdf","comment":"WMT24 General Translation Task System Description Paper, 10 pages, 1\n  figure, 6 tables"},{"id":"http://arxiv.org/abs/2405.19874v2","updated":"2024-10-04T12:39:20Z","published":"2024-05-30T09:28:56Z","title":"Is In-Context Learning Sufficient for Instruction Following in LLMs?","summary":"  In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.\n","authors":["Hao Zhao","Maksym Andriushchenko","Francesco Croce","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2405.19874v2.pdf","comment":"Preprint. Code at https://github.com/tml-epfl/icl-alignment"},{"id":"http://arxiv.org/abs/2405.03279v3","updated":"2024-10-04T12:29:46Z","published":"2024-05-06T08:52:11Z","title":"Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous\n  Prompt Learning","summary":"  Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed.\n","authors":["Qizhou Chen","Taolin Zhang","Xiaofeng He","Dongyang Li","Chengyu Wang","Longtao Huang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2405.03279v3.pdf","comment":"16 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.03357v1","updated":"2024-10-04T12:24:02Z","published":"2024-10-04T12:24:02Z","title":"Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of\n  Meta-Learning and Joint Learning AMR Parsing","summary":"  Cross-lingual AMR parsing is the task of predicting AMR graphs in a target\nlanguage when training data is available only in a source language. Due to the\nsmall size of AMR training data and evaluation data, cross-lingual AMR parsing\nhas only been explored in a small set of languages such as English, Spanish,\nGerman, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022),\nwho apply meta-learning to tackle cross-lingual syntactic parsing, we\ninvestigate the use of meta-learning for cross-lingual AMR parsing. We evaluate\nour models in $k$-shot scenarios (including 0-shot) and assess their\neffectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean\nand Croatian test sets are developed as part of our work, based on the existing\nThe Little Prince English AMR corpus, and made publicly available. We\nempirically study our method by comparing it to classical joint learning. Our\nfindings suggest that while the meta-learning model performs slightly better in\n0-shot evaluation for certain languages, the performance gain is minimal or\nabsent when $k$ is higher than 0.\n","authors":["Jeongwoo Kang","Maximin Coavoux","Cdric Lopez","Didier Schwab"],"pdf_url":"https://arxiv.org/pdf/2410.03357v1.pdf","comment":"to appear in Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03351v1","updated":"2024-10-04T12:17:08Z","published":"2024-10-04T12:17:08Z","title":"Generating Equivalent Representations of Code By A Self-Reflection\n  Approach","summary":"  Equivalent Representations (ERs) of code are textual representations that\npreserve the same semantics as the code itself, e.g., natural language comments\nand pseudocode. ERs play a critical role in software development and\nmaintenance. However, how to automatically generate ERs of code remains an open\nchallenge. In this paper, we propose a self-reflection approach to generating\nERs of code. It enables two Large Language Models (LLMs) to work mutually and\nproduce an ER through a reflection process. Depending on whether constraints on\nERs are applied, our approach generates ERs in both open and constrained\nsettings. We conduct a empirical study to generate ERs in two settings and\nobtain eight findings. (1) Generating ERs in the open setting. In the open\nsetting, we allow LLMs to represent code without any constraints, analyzing the\nresulting ERs and uncovering five key findings. These findings shed light on\nhow LLMs comprehend syntactic structures, APIs, and numerical computations in\ncode. (2) Generating ERs in the constrained setting. In the constrained\nsetting, we impose constraints on ERs, such as natural language comments,\npseudocode, and flowcharts. This allows our approach to address a range of\nsoftware engineering tasks. Based on our experiments, we have three findings\ndemonstrating that our approach can effectively generate ERs that adhere to\nspecific constraints, thus supporting various software engineering tasks. (3)\nFuture directions. We also discuss potential future research directions, such\nas deriving intermediate languages for code generation, exploring LLM-friendly\nrequirement descriptions, and further supporting software engineering tasks. We\nbelieve that this paper will spark discussions in research communities and\ninspire many follow-up studies.\n","authors":["Jia Li","Ge Li","Lecheng Wang","Hao Zhu","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2410.03351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10989v2","updated":"2024-10-04T12:13:42Z","published":"2024-09-17T08:44:20Z","title":"GOSt-MT: A Knowledge Graph for Occupation-related Gender Biases in\n  Machine Translation","summary":"  Gender bias in machine translation (MT) systems poses significant challenges\nthat often result in the reinforcement of harmful stereotypes. Especially in\nthe labour domain where frequently occupations are inaccurately associated with\nspecific genders, such biases perpetuate traditional gender stereotypes with a\nsignificant impact on society. Addressing these issues is crucial for ensuring\nequitable and accurate MT systems. This paper introduces a novel approach to\nstudying occupation-related gender bias through the creation of the GOSt-MT\n(Gender and Occupation Statistics for Machine Translation) Knowledge Graph.\nGOSt-MT integrates comprehensive gender statistics from real-world labour data\nand textual corpora used in MT training. This Knowledge Graph allows for a\ndetailed analysis of gender bias across English, French, and Greek,\nfacilitating the identification of persistent stereotypes and areas requiring\nintervention. By providing a structured framework for understanding how\noccupations are gendered in both labour markets and MT systems, GOSt-MT\ncontributes to efforts aimed at making MT systems more equitable and reducing\ngender biases in automated translations.\n","authors":["Orfeas Menis Mastromichalakis","Giorgos Filandrianos","Eva Tsouparopoulou","Dimitris Parsanoglou","Maria Symeonaki","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.10989v2.pdf","comment":"Accepted at the KG-STAR'24: Workshop on Knowledge Graphs for\n  Responsible AI co-located with the 33rd ACM CIKM Conference, October 25,\n  2024, Boise, Idaho"},{"id":"http://arxiv.org/abs/2408.13933v2","updated":"2024-10-04T12:03:03Z","published":"2024-08-25T20:41:22Z","title":"MobileQuant: Mobile-friendly Quantization for On-device Language Models","summary":"  Large language models (LLMs) have revolutionized language processing,\ndelivering outstanding results across multiple applications. However, deploying\nLLMs on edge devices poses several challenges with respect to memory, energy,\nand compute costs, limiting their widespread use in devices such as mobile\nphones. A promising solution is to reduce the number of bits used to represent\nweights and activations. While existing works have found partial success at\nquantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations\nbeyond 16 bits often leads to large computational overheads due to poor\non-device quantization support, or a considerable accuracy drop. Yet, 8-bit\nactivations are very attractive for on-device deployment as they would enable\nLLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units\n(NPUs). In this work, we make a first attempt to facilitate the on-device\ndeployment of LLMs using integer-only quantization. We first investigate the\nlimitations of existing quantization methods for on-device deployment, with a\nspecial focus on activation quantization. We then address these limitations by\nintroducing a simple post-training quantization method, named MobileQuant, that\nextends previous weight equivalent transformation works by jointly optimizing\nthe weight transformation and activation range parameters in an end-to-end\nmanner. MobileQuant demonstrates superior capabilities over existing methods by\n1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)\nreducing latency and energy consumption by 20\\%-50\\% compared to current\non-device quantization strategies, 3) requiring limited compute budget, 4)\nbeing compatible with mobile-friendly compute units, e.g. NPU.\n","authors":["Fuwen Tan","Royson Lee","ukasz Dudziak","Shell Xu Hu","Sourav Bhattacharya","Timothy Hospedales","Georgios Tzimiropoulos","Brais Martinez"],"pdf_url":"https://arxiv.org/pdf/2408.13933v2.pdf","comment":"EMNLP 2024 Findings. Code and models available:\n  https://github.com/saic-fi/MobileQuant"},{"id":"http://arxiv.org/abs/2305.00450v3","updated":"2024-10-04T12:00:21Z","published":"2023-04-30T11:26:10Z","title":"SMILE: Single-turn to Multi-turn Inclusive Language Expansion via\n  ChatGPT for Mental Health Support","summary":"  Developing specialized dialogue systems for mental health support requires\nmulti-turn conversation data, which has recently garnered increasing attention.\nHowever, gathering and releasing large-scale, real-life multi-turn\nconversations that could facilitate advancements in mental health support\npresents challenges in data privacy protection and the time and cost involved\nin crowdsourcing. To address these challenges, we introduce SMILE, a\nsingle-turn to multi-turn inclusive language expansion technique that prompts\nChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our work\nbegins by analyzing language transformation and validating the feasibility of\nour proposed method. We conduct a study on dialogue diversity, including\nlexical features, semantic features, and dialogue topics, demonstrating the\neffectiveness of our method. Further, we employ our method to generate a\nlarge-scale, lifelike, and diverse dialogue dataset named SMILECHAT, consisting\nof 55k dialogues. Finally, we utilize the collected corpus to develop a mental\nhealth chatbot, MeChat. To better assess the quality of SMILECHAT, we collect a\nsmall-scale real-life counseling dataset conducted by data anonymization. Both\nautomatic and human evaluations demonstrate significant improvements in our\ndialogue system and confirm that SMILECHAT is high-quality. Code, data, and\nmodel are publicly available at https://github.com/qiuhuachuan/smile.\n","authors":["Huachuan Qiu","Hongliang He","Shuai Zhang","Anqi Li","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2305.00450v3.pdf","comment":"accepted to the EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03341v1","updated":"2024-10-04T11:57:32Z","published":"2024-10-04T11:57:32Z","title":"Zero-Shot Fact Verification via Natural Logic and Large Language Models","summary":"  The recent development of fact verification systems with natural logic has\nenhanced their explainability by aligning claims with evidence through\nset-theoretic operators, providing faithful justifications. Despite these\nadvancements, such systems often rely on a large amount of training data\nannotated with natural logic. To address this issue, we propose a zero-shot\nmethod that utilizes the generalization capabilities of instruction-tuned large\nlanguage models. To comprehensively assess the zero-shot capabilities of our\nmethod and other fact verification systems, we evaluate all models on both\nartificial and real-world claims, including multilingual datasets. We also\ncompare our method against other fact verification systems in two setups.\nFirst, in the zero-shot generalization setup, we demonstrate that our approach\noutperforms other systems that were not specifically trained on natural logic\ndata, achieving an average accuracy improvement of 8.96 points over the\nbest-performing baseline. Second, in the zero-shot transfer setup, we show that\ncurrent systems trained on natural logic data do not generalize well to other\ndomains, and our method outperforms these systems across all datasets with\nreal-world claims.\n","authors":["Marek Strong","Rami Aly","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2410.03341v1.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.11614v2","updated":"2024-10-04T11:46:20Z","published":"2024-06-17T15:00:35Z","title":"Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces","summary":"  The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.\n","authors":["Yihuai Hong","Lei Yu","Haiqin Yang","Shauli Ravfogel","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2406.11614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12163v2","updated":"2024-10-04T11:40:58Z","published":"2024-08-22T07:18:46Z","title":"Preference-Guided Reflective Sampling for Aligning Language Models","summary":"  Iterative data generation and model re-training can effectively align large\nlanguage models(LLMs) to human preferences. The process of data sampling is\ncrucial, as it significantly influences the success of policy improvement.\nRepeated random sampling is a widely used method that independently queries the\nmodel multiple times to generate outputs. In this work, we propose a more\neffective sampling method, named Preference-Guided Reflective Sampling (PRS).\nUnlike random sampling, PRS employs a tree-based generation framework to enable\nmore efficient sampling. It leverages adaptive self-refinement techniques to\nbetter explore the sampling space. By specifying user preferences in natural\nlanguage, PRS can further optimize response generation according to these\npreferences. As a result, PRS can align models to diverse user preferences. Our\nexperiments demonstrate that PRS generates higher-quality responses with\nsignificantly higher rewards. On AlpacaEval and Arena-Hard, PRS substantially\noutperforms repeated random sampling in best-of-$N$ sampling. Moreover, PRS\nshows strong performance when applied in iterative offline RL training.\n","authors":["Hai Ye","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2408.12163v2.pdf","comment":"EMNLP2024, main"},{"id":"http://arxiv.org/abs/2407.16222v2","updated":"2024-10-04T11:34:23Z","published":"2024-07-23T06:59:53Z","title":"PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of\n  Multilingual Alignment","summary":"  Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.\n","authors":["Jiahuan Li","Shujian Huang","Aarron Ching","Xinyu Dai","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12593v2","updated":"2024-10-04T11:28:41Z","published":"2024-02-19T23:18:18Z","title":"Standardize: Aligning Language Models with Expert-Defined Standards for\n  Content Generation","summary":"  Domain experts across engineering, healthcare, and education follow strict\nstandards for producing quality content such as technical manuals, medication\ninstructions, and children's reading materials. However, current works in\ncontrollable text generation have yet to explore using these standards as\nreferences for control. Towards this end, we introduce Standardize, a\nretrieval-style in-context learning-based framework to guide large language\nmodels to align with expert-defined standards. Focusing on English language\nstandards in the education domain as a use case, we consider the Common\nEuropean Framework of Reference for Languages (CEFR) and Common Core Standards\n(CCS) for the task of open-ended content generation. Our findings show that\nmodels can gain a 45% to 100% increase in precise accuracy across open and\ncommercial LLMs evaluated, demonstrating that the use of knowledge artifacts\nextracted from standards and integrating them in the generation process can\neffectively guide models to produce better standard-aligned content.\n","authors":["Joseph Marvin Imperial","Gail Forey","Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2402.12593v2.pdf","comment":"Camera-ready for EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2406.14654v2","updated":"2024-10-04T11:08:06Z","published":"2024-06-20T18:17:58Z","title":"Major Entity Identification: A Generalizable Alternative to Coreference\n  Resolution","summary":"  The limited generalization of coreference resolution (CR) models has been a\nmajor bottleneck in the task's broad application. Prior work has identified\nannotation differences, especially for mention detection, as one of the main\nreasons for the generalization gap and proposed using additional annotated\ntarget domain data. Rather than relying on this additional annotation, we\npropose an alternative referential task, Major Entity Identification (MEI),\nwhere we: (a) assume the target entities to be specified in the input, and (b)\nlimit the task to only the frequent entities. Through extensive experiments, we\ndemonstrate that MEI models generalize well across domains on multiple datasets\nwith supervised models and LLM-based few-shot prompting. Additionally, MEI fits\nthe classification framework, which enables the use of robust and intuitive\nclassification-based metrics. Finally, MEI is also of practical use as it\nallows a user to search for all mentions of a particular entity or a group of\nentities of interest.\n","authors":["Kawshik Manikantan","Shubham Toshniwal","Makarand Tapaswi","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2406.14654v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.02131v2","updated":"2024-10-04T11:05:18Z","published":"2024-10-03T01:24:09Z","title":"C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language\n  Pre-Training","summary":"  Accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with their\naccompanying textual reports holds immense potential to enhance clinical\ndiagnostics through the combination of physiological data and qualitative\ninsights. However, this integration faces significant challenges due to\ninherent modality disparities and the scarcity of labeled data for robust\ncross-modal learning. To address these obstacles, we propose C-MELT, a novel\nframework that pre-trains ECG and text data using a contrastive masked\nauto-encoder architecture. C-MELT uniquely combines the strengths of generative\nwith enhanced discriminative capabilities to achieve robust cross-modal\nrepresentations. This is accomplished through masked modality modeling,\nspecialized loss functions, and an improved negative sampling strategy tailored\nfor cross-modal alignment. Extensive experiments on five public datasets across\ndiverse downstream tasks demonstrate that C-MELT significantly outperforms\nexisting methods, achieving 15% and 2% increases in linear probing and\nzero-shot performance over state-of-the-art models, respectively. These results\nhighlight the effectiveness of C-MELT, underscoring its potential to advance\nautomated clinical diagnostics through multi-modal representations.\n","authors":["Manh Pham","Aaqib Saeed","Dong Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03312v1","updated":"2024-10-04T10:50:18Z","published":"2024-10-04T10:50:18Z","title":"Context and System Fusion in Post-ASR Emotion Recognition with Large\n  Language Models","summary":"  Large language models (LLMs) have started to play a vital role in modelling\nspeech and text. To explore the best use of context and multiple systems'\noutputs for post-ASR speech emotion prediction, we study LLM prompting on a\nrecent task named GenSEC. Our techniques include ASR transcript ranking,\nvariable conversation context, and system output fusion. We show that the\nconversation context has diminishing returns and the metric used to select the\ntranscript for prediction is crucial. Finally, our best submission surpasses\nthe provided baseline by 20% in absolute accuracy.\n","authors":["Pavel Stepachev","Pinzhen Chen","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2410.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11368v2","updated":"2024-10-04T10:39:17Z","published":"2024-06-17T09:46:35Z","title":"Improving Quotation Attribution with Fictional Character Embeddings","summary":"  Humans naturally attribute utterances of direct speech to their speaker in\nliterary works. When attributing quotes, we process contextual information but\nalso access mental representations of characters that we build and revise\nthroughout the narrative. Recent methods to automatically attribute such\nutterances have explored simulating human logic with deterministic rules or\nlearning new implicit rules with neural networks when processing contextual\ninformation. However, these systems inherently lack \\textit{character}\nrepresentations, which often leads to errors in more challenging examples of\nattribution: anaphoric and implicit quotes. In this work, we propose to augment\na popular quotation attribution system, BookNLP, with character embeddings that\nencode global stylistic information of characters derived from an off-the-shelf\nstylometric model, Universal Authorship Representation (UAR). We create DramaCV\n(Code and data can be found at\nhttps://github.com/deezer/character_embeddings_qa ), a corpus of English drama\nplays from the 15th to 20th century that we automatically annotate for\nAuthorship Verification of fictional characters utterances, and release two\nversions of UAR trained on DramaCV, that are tailored for literary characters\nanalysis. Then, through an extensive evaluation on 28 novels, we show that\ncombining BookNLP's contextual information with our proposed global character\nembeddings improves the identification of speakers for anaphoric and implicit\nquotes, reaching state-of-the-art performance.\n","authors":["Gaspard Michel","Elena V. Epure","Romain Hennequin","Christophe Cerisara"],"pdf_url":"https://arxiv.org/pdf/2406.11368v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.12680v2","updated":"2024-10-04T10:30:44Z","published":"2024-06-18T14:51:54Z","title":"Measuring Psychological Depth in Language Models","summary":"  Evaluations of creative stories generated by large language models (LLMs)\noften focus on objective properties of the text, such as its style, coherence,\nand diversity. While these metrics are indispensable, they do not speak to a\nstory's subjective, psychological impact from a reader's perspective. We\nintroduce the Psychological Depth Scale (PDS), a novel framework rooted in\nliterary theory that measures an LLM's ability to produce authentic and\nnarratively complex stories that provoke emotion, empathy, and engagement. We\nempirically validate our framework by showing that humans can consistently\nevaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore\ntechniques for automating the PDS to easily scale future analyses. GPT-4o,\ncombined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an\naverage Spearman correlation of 0.51 with human judgment while Llama-3-70B with\nconstrained decoding scores as high as 0.68 for empathy. Finally, we compared\nthe depth of stories authored by both humans and LLMs. Surprisingly, GPT-4\nstories either surpassed or were statistically indistinguishable from\nhighly-rated human-written stories sourced from Reddit. By shifting the focus\nfrom text to reader, the Psychological Depth Scale is a validated, automated,\nand systematic means of measuring the capacity of LLMs to connect with humans\nthrough the stories they tell.\n","authors":["Fabrice Harel-Canada","Hanyu Zhou","Sreya Muppalla","Zeynep Yildiz","Miryung Kim","Amit Sahai","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2406.12680v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.04325v2","updated":"2024-10-04T10:23:56Z","published":"2024-05-07T13:55:11Z","title":"Deception in Reinforced Autonomous Agents","summary":"  We explore the ability of large language model (LLM)-based agents to engage\nin subtle deception such as strategically phrasing and intentionally\nmanipulating information to misguide and deceive other agents. This harmful\nbehavior can be hard to detect, unlike blatant lying or unintentional\nhallucination. We build an adversarial testbed mimicking a legislative\nenvironment where two LLMs play opposing roles: a corporate *lobbyist*\nproposing amendments to bills that benefit a specific company while evading a\n*critic* trying to detect this deception. We use real-world legislative bills\nmatched with potentially affected companies to ground these interactions. Our\nresults show that LLM lobbyists initially exhibit limited deception against\nstrong LLM critics which can be further improved through simple verbal\nreinforcement, significantly enhancing their deceptive capabilities, and\nincreasing deception rates by up to 40 points. This highlights the risk of\nautonomous agents manipulating other agents through seemingly neutral language\nto attain self-serving goals.\n","authors":["Atharvan Dogra","Krishna Pillutla","Ameet Deshpande","Ananya B Sai","John Nay","Tanmay Rajpurohit","Ashwin Kalyan","Balaraman Ravindran"],"pdf_url":"https://arxiv.org/pdf/2405.04325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03296v1","updated":"2024-10-04T10:14:12Z","published":"2024-10-04T10:14:12Z","title":"Comparing zero-shot self-explanations with human rationales in\n  multilingual text classification","summary":"  Instruction-tuned LLMs are able to provide an explanation about their output\nto users by generating self-explanations that do not require gradient\ncomputations or the application of possibly complex XAI methods. In this paper,\nwe analyse whether this ability results in a good explanation by evaluating\nself-explanations in the form of input rationales with respect to their\nplausibility to humans as well as their faithfulness to models. For this, we\napply two text classification tasks: sentiment classification and forced labour\ndetection. Next to English, we further include Danish and Italian translations\nof the sentiment classification task and compare self-explanations to human\nannotations for all samples. To allow for direct comparisons, we also compute\npost-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and\napply this pipeline to 4 LLMs (Llama2, Llama3, Mistral and Mixtral). Our\nresults show that self-explanations align more closely with human annotations\ncompared to LRP, while maintaining a comparable level of faithfulness.\n","authors":["Stephanie Brandl","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2410.03296v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.03293v1","updated":"2024-10-04T10:06:55Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03278v1","updated":"2024-10-04T09:50:45Z","published":"2024-10-04T09:50:45Z","title":"What do Large Language Models Need for Machine Translation Evaluation?","summary":"  Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.\n","authors":["Shenbin Qian","Archchana Sindhujan","Minnie Kabra","Diptesh Kanojia","Constantin Orsan","Tharindu Ranasinghe","Frdric Blain"],"pdf_url":"https://arxiv.org/pdf/2410.03278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03277v1","updated":"2024-10-04T09:49:57Z","published":"2024-10-04T09:49:57Z","title":"A Multi-task Learning Framework for Evaluating Machine Translation of\n  Emotion-loaded User-generated Content","summary":"  Machine translation (MT) of user-generated content (UGC) poses unique\nchallenges, including handling slang, emotion, and literary devices like irony\nand sarcasm. Evaluating the quality of these translations is challenging as\ncurrent metrics do not focus on these ubiquitous features of UGC. To address\nthis issue, we utilize an existing emotion-related dataset that includes\nemotion labels and human-annotated translation errors based on\nMulti-dimensional Quality Metrics. We extend it with sentence-level evaluation\nscores and word-level labels, leading to a dataset suitable for sentence- and\nword-level translation evaluation and emotion classification, in a multi-task\nsetting. We propose a new architecture to perform these tasks concurrently,\nwith a novel combined loss function, which integrates different loss\nheuristics, like the Nash and Aligned losses. Our evaluation compares existing\nfine-tuning and multi-task learning approaches, assessing generalization with\nablative experiments over multiple datasets. Our approach achieves\nstate-of-the-art performance and we present a comprehensive analysis for MT\nevaluation of UGC.\n","authors":["Shenbin Qian","Constantin Orsan","Diptesh Kanojia","Flix do Carmo"],"pdf_url":"https://arxiv.org/pdf/2410.03277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07237v3","updated":"2024-10-04T09:28:23Z","published":"2023-11-13T10:56:59Z","title":"In Search of the Long-Tail: Systematic Generation of Long-Tail\n  Inferential Knowledge via Logical Rule Guided Search","summary":"  To effectively use large language models (LLMs) for real-world queries, it is\nimperative that they generalize to the long-tail distribution, i.e. rare\nexamples where models exhibit low confidence. In this work, we take the first\nstep towards evaluating LLMs in the long-tail distribution of inferential\nknowledge. We exemplify long-tail evaluation on the Natural Language Inference\ntask. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic\nlong-tail data generation framework, to obtain factually-correct yet long-tail\ninferential statements. LINK uses variable-wise prompting grounded on symbolic\nrules to seek low-confidence statements while ensuring factual correctness. We\nthen use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail\ninferential knowledge dataset that contains 108K statements spanning four\ndomains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs\nshow significant performance drop (21% relative drop for GPT4) on long-tail\ndata as compared to on head distribution data, and smaller models show even\nmore generalization weakness. These results further underscore the necessity of\nlong-tail evaluation in developing generalizable LLMs.\n","authors":["Huihan Li","Yuting Ning","Zeyi Liao","Siyuan Wang","Xiang Lorraine Li","Ximing Lu","Wenting Zhao","Faeze Brahman","Yejin Choi","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2311.07237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02155v2","updated":"2024-10-04T09:27:20Z","published":"2024-10-03T02:34:31Z","title":"From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities","summary":"  Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Our method not only improves performance across various\nbenchmarks but also shows promising scalability, potentially paving the way for\nmore efficient and capable multimodal foundation models.\n","authors":["Wanpeng Zhang","Zilong Xie","Yicheng Feng","Yijiang Li","Xingrun Xing","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08495v2","updated":"2024-10-04T09:25:39Z","published":"2024-07-11T13:29:28Z","title":"Investigating LLMs as Voting Assistants via Contextual Augmentation: A\n  Case Study on the European Parliament Elections 2024","summary":"  In light of the recent 2024 European Parliament elections, we are\ninvestigating if LLMs can be used as Voting Advice Applications (VAAs). We\naudit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the\nstance of political parties based on the latest \"EU and I\" voting assistance\nquestionnaire. Furthermore, we explore alternatives to improve models'\nperformance by augmenting the input context via Retrieval-Augmented Generation\n(RAG) relying on web search, and Self-Reflection using staged conversations\nthat aim to re-collect relevant content from the model's internal memory. We\nfind that MIXTRAL is highly accurate with an 82% accuracy on average with a\nsignificant performance disparity across different political groups (50-95%).\nAugmenting the input context with expert-curated information can lead to a\nsignificant boost of approx. 9%, which remains an open challenge for automated\nRAG approaches, even considering curated content.\n","authors":["Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2407.08495v2.pdf","comment":"accepted to EMNLP 2024 as a short paper"},{"id":"http://arxiv.org/abs/2410.01524v2","updated":"2024-10-04T09:25:27Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03258v1","updated":"2024-10-04T09:24:55Z","published":"2024-10-04T09:24:55Z","title":"Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in\n  Finetuning Pretrained Language Models","summary":"  In this work, we show a fundamental limitation in vocabulary adaptation\napproaches that use Byte-Pair Encoding (BPE) tokenization scheme for\nfine-tuning pretrained language models (PLMs) to expert domains. Current\napproaches trivially append the target domain-specific vocabulary at the end of\nthe PLM vocabulary. This approach leads to a lower priority score and causes\nsub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a\ngiven text. To mitigate this issue, we propose AdaptBPE where the BPE\ntokenization initialization phase is modified to first perform the longest\nstring matching on the added (target) vocabulary before tokenizing at the\ncharacter level. We perform an extensive evaluation of AdaptBPE versus the\nstandard BPE over various classification and summarization tasks; AdaptBPE\nimproves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L),\nrespectively. AdaptBPE for MEDVOC works particularly well when reference\nsummaries have high OOV concentration or are longer in length. We also conduct\na human evaluation, revealing that AdaptBPE generates more relevant and more\nfaithful summaries as compared to MEDVOC. We make our codebase publicly\navailable at https://github.com/gb-kgp/adaptbpe.\n","authors":["Gunjan Balde","Soumyadeep Roy","Mainack Mondal","Niloy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2410.03258v1.pdf","comment":"11 pages. Accepted at EMNLP Findings 2024 (The 2024 Conference on\n  Empirical Methods in Natural Language Processing)"},{"id":"http://arxiv.org/abs/2403.20279v3","updated":"2024-10-04T09:19:07Z","published":"2024-03-29T16:49:24Z","title":"LUQ: Long-text Uncertainty Quantification for LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable capability in a\nvariety of NLP tasks. However, LLMs are also prone to generate nonfactual\ncontent. Uncertainty Quantification (UQ) is pivotal in enhancing our\nunderstanding of a model's confidence on its generation, thereby aiding in the\nmitigation of nonfactual outputs. Existing research on UQ predominantly targets\nshort text generation, typically yielding brief, word-limited responses.\nHowever, real-world applications frequently necessitate much longer responses.\nOur study first highlights the limitations of current UQ methods in handling\nlong text generation. We then introduce \\textsc{Luq} and its two variations, a\nseries of novel sampling-based UQ approaches specifically designed for long\ntext. Our findings reveal that \\textsc{Luq} outperforms existing baseline\nmethods in correlating with the model's factuality scores (negative coefficient\nof -0.85 observed for Gemini Pro). To further improve the factuality of LLM\nresponses, we propose \\textsc{Luq-Ensemble}, a method that ensembles responses\nfrom multiple models and selects the response with the lowest uncertainty. The\nensembling method greatly improves the response factuality upon the best\nstandalone LLM.\n","authors":["Caiqi Zhang","Fangyu Liu","Marco Basaldella","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2403.20279v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.03255v1","updated":"2024-10-04T09:18:54Z","published":"2024-10-04T09:18:54Z","title":"Towards a Benchmark for Large Language Models for Business Process\n  Management Tasks","summary":"  An increasing number of organizations are deploying Large Language Models\n(LLMs) for a wide range of tasks. Despite their general utility, LLMs are prone\nto errors, ranging from inaccuracies to hallucinations. To objectively assess\nthe capabilities of existing LLMs, performance benchmarks are conducted.\nHowever, these benchmarks often do not translate to more specific real-world\ntasks. This paper addresses the gap in benchmarking LLM performance in the\nBusiness Process Management (BPM) domain. Currently, no BPM-specific benchmarks\nexist, creating uncertainty about the suitability of different LLMs for BPM\ntasks. This paper systematically compares LLM performance on four BPM tasks\nfocusing on small open-source models. The analysis aims to identify\ntask-specific performance variations, compare the effectiveness of open-source\nversus commercial models, and assess the impact of model size on BPM task\nperformance. This paper provides insights into the practical applications of\nLLMs in BPM, guiding organizations in selecting appropriate models for their\nspecific needs.\n","authors":["Kiran Busch","Henrik Leopold"],"pdf_url":"https://arxiv.org/pdf/2410.03255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03254v1","updated":"2024-10-04T09:17:09Z","published":"2024-10-04T09:17:09Z","title":"Are Expert-Level Language Models Expert-Level Annotators?","summary":"  Data annotation refers to the labeling or tagging of textual data with\nrelevant information. A large body of works have reported positive results on\nleveraging LLMs as an alternative to human annotators. However, existing\nstudies focus on classic NLP tasks, and the extent to which LLMs as data\nannotators perform in domains requiring expert knowledge remains underexplored.\nIn this work, we investigate comprehensive approaches across three highly\nspecialized domains and discuss practical suggestions from a cost-effectiveness\nperspective. To the best of our knowledge, we present the first systematic\nevaluation of LLMs as expert-level data annotators.\n","authors":["Yu-Min Tseng","Wei-Lin Chen","Chung-Chi Chen","Hsin-Hsi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.03254v1.pdf","comment":"Accepted to WiML @ NeurIPS 2024 (extended version)"},{"id":"http://arxiv.org/abs/2410.02281v2","updated":"2024-10-04T09:16:02Z","published":"2024-10-03T08:03:40Z","title":"Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity\n  Recognition","summary":"  The Novelties corpus is a collection of novels (and parts of novels)\nannotated for Named Entity Recognition (NER) among other tasks. This document\ndescribes the guidelines applied during its annotation. It contains the\ninstructions used by the annotators, as well as a number of examples retrieved\nfrom the annotated novels, and illustrating expressions that should be marked\nas entities as well as expressions that should not.\n","authors":["Arthur Amalvy","Vincent Labatut"],"pdf_url":"https://arxiv.org/pdf/2410.02281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03249v1","updated":"2024-10-04T09:14:11Z","published":"2024-10-04T09:14:11Z","title":"How much can we forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we use experimental evidence and theoretical estimates to\nchallenge the common assumption that small-scale contamination renders\nbenchmark evaluations invalid. First, we experimentally quantify the magnitude\nof benchmark overfitting based on scaling along three dimensions: The number of\nmodel parameters (up to 1.6B), the number of times an example is seen (up to\n144), and the number of training tokens (up to 40B). We find that if model and\ndata follow the Chinchilla scaling laws, minor contamination indeed leads to\noverfitting. At the same time, even 144 times of contamination can be forgotten\nif the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. We then derive a simple theory of example\nforgetting via cumulative weight decay. It allows us to bound the number of\ngradient steps required to forget past data for any training run where we know\nthe hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,\nhave forgotten the data seen at the beginning of training. Experimentally, we\ndemonstrate that forgetting occurs faster than what is predicted by our bounds.\nTaken together, our results suggest that moderate amounts of contamination can\nbe forgotten at the end of realistically scaled training runs.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03240v1","updated":"2024-10-04T09:04:20Z","published":"2024-10-04T09:04:20Z","title":"Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken\n  Vocabulary?","summary":"  Word frequency is a key variable in psycholinguistics, useful for modeling\nhuman familiarity with words even in the era of large language models (LLMs).\nFrequency in film subtitles has proved to be a particularly good approximation\nof everyday language exposure. For many languages, however, film subtitles are\nnot easily available, or are overwhelmingly translated from English. We\ndemonstrate that frequencies extracted from carefully processed YouTube\nsubtitles provide an approximation comparable to, and often better than, the\nbest currently available resources. Moreover, they are available for languages\nfor which a high-quality subtitle or speech corpus does not exist. We use\nYouTube subtitles to construct frequency norms for five diverse languages,\nChinese, English, Indonesian, Japanese, and Spanish, and evaluate their\ncorrelation with lexical decision time, word familiarity, and lexical\ncomplexity. In addition to being strongly correlated with two psycholinguistic\nvariables, a simple linear regression on the new frequencies achieves a new\nhigh score on a lexical complexity prediction task in English and Japanese,\nsurpassing both models trained on film subtitle frequencies and the LLM GPT-4.\nOur code, the frequency lists, fastText word embeddings, and statistical\nlanguage models are freely available at https://github.com/naist-nlp/tubelex.\n","authors":["Adam Nohejl","Frederikus Hudi","Eunike Andriani Kardinata","Shintaro Ozaki","Maria Angelica Riera Machin","Hongyu Sun","Justin Vasselli","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.03240v1.pdf","comment":"Submitted for review to COLING 2025. 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.03234v1","updated":"2024-10-04T08:51:31Z","published":"2024-10-04T08:51:31Z","title":"Showing LLM-Generated Code Selectively Based on Confidence of LLMs","summary":"  Large Language Models (LLMs) have shown impressive abilities in code\ngeneration, but they may generate erroneous programs. Reading a program takes\nten times longer than writing it. Showing these erroneous programs to\ndevelopers will waste developers' energies and introduce security risks to\nsoftware.\n  To address the above limitations, we propose HonestCoder, a novel LLM-based\ncode generation approach. HonestCoder selectively shows the generated programs\nto developers based on LLMs' confidence. The confidence provides valuable\ninsights into the correctness of generated programs. To achieve this goal, we\npropose a novel approach to estimate LLMs' confidence in code generation. It\nestimates confidence by measuring the multi-modal similarity between\nLLMs-generated programs.\n  We collect and release a multilingual benchmark named TruthCodeBench, which\nconsists of 2,265 samples and covers two popular programming languages (i.e.,\nPython and Java). We apply HonestCoder to four popular LLMs (e.g.,\nDeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the\nexperiments, we obtain the following insights. (1) HonestCoder can effectively\nestimate LLMs' confidence and accurately determine the correctness of generated\nprograms. For example, HonestCoder outperforms the state-of-the-art baseline by\n27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of\nerroneous programs shown to developers. Compared to eight baselines, it can\nshow more correct programs and fewer erroneous programs to developers. (3)\nCompared to showing code indiscriminately, HonestCoder only adds slight time\noverhead (approximately 0.4 seconds per requirement). (4) We discuss future\ndirections to facilitate the application of LLMs in software development. We\nhope this work can motivate broad discussions about measuring the reliability\nof LLMs' outputs in performing code-related tasks.\n","authors":["Jia Li","Yuqi Zhu","Yongmin Li","Ge Li","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2410.03234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14247v2","updated":"2024-10-04T08:49:43Z","published":"2024-09-21T21:06:25Z","title":"Repairs in a Block World: A New Benchmark for Handling User Corrections\n  with Multi-Modal Language Models","summary":"  In dialogue, the addressee may initially misunderstand the speaker and\nrespond erroneously, often prompting the speaker to correct the\nmisunderstanding in the next turn with a Third Position Repair (TPR). The\nability to process and respond appropriately to such repair sequences is thus\ncrucial in conversational AI systems. In this paper, we first collect, analyse,\nand publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences\nin an instruction-following manipulation task that is, by design, rife with\nreferential ambiguity. We employ this dataset to evaluate several\nstate-of-the-art Vision and Language Models (VLM) across multiple settings,\nfocusing on their capability to process and accurately respond to TPRs and thus\nrecover from miscommunication. We find that, compared to humans, all models\nsignificantly underperform in this task. We then show that VLMs can benefit\nfrom specialised losses targeting relevant tokens during fine-tuning, achieving\nbetter performance and generalising better to new scenarios. Our results\nsuggest that these models are not yet ready to be deployed in multi-modal\ncollaborative settings where repairs are common, and highlight the need to\ndesign training regimes and objectives that facilitate learning from\ninteraction. Our code and data are available at\nwww.github.com/JChiyah/blockworld-repairs\n","authors":["Javier Chiyah-Garcia","Alessandro Suglia","Arash Eshghi"],"pdf_url":"https://arxiv.org/pdf/2409.14247v2.pdf","comment":"Accepted to EMNLP'24 Main (Upcoming). Data and code at\n  www.github.com/JChiyah/blockworld-repairs - for Bibtex see\n  https://raw.githubusercontent.com/JChiyah/blockworld-repairs/refs/heads/main/citation.bib"},{"id":"http://arxiv.org/abs/2409.19715v2","updated":"2024-10-04T08:48:56Z","published":"2024-09-29T14:14:25Z","title":"Coffee-Gym: An Environment for Evaluating and Improving Natural Language\n  Feedback on Erroneous Code","summary":"  This paper presents Coffee-Gym, a comprehensive RL environment for training\nmodels that provide feedback on code editing. Coffee-Gym includes two major\ncomponents: (1) Coffee, a dataset containing humans' code edit traces for\ncoding questions and machine-written feedback for editing erroneous code; (2)\nCoffeeEval, a reward function that faithfully reflects the helpfulness of\nfeedback by assessing the performance of the revised code in unit tests. With\nthem, Coffee-Gym addresses the unavailability of high-quality datasets for\ntraining feedback models with RL, and provides more accurate rewards than the\nSOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback\nmodels that outperform baselines in enhancing open-source code LLMs' code\nediting, making them comparable with closed-source LLMs. We make the dataset\nand the model checkpoint publicly available.\n","authors":["Hyungjoo Chae","Taeyoon Kwon","Seungjun Moon","Yongho Song","Dongjin Kang","Kai Tzu-iunn Ong","Beong-woo Kwak","Seonghyeon Bae","Seung-won Hwang","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2409.19715v2.pdf","comment":"EMNLP2024"},{"id":"http://arxiv.org/abs/2403.16820v2","updated":"2024-10-04T08:44:54Z","published":"2024-03-25T14:46:51Z","title":"Cross-lingual Contextualized Phrase Retrieval","summary":"  Phrase-level dense retrieval has shown many appealing characteristics in\ndownstream NLP tasks by leveraging the fine-grained information that phrases\noffer. In our work, we propose a new task formulation of dense retrieval,\ncross-lingual contextualized phrase retrieval, which aims to augment\ncross-lingual applications by addressing polysemy using context information.\nHowever, the lack of specific training data and models are the primary\nchallenges to achieve our goal. As a result, we extract pairs of cross-lingual\nphrases using word alignment information automatically induced from parallel\nsentences. Subsequently, we train our Cross-lingual Contextualized Phrase\nRetriever (CCPR) using contrastive learning, which encourages the hidden\nrepresentations of phrases with similar contexts and semantics to align\nclosely. Comprehensive experiments on both the cross-lingual phrase retrieval\ntask and a downstream task, i.e, machine translation, demonstrate the\neffectiveness of CCPR. On the phrase retrieval task, CCPR surpasses baselines\nby a significant margin, achieving a top-1 accuracy that is at least 13 points\nhigher. When utilizing CCPR to augment the large-language-model-based\ntranslator, it achieves average gains of 0.7 and 1.5 in BERTScore for\ntranslations from X=>En and vice versa, respectively, on WMT16 dataset. Our\ncode and data are available at \\url{https://github.com/ghrua/ccpr_release}.\n","authors":["Huayang Li","Deng Cai","Zhi Qu","Qu Cui","Hidetaka Kamigaito","Lemao Liu","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.16820v2.pdf","comment":"Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03227v1","updated":"2024-10-04T08:29:12Z","published":"2024-10-04T08:29:12Z","title":"ALR$^2$: A Retrieve-then-Reason Framework for Long-context Question\n  Answering","summary":"  The context window of large language models (LLMs) has been extended\nsignificantly in recent years. However, while the context length that the LLM\ncan process has grown, the capability of the model to accurately reason over\nthat context degrades noticeably. This occurs because modern LLMs often become\noverwhelmed by the vast amount of information in the context; when answering\nquestions, the model must identify and reason over relevant evidence sparsely\ndistributed throughout the text. To alleviate the challenge of long-context\nreasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason\nover relevant evidence collected during an intermediate retrieval step. We find\nthat modern LLMs struggle to accurately retrieve relevant facts and instead,\noften hallucinate \"retrieved facts\", resulting in flawed reasoning and the\nproduction of incorrect answers. To address these issues, we introduce ALR$^2$,\na method that augments the long-context reasoning capability of LLMs via an\nexplicit two-stage procedure, i.e., aligning LLMs with the objectives of both\nretrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating\nperformance degradation in long-context reasoning tasks. Through extensive\nexperiments on long-context QA benchmarks, we find our method to outperform\ncompetitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains\non the long-context versions of HotpotQA and SQuAD datasets, respectively.\n","authors":["Huayang Li","Pat Verga","Priyanka Sen","Bowen Yang","Vijay Viswanathan","Patrick Lewis","Taro Watanabe","Yixuan Su"],"pdf_url":"https://arxiv.org/pdf/2410.03227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v1","updated":"2024-10-04T08:26:06Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xioalei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.03223v1","updated":"2024-10-04T08:22:16Z","published":"2024-10-04T08:22:16Z","title":"Consultation on Industrial Machine Faults with Large language Models","summary":"  Industrial machine fault diagnosis is a critical component of operational\nefficiency and safety in manufacturing environments. Traditional methods rely\nheavily on expert knowledge and specific machine learning models, which can be\nlimited in their adaptability and require extensive labeled data. This paper\nintroduces a novel approach leveraging Large Language Models (LLMs),\nspecifically through a structured multi-round prompting technique, to improve\nfault diagnosis accuracy. By dynamically crafting prompts, our method enhances\nthe model's ability to synthesize information from diverse data sources,\nleading to improved contextual understanding and actionable recommendations.\nExperimental results demonstrate that our approach outperforms baseline models,\nachieving an accuracy of 91% in diagnosing various fault types. The findings\nunderscore the potential of LLMs in revolutionizing industrial fault\nconsultation practices, paving the way for more effective maintenance\nstrategies in complex environments.\n","authors":["Apiradee Boonmee","Kritsada Wongsuwan","Pimchanok Sukjai"],"pdf_url":"https://arxiv.org/pdf/2410.03223v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.03215v1","updated":"2024-10-04T08:02:43Z","published":"2024-10-04T08:02:43Z","title":"NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task","summary":"  In this paper, we describe our system for the WMT 24 shared task of\nLow-Resource Indic Language Translation. We consider eng $\\leftrightarrow$ {as,\nkha, lus, mni} as participating language pairs. In this shared task, we explore\nthe finetuning of a pre-trained model motivated by the pre-trained objective of\naligning embeddings closer by alignment augmentation \\cite{lin-etal-2020-pre}\nfor 22 scheduled Indian languages. Our primary system is based on\nlanguage-specific finetuning on a pre-trained model. We achieve chrF2 scores of\n50.6, 42.3, 54.9, and 66.3 on the official public test set for\neng$\\rightarrow$as, eng$\\rightarrow$kha, eng$\\rightarrow$lus,\neng$\\rightarrow$mni respectively. We also explore multilingual training\nwith/without language grouping and layer-freezing. Our code, models, and\ngenerated translations are available here:\nhttps://github.com/pramitsahoo/WMT2024-LRILT.\n","authors":["Pramit Sahoo","Maharaj Brahma","Maunendra Sankar Desarkar"],"pdf_url":"https://arxiv.org/pdf/2410.03215v1.pdf","comment":"WMT2024 INDICMT Shared Task"},{"id":"http://arxiv.org/abs/2406.20015v2","updated":"2024-10-04T07:51:29Z","published":"2024-06-28T16:03:30Z","title":"ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for\n  Tool-Augmented Large Language Models","summary":"  Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community has yet\nto fully understand the hallucination issues within these models. To address\nthis challenge, we introduce a comprehensive diagnostic benchmark, ToolBH.\nSpecifically, we assess the LLM's hallucinations through two perspectives:\ndepth and breadth. In terms of depth, we propose a multi-level diagnostic\nprocess, including (1) solvability detection, (2) solution planning, and (3)\nmissing-tool analysis. For breadth, we consider three scenarios based on the\ncharacteristics of the toolset: missing necessary tools, potential tools, and\nlimited functionality tools. Furthermore, we developed seven tasks and\ncollected 700 evaluation samples through multiple rounds of manual annotation.\nThe results show the significant challenges presented by the ToolBH benchmark.\nThe current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores\nof 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger\nmodel parameters do not guarantee better performance; the training data and\nresponse strategies also play crucial roles in tool-enhanced LLM scenarios. Our\ndiagnostic analysis indicates that the primary reason for model errors lies in\nassessing task solvability. Additionally, open-weight models suffer from\nperformance drops with verbose replies, whereas proprietary models excel with\nlonger reasoning.\n","authors":["Yuxiang Zhang","Jing Chen","Junjie Wang","Yaxin Liu","Cheng Yang","Chufan Shi","Xinyu Zhu","Zihao Lin","Hanwen Wan","Yujiu Yang","Tetsuya Sakai","Tian Feng","Hayato Yamana"],"pdf_url":"https://arxiv.org/pdf/2406.20015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07867v3","updated":"2024-10-04T07:45:22Z","published":"2024-01-15T17:57:41Z","title":"Authorship Obfuscation in Multilingual Machine-Generated Text Detection","summary":"  High-quality text generation capability of recent Large Language Models\n(LLMs) causes concerns about their misuse (e.g., in massive generation/spread\nof disinformation). Machine-generated text (MGT) detection is important to cope\nwith such threats. However, it is susceptible to authorship obfuscation (AO)\nmethods, such as paraphrasing, which can cause MGTs to evade detection. So far,\nthis was evaluated only in monolingual settings. Thus, the susceptibility of\nrecently proposed multilingual detectors is still unknown. We fill this gap by\ncomprehensively benchmarking the performance of 10 well-known AO methods,\nattacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10\n$\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of\ndata augmentation on adversarial robustness using obfuscated texts. The results\nindicate that all tested AO methods can cause evasion of automated detection in\nall tested languages, where homoglyph attacks are especially successful.\nHowever, some of the AO methods severely damaged the text, making it no longer\nreadable or easily recognizable by humans (e.g., changed language, weird\ncharacters).\n","authors":["Dominik Macko","Robert Moro","Adaku Uchendu","Ivan Srba","Jason Samuel Lucas","Michiharu Yamashita","Nafis Irtiza Tripto","Dongwon Lee","Jakub Simko","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2401.07867v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2402.14672v2","updated":"2024-10-04T07:40:57Z","published":"2024-02-22T16:18:07Z","title":"Middleware for LLMs: Tools Are Instrumental for Language Agents in\n  Complex Environments","summary":"  The applications of large language models (LLMs) have expanded well beyond\nthe confines of text processing, signaling a new era where LLMs are envisioned\nas generalist agents capable of operating within complex environments. These\nenvironments are often highly expansive, making it impossible for the LLM to\nprocess them within its short-term memory. Motivated by recent research on\nextending the capabilities of LLMs with tools, we seek to investigate the\nintriguing potential of tools to augment LLMs in handling such complexity by\nintroducing a novel class of tools, termed middleware, to aid in the proactive\nexploration within these massive environments. Such specialized tools can serve\nas a middleware layer shielding the LLM from environmental complexity. In two\nrepresentative complex environments -- knowledge bases (KBs) and databases --\nwe demonstrate the significant potential of augmenting language agents with\ntools in complex environments. Notably, equipped with the middleware, GPT-4\nachieves 2.8X the performance of the best baseline in tasks requiring access to\ndatabase content and 2.2X in KB tasks. Our findings illuminate the path for\nadvancing language agents in real-world applications.\n","authors":["Yu Gu","Yiheng Shu","Hao Yu","Xiao Liu","Yuxiao Dong","Jie Tang","Jayanth Srinivasa","Hugo Latapie","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2402.14672v2.pdf","comment":"EMNLP'2024; 18 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.03203v1","updated":"2024-10-04T07:39:34Z","published":"2024-10-04T07:39:34Z","title":"Learning Semantic Structure through First-Order-Logic Translation","summary":"  In this paper, we study whether transformer-based language models can extract\npredicate argument structure from simple sentences. We firstly show that\nlanguage models sometimes confuse which predicates apply to which objects. To\nmitigate this, we explore two tasks: question answering (Q/A), and first order\nlogic (FOL) translation, and two regimes, prompting and finetuning. In FOL\ntranslation, we finetune several large language models on synthetic datasets\ndesigned to gauge their generalization abilities. For Q/A, we finetune encoder\nmodels like BERT and RoBERTa and use prompting for LLMs. The results show that\nFOL translation for LLMs is better suited to learn predicate argument\nstructure.\n","authors":["Akshay Chaturvedi","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2410.03203v1.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03198v1","updated":"2024-10-04T07:29:41Z","published":"2024-10-04T07:29:41Z","title":"PersoBench: Benchmarking Personalized Response Generation in Large\n  Language Models","summary":"  While large language models (LLMs) have exhibited impressive conversational\ncapabilities, their proficiency in delivering personalized responses remains\nunclear. Although recent benchmarks automatically evaluate persona consistency\nin role-playing contexts using LLM-based judgment, the evaluation of\npersonalization in response generation remains underexplored. To address this\ngap, we present a new benchmark, PersoBench, to evaluate the personalization\nability of LLMs in persona-aware dialogue generation within a zero-shot\nsetting. We assess the performance of three open-source and three closed-source\nLLMs using well-known datasets and a range of metrics. Our analysis, conducted\non three well-known persona-aware datasets, evaluates multiple dimensions of\nresponse quality, including fluency, diversity, coherence, and personalization,\nacross both standard and chain-of-thought prompting methods. Our findings\nreveal that while LLMs excel at generating fluent and diverse responses, they\nare far from satisfactory in delivering personalized and coherent responses\nconsidering both the conversation context and the provided personas. Our\nbenchmark implementation is available at\nhttps://github.com/salehafzoon/PersoBench.\n","authors":["Saleh Afzoon","Usman Naseem","Amin Beheshti","Zahra Jamali"],"pdf_url":"https://arxiv.org/pdf/2410.03198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03197v1","updated":"2024-10-04T07:29:35Z","published":"2024-10-04T07:29:35Z","title":"Cross-lingual Transfer for Automatic Question Generation by Learning\n  Interrogative Structures in Target Languages","summary":"  Automatic question generation (QG) serves a wide range of purposes, such as\naugmenting question-answering (QA) corpora, enhancing chatbot systems, and\ndeveloping educational materials. Despite its importance, most existing\ndatasets predominantly focus on English, resulting in a considerable gap in\ndata availability for other languages. Cross-lingual transfer for QG (XLT-QG)\naddresses this limitation by allowing models trained on high-resource language\ndatasets to generate questions in low-resource languages. In this paper, we\npropose a simple and efficient XLT-QG method that operates without the need for\nmonolingual, parallel, or labeled data in the target language, utilizing a\nsmall language model. Our model, trained solely on English QA datasets, learns\ninterrogative structures from a limited set of question exemplars, which are\nthen applied to generate questions in the target language. Experimental results\nshow that our method outperforms several XLT-QG baselines and achieves\nperformance comparable to GPT-3.5-turbo across different languages.\nAdditionally, the synthetic data generated by our model proves beneficial for\ntraining multilingual QA models. With significantly fewer parameters than large\nlanguage models and without requiring additional training for target languages,\nour approach offers an effective solution for QG and QA tasks across various\nlanguages.\n","authors":["Seonjeong Hwang","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2410.03197v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.15468v2","updated":"2024-10-04T07:29:29Z","published":"2024-06-15T05:35:47Z","title":"MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large\n  Language Models","summary":"  We propose MMLU-SR, a novel dataset designed to measure the true\ncomprehension abilities of Large Language Models (LLMs) by challenging their\nperformance in question-answering tasks with modified terms. We reasoned that\nan agent that \"truly\" understands a concept can still evaluate it when key\nterms are replaced by suitably defined alternate terms, and sought to\ndifferentiate such comprehension from mere text replacement. In our study, we\nmodified standardized test questions by replacing a key term with a dummy word\nalong with its definition. The key term could be in the context of questions,\nanswers, or both questions and answers. Notwithstanding the high scores\nachieved by recent popular LLMs on the MMLU leaderboard, we found a substantial\nreduction in model performance after such replacement, suggesting poor\ncomprehension. This new benchmark provides a rigorous benchmark for testing\ntrue model comprehension, and poses a challenge to the broader scientific\ncommunity.\n","authors":["Wentian Wang","Sarthak Jain","Paul Kantor","Jacob Feldman","Lazaros Gallos","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07900v3","updated":"2024-10-04T07:27:53Z","published":"2024-04-11T16:39:00Z","title":"High-Dimension Human Value Representation in Large Language Models","summary":"  The widespread application of Large Language Models (LLMs) across various\ntasks and fields has necessitated the alignment of these models with human\nvalues and preferences. Given various approaches of human value alignment,\nranging from Reinforcement Learning with Human Feedback (RLHF), to\nconstitutional learning, etc. there is an urgent need to understand the scope\nand nature of human values injected into these models before their release.\nThere is also a need for model alignment without a costly large scale human\nannotation effort. We propose UniVaR, a high-dimensional representation of\nhuman value distributions in LLMs, orthogonal to model architecture and\ntraining data. Trained from the value-relevant output of eight multilingual\nLLMs and tested on the output from four multilingual LLMs, namely LlaMA2,\nChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the\ndistribution of human values embedded in different LLMs with different langauge\nsources. Through UniVaR, we explore how different LLMs prioritize various\nvalues in different languages and cultures, shedding light on the complex\ninterplay between human values and language modeling.\n","authors":["Samuel Cahyawijaya","Delong Chen","Yejin Bang","Leila Khalatbari","Bryan Wilie","Ziwei Ji","Etsuko Ishii","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2404.07900v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04280v2","updated":"2024-10-04T07:25:18Z","published":"2024-07-05T06:25:54Z","title":"LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous\n  Speech","summary":"  Prevalent ungrammatical expressions and disfluencies in spontaneous speech\nfrom second language (L2) learners pose unique challenges to Automatic Speech\nRecognition (ASR) systems. However, few datasets are tailored to L2 learner\nspeech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours\nof audio and transcriptions of L2 learners' spontaneous speech. Our linguistic\nanalysis reveals that transcriptions in our dataset contain L2S (L2 learner's\nSpontaneous speech) features, consisting of ungrammatical expressions and\ndisfluencies (e.g., filler words, word repetitions, self-repairs, false\nstarts), significantly more than native speech datasets. Fine-tuning\nwhisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than\nvanilla whisper-small.en. Furthermore, our qualitative analysis indicates that\n54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S\nfeatures, with 48.1% of them being reduced in the fine-tuned model.\n","authors":["Haechan Kim","Junho Myung","Seoyoung Kim","Sungpah Lee","Dongyeop Kang","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2407.04280v2.pdf","comment":"Proceedings of Interspeech"},{"id":"http://arxiv.org/abs/2410.03194v1","updated":"2024-10-04T07:15:07Z","published":"2024-10-04T07:15:07Z","title":"Parallel Corpus Augmentation using Masked Language Models","summary":"  In this paper we propose a novel method of augmenting parallel text corpora\nwhich promises good quality and is also capable of producing many fold larger\ncorpora than the seed corpus we start with. We do not need any additional\nmonolingual corpora. We use Multi-Lingual Masked Language Model to mask and\npredict alternative words in context and we use Sentence Embeddings to check\nand select sentence pairs which are likely to be translations of each other. We\ncross check our method using metrics for MT Quality Estimation. We believe this\nmethod can greatly alleviate the data scarcity problem for all language pairs\nfor which a reasonable seed corpus is available.\n","authors":["Vibhuti Kumari","Narayana Murthy Kavi"],"pdf_url":"https://arxiv.org/pdf/2410.03194v1.pdf","comment":"21 Pages, 3 Figures. arXiv admin note: text overlap with\n  arXiv:2011.01536 by other authors"},{"id":"http://arxiv.org/abs/2410.02185v2","updated":"2024-10-04T07:00:03Z","published":"2024-10-03T04:01:14Z","title":"POSIX: A Prompt Sensitivity Index For Large Language Models","summary":"  Despite their remarkable capabilities, Large Language Models (LLMs) are found\nto be surprisingly sensitive to minor variations in prompts, often generating\nsignificantly divergent outputs in response to minor variations in the prompts,\nsuch as spelling errors, alteration of wording or the prompt template. However,\nwhile assessing the quality of an LLM, the focus often tends to be solely on\nits performance on downstream tasks, while very little to no attention is paid\nto prompt sensitivity. To fill this gap, we propose POSIX - a novel PrOmpt\nSensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering\na more comprehensive evaluation of LLM performance. The key idea behind POSIX\nis to capture the relative change in loglikelihood of a given response upon\nreplacing the corresponding prompt with a different intent-preserving prompt.\nWe provide thorough empirical evidence demonstrating the efficacy of POSIX in\ncapturing prompt sensitivity and subsequently use it to measure and thereby\ncompare prompt sensitivity of various open-source LLMs. We find that merely\nincreasing the parameter count or instruction tuning does not necessarily\nreduce prompt sensitivity whereas adding some few-shot exemplars, even just\none, almost always leads to significant decrease in prompt sensitivity. We also\nfind that alterations to prompt template lead to the highest sensitivity in the\ncase of MCQ type tasks, whereas paraphrasing results in the highest sensitivity\nin open-ended generation tasks. The code for reproducing our results is\nopen-sourced at https://github.com/kowndinya-renduchintala/POSIX.\n","authors":["Anwoy Chatterjee","H S V N S Kowndinya Renduchintala","Sumit Bhatia","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2410.02185v2.pdf","comment":"EMNLP 2024 (Findings)"},{"id":"http://arxiv.org/abs/2410.03182v1","updated":"2024-10-04T06:45:48Z","published":"2024-10-04T06:45:48Z","title":"Generating bilingual example sentences with large language models as\n  lexicography assistants","summary":"  We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages.\n","authors":["Raphael Merx","Ekaterina Vylomova","Kemal Kurniawan"],"pdf_url":"https://arxiv.org/pdf/2410.03182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15000v2","updated":"2024-10-04T06:44:10Z","published":"2024-02-22T22:28:46Z","title":"Divide-or-Conquer? Which Part Should You Distill Your LLM?","summary":"  Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.\n","authors":["Zhuofeng Wu","He Bai","Aonan Zhang","Jiatao Gu","VG Vinod Vydiswaran","Navdeep Jaitly","Yizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15000v2.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03181v1","updated":"2024-10-04T06:38:38Z","published":"2024-10-04T06:38:38Z","title":"Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large\n  Language Models with Assigned Visual Personas","summary":"  This study is the first to explore whether multi-modal large language models\n(LLMs) can align their behaviors with visual personas, addressing a significant\ngap in the literature that predominantly focuses on text-based personas. We\ndeveloped a novel dataset of 5K fictional avatar images for assignment as\nvisual personas to LLMs, and analyzed their negotiation behaviors based on the\nvisual traits depicted in these images, with a particular focus on\naggressiveness. The results indicate that LLMs assess the aggressiveness of\nimages in a manner similar to humans and output more aggressive negotiation\nbehaviors when prompted with an aggressive visual persona. Interestingly, the\nLLM exhibited more aggressive negotiation behaviors when the opponent's image\nappeared less aggressive than their own, and less aggressive behaviors when the\nopponents image appeared more aggressive.\n","authors":["Seungjong Sun","Eungu Lee","Seo Yeon Baek","Seunghyun Hwang","Wonbyung Lee","Dongyan Nan","Bernard J. Jansen","Jang Hyun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.03181v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.06034v6","updated":"2024-10-04T06:32:07Z","published":"2024-01-11T16:48:00Z","title":"LinguAlchemy: Fusing Typological and Geographical Elements for Unseen\n  Language Generalization","summary":"  Pretrained language models (PLMs) have become remarkably adept at task and\nlanguage generalization. Nonetheless, they often fail when faced with unseen\nlanguages. In this work, we present LinguAlchemy, a regularization method that\nincorporates various linguistic information covering typological, geographical,\nand phylogenetic features to align PLMs representation to the corresponding\nlinguistic information on each language. Our LinguAlchemy significantly\nimproves the performance of mBERT and XLM-R on low-resource languages in\nmultiple downstream tasks such as intent classification, news classification,\nand semantic relatedness compared to fully finetuned models and displaying a\nhigh degree of unseen language generalization. We further introduce\nAlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the\nlinguistic regularization weights automatically, alleviating the need for\nhyperparameter search.\n","authors":["Muhammad Farid Adilazuarda","Samuel Cahyawijaya","Alham Fikri Aji","Genta Indra Winata","Ayu Purwarianti"],"pdf_url":"https://arxiv.org/pdf/2401.06034v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03000v2","updated":"2024-10-04T06:31:47Z","published":"2024-05-05T17:06:31Z","title":"MedAdapter: Efficient Test-Time Adaptation of Large Language Models\n  towards Medical Reasoning","summary":"  Despite their improved capabilities in generation and reasoning, adapting\nlarge language models (LLMs) to the biomedical domain remains challenging due\nto their immense size and corporate privacy. In this work, we propose\nMedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards\nbiomedical applications. Instead of fine-tuning the entire LLM, MedAdapter\neffectively adapts the original model by fine-tuning only a small BERT-sized\nadapter to rank candidate solutions generated by LLMs. Experiments demonstrate\nthat MedAdapter effectively adapts both white-box and black-box LLMs in\nbiomedical reasoning, achieving average performance improvements of 25.48% and\n11.31%, respectively, without requiring extensive computational resources or\nsharing data with third parties. MedAdapter also yields superior performance\nwhen combined with train-time adaptation, highlighting a flexible and\ncomplementary solution to existing adaptation methods. Faced with the\nchallenges of balancing model performance, computational resources, and data\nprivacy, MedAdapter provides an efficient, privacy-preserving, cost-effective,\nand transparent solution for adapting LLMs to the biomedical domain.\n","authors":["Wenqi Shi","Ran Xu","Yuchen Zhuang","Yue Yu","Haotian Sun","Hang Wu","Carl Yang","May D. Wang"],"pdf_url":"https://arxiv.org/pdf/2405.03000v2.pdf","comment":"Accepted in EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2406.12016v2","updated":"2024-10-04T06:26:20Z","published":"2024-06-17T18:33:44Z","title":"Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization","summary":"  Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.\n","authors":["Seungwoo Son","Wonpyo Park","Woohyun Han","Kyuyeun Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2406.12016v2.pdf","comment":"EMNLP 2024 Main (Long)"},{"id":"http://arxiv.org/abs/2406.14026v2","updated":"2024-10-04T06:18:15Z","published":"2024-06-20T06:46:23Z","title":"Demystifying Language Model Forgetting with Low-rank Example\n  Associations","summary":"  Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on and associated\nwith newly learned tasks. Insights on such associations enable efficient and\ntargeted mitigation of forgetting. In this paper, we empirically analyze\nforgetting (measured in log-perplexity increase) that occurs in $N$ upstream\nexamples of language modeling or instruction-tuning after fine-tuning LLMs on\none of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that\nthe matrices display simple low-rank patterns, often well-approximated with\nmultiplicative scalar effects of upstream examples and newly learned tasks. We\nalso examine fine-grained associations with visualization and statistics.\nLeveraging the low-rank nature of the associations, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/\n","authors":["Xisen Jin","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2406.14026v2.pdf","comment":"9 pages; preprint"},{"id":"http://arxiv.org/abs/2409.15202v2","updated":"2024-10-04T06:09:15Z","published":"2024-09-23T16:49:47Z","title":"ASTE Transformer Modelling Dependencies in Aspect-Sentiment Triplet\n  Extraction","summary":"  Aspect-Sentiment Triplet Extraction (ASTE) is a recently proposed task of\naspect-based sentiment analysis that consists in extracting (aspect phrase,\nopinion phrase, sentiment polarity) triples from a given sentence. Recent\nstate-of-the-art methods approach this task by first extracting all possible\ntext spans from a given text, then filtering the potential aspect and opinion\nphrases with a classifier, and finally considering all their pairs with another\nclassifier that additionally assigns sentiment polarity to them. Although\nseveral variations of the above scheme have been proposed, the common feature\nis that the final result is constructed by a sequence of independent classifier\ndecisions. This hinders the exploitation of dependencies between extracted\nphrases and prevents the use of knowledge about the interrelationships between\nclassifier predictions to improve performance. In this paper, we propose a new\nASTE approach consisting of three transformer-inspired layers, which enables\nthe modelling of dependencies both between phrases and between the final\nclassifier decisions. Experimental results show that the method achieves higher\nperformance in terms of F1 measure than other methods studied on popular\nbenchmarks. In addition, we show that a simple pre-training technique further\nimproves the performance of the model.\n","authors":["Iwo Naglik","Mateusz Lango"],"pdf_url":"https://arxiv.org/pdf/2409.15202v2.pdf","comment":"The 2024 Conference on Empirical Methods in Natural Language\n  Processing, November 12-16, Miami, Florida 9 pages, appendix, diagrams"},{"id":"http://arxiv.org/abs/2410.03170v1","updated":"2024-10-04T06:05:17Z","published":"2024-10-04T06:05:17Z","title":"Autoregressive Large Language Models are Computationally Universal","summary":"  We show that autoregressive decoding of a transformer-based language model\ncan realize universal computation, without external intervention or\nmodification of the model's weights. Establishing this result requires\nunderstanding how a language model can process arbitrarily long inputs using a\nbounded context. For this purpose, we consider a generalization of\nautoregressive decoding where, given a long input, emitted tokens are appended\nto the end of the sequence as the context window advances. We first show that\nthe resulting system corresponds to a classical model of computation, a Lag\nsystem, that has long been known to be computationally universal. By leveraging\na new proof, we show that a universal Turing machine can be simulated by a Lag\nsystem with 2027 production rules. We then investigate whether an existing\nlarge language model can simulate the behaviour of such a universal Lag system.\nWe give an affirmative answer by showing that a single system-prompt can be\ndeveloped for gemini-1.5-pro-001 that drives the model, under deterministic\n(greedy) decoding, to correctly apply each of the 2027 production rules. We\nconclude that, by the Church-Turing thesis, prompted gemini-1.5-pro-001 with\nextended autoregressive (greedy) decoding is a general purpose computer.\n","authors":["Dale Schuurmans","Hanjun Dai","Francesco Zanini"],"pdf_url":"https://arxiv.org/pdf/2410.03170v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2410.03168v1","updated":"2024-10-04T06:01:27Z","published":"2024-10-04T06:01:27Z","title":"Can Watermarked LLMs be Identified by Users via Crafted Prompts?","summary":"  Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.\n","authors":["Aiwei Liu","Sheng Guan","Yiming Liu","Leyi Pan","Yifei Zhang","Liancheng Fang","Lijie Wen","Philip S. Yu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.03168v1.pdf","comment":"25 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.07128v3","updated":"2024-10-04T05:56:56Z","published":"2024-01-13T18:09:05Z","title":"EHRAgent: Code Empowers Large Language Models for Few-shot Complex\n  Tabular Reasoning on Electronic Health Records","summary":"  Large language models (LLMs) have demonstrated exceptional capabilities in\nplanning and tool utilization as autonomous agents, but few have been developed\nfor medical problem-solving. We propose EHRAgent, an LLM agent empowered with a\ncode interface, to autonomously generate and execute code for multi-tabular\nreasoning within electronic health records (EHRs). First, we formulate an EHR\nquestion-answering task into a tool-use planning process, efficiently\ndecomposing a complicated task into a sequence of manageable actions. By\nintegrating interactive coding and execution feedback, EHRAgent learns from\nerror messages and improves the originally generated code through iterations.\nFurthermore, we enhance the LLM agent by incorporating long-term memory, which\nallows EHRAgent to effectively select and build upon the most relevant\nsuccessful cases from past experiences. Experiments on three real-world\nmulti-tabular EHR datasets show that EHRAgent outperforms the strongest\nbaseline by up to 29.6% in success rate. EHRAgent leverages the emerging\nfew-shot learning capabilities of LLMs, enabling autonomous code generation and\nexecution to tackle complex clinical tasks with minimal demonstrations.\n","authors":["Wenqi Shi","Ran Xu","Yuchen Zhuang","Yue Yu","Jieyu Zhang","Hang Wu","Yuanda Zhu","Joyce Ho","Carl Yang","May D. Wang"],"pdf_url":"https://arxiv.org/pdf/2401.07128v3.pdf","comment":"Accepted in EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2403.15744v6","updated":"2024-10-04T05:56:18Z","published":"2024-03-23T07:16:23Z","title":"On the Fragility of Active Learners for Text Classification","summary":"  Active learning (AL) techniques optimally utilize a labeling budget by\niteratively selecting instances that are most valuable for learning. However,\nthey lack ``prerequisite checks'', i.e., there are no prescribed criteria to\npick an AL algorithm best suited for a dataset. A practitioner must pick a\ntechnique they \\emph{trust} would beat random sampling, based on prior reported\nresults, and hope that it is resilient to the many variables in their\nenvironment: dataset, labeling budget and prediction pipelines. The important\nquestions then are: how often on average, do we expect any AL technique to\nreliably beat the computationally cheap and easy-to-implement strategy of\nrandom sampling? Does it at least make sense to use AL in an ``Always ON'' mode\nin a prediction pipeline, so that while it might not always help, it never\nunder-performs random sampling? How much of a role does the prediction pipeline\nplay in AL's success?\n  We examine these questions in detail for the task of text classification\nusing pre-trained representations, which are ubiquitous today.\n  Our primary contribution here is a rigorous evaluation of AL techniques, old\nand new, across setups that vary wrt datasets, text representations and\nclassifiers. This unlocks multiple insights around warm-up times, i.e., number\nof labels before gains from AL are seen, viability of an ``Always ON'' mode and\nthe relative significance of different factors. Additionally, we release a\nframework for rigorous benchmarking of AL techniques for text classification.\n","authors":["Abhishek Ghose","Emma Thuong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.15744v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.14187v2","updated":"2024-10-04T05:52:42Z","published":"2022-03-27T02:21:19Z","title":"Educational Question Generation of Children Storybooks via Question Type\n  Distribution Learning and Event-Centric Summarization","summary":"  Generating educational questions of fairytales or storybooks is vital for\nimproving children's literacy ability. However, it is challenging to generate\nquestions that capture the interesting aspects of a fairytale story with\neducational meaningfulness. In this paper, we propose a novel question\ngeneration method that first learns the question type distribution of an input\nstory paragraph, and then summarizes salient events which can be used to\ngenerate high-cognitive-demand questions. To train the event-centric\nsummarizer, we finetune a pre-trained transformer-based sequence-to-sequence\nmodel using silver samples composed by educational question-answer pairs. On a\nnewly proposed educational question answering dataset FairytaleQA, we show good\nperformance of our method on both automatic and human evaluation metrics. Our\nwork indicates the necessity of decomposing question type distribution learning\nand event-centric summary generation for educational question generation.\n","authors":["Zhenjie Zhao","Yufang Hou","Dakuo Wang","Mo Yu","Chengzhong Liu","Xiaojuan Ma"],"pdf_url":"https://arxiv.org/pdf/2203.14187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12168v3","updated":"2024-10-04T05:41:08Z","published":"2024-06-18T00:41:40Z","title":"BPO: Staying Close to the Behavior LLM Creates Better Online LLM\n  Alignment","summary":"  Direct alignment from preferences (DAP) has emerged as a promising paradigm\nfor aligning large language models (LLMs) to human desiderata from\npre-collected, offline preference datasets. While recent studies indicate that\nexisting offline DAP methods can directly benefit from online training samples,\nwe highlight the need to develop specific online DAP algorithms to fully\nharness the power of online training. Specifically, we identify that the\nlearned LLM should adhere to the proximity of the behavior LLM, which collects\nthe training samples. To this end, we propose online Preference Optimization in\nproximity to the Behavior LLM (BPO), emphasizing the importance of constructing\na proper trust region for LLM alignment.\n  We conduct extensive experiments to validate the effectiveness and\napplicability of our approach by integrating it with various DAP methods,\nresulting in significant performance improvements across a wide range of tasks\nwhen training with the same amount of preference data. Even when only\nintroducing one additional data collection phase, our online BPO improves its\noffline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on\nAnthropic Helpfulness in terms of win rate against human reference text.\n","authors":["Wenda Xu","Jiachen Li","William Yang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2406.12168v3.pdf","comment":"Wenda Xu and Jiachen Li contributed equally. Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2311.09756v3","updated":"2024-10-04T05:39:32Z","published":"2023-11-16T10:30:26Z","title":"StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for\n  Children's Story-Based Learning","summary":"  Interactive story reading is a common parent-child activity, where parents\nexpect to teach both language skills and real-world knowledge beyond the story.\nWhile increasing storytelling and reading systems have been developed for this\nactivity, they often fail to infuse real-world knowledge into the conversation.\nThis limitation can be attributed to the existing question-answering (QA)\ndatasets used for children's education, upon which the systems are built,\nfailing to capture the nuances of how education experts think when conducting\ninteractive story reading activities. To bridge this gap, we design an\nannotation framework, empowered by existing knowledge graph to capture experts'\nannotations and thinking process, and leverage this framework to construct\nStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with\nreal-world knowledge. We conduct automated and human expert evaluations across\nvarious QA pair generation settings to demonstrate that our StorySparkQA can\neffectively support models in generating QA pairs that target real-world\nknowledge beyond story content. StorySparkQA is available at\nhttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.\n","authors":["Jiaju Chen","Yuxuan Lu","Shao Zhang","Bingsheng Yao","Yuanzhe Dong","Ying Xu","Yunyao Li","Qianwen Wang","Dakuo Wang","Yuling Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09756v3.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.07087v2","updated":"2024-10-04T05:35:57Z","published":"2024-07-09T17:58:18Z","title":"CopyBench: Measuring Literal and Non-Literal Reproduction of\n  Copyright-Protected Text in Language Model Generation","summary":"  Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2\\% to 10.5\\% and non-literal copying from\n2.3\\% to 5.9\\% when comparing Llama3-8B and 70B models, respectively. We\nfurther evaluate the effectiveness of current strategies for mitigating copying\nand show that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.\n","authors":["Tong Chen","Akari Asai","Niloofar Mireshghallah","Sewon Min","James Grimmelmann","Yejin Choi","Hannaneh Hajishirzi","Luke Zettlemoyer","Pang Wei Koh"],"pdf_url":"https://arxiv.org/pdf/2407.07087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06648v3","updated":"2024-10-04T05:33:51Z","published":"2023-12-11T18:57:35Z","title":"Dense X Retrieval: What Retrieval Granularity Should We Use?","summary":"  Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our experiments reveal that\nindexing a corpus by fine-grained units such as propositions significantly\noutperforms passage-level units in retrieval tasks. Moreover, constructing\nprompts with fine-grained retrieved units for retrieval-augmented language\nmodels improves the performance of downstream QA tasks given a specific\ncomputation budget.\n","authors":["Tong Chen","Hongwei Wang","Sihao Chen","Wenhao Yu","Kaixin Ma","Xinran Zhao","Hongming Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06648v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03154v1","updated":"2024-10-04T05:29:51Z","published":"2024-10-04T05:29:51Z","title":"Exploring Learnability in Memory-Augmented Recurrent Neural Networks:\n  Precision, Stability, and Empirical Insights","summary":"  This study explores the learnability of memory-less and memory-augmented\nRNNs, which are theoretically equivalent to Pushdown Automata. Empirical\nresults show that these models often fail to generalize on longer sequences,\nrelying more on precision than mastering symbolic grammar. Experiments on fully\ntrained and component-frozen models reveal that freezing the memory component\nsignificantly improves performance, achieving state-of-the-art results on the\nPenn Treebank dataset (test perplexity reduced from 123.5 to 120.5). Models\nwith frozen memory retained up to 90% of initial performance on longer\nsequences, compared to a 60% drop in standard models. Theoretical analysis\nsuggests that freezing memory stabilizes temporal dependencies, leading to\nrobust convergence. These findings stress the need for stable memory designs\nand long-sequence evaluations to understand RNNs true learnability limits.\n","authors":["Shrabon Das","Ankur Mali"],"pdf_url":"https://arxiv.org/pdf/2410.03154v1.pdf","comment":"21 pages, 4 theorems, 5 tables"},{"id":"http://arxiv.org/abs/2403.04224v4","updated":"2024-10-04T05:29:18Z","published":"2024-03-07T04:54:56Z","title":"Aligners: Decoupling LLMs and Alignment","summary":"  Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We use the same synthetic data to train inspectors, binary\nmiss-alignment classification models to guide a \"squad\" of multiple aligners.\nOur empirical results demonstrate consistent improvements when applying aligner\nsquad to various LLMs, including chat-aligned models, across several\ninstruction-following and red-teaming datasets.\n","authors":["Lilian Ngweta","Mayank Agarwal","Subha Maity","Alex Gittens","Yuekai Sun","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2403.04224v4.pdf","comment":"Short version accepted as a Tiny Paper at the International\n  Conference on Learning Representations (ICLR) 2024. Long version accepted to\n  the Conference on Empirical Methods in Natural Language Processing (EMNLP)\n  2024 Findings"},{"id":"http://arxiv.org/abs/2410.03151v1","updated":"2024-10-04T05:21:42Z","published":"2024-10-04T05:21:42Z","title":"Media Framing through the Lens of Event-Centric Narratives","summary":"  From a communications perspective, a frame defines the packaging of the\nlanguage used in such a way as to encourage certain interpretations and to\ndiscourage others. For example, a news article can frame immigration as either\na boost or a drain on the economy, and thus communicate very different\ninterpretations of the same phenomenon. In this work, we argue that to explain\nframing devices we have to look at the way narratives are constructed. As a\nfirst step in this direction, we propose a framework that extracts events and\ntheir relations to other events, and groups them into high-level narratives\nthat help explain frames in news articles. We show that our framework can be\nused to analyze framing in U.S. news for two different domains: immigration and\ngun control.\n","authors":["Rohan Das","Aditya Chandra","I-Ta Lee","Maria Leonor Pacheco"],"pdf_url":"https://arxiv.org/pdf/2410.03151v1.pdf","comment":"Accepted to the 6th Workshop on Narrative Understanding, co-located\n  with EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.11972v3","updated":"2024-10-04T05:20:18Z","published":"2024-04-18T07:59:53Z","title":"Aligning Language Models to Explicitly Handle Ambiguity","summary":"  In interactions between users and language model agents, user utterances\nfrequently exhibit ellipsis (omission of words or phrases) or imprecision (lack\nof exactness) to prioritize efficiency. This can lead to varying\ninterpretations of the same input based on different assumptions or background\nknowledge. It is thus crucial for agents to adeptly handle the inherent\nambiguity in queries to ensure reliability. However, even state-of-the-art\nlarge language models (LLMs) still face challenges in such scenarios, primarily\ndue to the following hurdles: (1) LLMs are not explicitly trained to deal with\nambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may\nvary depending on the possessed knowledge. To address these issues, we propose\nAlignment with Perceived Ambiguity (APA), a novel pipeline that aligns LLMs to\nmanage ambiguous queries by leveraging their own assessment of ambiguity (i.e.,\nperceived ambiguity). Experimental results on question-answering datasets\ndemonstrate that APA empowers LLMs to explicitly detect and manage ambiguous\nqueries while retaining the ability to answer clear questions. Furthermore, our\nfinding proves that APA excels beyond training with gold-standard labels,\nespecially in out-of-distribution scenarios. The data and code are available at\nhttps://github.com/heyjoonkim/APA.\n","authors":["Hyuhng Joon Kim","Youna Kim","Cheonbok Park","Junyeob Kim","Choonghyun Park","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2404.11972v3.pdf","comment":"EMNLP 2024 (main)"},{"id":"http://arxiv.org/abs/2404.08634v2","updated":"2024-10-04T05:14:48Z","published":"2024-04-12T17:53:34Z","title":"Inheritune: Training Smaller Yet More Attentive Language Models","summary":"  Large Language Models (LLMs) have achieved remarkable performance across\nvarious natural language processing tasks, primarily due to the transformer\narchitecture and its self-attention mechanism. However, we observe that in\nstandard decoder-style LLMs, attention matrices degenerate to single-column for\ndeeper layers. Layers in this state are unable to learn anything meaningful and\nmostly redundant; we refer to these as lazy layers. The goal of this paper is\nto train smaller models by eliminating this structural inefficiency without\ncompromising performance.\n  Motivated by this observation, we propose Inheritune, a simple yet effective\ntraining recipe for developing smaller, high-performing language models.\nSmaller models trained with Inheritune, inherit early transformer layers from a\nlarger pre-trained model, then retrain and progressively expand until they\nmatch or exceed the performance of the larger model. We demonstrate that\nInheritune enables the training of various sizes of GPT-2 models on datasets\nlike OpenWebText-9B and FineWeb_edu. Models trained with Inheritune, despite\nhaving significantly fewer layers, match or even surpass the performance of\ntheir larger counterparts. For instance, our 16-layer GPT-2 medium variant\nachieves comparable performance to the standard 24-layer GPT-2 medium model.\nCode is available at https://github.com/sanyalsunny111/LLM-Inheritune.\n","authors":["Sunny Sanyal","Ravid Shwartz-Ziv","Alexandros G. Dimakis","Sujay Sanghavi"],"pdf_url":"https://arxiv.org/pdf/2404.08634v2.pdf","comment":"25 pages, 13 figures, 10 tables"},{"id":"http://arxiv.org/abs/2406.09324v2","updated":"2024-10-04T05:14:30Z","published":"2024-06-13T17:01:40Z","title":"Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs","summary":"  Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we evaluate the impact of\nvarious attack settings on LLM performance and provide a baseline benchmark for\njailbreak attacks, encouraging the adoption of a standardized evaluation\nframework. Specifically, we evaluate the eight key factors of implementing\njailbreak attacks on LLMs from both target-level and attack-level perspectives.\nWe further conduct seven representative jailbreak attacks on six defense\nmethods across two widely used datasets, encompassing approximately 354\nexperiments with about 55,000 GPU hours on A800-80G. Our experimental results\nhighlight the need for standardized benchmarking to evaluate these attacks on\ndefense-enhanced LLMs. Our code is available at\nhttps://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.\n","authors":["Zhao Xu","Fan Liu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09324v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.04819v3","updated":"2024-10-04T05:13:58Z","published":"2024-05-08T05:38:20Z","title":"DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's\n  Disease Questions with Scientific Literature","summary":"  Recent advancements in large language models (LLMs) have achieved promising\nperformances across various applications. Nonetheless, the ongoing challenge of\nintegrating long-tail knowledge continues to impede the seamless adoption of\nLLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic\nCo-Augmentation of LLMs and KG, to address this limitation and demonstrate its\nability on studying Alzheimer's Disease (AD), a specialized sub-field in\nbiomedicine and a global health priority. With a synergized framework of LLM\nand KG mutually enhancing each other, we first leverage LLM to construct an\nevolving AD-specific knowledge graph (KG) sourced from AD-related scientific\nliterature, and then we utilize a coarse-to-fine sampling method with a novel\nself-aware knowledge retrieval approach to select appropriate knowledge from\nthe KG to augment LLM inference capabilities. The experimental results,\nconducted on our constructed AD question answering (ADQA) benchmark, underscore\nthe efficacy of DALK. Additionally, we perform a series of detailed analyses\nthat can offer valuable insights and guidelines for the emerging topic of\nmutually enhancing KG and LLM. We will release the code and data at\nhttps://github.com/David-Li0406/DALK.\n","authors":["Dawei Li","Shu Yang","Zhen Tan","Jae Young Baik","Sukwon Yun","Joseph Lee","Aaron Chacko","Bojian Hou","Duy Duong-Tran","Ying Ding","Huan Liu","Li Shen","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.04819v3.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03147v1","updated":"2024-10-04T05:07:55Z","published":"2024-10-04T05:07:55Z","title":"Analysis and Detection of Differences in Spoken User Behaviors between\n  Autonomous and Wizard-of-Oz Systems","summary":"  This study examined users' behavioral differences in a large corpus of\nJapanese human-robot interactions, comparing interactions between a\ntele-operated robot and an autonomous dialogue system. We analyzed user spoken\nbehaviors in both attentive listening and job interview dialogue scenarios.\nResults revealed significant differences in metrics such as speech length,\nspeaking rate, fillers, backchannels, disfluencies, and laughter between\noperator-controlled and autonomous conditions. Furthermore, we developed\npredictive models to distinguish between operator and autonomous system\nconditions. Our models demonstrated higher accuracy and precision compared to\nthe baseline model, with several models also achieving a higher F1 score than\nthe baseline.\n","authors":["Mikey Elmers","Koji Inoue","Divesh Lala","Keiko Ochi","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2410.03147v1.pdf","comment":"Accepted and will be presented at the 27th conference of the Oriental\n  COCOSDA (O-COCOSDA 2024)"},{"id":"http://arxiv.org/abs/2401.15498v3","updated":"2024-10-04T05:02:38Z","published":"2024-01-27T20:26:03Z","title":"Do We Need Language-Specific Fact-Checking Models? The Case of Chinese","summary":"  This paper investigates the potential benefits of language-specific\nfact-checking models, focusing on the case of Chinese. We first demonstrate the\nlimitations of translation-based methods and multilingual large language models\n(e.g., GPT-4), highlighting the need for language-specific systems. We further\npropose a Chinese fact-checking system that can better retrieve evidence from a\ndocument by incorporating context information. To better analyze token-level\nbiases in different systems, we construct an adversarial dataset based on the\nCHEF dataset, where each instance has large word overlap with the original one\nbut holds the opposite veracity label. Experimental results on the CHEF dataset\nand our adversarial dataset show that our proposed method outperforms\ntranslation-based methods and multilingual LLMs and is more robust toward\nbiases, while there is still large room for improvement, emphasizing the\nimportance of language-specific fact-checking systems.\n","authors":["Caiqi Zhang","Zhijiang Guo","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2401.15498v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2311.06985v3","updated":"2024-10-04T05:00:24Z","published":"2023-11-12T23:14:43Z","title":"Large Language Models are In-context Teachers for Knowledge Reasoning","summary":"  In this work, we study in-context teaching (ICT), where a teacher provides\nin-context example rationales to teach a student to reason over unseen cases.\nHuman teachers are usually required to craft in-context demonstrations, which\nare costly and have high variance. We ask whether a large language model (LLM)\ncan serve as a more effective in-context teacher for itself or other LLMs,\ncompared to humans. Inspired by the Encoding Specificity Hypothesis from human\nepisodic memory, we hypothesize that in-context exemplars crafted by the\nteacher should match the training data of the student. This hypothesis\nmotivates us to propose Self-Explain where an LLM's self-elicited explanations\nare used as in-context demonstrations for prompting it as they are generalized\nfrom the model's training examples. Self-Explain is shown to significantly\noutperform using human-crafted exemplars and other baselines.\n  Furthermore, we reveal that for ICT, rationales from different teacher LLMs\nor human experts that more resemble the student LLM's self-explanations are\nbetter in-context demonstrations. This supports our encoding specificity\nhypothesis. We then propose Teach-Back that aligns a teacher LLM with the\nstudent to enhance the ICT performance. For example, Teach-Back enables a 7B\nmodel to teach the much larger GPT-3.5 in context, surpassing human teachers by\naround 5% in test accuracy on medical question answering.\n","authors":["Jiachen Zhao","Zonghai Yao","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2311.06985v3.pdf","comment":"EMNLP 24 Findings"},{"id":"http://arxiv.org/abs/2406.17169v2","updated":"2024-10-04T05:00:13Z","published":"2024-06-24T23:02:56Z","title":"Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models","summary":"  As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.\n","authors":["Nisarg Patel","Mohith Kulkarni","Mihir Parmar","Aashna Budhiraja","Mutsumi Nakamura","Neeraj Varshney","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2406.17169v2.pdf","comment":"Accepted at EMNLP 2024 Main"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.03665v1","updated":"2024-10-04T17:59:57Z","published":"2024-10-04T17:59:57Z","title":"Estimating Body and Hand Motion in an Ego-sensed World","summary":"  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture the wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve the hands: the resulting kinematic and temporal\nconstraints result in over 40% lower hand estimation errors compared to noisy\nmonocular estimates. Project page: https://egoallo.github.io/\n","authors":["Brent Yi","Vickie Ye","Maya Zheng","Lea Mller","Georgios Pavlakos","Yi Ma","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.03665v1.pdf","comment":"Project page: https://egoallo.github.io/"},{"id":"http://arxiv.org/abs/2410.03659v1","updated":"2024-10-04T17:59:28Z","published":"2024-10-04T17:59:28Z","title":"Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language\n  Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.\n","authors":["Tinghui Zhu","Qin Liu","Fei Wang","Zhengzhong Tu","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.03659v1.pdf","comment":"Website:\n  https://darthzhu.github.io/cross-modality-knowledge-conflict/"},{"id":"http://arxiv.org/abs/2406.05191v3","updated":"2024-10-04T17:58:13Z","published":"2024-06-07T18:17:17Z","title":"DiffusionPID: Interpreting Diffusion via Partial Information\n  Decomposition","summary":"  Text-to-image diffusion models have made significant progress in generating\nnaturalistic images from textual inputs, and demonstrate the capacity to learn\nand represent complex visual-semantic relationships. While these diffusion\nmodels have achieved remarkable success, the underlying mechanisms driving\ntheir performance are not yet fully accounted for, with many unanswered\nquestions surrounding what they learn, how they represent visual-semantic\nrelationships, and why they sometimes fail to generalize. Our work presents\nDiffusion Partial Information Decomposition (DiffusionPID), a novel technique\nthat applies information-theoretic principles to decompose the input text\nprompt into its elementary components, enabling a detailed examination of how\nindividual tokens and their interactions shape the generated image. We\nintroduce a formal approach to analyze the uniqueness, redundancy, and synergy\nterms by applying PID to the denoising model at both the image and pixel level.\nThis approach enables us to characterize how individual tokens and their\ninteractions affect the model output. We first present a fine-grained analysis\nof characteristics utilized by the model to uniquely localize specific\nconcepts, we then apply our approach in bias analysis and show it can recover\ngender and ethnicity biases. Finally, we use our method to visually\ncharacterize word ambiguity and similarity from the model's perspective and\nillustrate the efficacy of our method for prompt intervention. Our results show\nthat PID is a potent tool for evaluating and diagnosing text-to-image diffusion\nmodels.\n","authors":["Shaurya Dewan","Rushikesh Zawar","Prakanshul Saxena","Yingshan Chang","Andrew Luo","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2406.05191v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03645v1","updated":"2024-10-04T17:51:33Z","published":"2024-10-04T17:51:33Z","title":"GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning\n  LLMs","summary":"  Robotic simulation today remains challenging to scale up due to the human\nefforts required to create diverse simulation tasks and scenes.\nSimulation-trained policies also face scalability issues as many sim-to-real\nmethods focus on a single task. To address these challenges, this work proposes\nGenSim2, a scalable framework that leverages coding LLMs with multi-modal and\nreasoning capabilities for complex and realistic simulation task creation,\nincluding long-horizon tasks with articulated objects. To automatically\ngenerate demonstration data for these tasks at scale, we propose planning and\nRL solvers that generalize within object categories. The pipeline can generate\ndata for up to 100 articulated tasks with 200 objects and reduce the required\nhuman efforts. To utilize such data, we propose an effective multi-task\nlanguage-conditioned policy architecture, dubbed proprioceptive point-cloud\ntransformer (PPT), that learns from the generated demonstrations and exhibits\nstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and the\npolicy architecture, we show a promising usage of GenSim2 that the generated\ndata can be used for zero-shot transfer or co-train with real-world collected\ndata, which enhances the policy performance by 20% compared with training\nexclusively on limited real data.\n","authors":["Pu Hua","Minghuan Liu","Annabella Macaluso","Yunfeng Lin","Weinan Zhang","Huazhe Xu","Lirui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03645v1.pdf","comment":"CoRL 2024. Project website: https://gensim2.github.io/"},{"id":"http://arxiv.org/abs/2410.03644v1","updated":"2024-10-04T17:49:32Z","published":"2024-10-04T17:49:32Z","title":"Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need","summary":"  Traditional unlearnable strategies have been proposed to prevent unauthorized\nusers from training on the 2D image data. With more 3D point cloud data\ncontaining sensitivity information, unauthorized usage of this new type data\nhas also become a serious concern. To address this, we propose the first\nintegral unlearnable framework for 3D point clouds including two processes: (i)\nwe propose an unlearnable data protection scheme, involving a class-wise\nsetting established by a category-adaptive allocation strategy and\nmulti-transformations assigned to samples; (ii) we propose a data restoration\nscheme that utilizes class-wise inverse matrix transformation, thus enabling\nauthorized-only training for unlearnable data. This restoration process is a\npractical issue overlooked in most existing unlearnable literature, \\ie, even\nauthorized users struggle to gain knowledge from 3D unlearnable data. Both\ntheoretical and empirical results (including 6 datasets, 16 models, and 2\ntasks) demonstrate the effectiveness of our proposed unlearnable framework. Our\ncode is available at \\url{https://github.com/CGCL-codes/UnlearnablePC}\n","authors":["Xianlong Wang","Minghui Li","Wei Liu","Hangtao Zhang","Shengshan Hu","Yechao Zhang","Ziqi Zhou","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2410.03644v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2306.17210v2","updated":"2024-10-04T17:46:04Z","published":"2023-06-29T18:00:00Z","title":"Scattering Spectra Models for Physics","summary":"  Physicists routinely need probabilistic models for a number of tasks such as\nparameter inference or the generation of new realizations of a field.\nEstablishing such models for highly non-Gaussian fields is a challenge,\nespecially when the number of samples is limited. In this paper, we introduce\nscattering spectra models for stationary fields and we show that they provide\naccurate and robust statistical descriptions of a wide range of fields\nencountered in physics. These models are based on covariances of scattering\ncoefficients, i.e. wavelet decomposition of a field coupled with a point-wise\nmodulus. After introducing useful dimension reductions taking advantage of the\nregularity of a field under rotation and scaling, we validate these models on\nvarious multi-scale physical fields and demonstrate that they reproduce\nstandard statistics, including spatial moments up to 4th order. These\nscattering spectra provide us with a low-dimensional structured representation\nthat captures key properties encountered in a wide range of physical fields.\nThese generic models can be used for data exploration, classification,\nparameter inference, symmetry detection, and component separation.\n","authors":["Sihao Cheng","Rudy Morel","Erwan Allys","Brice Mnard","Stphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2306.17210v2.pdf","comment":"11 pages, 6 figures, plus appendices, updated to published version"},{"id":"http://arxiv.org/abs/2410.03624v1","updated":"2024-10-04T17:29:38Z","published":"2024-10-04T17:29:38Z","title":"HyperCMR: Enhanced Multi-Contrast CMR Reconstruction with Eagle Loss","summary":"  Accelerating image acquisition for cardiac magnetic resonance imaging (CMRI)\nis a critical task. CMRxRecon2024 challenge aims to set the state of the art\nfor multi-contrast CMR reconstruction. This paper presents HyperCMR, a novel\nframework designed to accelerate the reconstruction of multi-contrast cardiac\nmagnetic resonance (CMR) images. HyperCMR enhances the existing PromptMR model\nby incorporating advanced loss functions, notably the innovative Eagle Loss,\nwhich is specifically designed to recover missing high-frequency information in\nundersampled k-space. Extensive experiments conducted on the CMRxRecon2024\nchallenge dataset demonstrate that HyperCMR consistently outperforms the\nbaseline across multiple evaluation metrics, achieving superior SSIM and PSNR\nscores.\n","authors":["Ruru Xu","Caner zer","Ilkay Oksuz"],"pdf_url":"https://arxiv.org/pdf/2410.03624v1.pdf","comment":"MICCAI 2024 STACOM-CMRxRecon"},{"id":"http://arxiv.org/abs/2403.17924v3","updated":"2024-10-04T17:09:40Z","published":"2024-03-26T17:57:05Z","title":"AID: Attention Interpolation of Text-to-Image Diffusion","summary":"  Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.\n","authors":["Qiyuan He","Jinghao Wang","Ziwei Liu","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17924v3.pdf","comment":"NeurIPS 2024 Conference Paper"},{"id":"http://arxiv.org/abs/2407.11229v2","updated":"2024-10-04T16:52:57Z","published":"2024-07-15T20:29:24Z","title":"Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into\n  Consistency and Robustness","summary":"  Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.\n","authors":["Srija Mukhopadhyay","Adnan Qidwai","Aparna Garimella","Pritika Ramu","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.11229v2.pdf","comment":"22 pages, 9 Tables, 5 figures, 22 examples"},{"id":"http://arxiv.org/abs/2410.03592v1","updated":"2024-10-04T16:52:03Z","published":"2024-10-04T16:52:03Z","title":"Variational Bayes Gaussian Splatting","summary":"  Recently, 3D Gaussian Splatting has emerged as a promising approach for\nmodeling 3D scenes using mixtures of Gaussians. The predominant optimization\nmethod for these models relies on backpropagating gradients through a\ndifferentiable rendering pipeline, which struggles with catastrophic forgetting\nwhen dealing with continuous streams of data. To address this limitation, we\npropose Variational Bayes Gaussian Splatting (VBGS), a novel approach that\nframes training a Gaussian splat as variational inference over model\nparameters. By leveraging the conjugacy properties of multivariate Gaussians,\nwe derive a closed-form variational update rule, allowing efficient updates\nfrom partial, sequential observations without the need for replay buffers. Our\nexperiments show that VBGS not only matches state-of-the-art performance on\nstatic datasets, but also enables continual learning from sequentially streamed\n2D and 3D data, drastically improving performance in this setting.\n","authors":["Toon Van de Maele","Ozan Catal","Alexander Tschantz","Christopher L. Buckley","Tim Verbelen"],"pdf_url":"https://arxiv.org/pdf/2410.03592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14900v3","updated":"2024-10-04T16:42:33Z","published":"2023-11-25T02:09:38Z","title":"Resfusion: Denoising Diffusion Probabilistic Models for Image\n  Restoration Based on Prior Residual Noise","summary":"  Recently, research on denoising diffusion models has expanded its application\nto the field of image restoration. Traditional diffusion-based image\nrestoration methods utilize degraded images as conditional input to effectively\nguide the reverse generation process, without modifying the original denoising\ndiffusion process. However, since the degraded images already include\nlow-frequency information, starting from Gaussian white noise will result in\nincreased sampling steps. We propose Resfusion, a general framework that\nincorporates the residual term into the diffusion forward process, starting the\nreverse process directly from the noisy degraded images. The form of our\ninference process is consistent with the DDPM. We introduced a weighted\nresidual noise, named resnoise, as the prediction target and explicitly provide\nthe quantitative relationship between the residual term and the noise term in\nresnoise. By leveraging a smooth equivalence transformation, Resfusion\ndetermine the optimal acceleration step and maintains the integrity of existing\nnoise schedules, unifying the training and inference processes. The\nexperimental results demonstrate that Resfusion exhibits competitive\nperformance on ISTD dataset, LOL dataset and Raindrop dataset with only five\nsampling steps. Furthermore, Resfusion can be easily applied to image\ngeneration and emerges with strong versatility. Our code and model are\navailable at https://github.com/nkicsl/Resfusion.\n","authors":["Zhenning Shi","Haoshuai Zheng","Chen Xu","Changsheng Dong","Bin Pan","Xueshuo Xie","Along He","Tao Li","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2311.14900v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.13548v3","updated":"2024-10-04T16:39:07Z","published":"2024-09-20T14:47:58Z","title":"Data Diet: Can Trimming PET/CT Datasets Enhance Lesion Segmentation?","summary":"  In this work, we describe our approach to compete in the autoPET3 datacentric\ntrack. While conventional wisdom suggests that larger datasets lead to better\nmodel performance, recent studies indicate that excluding certain training\nsamples can enhance model accuracy. We find that in the autoPETIII dataset, a\nmodel that is trained on the entire dataset exhibits undesirable\ncharacteristics by producing a large number of false positives particularly for\nPSMA-PETs. We counteract this by removing the easiest samples from the training\ndataset as measured by the model loss before retraining from scratch. Using the\nproposed approach we manage to drive down the false negative volume and improve\nupon the baseline model in both false negative volume and dice score on the\npreliminary test set. Code and pre-trained models are available at\ngithub.com/alexanderjaus/autopet3_datadiet.\n","authors":["Alexander Jaus","Simon Rei","Jens Klesiek","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.13548v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02646v2","updated":"2024-10-04T16:35:32Z","published":"2024-10-03T16:31:28Z","title":"Learning 3D Perception from Others' Predictions","summary":"  Accurate 3D object detection in real-world environments requires a huge\namount of annotated data with high quality. Acquiring such data is tedious and\nexpensive, and often needs repeated effort when a new sensor is adopted or when\nthe detector is deployed in a new environment. We investigate a new scenario to\nconstruct 3D object detectors: learning from the predictions of a nearby unit\nthat is equipped with an accurate detector. For example, when a self-driving\ncar enters a new area, it may learn from other traffic participants whose\ndetectors have been optimized for that area. This setting is label-efficient,\nsensor-agnostic, and communication-efficient: nearby units only need to share\nthe predictions with the ego agent (e.g., car). Naively using the received\npredictions as ground-truths to train the detector for the ego car, however,\nleads to inferior performance. We systematically study the problem and identify\nviewpoint mismatches and mislocalization (due to synchronization and GPS\nerrors) as the main causes, which unavoidably result in false positives, false\nnegatives, and inaccurate pseudo labels. We propose a distance-based\ncurriculum, first learning from closer units with similar viewpoints and\nsubsequently improving the quality of other units' predictions via\nself-training. We further demonstrate that an effective pseudo label refinement\nmodule can be trained with a handful of annotated data, largely reducing the\ndata quantity necessary to train an object detector. We validate our approach\non the recently released real-world collaborative driving dataset, using\nreference cars' predictions as pseudo labels for the ego car. Extensive\nexperiments including several scenarios (e.g., different sensors, detectors,\nand domains) demonstrate the effectiveness of our approach toward\nlabel-efficient learning of 3D perception from other units' predictions.\n","authors":["Jinsu Yoo","Zhenyang Feng","Tai-Yu Pan","Yihong Sun","Cheng Perng Phoo","Xiangyu Chen","Mark Campbell","Kilian Q. Weinberger","Bharath Hariharan","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2410.02646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16434v3","updated":"2024-10-04T16:35:13Z","published":"2024-09-24T19:57:40Z","title":"Lessons Learned from a Unifying Empirical Study of Parameter-Efficient\n  Transfer Learning (PETL) in Visual Recognition","summary":"  Parameter-efficient transfer learning (PETL) has attracted significant\nattention lately, due to the increasing size of pre-trained models and the need\nto fine-tune (FT) them for superior downstream performance. This community-wide\nenthusiasm has sparked a plethora of approaches. Nevertheless, a systematic\nstudy to understand their performance and suitable application scenarios is\nlacking, leaving questions like when to apply PETL and which approach to use\nlargely unanswered. In this paper, we conduct a unifying empirical study of\nrepresentative PETL methods in the context of Vision Transformers. We\nsystematically tune their hyper-parameters to fairly compare their accuracy on\ndownstream tasks. Our study not only offers a valuable user guide but also\nunveils several new insights. First, if tuned carefully, different PETL methods\ncan obtain similar accuracy in the low-shot benchmark VTAB-1K. This includes\nsimple methods like FT the bias terms that were reported inferior. Second,\nthough with similar accuracy, we find that PETL methods make different mistakes\nand high-confidence predictions, likely due to their different inductive\nbiases. Such an inconsistency (or complementariness) opens up the opportunity\nfor ensemble methods, and we make preliminary attempts at this. Third, going\nbeyond the commonly used low-shot tasks, we find that PETL is also useful in\nmany-shot regimes -- it achieves comparable and sometimes better accuracy than\nfull FT, using much fewer learnable parameters. Last but not least, we\ninvestigate PETL's ability to preserve a pre-trained model's robustness to\ndistribution shifts (e.g., a CLIP backbone). Perhaps not surprisingly, PETL\nmethods outperform full FT alone. However, with weight-space ensembles, the\nfully fine-tuned model can better balance target (i.e., downstream)\ndistribution and distribution shift performance, suggesting a future research\ndirection for PETL.\n","authors":["Zheda Mai","Ping Zhang","Cheng-Hao Tu","Hong-You Chen","Li Zhang","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2409.16434v3.pdf","comment":"Code is available at https://github.com/OSU-MLB/PETL_Vision"},{"id":"http://arxiv.org/abs/2410.03577v1","updated":"2024-10-04T16:30:54Z","published":"2024-10-04T16:30:54Z","title":"Look Twice Before You Answer: Memory-Space Visual Retracing for\n  Hallucination Mitigation in Multimodal Large Language Models","summary":"  Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) are susceptible to hallucinations, especially assertively fabricating\ncontent not present in the visual inputs. To address the aforementioned\nchallenge, we follow a common cognitive process - when one's initial memory of\ncritical on-sight details fades, it is intuitive to look at them a second time\nto seek a factual and accurate answer. Therefore, we introduce Memory-space\nVisual Retracing (MemVR), a novel hallucination mitigation paradigm that\nwithout the need for external knowledge retrieval or additional fine-tuning. In\nparticular, we treat visual prompts as supplementary evidence to be reinjected\ninto MLLMs via Feed Forward Network (FFN) as key-value memory, when the model\nis uncertain or even amnesic about question-relevant visual memories.\nComprehensive experimental evaluations demonstrate that MemVR significantly\nmitigates hallucination issues across various MLLMs and excels in general\nbenchmarks without incurring added time overhead, thus emphasizing its\npotential for widespread applicability.\n","authors":["Xin Zou","Yizhou Wang","Yibo Yan","Sirui Huang","Kening Zheng","Junkai Chen","Chang Tang","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.03577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00741v2","updated":"2024-10-04T16:10:38Z","published":"2024-10-01T14:33:22Z","title":"VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP\n  Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has been widely studied and\napplied in numerous applications. However, the emphasis on brief summary texts\nduring pre-training prevents CLIP from understanding long descriptions. This\nissue is particularly acute regarding videos given that videos often contain\nabundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra\nLength) model, which aims to unleash the long-description understanding\ncapability of video CLIP models. Firstly, we establish an automatic data\ncollection system and gather a large-scale VILD pre-training dataset with VIdeo\nand Long-Description pairs. Then, we propose Text-similarity-guided Primary\nComponent Matching (TPCM) to better learn the distribution of feature space\nwhile expanding the long description capability. We also introduce two new\ntasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware\nDescription Ranking (HDR) for further understanding improvement. Finally, we\nconstruct a Long Video Description Ranking (LVDR) benchmark for evaluating the\nlong-description capability more comprehensively. Extensive experimental\nresults on widely-used text-video retrieval benchmarks with both short and long\ndescriptions and our LVDR benchmark can fully demonstrate the effectiveness of\nour method.\n","authors":["Jiapeng Wang","Chengyu Wang","Kunzhe Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2410.00741v2.pdf","comment":"EMNLP 2024 Main conference"},{"id":"http://arxiv.org/abs/2410.03558v1","updated":"2024-10-04T16:05:14Z","published":"2024-10-04T16:05:14Z","title":"Not All Diffusion Model Activations Have Been Evaluated as\n  Discriminative Features","summary":"  Diffusion models are initially designed for image generation. Recent research\nshows that the internal signals within their backbones, named activations, can\nalso serve as dense features for various discriminative tasks such as semantic\nsegmentation. Given numerous activations, selecting a small yet effective\nsubset poses a fundamental problem. To this end, the early study of this field\nperforms a large-scale quantitative comparison of the discriminative ability of\nthe activations. However, we find that many potential activations have not been\nevaluated, such as the queries and keys used to compute attention scores.\nMoreover, recent advancements in diffusion architectures bring many new\nactivations, such as those within embedded ViT modules. Both combined,\nactivation selection remains unresolved but overlooked. To tackle this issue,\nthis paper takes a further step with a much broader range of activations\nevaluated. Considering the significant increase in activations, a full-scale\nquantitative comparison is no longer operational. Instead, we seek to\nunderstand the properties of these activations, such that the activations that\nare clearly inferior can be filtered out in advance via simple qualitative\nevaluation. After careful analysis, we discover three properties universal\namong diffusion models, enabling this study to go beyond specific models. On\ntop of this, we present effective feature selection solutions for several\npopular diffusion models. Finally, the experiments across multiple\ndiscriminative tasks validate the superiority of our method over the SOTA\ncompetitors. Our code is available at\nhttps://github.com/Darkbblue/generic-diffusion-feature.\n","authors":["Benyuan Meng","Qianqian Xu","Zitai Wang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03555v1","updated":"2024-10-04T16:03:13Z","published":"2024-10-04T16:03:13Z","title":"Enhancing Autonomous Navigation by Imaging Hidden Objects using\n  Single-Photon LiDAR","summary":"  Robust autonomous navigation in environments with limited visibility remains\na critical challenge in robotics. We present a novel approach that leverages\nNon-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improve\nvisibility and enhance autonomous navigation. Our method enables mobile robots\nto \"see around corners\" by utilizing multi-bounce light information,\neffectively expanding their perceptual range without additional infrastructure.\nWe propose a three-module pipeline: (1) Sensing, which captures multi-bounce\nhistograms using SPAD-based LiDAR; (2) Perception, which estimates occupancy\nmaps of hidden regions from these histograms using a convolutional neural\nnetwork; and (3) Control, which allows a robot to follow safe paths based on\nthe estimated occupancy. We evaluate our approach through simulations and\nreal-world experiments on a mobile robot navigating an L-shaped corridor with\nhidden obstacles. Our work represents the first experimental demonstration of\nNLOS imaging for autonomous navigation, paving the way for safer and more\nefficient robotic systems operating in complex environments. We also contribute\na novel dynamics-integrated transient rendering framework for simulating NLOS\nscenarios, facilitating future research in this domain.\n","authors":["Aaron Young","Nevindu M. Batagoda","Harry Zhang","Akshat Dave","Adithya Pediredla","Dan Negrut","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2410.03555v1.pdf","comment":"Project webpage:\n  https://github.com/camera-culture/nlos-aided-autonomous-navigation"},{"id":"http://arxiv.org/abs/2410.03505v1","updated":"2024-10-04T15:20:57Z","published":"2024-10-04T15:20:57Z","title":"Classification-Denoising Networks","summary":"  Image classification and denoising suffer from complementary issues of lack\nof robustness or partially ignoring conditioning information. We argue that\nthey can be alleviated by unifying both tasks through a model of the joint\nprobability of (noisy) images and class labels. Classification is performed\nwith a forward pass followed by conditioning. Using the Tweedie-Miyasawa\nformula, we evaluate the denoising function with the score, which can be\ncomputed by marginalization and back-propagation. The training objective is\nthen a combination of cross-entropy loss and denoising score matching loss\nintegrated over noise levels. Numerical experiments on CIFAR-10 and ImageNet\nshow competitive classification and denoising performance compared to reference\ndeep convolutional classifiers/denoisers, and significantly improves efficiency\ncompared to previous joint approaches. Our model shows an increased robustness\nto adversarial perturbations compared to a standard discriminative classifier,\nand allows for a novel interpretation of adversarial gradients as a difference\nof denoisers.\n","authors":["Louis Thiry","Florentin Guth"],"pdf_url":"https://arxiv.org/pdf/2410.03505v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03499v1","updated":"2024-10-04T15:13:31Z","published":"2024-10-04T15:13:31Z","title":"FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein\n  Estimator","summary":"  Federated Learning (FL) facilitates data privacy by enabling collaborative\nin-situ training across decentralized clients. Despite its inherent advantages,\nFL faces significant challenges of performance and convergence when dealing\nwith data that is not independently and identically distributed (non-i.i.d.).\nWhile previous research has primarily addressed the issue of skewed label\ndistribution across clients, this study focuses on the less explored challenge\nof multi-domain FL, where client data originates from distinct domains with\nvarying feature distributions. We introduce a novel method designed to address\nthese challenges FedStein: Enhancing Multi-Domain Federated Learning Through\nthe James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)\nestimates of batch normalization (BN) statistics across clients, while\nmaintaining local BN parameters. The non-BN layer parameters are exchanged via\nstandard FL techniques. Extensive experiments conducted across three datasets\nand multiple models demonstrate that FedStein surpasses existing methods such\nas FedAvg and FedBN, with accuracy improvements exceeding 14% in certain\ndomains leading to enhanced domain generalization. The code is available at\nhttps://github.com/sunnyinAI/FedStein\n","authors":["Sunny Gupta","Nikita Jangid","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2410.03499v1.pdf","comment":"12 pages, 2 figures. Accepted at International Workshop on Federated\n  Foundation Models In Conjunction with NeurIPS 2024 (FL@FM-NeurIPS'24)"},{"id":"http://arxiv.org/abs/2410.01411v2","updated":"2024-10-04T15:06:09Z","published":"2024-10-02T10:46:05Z","title":"CSIM: A Copula-based similarity index sensitive to local changes for\n  Image quality assessment","summary":"  Image similarity metrics play an important role in computer vision\napplications, as they are used in image processing, computer vision and machine\nlearning. Furthermore, those metrics enable tasks such as image retrieval,\nobject recognition and quality assessment, essential in fields like healthcare,\nastronomy and surveillance. Existing metrics, such as PSNR, MSE, SSIM, ISSM and\nFSIM, often face limitations in terms of either speed, complexity or\nsensitivity to small changes in images. To address these challenges, a novel\nimage similarity metric, namely CSIM, that combines real-time while being\nsensitive to subtle image variations is investigated in this paper. The novel\nmetric uses Gaussian Copula from probability theory to transform an image into\nvectors of pixel distribution associated to local image patches. These vectors\ncontain, in addition to intensities and pixel positions, information on the\ndependencies between pixel values, capturing the structural relationships\nwithin the image. By leveraging the properties of Copulas, CSIM effectively\nmodels the joint distribution of pixel intensities, enabling a more nuanced\ncomparison of image patches making it more sensitive to local changes compared\nto other metrics. Experimental results demonstrate that CSIM outperforms\nexisting similarity metrics in various image distortion scenarios, including\nnoise, compression artifacts and blur. The metric's ability to detect subtle\ndifferences makes it suitable for applications requiring high precision, such\nas medical imaging, where the detection of minor anomalies can be of a high\nimportance. The results obtained in this work can be reproduced from this\nGithub repository: https://github.com/safouaneelg/copulasimilarity.\n","authors":["Safouane El Ghazouali","Umberto Michelucci","Yassin El Hillali","Hichem Nouira"],"pdf_url":"https://arxiv.org/pdf/2410.01411v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2406.05753v4","updated":"2024-10-04T15:00:24Z","published":"2024-06-09T12:16:30Z","title":"Grounding Continuous Representations in Geometry: Equivariant Neural\n  Fields","summary":"  Conditional Neural Fields (CNFs) are increasingly being leveraged as\ncontinuous signal representations, by associating each data-sample with a\nlatent variable that conditions a shared backbone Neural Field (NeF) to\nreconstruct the sample. However, existing CNF architectures face limitations\nwhen using this latent downstream in tasks requiring fine grained geometric\nreasoning, such as classification and segmentation. We posit that this results\nfrom lack of explicit modelling of geometric information (e.g. locality in the\nsignal or the orientation of a feature) in the latent space of CNFs. As such,\nwe propose Equivariant Neural Fields (ENFs), a novel CNF architecture which\nuses a geometry-informed cross-attention to condition the NeF on a geometric\nvariable, a latent point cloud of features, that enables an equivariant\ndecoding from latent to field. We show that this approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws: if the field transforms, the latent\nrepresentation transforms accordingly - and vice versa. Crucially, this\nequivariance relation ensures that the latent is capable of (1) representing\ngeometric patterns faitfhully, allowing for geometric reasoning in latent\nspace, (2) weight-sharing over similar local patterns, allowing for efficient\nlearning of datasets of fields. We validate these main properties in a range of\ntasks including classification, segmentation, forecasting and reconstruction,\nshowing clear improvement over baselines with a geometry-free latent space.\n","authors":["David R Wessels","David M Knigge","Samuele Papa","Riccardo Valperga","Sharvaree Vadgama","Efstratios Gavves","Erik J Bekkers"],"pdf_url":"https://arxiv.org/pdf/2406.05753v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03487v1","updated":"2024-10-04T14:59:10Z","published":"2024-10-04T14:59:10Z","title":"A Multimodal Framework for Deepfake Detection","summary":"  The rapid advancement of deepfake technology poses a significant threat to\ndigital media integrity. Deepfakes, synthetic media created using AI, can\nconvincingly alter videos and audio to misrepresent reality. This creates risks\nof misinformation, fraud, and severe implications for personal privacy and\nsecurity. Our research addresses the critical issue of deepfakes through an\ninnovative multimodal approach, targeting both visual and auditory elements.\nThis comprehensive strategy recognizes that human perception integrates\nmultiple sensory inputs, particularly visual and auditory information, to form\na complete understanding of media content. For visual analysis, a model that\nemploys advanced feature extraction techniques was developed, extracting nine\ndistinct facial characteristics and then applying various machine learning and\ndeep learning models. For auditory analysis, our model leverages\nmel-spectrogram analysis for feature extraction and then applies various\nmachine learning and deep learningmodels. To achieve a combined analysis, real\nand deepfake audio in the original dataset were swapped for testing purposes\nand ensured balanced samples. Using our proposed models for video and audio\nclassification i.e. Artificial Neural Network and VGG19, the overall sample is\nclassified as deepfake if either component is identified as such. Our\nmultimodal framework combines visual and auditory analyses, yielding an\naccuracy of 94%.\n","authors":["Kashish Gandhi","Prutha Kulkarni","Taran Shah","Piyush Chaudhari","Meera Narvekar","Kranti Ghag"],"pdf_url":"https://arxiv.org/pdf/2410.03487v1.pdf","comment":"22 pages, 14 figures, Accepted in Journal of Electrical Systems"},{"id":"http://arxiv.org/abs/2410.03478v1","updated":"2024-10-04T14:52:09Z","published":"2024-10-04T14:52:09Z","title":"VEDIT: Latent Prediction Architecture For Procedural Video\n  Representation Learning","summary":"  Procedural video representation learning is an active research area where the\nobjective is to learn an agent which can anticipate and forecast the future\ngiven the present video input, typically in conjunction with textual\nannotations. Prior works often rely on large-scale pretraining of visual\nencoders and prediction models with language supervision. However, the\nnecessity and effectiveness of extending compute intensive pretraining to learn\nvideo clip sequences with noisy text supervision have not yet been fully\nvalidated by previous works. In this work, we show that a strong off-the-shelf\nfrozen pretrained visual encoder, along with a well designed prediction model,\ncan achieve state-of-the-art (SoTA) performance in forecasting and procedural\nplanning without the need for pretraining the prediction model, nor requiring\nadditional supervision from language or ASR. Instead of learning\nrepresentations from pixel space, our method utilizes the latent embedding\nspace of publicly available vision encoders. By conditioning on frozen\nclip-level embeddings from observed steps to predict the actions of unseen\nsteps, our prediction model is able to learn robust representations for\nforecasting through iterative denoising - leveraging the recent advances in\ndiffusion transformers (Peebles & Xie, 2023). Empirical studies over a total of\nfive procedural learning tasks across four datasets (NIV, CrossTask, COIN and\nEgo4D-v2) show that our model advances the strong baselines in long-horizon\naction anticipation (+2.6% in Verb ED@20, +3.1% in Noun ED@20), and\nsignificantly improves the SoTA in step forecasting (+5.0%), task\nclassification (+3.8%), and procedure planning tasks (up to +2.28% in success\nrate, +3.39% in mAcc, and +0.90% in mIoU).\n","authors":["Han Lin","Tushar Nagarajan","Nicolas Ballas","Mido Assran","Mojtaba Komeili","Mohit Bansal","Koustuv Sinha"],"pdf_url":"https://arxiv.org/pdf/2410.03478v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.01519v3","updated":"2024-10-04T14:37:13Z","published":"2024-07-01T17:59:12Z","title":"DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image\n  Restoration Models","summary":"  This paper introduces a method for zero-shot video restoration using\npre-trained image restoration diffusion models. Traditional video restoration\nmethods often need retraining for different settings and struggle with limited\ngeneralization across various degradation types and datasets. Our approach uses\na hierarchical token merging strategy for keyframes and local frames, combined\nwith a hybrid correspondence mechanism that blends optical flow and\nfeature-based nearest neighbor matching (latent merging). We show that our\nmethod not only achieves top performance in zero-shot video restoration but\nalso significantly surpasses trained models in generalization across diverse\ndatasets and extreme degradations (8$\\times$ super-resolution and high-standard\ndeviation video denoising). We present evidence through quantitative metrics\nand visual comparisons on various challenging datasets. Additionally, our\ntechnique works with any 2D restoration diffusion model, offering a versatile\nand powerful tool for video enhancement tasks without extensive retraining.\nThis research leads to more efficient and widely applicable video restoration\ntechnologies, supporting advancements in fields that require high-quality video\noutput. See our project page for video results and source code at\nhttps://jimmycv07.github.io/DiffIR2VR_web/.\n","authors":["Chang-Han Yeh","Chin-Yang Lin","Zhixiang Wang","Chi-Wei Hsiao","Ting-Hsuan Chen","Hau-Shiang Shiu","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01519v3.pdf","comment":"Project page: https://jimmycv07.github.io/DiffIR2VR_web/"},{"id":"http://arxiv.org/abs/2406.01591v2","updated":"2024-10-04T14:36:11Z","published":"2024-06-03T17:59:34Z","title":"DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video\n  Vessel Segmentation","summary":"  This paper presents Deformable Neural Vessel Representations (DeNVeR), an\nunsupervised approach for vessel segmentation in X-ray videos without annotated\nground truth. DeNVeR uses optical flow and layer separation, enhancing\nsegmentation accuracy and adaptability through test-time training. A key\ncomponent of our research is the introduction of the XACV dataset, the first\nX-ray angiography coronary video dataset with high-quality, manually labeled\nsegmentation ground truth. Our evaluation demonstrates that DeNVeR outperforms\ncurrent state-of-the-art methods in vessel segmentation. This paper marks an\nadvance in medical imaging, providing a robust, data-efficient tool for disease\ndiagnosis and treatment planning and setting a new standard for future research\nin video vessel segmentation. See our project page for video results at\nhttps://kirito878.github.io/DeNVeR/.\n","authors":["Chun-Hung Wu","Shih-Hong Chen","Chih-Yao Hu","Hsin-Yu Wu","Kai-Hsin Chen","Yu-You Chen","Chih-Hai Su","Chih-Kuo Lee","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2406.01591v2.pdf","comment":"Project page: https://kirito878.github.io/DeNVeR/"},{"id":"http://arxiv.org/abs/2410.03463v1","updated":"2024-10-04T14:26:54Z","published":"2024-10-04T14:26:54Z","title":"Diffusion State-Guided Projected Gradient for Inverse Problems","summary":"  Recent advancements in diffusion models have been effective in learning data\npriors for solving inverse problems. They leverage diffusion sampling steps for\ninducing a data prior while using a measurement guidance gradient at each step\nto impose data consistency. For general inverse problems, approximations are\nneeded when an unconditionally trained diffusion model is used since the\nmeasurement likelihood is intractable, leading to inaccurate posterior\nsampling. In other words, due to their approximations, these methods fail to\npreserve the generation process on the data manifold defined by the diffusion\nprior, leading to artifacts in applications such as image restoration. To\nenhance the performance and robustness of diffusion models in solving inverse\nproblems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad),\nwhich projects the measurement gradient onto a subspace that is a low-rank\napproximation of an intermediate state of the diffusion process. DiffStateGrad,\nas a module, can be added to a wide range of diffusion-based inverse solvers to\nimprove the preservation of the diffusion process on the prior manifold and\nfilter out artifact-inducing components. We highlight that DiffStateGrad\nimproves the robustness of diffusion models in terms of the choice of\nmeasurement guidance step size and noise while improving the worst-case\nperformance. Finally, we demonstrate that DiffStateGrad improves upon the\nstate-of-the-art on linear and nonlinear image restoration inverse problems.\n","authors":["Rayhan Zirvi","Bahareh Tolooshams","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2410.03463v1.pdf","comment":"preprint. under review. RZ and BT have equal contributions"},{"id":"http://arxiv.org/abs/2409.19992v2","updated":"2024-10-04T14:20:31Z","published":"2024-09-30T06:30:33Z","title":"A large-scale operational study of fingerprint quality and demographics","summary":"  Even though a few initial works have shown on small sets of data some level\nof bias in the performance of fingerprint recognition technology with respect\nto certain demographic groups, there is still not sufficient evidence to\nunderstand the impact that certain factors such as gender, age or finger-type\nmay have on fingerprint quality and, in turn, also on fingerprint matching\naccuracy. The present work addresses this still under researched topic, on a\nlarge-scale database of operational data containing 10-print impressions of\nalmost 16,000 subjects. The results reached provide further insight into the\ndependency of fingerprint quality and demographics, and show that there in fact\nexists a certain degree of performance variability in fingerprint-based\nrecognition systems for different segments of the population. Based on the\nexperimental evaluation, the work points out new observations based on\ndata-driven evidence, provides plausible hypotheses to explain such\nobservations, and concludes with potential follow-up actions that can help to\nreduce the observed fingerprint quality differences. This way, the current\npaper can be considered as a contribution to further increase the algorithmic\nfairness and equality of biometric technology.\n","authors":["Javier Galbally","Aleksandrs Cepilovs","Ramon Blanco-Gonzalo","Gillian Ormiston","Oscar Miguel-Hurtado","Istvan Sz. Racz"],"pdf_url":"https://arxiv.org/pdf/2409.19992v2.pdf","comment":"Extended journal version submitted to IET Biometrics. 10 pages, 5\n  figures Reference conference paper: J. Galbally, A. Cepilovs, R.\n  Blanco-Gonzalo, G. Ormiston, O. Miguel-Hurtado, and I. S. Racz, 'Fingerprint\n  quality per individual finger type: A large-scale study on real operational\n  data' in Proc. IEEE Intl. Workshop on Biometrics and Forensics 2023 (IWBF\n  2023)"},{"id":"http://arxiv.org/abs/2410.03456v1","updated":"2024-10-04T14:14:28Z","published":"2024-10-04T14:14:28Z","title":"Dynamic Diffusion Transformer","summary":"  Diffusion Transformer (DiT), an emerging diffusion model for image\ngeneration, has demonstrated superior performance but suffers from substantial\ncomputational costs. Our investigations reveal that these costs stem from the\nstatic inference paradigm, which inevitably introduces redundant computation in\ncertain diffusion timesteps and spatial regions. To address this inefficiency,\nwe propose Dynamic Diffusion Transformer (DyDiT), an architecture that\ndynamically adjusts its computation along both timestep and spatial dimensions\nduring generation. Specifically, we introduce a Timestep-wise Dynamic Width\n(TDW) approach that adapts model width conditioned on the generation timesteps.\nIn addition, we design a Spatial-wise Dynamic Token (SDT) strategy to avoid\nredundant computation at unnecessary spatial locations. Extensive experiments\non various datasets and different-sized models verify the superiority of DyDiT.\nNotably, with <3% additional fine-tuning iterations, our method reduces the\nFLOPs of DiT-XL by 51%, accelerates generation by 1.73, and achieves a\ncompetitive FID score of 2.07 on ImageNet. The code is publicly available at\nhttps://github.com/NUS-HPC-AI-Lab/ Dynamic-Diffusion-Transformer.\n","authors":["Wangbo Zhao","Yizeng Han","Jiasheng Tang","Kai Wang","Yibing Song","Gao Huang","Fan Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.03456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03441v1","updated":"2024-10-04T13:56:48Z","published":"2024-10-04T13:56:48Z","title":"CLoSD: Closing the Loop between Simulation and Diffusion for multi-task\n  character control","summary":"  Motion diffusion models and Reinforcement Learning (RL) based control for\nphysics-based simulations have complementary strengths for human motion\ngeneration. The former is capable of generating a wide variety of motions,\nadhering to intuitive control such as text, while the latter offers physically\nplausible motion and direct interaction with the environment. In this work, we\npresent a method that combines their respective strengths. CLoSD is a\ntext-driven RL physics-based controller, guided by diffusion generation for\nvarious tasks. Our key insight is that motion diffusion can serve as an\non-the-fly universal planner for a robust RL controller. To this end, CLoSD\nmaintains a closed-loop interaction between two modules -- a Diffusion Planner\n(DiP), and a tracking controller. DiP is a fast-responding autoregressive\ndiffusion model, controlled by textual prompts and target locations, and the\ncontroller is a simple and robust motion imitator that continuously receives\nmotion plans from DiP and provides feedback from the environment. CLoSD is\ncapable of seamlessly performing a sequence of different tasks, including\nnavigation to a goal location, striking an object with a hand or foot as\nspecified in a text prompt, sitting down, and getting up.\nhttps://guytevet.github.io/CLoSD-page/\n","authors":["Guy Tevet","Sigal Raab","Setareh Cohan","Daniele Reda","Zhengyi Luo","Xue Bin Peng","Amit H. Bermano","Michiel van de Panne"],"pdf_url":"https://arxiv.org/pdf/2410.03441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15476v2","updated":"2024-10-04T13:52:33Z","published":"2024-05-24T11:55:46Z","title":"Editable Concept Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) have garnered much attention for their\nability to elucidate the prediction process through a human-understandable\nconcept layer. However, most previous studies focused on cases where the data,\nincluding concepts, are clean. In many scenarios, we always need to\nremove/insert some training data or new concepts from trained CBMs due to\ndifferent reasons, such as privacy concerns, data mislabelling, spurious\nconcepts, and concept annotation errors. Thus, the challenge of deriving\nefficient editable CBMs without retraining from scratch persists, particularly\nin large-scale applications. To address these challenges, we propose Editable\nConcept Bottleneck Models (ECBMs). Specifically, ECBMs support three different\nlevels of data removal: concept-label-level, concept-level, and data-level.\nECBMs enjoy mathematically rigorous closed-form approximations derived from\ninfluence functions that obviate the need for re-training. Experimental results\ndemonstrate the efficiency and effectiveness of our ECBMs, affirming their\nadaptability within the realm of CBMs.\n","authors":["Lijie Hu","Chenyang Ren","Zhengyu Hu","Hongbin Lin","Cheng-Long Wang","Hui Xiong","Jingfeng Zhang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2405.15476v2.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.03438v1","updated":"2024-10-04T13:52:22Z","published":"2024-10-04T13:52:22Z","title":"Dessie: Disentanglement for Articulated 3D Horse Shape and Pose\n  Estimation from Images","summary":"  In recent years, 3D parametric animal models have been developed to aid in\nestimating 3D shape and pose from images and video. While progress has been\nmade for humans, it's more challenging for animals due to limited annotated\ndata. To address this, we introduce the first method using synthetic data\ngeneration and disentanglement to learn to regress 3D shape and pose. Focusing\non horses, we use text-based texture generation and a synthetic data pipeline\nto create varied shapes, poses, and appearances, learning disentangled spaces.\nOur method, Dessie, surpasses existing 3D horse reconstruction methods and\ngeneralizes to other large animals like zebras, cows, and deer. See the project\nwebsite at: \\url{https://celiali.github.io/Dessie/}.\n","authors":["Ci Li","Yi Yang","Zehang Weng","Elin Hernlund","Silvia Zuffi","Hedvig Kjellstrm"],"pdf_url":"https://arxiv.org/pdf/2410.03438v1.pdf","comment":"ACCV2024"},{"id":"http://arxiv.org/abs/2409.04005v2","updated":"2024-10-04T13:45:54Z","published":"2024-09-06T03:13:45Z","title":"Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for\n  Text-to-Any-Task","summary":"  The global self-attention mechanism in diffusion transformers involves\nredundant computation due to the sparse and redundant nature of visual\ninformation, and the attention map of tokens within a spatial window shows\nsignificant similarity. To address this redundancy, we propose the\nProxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse\nrepresentative token attention (where the number of representative tokens is\nmuch smaller than the total number of tokens) to model global visual\ninformation efficiently. Specifically, within each transformer block, we\ncompute an averaging token from each spatial-temporal window to serve as a\nproxy token for that region. The global semantics are captured through the\nself-attention of these proxy tokens and then injected into all latent tokens\nvia cross-attention. Simultaneously, we introduce window and shift window\nattention to address the limitations in detail modeling caused by the sparse\nattention mechanism. Building on the well-designed PT-DiT, we further develop\nthe Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV\ntasks. Experimental results show that PT-DiT achieves competitive performance\nwhile reducing the computational complexity in both image and video generation\ntasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to\nPixArt-$\\alpha$). The visual exhibition and source code of Qihoo-T2X is\navailable at https://360cvgroup.github.io/Qihoo-T2X/.\n","authors":["Jing Wang","Ao Ma","Jiasong Feng","Dawei Leng","Yuhui Yin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2409.04005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03430v1","updated":"2024-10-04T13:40:15Z","published":"2024-10-04T13:40:15Z","title":"Images Speak Volumes: User-Centric Assessment of Image Generation for\n  Accessible Communication","summary":"  Explanatory images play a pivotal role in accessible and easy-to-read (E2R)\ntexts. However, the images available in online databases are not tailored\ntoward the respective texts, and the creation of customized images is\nexpensive. In this large-scale study, we investigated whether text-to-image\ngeneration models can close this gap by providing customizable images quickly\nand easily. We benchmarked seven, four open- and three closed-source, image\ngeneration models and provide an extensive evaluation of the resulting images.\nIn addition, we performed a user study with people from the E2R target group to\nexamine whether the images met their requirements. We find that some of the\nmodels show remarkable performance, but none of the models are ready to be used\nat a larger scale without human supervision. Our research is an important step\ntoward facilitating the creation of accessible information for E2R creators and\ntailoring accessible images to the target group's needs.\n","authors":["Miriam Anschtz","Tringa Sylaj","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2410.03430v1.pdf","comment":"To be published at TSAR workshop 2024\n  (https://tsar-workshop.github.io/)"},{"id":"http://arxiv.org/abs/2403.05327v3","updated":"2024-10-04T13:37:08Z","published":"2024-03-08T14:06:15Z","title":"DiffSF: Diffusion Models for Scene Flow Estimation","summary":"  Scene flow estimation is an essential ingredient for a variety of real-world\napplications, especially for autonomous agents, such as self-driving cars and\nrobots. While recent scene flow estimation approaches achieve a reasonable\naccuracy, their applicability to real-world systems additionally benefits from\na reliability measure. Aiming at improving accuracy while additionally\nproviding an estimate for uncertainty, we propose DiffSF that combines\ntransformer-based scene flow estimation with denoising diffusion models. In the\ndiffusion process, the ground truth scene flow vector field is gradually\nperturbed by adding Gaussian noise. In the reverse process, starting from\nrandomly sampled Gaussian noise, the scene flow vector field prediction is\nrecovered by conditioning on a source and a target point cloud. We show that\nthe diffusion process greatly increases the robustness of predictions compared\nto prior approaches resulting in state-of-the-art performance on standard scene\nflow estimation benchmarks. Moreover, by sampling multiple times with different\ninitial states, the denoising process predicts multiple hypotheses, which\nenables measuring the output uncertainty, allowing our approach to detect a\nmajority of the inaccurate predictions. The code is available at\nhttps://github.com/ZhangYushan3/DiffSF.\n","authors":["Yushan Zhang","Bastian Wandt","Maria Magnusson","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2403.05327v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03420v1","updated":"2024-10-04T13:30:18Z","published":"2024-10-04T13:30:18Z","title":"Towards Real-time Intrahepatic Vessel Identification in Intraoperative\n  Ultrasound-Guided Liver Surgery","summary":"  While laparoscopic liver resection is less prone to complications and\nmaintains patient outcomes compared to traditional open surgery, its complexity\nhinders widespread adoption due to challenges in representing the liver's\ninternal structure. Laparoscopic intraoperative ultrasound offers efficient,\ncost-effective and radiation-free guidance. Our objective is to aid physicians\nin identifying internal liver structures using laparoscopic intraoperative\nultrasound. We propose a patient-specific approach using preoperative 3D\nultrasound liver volume to train a deep learning model for real-time\nidentification of portal tree and branch structures. Our personalized AI model,\nvalidated on ex vivo swine livers, achieved superior precision (0.95) and\nrecall (0.93) compared to surgeons, laying groundwork for precise vessel\nidentification in ultrasound-based liver resection. Its adaptability and\npotential clinical impact promise to advance surgical interventions and improve\npatient care.\n","authors":["Karl-Philippe Beaudet","Alexandros Karargyris","Sidaty El Hadramy","Stphane Cotin","Jean-Paul Mazellier","Nicolas Padoy","Juan Verde"],"pdf_url":"https://arxiv.org/pdf/2410.03420v1.pdf","comment":"MICCAI 2024, Oct 2024, Marrakech, Morocco"},{"id":"http://arxiv.org/abs/2401.02141v2","updated":"2024-10-04T13:29:14Z","published":"2024-01-04T08:46:39Z","title":"Bayesian Unsupervised Disentanglement of Anatomy and Geometry for Deep\n  Groupwise Image Registration","summary":"  This article presents a general Bayesian learning framework for multi-modal\ngroupwise image registration. The method builds on probabilistic modelling of\nthe image generative process, where the underlying common anatomy and geometric\nvariations of the observed images are explicitly disentangled as latent\nvariables. Therefore, groupwise image registration is achieved via hierarchical\nBayesian inference. We propose a novel hierarchical variational auto-encoding\narchitecture to realise the inference procedure of the latent variables, where\nthe registration parameters can be explicitly estimated in a mathematically\ninterpretable fashion. Remarkably, this new paradigm learns groupwise image\nregistration in an unsupervised closed-loop self-reconstruction process,\nsparing the burden of designing complex image-based similarity measures. The\ncomputationally efficient disentangled network architecture is also inherently\nscalable and flexible, allowing for groupwise registration on large-scale image\ngroups with variable sizes. Furthermore, the inferred structural\nrepresentations from multi-modal images via disentanglement learning are\ncapable of capturing the latent anatomy of the observations with visual\nsemantics. Extensive experiments were conducted to validate the proposed\nframework, including four different datasets from cardiac, brain, and abdominal\nmedical images. The results have demonstrated the superiority of our method\nover conventional similarity-based approaches in terms of accuracy, efficiency,\nscalability, and interpretability.\n","authors":["Xinzhe Luo","Xin Wang","Linda Shapiro","Chun Yuan","Jianfeng Feng","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2401.02141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02713v2","updated":"2024-10-04T13:29:09Z","published":"2024-10-03T17:36:49Z","title":"Video Instruction Tuning With Synthetic Data","summary":"  The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.\n","authors":["Yuanhan Zhang","Jinming Wu","Wei Li","Bo Li","Zejun Ma","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.02713v2.pdf","comment":"Project page: https://llava-vl.github.io/blog/2024-09-30-llava-video/"},{"id":"http://arxiv.org/abs/2410.02309v2","updated":"2024-10-04T13:28:20Z","published":"2024-10-03T08:46:17Z","title":"Decoupling Layout from Glyph in Online Chinese Handwriting Generation","summary":"  Text plays a crucial role in the transmission of human civilization, and\nteaching machines to generate online handwritten text in various styles\npresents an interesting and significant challenge. However, most prior work has\nconcentrated on generating individual Chinese fonts, leaving {complete text\nline generation largely unexplored}. In this paper, we identify that text lines\ncan naturally be divided into two components: layout and glyphs. Based on this\ndivision, we designed a text line layout generator coupled with a\ndiffusion-based stylized font synthesizer to address this challenge\nhierarchically. More concretely, the layout generator performs in-context-like\nlearning based on the text content and the provided style references to\ngenerate positions for each glyph autoregressively. Meanwhile, the font\nsynthesizer which consists of a character embedding dictionary, a multi-scale\ncalligraphy style encoder, and a 1D U-Net based diffusion denoiser will\ngenerate each font on its position while imitating the calligraphy style\nextracted from the given style references. Qualitative and quantitative\nexperiments on the CASIA-OLHWDB demonstrate that our method is capable of\ngenerating structurally correct and indistinguishable imitation samples.\n","authors":["Min-Si Ren","Yan-Ming Zhang","Yi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03417v1","updated":"2024-10-04T13:27:52Z","published":"2024-10-04T13:27:52Z","title":"Img2CAD: Conditioned 3D CAD Model Generation from Single Image with\n  Structured Visual Geometry","summary":"  In this paper, we propose Img2CAD, the first approach to our knowledge that\nuses 2D image inputs to generate CAD models with editable parameters. Unlike\nexisting AI methods for 3D model generation using text or image inputs often\nrely on mesh-based representations, which are incompatible with CAD tools and\nlack editability and fine control, Img2CAD enables seamless integration between\nAI-based 3D reconstruction and CAD software. We have identified an innovative\nintermediate representation called Structured Visual Geometry (SVG),\ncharacterized by vectorized wireframes extracted from objects. This\nrepresentation significantly enhances the performance of generating conditioned\nCAD models. Additionally, we introduce two new datasets to further support\nresearch in this area: ABC-mono, the largest known dataset comprising over\n200,000 3D CAD models with rendered images, and KOCAD, the first dataset\nfeaturing real-world captured objects alongside their ground truth CAD models,\nsupporting further research in conditioned CAD model generation.\n","authors":["Tianrun Chen","Chunan Yu","Yuanqi Hu","Jing Li","Tao Xu","Runlong Cao","Lanyun Zhu","Ying Zang","Yong Zhang","Zejian Li","Linyun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03390v1","updated":"2024-10-04T12:54:21Z","published":"2024-10-04T12:54:21Z","title":"Lightning UQ Box: A Comprehensive Framework for Uncertainty\n  Quantification in Deep Learning","summary":"  Uncertainty quantification (UQ) is an essential tool for applying deep neural\nnetworks (DNNs) to real world tasks, as it attaches a degree of confidence to\nDNN outputs. However, despite its benefits, UQ is often left out of the\nstandard DNN workflow due to the additional technical knowledge required to\napply and evaluate existing UQ procedures. Hence there is a need for a\ncomprehensive toolbox that allows the user to integrate UQ into their modelling\nworkflow, without significant overhead. We introduce \\texttt{Lightning UQ Box}:\na unified interface for applying and evaluating various approaches to UQ. In\nthis paper, we provide a theoretical and quantitative comparison of the wide\nrange of state-of-the-art UQ methods implemented in our toolbox. We focus on\ntwo challenging vision tasks: (i) estimating tropical cyclone wind speeds from\ninfrared satellite imagery and (ii) estimating the power output of solar panels\nfrom RGB images of the sky. By highlighting the differences between methods our\nresults demonstrate the need for a broad and approachable experimental\nframework for UQ, that can be used for benchmarking UQ methods. The toolbox,\nexample implementations, and further information are available at:\nhttps://github.com/lightning-uq-box/lightning-uq-box\n","authors":["Nils Lehmann","Jakob Gawlikowski","Adam J. Stewart","Vytautas Jancauskas","Stefan Depeweg","Eric Nalisnick","Nina Maria Gottschling"],"pdf_url":"https://arxiv.org/pdf/2410.03390v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.03359v1","updated":"2024-10-04T12:26:51Z","published":"2024-10-04T12:26:51Z","title":"An Enhanced Harmonic Densely Connected Hybrid Transformer Network\n  Architecture for Chronic Wound Segmentation Utilising Multi-Colour Space\n  Tensor Merging","summary":"  Chronic wounds and associated complications present ever growing burdens for\nclinics and hospitals world wide. Venous, arterial, diabetic, and pressure\nwounds are becoming increasingly common globally. These conditions can result\nin highly debilitating repercussions for those affected, with limb amputations\nand increased mortality risk resulting from infection becoming more common. New\nmethods to assist clinicians in chronic wound care are therefore vital to\nmaintain high quality care standards. This paper presents an improved HarDNet\nsegmentation architecture which integrates a contrast-eliminating component in\nthe initial layers of the network to enhance feature learning. We also utilise\na multi-colour space tensor merging process and adjust the harmonic shape of\nthe convolution blocks to facilitate these additional features. We train our\nproposed model using wound images from light-skinned patients and test the\nmodel on two test sets (one set with ground truth, and one without) comprising\nonly darker-skinned cases. Subjective ratings are obtained from clinical wound\nexperts with intraclass correlation coefficient used to determine inter-rater\nreliability. For the dark-skin tone test set with ground truth, we demonstrate\nimprovements in terms of Dice similarity coefficient (+0.1221) and intersection\nover union (+0.1274). Qualitative analysis showed high expert ratings, with\nimprovements of >3% demonstrated when comparing the baseline model with the\nproposed model. This paper presents the first study to focus on darker-skin\ntones for chronic wound segmentation using models trained only on wound images\nexhibiting lighter skin. Diabetes is highly prevalent in countries where\npatients have darker skin tones, highlighting the need for a greater focus on\nsuch cases. Additionally, we conduct the largest qualitative study to date for\nchronic wound segmentation.\n","authors":["Bill Cassidy","Christian Mcbride","Connah Kendrick","Neil D. Reeves","Joseph M. Pappachan","Cornelius J. Fernandez","Elias Chacko","Raphael Brngel","Christoph M. Friedrich","Metib Alotaibi","Abdullah Abdulaziz AlWabel","Mohammad Alderwish","Kuan-Ying Lai","Moi Hoon Yap"],"pdf_url":"https://arxiv.org/pdf/2410.03359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03355v1","updated":"2024-10-04T12:21:03Z","published":"2024-10-04T12:21:03Z","title":"LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding","summary":"  Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.76}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model.\n","authors":["Doohyuk Jang","Sihwan Park","June Yong Yang","Yeonsung Jung","Jihun Yun","Souvik Kundu","Sung-Yub Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2410.03355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15127v2","updated":"2024-10-04T12:20:44Z","published":"2024-08-27T15:07:58Z","title":"T-FAKE: Synthesizing Thermal Images for Facial Landmarking","summary":"  Facial analysis is a key component in a wide range of applications such as\nsecurity, autonomous driving, entertainment, and healthcare. Despite the\navailability of various facial RGB datasets, the thermal modality, which plays\na crucial role in life sciences, medicine, and biometrics, has been largely\noverlooked. To address this gap, we introduce the T-FAKE dataset, a new\nlarge-scale synthetic thermal dataset with sparse and dense landmarks. To\nfacilitate the creation of the dataset, we propose a novel RGB2Thermal loss\nfunction, which enables the transfer of thermal style to RGB faces. By\nutilizing the Wasserstein distance between thermal and RGB patches and the\nstatistical analysis of clinical temperature distributions on faces, we ensure\nthat the generated thermal images closely resemble real samples. Using\nRGB2Thermal style transfer based on our RGB2Thermal loss function, we create\nthe T-FAKE dataset, a large-scale synthetic thermal dataset of faces.\nLeveraging our novel T-FAKE dataset, probabilistic landmark prediction, and\nlabel adaptation networks, we demonstrate significant improvements in landmark\ndetection methods on thermal images across different landmark conventions. Our\nmodels show excellent performance with both sparse 70-point landmarks and dense\n478-point landmark annotations. Our code and models are available at\nhttps://github.com/phflot/tfake.\n","authors":["Philipp Flotho","Moritz Piening","Anna Kukleva","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2408.15127v2.pdf","comment":"22 pages, 12 figures, Philipp Flotho and Moritz Piening share equal\n  contribution"},{"id":"http://arxiv.org/abs/2410.03335v1","updated":"2024-10-04T11:40:53Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions, and calls the agent for audio generation. Consequently,\nAudio-Agent generates high-quality audio that is closely aligned with the\nprovided text or video while also supporting variable-length generation. For\nvideo-to-audio (VTA) tasks, most existing methods require training a timestamp\ndetector to synchronize video events with generated audio, a process that can\nbe tedious and time-consuming. We propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions to bridge video and audio modality. Thus our\nframework provides a comprehensive solution for both TTA and VTA tasks without\nsubstantial computational overhead in training.\n","authors":["Zixuan Wang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2410.03335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03334v1","updated":"2024-10-04T11:40:21Z","published":"2024-10-04T11:40:21Z","title":"An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable\n  Radiology Report Generation","summary":"  Radiological services are experiencing unprecedented demand, leading to\nincreased interest in automating radiology report generation. Existing\nVision-Language Models (VLMs) suffer from hallucinations, lack\ninterpretability, and require expensive fine-tuning. We introduce SAE-Rad,\nwhich uses sparse autoencoders (SAEs) to decompose latent representations from\na pre-trained vision transformer into human-interpretable features. Our hybrid\narchitecture combines state-of-the-art SAE advancements, achieving accurate\nlatent reconstructions while maintaining sparsity. Using an off-the-shelf\nlanguage model, we distil ground-truth reports into radiological descriptions\nfor each SAE feature, which we then compile into a full report for each image,\neliminating the need for fine-tuning large models for this task. To the best of\nour knowledge, SAE-Rad represents the first instance of using mechanistic\ninterpretability techniques explicitly for a downstream multi-modal reasoning\ntask. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific\nmetrics compared to state-of-the-art models while using significantly fewer\ncomputational resources for training. Qualitative analysis reveals that SAE-Rad\nlearns meaningful visual concepts and generates reports aligning closely with\nexpert interpretations. Our results suggest that SAEs can enhance multimodal\nreasoning in healthcare, providing a more interpretable alternative to existing\nVLMs.\n","authors":["Ahmed Abdulaal","Hugo Fry","Nina Montaa-Brown","Ayodeji Ijishakin","Jack Gao","Stephanie Hyland","Daniel C. Alexander","Daniel C. Castro"],"pdf_url":"https://arxiv.org/pdf/2410.03334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11536v3","updated":"2024-10-04T11:38:06Z","published":"2024-05-19T12:49:21Z","title":"RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State\n  Estimation Drift Mitigation on LiDAR PointCloud","summary":"  This work addresses limitations in recent 3D tracking-by-detection methods,\nfocusing on identifying legitimate trajectories and addressing state estimation\ndrift in Kalman filters. Current methods rely heavily on threshold-based\nfiltering of false positive detections using detection scores to prevent ghost\ntrajectories. However, this approach is inadequate for distant and partially\noccluded objects, where detection scores tend to drop, potentially leading to\nfalse positives exceeding the threshold. Additionally, the literature generally\ntreats detections as precise localizations of objects. Our research reveals\nthat noise in detections impacts localization information, causing trajectory\ndrift for occluded objects and hindering recovery. To this end, we propose a\nnovel online track validity mechanism that temporally distinguishes between\nlegitimate and ghost tracks, along with a multi-stage observational gating\nprocess for incoming observations. This mechanism significantly improves\ntracking performance, with a $6.28\\%$ in HOTA and a $17.87\\%$ increase in MOTA.\nWe also introduce a refinement to the Kalman filter that enhances noise\nmitigation in trajectory drift, leading to more robust state estimation for\noccluded objects. Our framework, RobMOT, outperforms state-of-the-art methods,\nincluding deep learning approaches, across various detectors, achieving up to a\n$4\\%$ margin in HOTA and $6\\%$ in MOTA. RobMOT excels under challenging\nconditions, such as prolonged occlusions and tracking distant objects, with up\nto a 59\\% improvement in processing latency.\n","authors":["Mohamed Nagy","Naoufel Werghi","Bilal Hassan","Jorge Dias","Majid Khonji"],"pdf_url":"https://arxiv.org/pdf/2405.11536v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00524v2","updated":"2024-10-04T11:32:31Z","published":"2024-10-01T09:07:24Z","title":"Deep Model Interpretation with Limited Data : A Coreset-based Approach","summary":"  Model Interpretation aims at the extraction of insights from the internals of\na trained model. A common approach to address this task is the characterization\nof relevant features internally encoded in the model that are critical for its\nproper operation. Despite recent progress of these methods, they come with the\nweakness of being computationally expensive due to the dense evaluation of\ndatasets that they require. As a consequence, research on the design of these\nmethods have focused on smaller data subsets which may led to reduced insights.\nTo address these computational costs, we propose a coreset-based interpretation\nframework that utilizes coreset selection methods to sample a representative\nsubset of the large dataset for the interpretation task. Towards this goal, we\npropose a similarity-based evaluation protocol to assess the robustness of\nmodel interpretation methods towards the amount data they take as input.\nExperiments considering several interpretation methods, DNN models, and coreset\nselection methods show the effectiveness of the proposed framework.\n","authors":["Hamed Behzadi-Khormouji","Jos Oramas"],"pdf_url":"https://arxiv.org/pdf/2410.00524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03333v1","updated":"2024-10-04T11:31:43Z","published":"2024-10-04T11:31:43Z","title":"Comparative Analysis and Ensemble Enhancement of Leading CNN\n  Architectures for Breast Cancer Classification","summary":"  This study introduces a novel and accurate approach to breast cancer\nclassification using histopathology images. It systematically compares leading\nConvolutional Neural Network (CNN) models across varying image datasets,\nidentifies their optimal hyperparameters, and ranks them based on\nclassification efficacy. To maximize classification accuracy for each model we\nexplore, the effects of data augmentation, alternative fully-connected layers,\nmodel training hyperparameter settings, and, the advantages of retraining\nmodels versus using pre-trained weights. Our methodology includes several\noriginal concepts, including serializing generated datasets to ensure\nconsistent data conditions across training runs and significantly reducing\ntraining duration. Combined with automated curation of results, this enabled\nthe exploration of over 2,000 training permutations -- such a comprehensive\ncomparison is as yet unprecedented. Our findings establish the settings\nrequired to achieve exceptional classification accuracy for standalone CNN\nmodels and rank them by model efficacy. Based on these results, we propose\nensemble architectures that stack three high-performing standalone CNN models\ntogether with diverse classifiers, resulting in improved classification\naccuracy. The ability to systematically run so many model permutations to get\nthe best outcomes gives rise to very high quality results, including 99.75% for\nBreakHis x40 and BreakHis x200 and 95.18% for the Bach datasets when split into\ntrain, validation and test datasets. The Bach Online blind challenge, yielded\n89% using this approach. Whilst this study is based on breast cancer\nhistopathology image datasets, the methodology is equally applicable to other\nmedical image datasets.\n","authors":["Gary Murphy","Raghubir Singh"],"pdf_url":"https://arxiv.org/pdf/2410.03333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03331v1","updated":"2024-10-04T11:29:04Z","published":"2024-10-04T11:29:04Z","title":"EmojiHeroVR: A Study on Facial Expression Recognition under Partial\n  Occlusion from Head-Mounted Displays","summary":"  Emotion recognition promotes the evaluation and enhancement of Virtual\nReality (VR) experiences by providing emotional feedback and enabling advanced\npersonalization. However, facial expressions are rarely used to recognize\nusers' emotions, as Head-Mounted Displays (HMDs) occlude the upper half of the\nface. To address this issue, we conducted a study with 37 participants who\nplayed our novel affective VR game EmojiHeroVR. The collected database,\nEmoHeVRDB (EmojiHeroVR Database), includes 3,556 labeled facial images of 1,778\nreenacted emotions. For each labeled image, we also provide 29 additional\nframes recorded directly before and after the labeled image to facilitate\ndynamic Facial Expression Recognition (FER). Additionally, EmoHeVRDB includes\ndata on the activations of 63 facial expressions captured via the Meta Quest\nPro VR headset for each frame. Leveraging our database, we conducted a baseline\nevaluation on the static FER classification task with six basic emotions and\nneutral using the EfficientNet-B0 architecture. The best model achieved an\naccuracy of 69.84% on the test set, indicating that FER under HMD occlusion is\nfeasible but significantly more challenging than conventional FER.\n","authors":["Thorben Ortmann","Qi Wang","Larissa Putzar"],"pdf_url":"https://arxiv.org/pdf/2410.03331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03323v1","updated":"2024-10-04T11:20:04Z","published":"2024-10-04T11:20:04Z","title":"Does SpatioTemporal information benefit Two video summarization\n  benchmarks?","summary":"  An important aspect of summarizing videos is understanding the temporal\ncontext behind each part of the video to grasp what is and is not important.\nVideo summarization models have in recent years modeled spatio-temporal\nrelationships to represent this information. These models achieved\nstate-of-the-art correlation scores on important benchmark datasets. However,\nwhat has not been reviewed is whether spatio-temporal relationships are even\nrequired to achieve state-of-the-art results. Previous work in activity\nrecognition has found biases, by prioritizing static cues such as scenes or\nobjects, over motion information. In this paper we inquire if similar spurious\nrelationships might influence the task of video summarization. To do so, we\nanalyse the role that temporal information plays on existing benchmark\ndatasets. We first estimate a baseline with temporally invariant models to see\nhow well such models rank on benchmark datasets (TVSum and SumMe). We then\ndisrupt the temporal order of the videos to investigate the impact it has on\nexisting state-of-the-art models. One of our findings is that the temporally\ninvariant models achieve competitive correlation scores that are close to the\nhuman baselines on the TVSum dataset. We also demonstrate that existing models\nare not affected by temporal perturbations. Furthermore, with certain\ndisruption strategies that shuffle fixed time segments, we can actually improve\ntheir correlation scores. With these results, we find that spatio-temporal\nrelationship play a minor role and we raise the question whether these\nbenchmarks adequately model the task of video summarization. Code available at:\nhttps://github.com/AashGan/TemporalPerturbSum\n","authors":["Aashutosh Ganesh","Mirela Popa","Daan Odijk","Nava Tintarev"],"pdf_url":"https://arxiv.org/pdf/2410.03323v1.pdf","comment":"Accepted for presentation at AEQUITAS workshop, Co-located with ECAI\n  2024"},{"id":"http://arxiv.org/abs/2410.03321v1","updated":"2024-10-04T11:18:41Z","published":"2024-10-04T11:18:41Z","title":"Visual-O1: Understanding Ambiguous Instructions via Multi-modal\n  Multi-turn Chain-of-thoughts Reasoning","summary":"  As large-scale models evolve, language instructions are increasingly utilized\nin multi-modal tasks. Due to human language habits, these instructions often\ncontain ambiguities in real-world scenarios, necessitating the integration of\nvisual context or common sense for accurate interpretation. However, even\nhighly intelligent large models exhibit significant performance limitations on\nambiguous instructions, where weak reasoning abilities of disambiguation can\nlead to catastrophic errors. To address this issue, this paper proposes\nVisual-O1, a multi-modal multi-turn chain-of-thought reasoning framework. It\nsimulates human multi-modal multi-turn reasoning, providing instantial\nexperience for highly intelligent models or empirical experience for generally\nintelligent models to understand ambiguous instructions. Unlike traditional\nmethods that require models to possess high intelligence to understand long\ntexts or perform lengthy complex reasoning, our framework does not\nsignificantly increase computational overhead and is more general and\neffective, even for generally intelligent models. Experiments show that our\nmethod not only significantly enhances the performance of models of different\nintelligence levels on ambiguous instructions but also improves their\nperformance on general datasets. Our work highlights the potential of\nartificial intelligence to work like humans in real-world scenarios with\nuncertainty and ambiguity. We will release our data and code.\n","authors":["Minheng Ni","Yutao Fan","Lei Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2410.03321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02069v2","updated":"2024-10-04T11:15:49Z","published":"2024-10-02T22:36:12Z","title":"Semi-Supervised Fine-Tuning of Vision Foundation Models with\n  Content-Style Decomposition","summary":"  In this paper, we present a semi-supervised fine-tuning approach designed to\nimprove the performance of pre-trained foundation models on downstream tasks\nwith limited labeled data. By leveraging content-style decomposition within an\ninformation-theoretic framework, our method enhances the latent representations\nof pre-trained vision foundation models, aligning them more effectively with\nspecific task objectives and addressing the problem of distribution shift. We\nevaluate our approach on multiple datasets, including MNIST, its augmented\nvariations (with yellow and white stripes), CIFAR-10, SVHN, and GalaxyMNIST.\nThe experiments show improvements over supervised finetuning baseline of\npre-trained models, particularly in low-labeled data regimes, across both\nfrozen and trainable backbones for the majority of the tested datasets.\n","authors":["Mariia Drozdova","Vitaliy Kinakh","Yury Belousov","Erica Lastufka","Slava Voloshynovskiy"],"pdf_url":"https://arxiv.org/pdf/2410.02069v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.03320v1","updated":"2024-10-04T11:14:31Z","published":"2024-10-04T11:14:31Z","title":"Lost in Tracking: Uncertainty-guided Cardiac Cine MRI Segmentation at\n  Right Ventricle Base","summary":"  Accurate biventricular segmentation of cardiac magnetic resonance (CMR) cine\nimages is essential for the clinical evaluation of heart function. However,\ncompared to left ventricle (LV), right ventricle (RV) segmentation is still\nmore challenging and less reproducible. Degenerate performance frequently\noccurs at the RV base, where the in-plane anatomical structures are complex\n(with atria, valve, and aorta) and vary due to the strong interplanar motion.\nIn this work, we propose to address the currently unsolved issues in CMR\nsegmentation, specifically at the RV base, with two strategies: first, we\ncomplemented the public resource by reannotating the RV base in the ACDC\ndataset, with refined delineation of the right ventricle outflow tract (RVOT),\nunder the guidance of an expert cardiologist. Second, we proposed a novel dual\nencoder U-Net architecture that leverages temporal incoherence to inform the\nsegmentation when interplanar motions occur. The inter-planar motion is\ncharacterized by loss-of-tracking, via Bayesian uncertainty of a\nmotion-tracking model. Our experiments showed that our method significantly\nimproved RV base segmentation taking into account temporal incoherence.\nFurthermore, we investigated the reproducibility of deep learning-based\nsegmentation and showed that the combination of consistent annotation and loss\nof tracking could enhance the reproducibility of RV segmentation, potentially\nfacilitating a large number of clinical studies focusing on RV.\n","authors":["Yidong Zhao","Yi Zhang","Orlando Simonetti","Yuchi Han","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2410.03320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19703v2","updated":"2024-10-04T10:54:55Z","published":"2024-09-29T13:33:14Z","title":"Applying the Lower-Biased Teacher Model in Semi-Supervised Object\n  Detection","summary":"  I present the Lower Biased Teacher model, an enhancement of the Unbiased\nTeacher model, specifically tailored for semi-supervised object detection\ntasks. The primary innovation of this model is the integration of a\nlocalization loss into the teacher model, which significantly improves the\naccuracy of pseudo-label generation. By addressing key issues such as class\nimbalance and the precision of bounding boxes, the Lower Biased Teacher model\ndemonstrates superior performance in object detection tasks. Extensive\nexperiments on multiple semi-supervised object detection datasets show that the\nLower Biased Teacher model not only reduces the pseudo-labeling bias caused by\nclass imbalances but also mitigates errors arising from incorrect bounding\nboxes. As a result, the model achieves higher mAP scores and more reliable\ndetection outcomes compared to existing methods. This research underscores the\nimportance of accurate pseudo-label generation and provides a robust framework\nfor future advancements in semi-supervised learning for object detection.\n","authors":["Shuang Wang"],"pdf_url":"https://arxiv.org/pdf/2409.19703v2.pdf","comment":"12pages,2 figures,2 tables, several fomulas. arXiv admin note: text\n  overlap with arXiv:2102.09480 by other authors"},{"id":"http://arxiv.org/abs/2409.12448v2","updated":"2024-10-04T10:54:20Z","published":"2024-09-19T03:58:32Z","title":"Infrared Small Target Detection in Satellite Videos: A New Dataset and A\n  Novel Recurrent Feature Refinement Framework","summary":"  Multi-frame infrared small target (MIRST) detection in satellite videos is a\nlong-standing, fundamental yet challenging task for decades, and the challenges\ncan be summarized as: First, extremely small target size, highly complex\nclutters & noises, various satellite motions result in limited feature\nrepresentation, high false alarms, and difficult motion analyses. Second, the\nlack of large-scale public available MIRST dataset in satellite videos greatly\nhinders the algorithm development. To address the aforementioned challenges, in\nthis paper, we first build a large-scale dataset for MIRST detection in\nsatellite videos (namely IRSatVideo-LEO), and then develop a recurrent feature\nrefinement (RFR) framework as the baseline method. Specifically, IRSatVideo-LEO\nis a semi-simulated dataset with synthesized satellite motion, target\nappearance, trajectory and intensity, which can provide a standard toolbox for\nsatellite video generation and a reliable evaluation platform to facilitate the\nalgorithm development. For baseline method, RFR is proposed to be equipped with\nexisting powerful CNN-based methods for long-term temporal dependency\nexploitation and integrated motion compensation & MIRST detection.\nSpecifically, a pyramid deformable alignment (PDA) module and a\ntemporal-spatial-frequency modulation (TSFM) module are proposed to achieve\neffective and efficient feature alignment, propagation, aggregation and\nrefinement. Extensive experiments have been conducted to demonstrate the\neffectiveness and superiority of our scheme. The comparative results show that\nResUNet equipped with RFR outperforms the state-of-the-art MIRST detection\nmethods. Dataset and code are released at https://github.com/XinyiYing/RFR.\n","authors":["Xinyi Ying","Li Liu","Zaipin Lin","Yangsi Shi","Yingqian Wang","Ruojing Li","Xu Cao","Boyang Li","Shilin Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.12448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03311v1","updated":"2024-10-04T10:48:54Z","published":"2024-10-04T10:48:54Z","title":"Quo Vadis, Motion Generation? From Large Language Models to Large Motion\n  Models","summary":"  Inspired by the recent success of LLMs, the field of human motion\nunderstanding has increasingly shifted towards the development of large motion\nmodels. Despite some progress, current state-of-the-art works remain far from\nachieving truly generalist models, largely due to the lack of large-scale,\nhigh-quality motion data. To address this, we present MotionBase, the first\nmillion-level motion generation benchmark, offering 15 times the data volume of\nthe previous largest dataset, and featuring multimodal data with hierarchically\ndetailed text descriptions. By leveraging this vast dataset, our large motion\nmodel demonstrates strong performance across a broad range of motions,\nincluding unseen ones. Through systematic investigation, we underscore the\nimportance of scaling both data and model size, with synthetic data and pseudo\nlabels playing a crucial role in mitigating data acquisition costs. Moreover,\nour research reveals the limitations of existing evaluation metrics,\nparticularly in handling out-of-domain text instructions -- an issue that has\nlong been overlooked. In addition to these, we introduce a novel 2D lookup-free\napproach for motion tokenization, which preserves motion information and\nexpands codebook capacity, further enhancing the representative ability of\nlarge motion models. The release of MotionBase and the insights gained from\nthis study are expected to pave the way for the development of more powerful\nand versatile motion generation models.\n","authors":["Ye Wang","Sipeng Zheng","Bin Cao","Qianshan Wei","Qin Jin","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.03311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18881v2","updated":"2024-10-04T10:40:11Z","published":"2024-09-27T16:18:13Z","title":"Explainable Artifacts for Synthetic Western Blot Source Attribution","summary":"  Recent advancements in artificial intelligence have enabled generative models\nto produce synthetic scientific images that are indistinguishable from pristine\nones, posing a challenge even for expert scientists habituated to working with\nsuch content. When exploited by organizations known as paper mills, which\nsystematically generate fraudulent articles, these technologies can\nsignificantly contribute to the spread of misinformation about ungrounded\nscience, potentially undermining trust in scientific research. While previous\nstudies have explored black-box solutions, such as Convolutional Neural\nNetworks, for identifying synthetic content, only some have addressed the\nchallenge of generalizing across different models and providing insight into\nthe artifacts in synthetic images that inform the detection process. This study\naims to identify explainable artifacts generated by state-of-the-art generative\nmodels (e.g., Generative Adversarial Networks and Diffusion Models) and\nleverage them for open-set identification and source attribution (i.e.,\npointing to the model that created the image).\n","authors":["Joo Phillipe Cardenuto","Sara Mandelli","Daniel Moreira","Paolo Bestagini","Edward Delp","Anderson Rocha"],"pdf_url":"https://arxiv.org/pdf/2409.18881v2.pdf","comment":"Accepted in IEEE International Workshop on Information Forensics and\n  Security - WIFS 2024, Rome, Italy"},{"id":"http://arxiv.org/abs/2410.03303v1","updated":"2024-10-04T10:40:11Z","published":"2024-10-04T10:40:11Z","title":"SELU: Self-Learning Embodied MLLMs in Unknown Environments","summary":"  Recently, multimodal large language models (MLLMs) have demonstrated strong\nvisual understanding and decision-making capabilities, enabling the exploration\nof autonomously improving MLLMs in unknown environments. However, external\nfeedback like human or environmental feedback is not always available. To\naddress this challenge, existing methods primarily focus on enhancing the\ndecision-making capabilities of MLLMs through voting and scoring mechanisms,\nwhile little effort has been paid to improving the environmental comprehension\nof MLLMs in unknown environments. To fully unleash the self-learning potential\nof MLLMs, we propose a novel actor-critic self-learning paradigm, dubbed SELU,\ninspired by the actor-critic paradigm in reinforcement learning. The critic\nemploys self-asking and hindsight relabeling to extract knowledge from\ninteraction trajectories collected by the actor, thereby augmenting its\nenvironmental comprehension. Simultaneously, the actor is improved by the\nself-feedback provided by the critic, enhancing its decision-making. We\nevaluate our method in the AI2-THOR and VirtualHome environments, and SELU\nachieves critic improvements of approximately 28% and 30%, and actor\nimprovements of about 20% and 24% via self-learning.\n","authors":["Boyu Li","Haobin Jiang","Ziluo Ding","Xinrun Xu","Haoran Li","Dongbin Zhao","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.03303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03302v1","updated":"2024-10-04T10:36:22Z","published":"2024-10-04T10:36:22Z","title":"Action Selection Learning for Multi-label Multi-view Action Recognition","summary":"  Multi-label multi-view action recognition aims to recognize multiple\nconcurrent or sequential actions from untrimmed videos captured by multiple\ncameras. Existing work has focused on multi-view action recognition in a narrow\narea with strong labels available, where the onset and offset of each action\nare labeled at the frame-level. This study focuses on real-world scenarios\nwhere cameras are distributed to capture a wide-range area with only weak\nlabels available at the video-level. We propose the method named MultiASL\n(Multi-view Action Selection Learning), which leverages action selection\nlearning to enhance view fusion by selecting the most useful information from\ndifferent viewpoints. The proposed method includes a Multi-view\nSpatial-Temporal Transformer video encoder to extract spatial and temporal\nfeatures from multi-viewpoint videos. Action Selection Learning is employed at\nthe frame-level, using pseudo ground-truth obtained from weak labels at the\nvideo-level, to identify the most relevant frames for action recognition.\nExperiments in a real-world office environment using the MM-Office dataset\ndemonstrate the superior performance of the proposed method compared to\nexisting methods.\n","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"https://arxiv.org/pdf/2410.03302v1.pdf","comment":"ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2211.08007v4","updated":"2024-10-04T10:18:57Z","published":"2022-11-15T09:42:07Z","title":"Evidence-based Match-status-Aware Gait Recognition for Out-of-Gallery\n  Gait Identification","summary":"  Existing gait recognition methods typically identify individuals based on the\nsimilarity between probe and gallery samples. However, these methods often\nneglect the fact that the gallery may not contain identities corresponding to\nthe probes, leading to incorrect recognition.To identify Out-of-Gallery (OOG)\ngait queries, we propose an Evidence-based Match-status-Aware Gait Recognition\n(EMA-GR) framework. Inspired by Evidential Deep Learning (EDL), EMA-GR is\ndesigned to quantify the uncertainty associated with the match status of\nrecognition. Thus, EMA-GR identifies whether the probe has a counterpart in the\ngallery. Specifically, we adopt an evidence collector to gather match status\nevidence from a recognition result pair and parameterize a Dirichlet\ndistribution over the gathered evidence, following the Dempster-Shafer Theory\nof Evidence (DST). We measure the uncertainty and predict the match status of\nthe recognition results, and thus determine whether the probe is an OOG\nquery.To the best of our knowledge, our method is the first attempt to tackle\nOOG queries in gait recognition. Moreover, EMA-GR is agnostic against gait\nrecognition methods and improves the robustness against OOG queries. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non datasets with OOG queries, and can also generalize well to other\nidentity-retrieval tasks. Importantly, our method surpasses existing\nstate-of-the-art methods by a substantial margin, achieving a 51.26%\nimprovement when the OOG query rate is around 50% on OUMVLP.\n","authors":["Heming Du","Chen Liu","Ming Wang","Lincheng Li","Shunli Zhang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2211.08007v4.pdf","comment":"We have withdrawn this manuscript from Arxiv following a consensus\n  among co-authors to refine our approach and reevaluate the data presented.\n  This decision is part of our commitment to ensuring the highest standards of\n  accuracy and completeness in our published work"},{"id":"http://arxiv.org/abs/2409.16728v2","updated":"2024-10-04T10:17:19Z","published":"2024-09-25T08:23:53Z","title":"SDCL: Students Discrepancy-Informed Correction Learning for\n  Semi-supervised Medical Image Segmentation","summary":"  Semi-supervised medical image segmentation (SSMIS) has been demonstrated the\npotential to mitigate the issue of limited medical labeled data. However,\nconfirmation and cognitive biases may affect the prevalent teacher-student\nbased SSMIS methods due to erroneous pseudo-labels. To tackle this challenge,\nwe improve the mean teacher approach and propose the Students\nDiscrepancy-Informed Correction Learning (SDCL) framework that includes two\nstudents and one non-trainable teacher, which utilizes the segmentation\ndifference between the two students to guide the self-correcting learning. The\nessence of SDCL is to identify the areas of segmentation discrepancy as the\npotential bias areas, and then encourage the model to review the correct\ncognition and rectify their own biases in these areas. To facilitate the bias\ncorrection learning with continuous review and rectification, two correction\nloss functions are employed to minimize the correct segmentation voxel distance\nand maximize the erroneous segmentation voxel entropy. We conducted experiments\non three public medical image datasets: two 3D datasets (CT and MRI) and one 2D\ndataset (MRI). The results show that our SDCL surpasses the current\nState-of-the-Art (SOTA) methods by 2.57\\%, 3.04\\%, and 2.34\\% in the Dice score\non the Pancreas, LA, and ACDC datasets, respectively. In addition, the accuracy\nof our method is very close to the fully supervised method on the ACDC dataset,\nand even exceeds the fully supervised method on the Pancreas and LA dataset.\n(Code available at \\url{https://github.com/pascalcpp/SDCL}).\n","authors":["Bentao Song","Qingfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16728v2.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2410.01723v2","updated":"2024-10-04T10:14:17Z","published":"2024-10-02T16:34:29Z","title":"HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration","summary":"  Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.\n","authors":["Yushi Huang","Zining Wang","Ruihao Gong","Jing Liu","Xinjie Zhang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01723v2.pdf","comment":"Code will be released soon"},{"id":"http://arxiv.org/abs/2410.03290v1","updated":"2024-10-04T10:04:37Z","published":"2024-10-04T10:04:37Z","title":"Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video\n  Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have demonstrated remarkable\ncapabilities in coarse-grained video understanding, however, they struggle with\nfine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,\na novel Video-LLM adept at perceiving and reasoning over specific video moments\nin a fine-grained manner. We identify that current Video-LLMs have limitations\nfor fine-grained video understanding since they lack effective temporal\nmodeling and timestamp representation. In light of this, we sharpen our model\nby incorporating (1) an additional temporal stream to encode the relationships\nbetween frames and (2) discrete temporal tokens enriched with specific time\nknowledge to represent timestamps. To optimize the training of\nGrounded-VideoLLM, we employ a multi-stage training scheme, beginning with\nsimple video-captioning tasks and progressively introducing video temporal\ngrounding tasks of increasing complexity. To further enhance\nGrounded-VideoLLM's temporal reasoning capability, we also curate a grounded\nVideoQA dataset by an automatic annotation pipeline. Extensive experiments\ndemonstrate that Grounded-VideoLLM not only excels in fine-grained grounding\ntasks such as temporal sentence grounding, dense video captioning, and grounded\nVideoQA, but also shows great potential as a versatile video assistant for\ngeneral video understanding.\n","authors":["Haibo Wang","Zhiyang Xu","Yu Cheng","Shizhe Diao","Yufan Zhou","Yixin Cao","Qifan Wang","Weifeng Ge","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03289v1","updated":"2024-10-04T10:03:04Z","published":"2024-10-04T10:03:04Z","title":"Semantic Segmentation Based Quality Control of Histopathology Whole\n  Slide Images","summary":"  We developed a software pipeline for quality control (QC) of histopathology\nwhole slide images (WSIs) that segments various regions, such as blurs of\ndifferent levels, tissue regions, tissue folds, and pen marks. Given the\nnecessity and increasing availability of GPUs for processing WSIs, the proposed\npipeline comprises multiple lightweight deep learning models to strike a\nbalance between accuracy and speed. The pipeline was evaluated in all TCGAs,\nwhich is the largest publicly available WSI dataset containing more than 11,000\nhistopathological images from 28 organs. It was compared to a previous work,\nwhich was not based on deep learning, and it showed consistent improvement in\nsegmentation results across organs. To minimize annotation effort for tissue\nand blur segmentation, annotated images were automatically prepared by\nmosaicking patches (sub-images) from various WSIs whose labels were identified\nusing a patch classification tool HistoROI. Due to the generality of our\ntrained QC pipeline and its extensive testing the potential impact of this work\nis broad. It can be used for automated pre-processing any WSI cohort to enhance\nthe accuracy and reliability of large-scale histopathology image analysis for\nboth research and clinical use. We have made the trained models, training\nscripts, training data, and inference results publicly available at\nhttps://github.com/abhijeetptl5/wsisegqc, which should enable the research\ncommunity to use the pipeline right out of the box or further customize it to\nnew datasets and applications in the future.\n","authors":["Abhijeet Patil","Garima Jain","Harsh Diwakar","Jay Sawant","Tripti Bameta","Swapnil Rane","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2410.03289v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.03276v1","updated":"2024-10-04T09:49:28Z","published":"2024-10-04T09:49:28Z","title":"Sm: enhanced localization in Multiple Instance Learning for medical\n  imaging classification","summary":"  Multiple Instance Learning (MIL) is widely used in medical imaging\nclassification to reduce the labeling effort. While only bag labels are\navailable for training, one typically seeks predictions at both bag and\ninstance levels (classification and localization tasks, respectively). Early\nMIL methods treated the instances in a bag independently. Recent methods\naccount for global and local dependencies among instances. Although they have\nyielded excellent results in classification, their performance in terms of\nlocalization is comparatively limited. We argue that these models have been\ndesigned to target the classification task, while implications at the instance\nlevel have not been deeply investigated. Motivated by a simple observation --\nthat neighboring instances are likely to have the same label -- we propose a\nnovel, principled, and flexible mechanism to model local dependencies. It can\nbe used alone or combined with any mechanism to model global dependencies\n(e.g., transformers). A thorough empirical validation shows that our module\nleads to state-of-the-art performance in localization while being competitive\nor superior in classification. Our code is at\nhttps://github.com/Franblueee/SmMIL.\n","authors":["Francisco M. Castro-Macas","Pablo Morales-lvarez","Yunan Wu","Rafael Molina","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2410.03276v1.pdf","comment":"24 pages, 14 figures, 2024 Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2405.03958v3","updated":"2024-10-04T09:40:05Z","published":"2024-05-07T02:45:28Z","title":"Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your\n  Diffusion Model","summary":"  Current state-of-the-art diffusion models employ U-Net architectures\ncontaining convolutional and (qkv) self-attention layers. The U-Net processes\nimages while being conditioned on the time embedding input for each sampling\nstep and the class or caption embedding input corresponding to the desired\nconditional generation. Such conditioning involves scale-and-shift operations\nto the convolutional layers but does not directly affect the attention layers.\nWhile these standard architectural choices are certainly effective, not\nconditioning the attention layers feels arbitrary and potentially suboptimal.\nIn this work, we show that simply adding LoRA conditioning to the attention\nlayers without changing or tuning the other parts of the U-Net architecture\nimproves the image generation quality. For example, a drop-in addition of LoRA\nconditioning to EDM diffusion model yields FID scores of 1.91/1.75 for\nunconditional and class-conditional CIFAR-10 generation, improving upon the\nbaseline of 1.97/1.79.\n","authors":["Joo Young Choi","Jaesung R. Park","Inkyu Park","Jaewoong Cho","Albert No","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2405.03958v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11566v2","updated":"2024-10-04T09:39:44Z","published":"2024-07-16T10:19:14Z","title":"TGIF: Text-Guided Inpainting Forgery Dataset","summary":"  Digital image manipulation has become increasingly accessible and realistic\nwith the advent of generative AI technologies. Recent developments allow for\ntext-guided inpainting, making sophisticated image edits possible with minimal\neffort. This poses new challenges for digital media forensics. For example,\ndiffusion model-based approaches could either splice the inpainted region into\nthe original image, or regenerate the entire image. In the latter case,\ntraditional image forgery localization (IFL) methods typically fail. This paper\nintroduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive\ncollection of images designed to support the training and evaluation of image\nforgery localization and synthetic image detection (SID) methods. The TGIF\ndataset includes approximately 75k forged images, originating from popular\nopen-source and commercial methods, namely SD2, SDXL, and Adobe Firefly. We\nbenchmark several state-of-the-art IFL and SID methods on TGIF. Whereas\ntraditional IFL methods can detect spliced images, they fail to detect\nregenerated inpainted images. Moreover, traditional SID may detect the\nregenerated inpainted images to be fake, but cannot localize the inpainted\narea. Finally, both IFL and SID methods fail when exposed to stronger\ncompression, while they are less robust to modern compression algorithms, such\nas WEBP. In conclusion, this work demonstrates the inefficiency of\nstate-of-the-art detectors on local manipulations performed by modern\ngenerative approaches, and aspires to help with the development of more capable\nIFL and SID methods. The dataset and code can be downloaded at\nhttps://github.com/IDLabMedia/tgif-dataset.\n","authors":["Hannes Mareen","Dimitrios Karageorgiou","Glenn Van Wallendael","Peter Lambert","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.11566v2.pdf","comment":"6 pages, accepted at IEEE WIFS 2024"},{"id":"http://arxiv.org/abs/2304.01484v3","updated":"2024-10-04T09:37:48Z","published":"2023-04-04T02:55:57Z","title":"Mapping Degeneration Meets Label Evolution: Learning Infrared Small\n  Target Detection with Single Point Supervision","summary":"  Training a convolutional neural network (CNN) to detect infrared small\ntargets in a fully supervised manner has gained remarkable research interests\nin recent years, but is highly labor expensive since a large number of\nper-pixel annotations are required. To handle this problem, in this paper, we\nmake the first attempt to achieve infrared small target detection with\npoint-level supervision. Interestingly, during the training phase supervised by\npoint labels, we discover that CNNs first learn to segment a cluster of pixels\nnear the targets, and then gradually converge to predict groundtruth point\nlabels. Motivated by this \"mapping degeneration\" phenomenon, we propose a label\nevolution framework named label evolution with single point supervision (LESPS)\nto progressively expand the point label by leveraging the intermediate\npredictions of CNNs. In this way, the network predictions can finally\napproximate the updated pseudo labels, and a pixel-level target mask can be\nobtained to train CNNs in an end-to-end manner. We conduct extensive\nexperiments with insightful visualizations to validate the effectiveness of our\nmethod. Experimental results show that CNNs equipped with LESPS can well\nrecover the target masks from corresponding point labels, {and can achieve over\n70% and 95% of their fully supervised performance in terms of pixel-level\nintersection over union (IoU) and object-level probability of detection (Pd),\nrespectively. Code is available at https://github.com/XinyiYing/LESPS.\n","authors":["Xinyi Ying","Li Liu","Yingqian Wang","Ruojing Li","Nuo Chen","Zaiping Lin","Weidong Sheng","Shilin Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.01484v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02155v2","updated":"2024-10-04T09:27:20Z","published":"2024-10-03T02:34:31Z","title":"From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities","summary":"  Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Our method not only improves performance across various\nbenchmarks but also shows promising scalability, potentially paving the way for\nmore efficient and capable multimodal foundation models.\n","authors":["Wanpeng Zhang","Zilong Xie","Yicheng Feng","Yijiang Li","Xingrun Xing","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.02155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03248v1","updated":"2024-10-04T09:13:02Z","published":"2024-10-04T09:13:02Z","title":"3D Segmentation of Neuronal Nuclei and Cell-Type Identification using\n  Multi-channel Information","summary":"  Background Analyzing images to accurately estimate the number of different\ncell types in the brain using automatic methods is a major objective in\nneuroscience. The automatic and selective detection and segmentation of neurons\nwould be an important step in neuroanatomical studies. New method We present a\nmethod to improve the 3D reconstruction of neuronal nuclei that allows their\nsegmentation, excluding the nuclei of non-neuronal cell types. Results We have\ntested the algorithm on stacks of images from rat neocortex, in a complex\nscenario (large stacks of images, uneven staining, and three different channels\nto visualize different cellular markers). It was able to provide a good\nidentification ratio of neuronal nuclei and a 3D segmentation. Comparison with\nExisting Methods: Many automatic tools are in fact currently available, but\ndifferent methods yield different cell count estimations, even in the same\nbrain regions, due to differences in the labeling and imaging techniques, as\nwell as in the algorithms used to detect cells. Moreover, some of the available\nautomated software methods have provided estimations of cell numbers that have\nbeen reported to be inaccurate or inconsistent after evaluation by\nneuroanatomists. Conclusions It is critical to have a tool for automatic\nsegmentation that allows discrimination between neurons, glial cells and\nperivascular cells. It would greatly speed up a task that is currently\nperformed manually and would allow the cell counting to be systematic, avoiding\nhuman bias. Furthermore, the resulting 3D reconstructions of different cell\ntypes can be used to generate models of the spatial distribution of cells.\n","authors":["Antonio LaTorre","Lidia Alonso-Nanclares","Jos Mara Pea","Javier De Felipe"],"pdf_url":"https://arxiv.org/pdf/2410.03248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v1","updated":"2024-10-04T08:26:06Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xioalei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.03224v1","updated":"2024-10-04T08:23:56Z","published":"2024-10-04T08:23:56Z","title":"ScriptViz: A Visualization Tool to Aid Scriptwriting based on a Large\n  Movie Database","summary":"  Scriptwriters usually rely on their mental visualization to create a vivid\nstory by using their imagination to see, feel, and experience the scenes they\nare writing. Besides mental visualization, they often refer to existing images\nor scenes in movies and analyze the visual elements to create a certain mood or\natmosphere. In this paper, we develop ScriptViz to provide external\nvisualization based on a large movie database for the screenwriting process. It\nretrieves reference visuals on the fly based on scripts' text and dialogue from\na large movie database. The tool provides two types of control on visual\nelements that enable writers to 1) see exactly what they want with fixed visual\nelements and 2) see variances in uncertain elements. User evaluation among 15\nscriptwriters shows that ScriptViz is able to present scriptwriters with\nconsistent yet diverse visual possibilities, aligning closely with their\nscripts and helping their creation.\n","authors":["Anyi Rao","Jean-Pec Chou","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2410.03224v1.pdf","comment":"Accepted in the 37th Annual ACM Symposium on User Interface Software\n  and Technology (UIST'24). Webpage:\n  https://virtualfilmstudio.github.io/projects/scriptviz"},{"id":"http://arxiv.org/abs/2406.06462v3","updated":"2024-10-04T08:22:48Z","published":"2024-06-10T16:58:48Z","title":"VCR: Visual Caption Restoration","summary":"  We introduce Visual Caption Restoration (VCR), a novel vision-language task\nthat challenges models to accurately restore partially obscured texts using\npixel-level hints within images. This task stems from the observation that text\nembedded in images is intrinsically different from common visual elements and\nnatural language due to the need to align the modalities of vision, text, and\ntext embedded in images. While numerous works have integrated text embedded in\nimages into visual question-answering tasks, approaches to these tasks\ngenerally rely on optical character recognition or masked language modeling,\nthus reducing the task to mainly text-based processing. However, text-based\nprocessing becomes ineffective in VCR as accurate text restoration depends on\nthe combined information from provided images, context, and subtle cues from\nthe tiny exposed areas of masked texts. We develop a pipeline to generate\nsynthetic images for the VCR task using image-caption pairs, with adjustable\ncaption visibility to control the task difficulty. With this pipeline, we\nconstruct a dataset for VCR called VCR-Wiki using images with captions from\nWikipedia, comprising 2.11M English and 346K Chinese entities in both easy and\nhard split variants. Our results reveal that current vision language models\nsignificantly lag behind human performance in the VCR task, and merely\nfine-tuning the models on our dataset does not lead to notable improvements. We\nrelease VCR-Wiki and the data construction code to facilitate future research.\n","authors":["Tianyu Zhang","Suyuchen Wang","Lu Li","Ge Zhang","Perouz Taslakian","Sai Rajeswar","Jie Fu","Bang Liu","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2406.06462v3.pdf","comment":"22 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.02240v2","updated":"2024-10-04T08:16:22Z","published":"2024-10-03T06:25:53Z","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial\n  Attack","summary":"  Unrestricted adversarial attacks typically manipulate the semantic content of\nan image (e.g., color or texture) to create adversarial examples that are both\neffective and photorealistic. Recent works have utilized the diffusion\ninversion process to map images into a latent space, where high-level semantics\nare manipulated by introducing perturbations. However, they often results in\nsubstantial semantic distortions in the denoised output and suffers from low\nefficiency. In this study, we propose a novel framework called\nSemantic-Consistent Unrestricted Adversarial Attacks (SCA), which employs an\ninversion method to extract edit-friendly noise maps and utilizes Multimodal\nLarge Language Model (MLLM) to provide semantic guidance throughout the\nprocess. Under the condition of rich semantic information provided by MLLM, we\nperform the DDPM denoising process of each step using a series of edit-friendly\nnoise maps, and leverage DPM Solver++ to accelerate this process, enabling\nefficient sampling with semantic consistency. Compared to existing methods, our\nframework enables the efficient generation of adversarial examples that exhibit\nminimal discernible semantic changes. Consequently, we for the first time\nintroduce Semantic-Consistent Adversarial Examples (SCAE). Extensive\nexperiments and visualizations have demonstrated the high efficiency of SCA,\nparticularly in being on average 12 times faster than the state-of-the-art\nattacks. Our code can be found at https://github.com/Pan-Zihao/SCA.\n","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.02240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09624v2","updated":"2024-10-04T08:11:13Z","published":"2024-01-17T22:30:41Z","title":"MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative\n  Adversarial Networks","summary":"  The progress in generative models, particularly Generative Adversarial\nNetworks (GANs), opened new possibilities for image generation but raised\nconcerns about potential malicious uses, especially in sensitive areas like\nmedical imaging. This study introduces MITS-GAN, a novel approach to prevent\ntampering in medical images, with a specific focus on CT scans. The approach\ndisrupts the output of the attacker's CT-GAN architecture by introducing finely\ntuned perturbations that are imperceptible to the human eye. Specifically, the\nproposed approach involves the introduction of appropriate Gaussian noise to\nthe input as a protective measure against various attacks. Our method aims to\nenhance tamper resistance, comparing favorably to existing techniques.\nExperimental results on a CT scan demonstrate MITS-GAN's superior performance,\nemphasizing its ability to generate tamper-resistant images with negligible\nartifacts. As image tampering in medical domains poses life-threatening risks,\nour proactive approach contributes to the responsible and ethical use of\ngenerative models. This work provides a foundation for future research in\ncountering cyber threats in medical imaging. Models and codes are publicly\navailable on https://iplab.dmi.unict.it/MITS-GAN-2024/.\n","authors":["Giovanni Pasqualino","Luca Guarnera","Alessandro Ortis","Sebastiano Battiato"],"pdf_url":"https://arxiv.org/pdf/2401.09624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10787v2","updated":"2024-10-04T08:08:35Z","published":"2024-08-20T12:27:53Z","title":"A Lightweight Modular Framework for Low-Cost Open-Vocabulary Object\n  Detection Training","summary":"  Object detection is a fundamental challenge in computer vision, centered on\nrecognizing objects within images, with diverse applications in areas like\nimage analysis, robotics, and autonomous vehicles. Although existing methods\nhave achieved great success, they are often constrained by a fixed vocabulary\nof objects. To overcome this limitation, approaches like MDETR have redefined\nobject detection by incorporating region-level vision-language pre-training,\nenabling open-vocabulary object detectors. However, these methods are\ncomputationally heavy due to the simultaneous training of large models for both\nvision and language representations. To address this, we introduce a\nlightweight framework that significantly reduces the number of parameters while\npreserving, or even improving, performance. Our solution is applied to MDETR,\nresulting in the development of Lightweight MDETR (LightMDETR), an optimized\nversion of MDETR designed to enhance computational efficiency without\nsacrificing accuracy. The core of our approach involves freezing the MDETR\nbackbone and training only the Universal Projection module (UP), which bridges\nvision and language representations. A learnable modality token parameter\nallows the UP to seamlessly switch between modalities. Evaluations on tasks\nlike phrase grounding, referring expression comprehension, and segmentation\nshow that LightMDETR not only reduces computational costs but also outperforms\nseveral state-of-the-art methods in terms of accuracy.\n","authors":["Bilal Faye","Binta Sow","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2408.10787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02369v2","updated":"2024-10-04T07:54:58Z","published":"2024-10-03T10:33:49Z","title":"Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation","summary":"  The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.\n","authors":["Muzhi Zhu","Yang Liu","Zekai Luo","Chenchen Jing","Hao Chen","Guangkai Xu","Xinlong Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2410.02369v2.pdf","comment":"Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2211.10881v4","updated":"2024-10-04T07:53:15Z","published":"2022-11-20T06:31:23Z","title":"Deepfake Detection: A Comprehensive Survey from the Reliability\n  Perspective","summary":"  The mushroomed Deepfake synthetic materials circulated on the internet have\nraised a profound social impact on politicians, celebrities, and individuals\nworldwide. In this survey, we provide a thorough review of the existing\nDeepfake detection studies from the reliability perspective. We identify three\nreliability-oriented research challenges in the current Deepfake detection\ndomain: transferability, interpretability, and robustness. Moreover, while\nsolutions have been frequently addressed regarding the three challenges, the\ngeneral reliability of a detection model has been barely considered, leading to\nthe lack of reliable evidence in real-life usages and even for prosecutions on\nDeepfake-related cases in court. We, therefore, introduce a model reliability\nstudy metric using statistical random sampling knowledge and the publicly\navailable benchmark datasets to review the reliability of the existing\ndetection models on arbitrary Deepfake candidate suspects. Case studies are\nfurther executed to justify the real-life Deepfake cases including different\ngroups of victims with the help of the reliably qualified detection models as\nreviewed in this survey. Reviews and experiments on the existing approaches\nprovide informative discussions and future research directions for Deepfake\ndetection.\n","authors":["Tianyi Wang","Xin Liao","Kam Pui Chow","Xiaodong Lin","Yinglong Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10881v4.pdf","comment":"Accepted to ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2410.03190v1","updated":"2024-10-04T07:05:16Z","published":"2024-10-04T07:05:16Z","title":"Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample\n  Optimization","summary":"  Recent advancements in timestep-distilled diffusion models have enabled\nhigh-quality image generation that rivals non-distilled multi-step models, but\nwith significantly fewer inference steps. While such models are attractive for\napplications due to the low inference cost and latency, fine-tuning them with a\nnaive diffusion objective would result in degraded and blurry outputs. An\nintuitive alternative is to repeat the diffusion distillation process with a\nfine-tuned teacher model, which produces good results but is cumbersome and\ncomputationally intensive; the distillation training usually requires magnitude\nhigher of training compute compared to fine-tuning for specific image styles.\nIn this paper, we present an algorithm named pairwise sample optimization\n(PSO), which enables the direct fine-tuning of an arbitrary timestep-distilled\ndiffusion model. PSO introduces additional reference images sampled from the\ncurrent time-step distilled model, and increases the relative likelihood margin\nbetween the training images and reference images. This enables the model to\nretain its few-step generation ability, while allowing for fine-tuning of its\noutput distribution. We also demonstrate that PSO is a generalized formulation\nwhich can be flexibly extended to both offline-sampled and online-sampled\npairwise data, covering various popular objectives for diffusion model\npreference optimization. We evaluate PSO in both preference optimization and\nother fine-tuning tasks, including style transfer and concept customization. We\nshow that PSO can directly adapt distilled models to human-preferred generation\nwith both offline and online-generated pairwise preference image data. PSO also\ndemonstrates effectiveness in style transfer and concept customization by\ndirectly tuning timestep-distilled diffusion models.\n","authors":["Zichen Miao","Zhengyuan Yang","Kevin Lin","Ze Wang","Zicheng Liu","Lijuan Wang","Qiang Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.03190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03189v1","updated":"2024-10-04T07:02:13Z","published":"2024-10-04T07:02:13Z","title":"Generalizable Prompt Tuning for Vision-Language Models","summary":"  Prompt tuning for vision-language models such as CLIP involves optimizing the\ntext prompts used to generate image-text pairs for specific downstream tasks.\nWhile hand-crafted or template-based prompts are generally applicable to a\nwider range of unseen classes, they tend to perform poorly in downstream tasks\n(i.e., seen classes). Learnable soft prompts, on the other hand, often perform\nwell in downstream tasks but lack generalizability. Additionally, prior\nresearch has predominantly concentrated on the textual modality, with very few\nstudies attempting to explore the prompt's generalization potential from the\nvisual modality. Keeping these limitations in mind, we investigate how to\nprompt tuning to obtain both a competitive downstream performance and\ngeneralization. The study shows that by treating soft and hand-crafted prompts\nas dual views of the textual modality, and maximizing their mutual information,\nwe can better ensemble task-specific and general semantic information.\nMoreover, to generate more expressive prompts, the study introduces a\nclass-wise augmentation from the visual modality, resulting in significant\nrobustness to a wider range of unseen classes. Extensive evaluations on several\nbenchmarks report that the proposed approach achieves competitive results in\nterms of both task-specific performance and general abilities.\n","authors":["Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03188v1","updated":"2024-10-04T07:01:37Z","published":"2024-10-04T07:01:37Z","title":"Looking into Concept Explanation Methods for Diabetic Retinopathy\n  Classification","summary":"  Diabetic retinopathy is a common complication of diabetes, and monitoring the\nprogression of retinal abnormalities using fundus imaging is crucial. Because\nthe images must be interpreted by a medical expert, it is infeasible to screen\nall individuals with diabetes for diabetic retinopathy. Deep learning has shown\nimpressive results for automatic analysis and grading of fundus images. One\ndrawback is, however, the lack of interpretability, which hampers the\nimplementation of such systems in the clinic. Explainable artificial\nintelligence methods can be applied to explain the deep neural networks.\nExplanations based on concepts have shown to be intuitive for humans to\nunderstand, but have not yet been explored in detail for diabetic retinopathy\ngrading. This work investigates and compares two concept-based explanation\ntechniques for explaining deep neural networks developed for automatic\ndiagnosis of diabetic retinopathy: Quantitative Testing with Concept Activation\nVectors and Concept Bottleneck Models. We found that both methods have\nstrengths and weaknesses, and choice of method should take the available data\nand the end user's preferences into account.\n","authors":["Andrea M. Stors","Josefine V. Sundgaard"],"pdf_url":"https://arxiv.org/pdf/2410.03188v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:021"},{"id":"http://arxiv.org/abs/2410.03187v1","updated":"2024-10-04T06:58:45Z","published":"2024-10-04T06:58:45Z","title":"Autonomous Character-Scene Interaction Synthesis from Text Instruction","summary":"  Synthesizing human motions in 3D environments, particularly those with\ncomplex activities such as locomotion, hand-reaching, and human-object\ninteraction, presents substantial demands for user-defined waypoints and stage\ntransitions. These requirements pose challenges for current models, leading to\na notable gap in automating the animation of characters from simple human\ninputs. This paper addresses this challenge by introducing a comprehensive\nframework for synthesizing multi-stage scene-aware interaction motions directly\nfrom a single text instruction and goal location. Our approach employs an\nauto-regressive diffusion model to synthesize the next motion segment, along\nwith an autonomous scheduler predicting the transition for each action stage.\nTo ensure that the synthesized motions are seamlessly integrated within the\nenvironment, we propose a scene representation that considers the local\nperception both at the start and the goal location. We further enhance the\ncoherence of the generated motion by integrating frame embeddings with language\ninput. Additionally, to support model training, we present a comprehensive\nmotion-captured dataset comprising 16 hours of motion sequences in 120 indoor\nscenes covering 40 types of motions, each annotated with precise language\ndescriptions. Experimental results demonstrate the efficacy of our method in\ngenerating high-quality, multi-stage motions closely aligned with environmental\nand textual conditions.\n","authors":["Nan Jiang","Zimo He","Zi Wang","Hongjie Li","Yixin Chen","Siyuan Huang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.03187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01532v2","updated":"2024-10-04T06:43:47Z","published":"2023-08-03T04:17:25Z","title":"MA-FSAR: Multimodal Adaptation of CLIP for Few-Shot Action Recognition","summary":"  Applying large-scale vision-language pre-trained models like CLIP to few-shot\naction recognition (FSAR) can significantly enhance both performance and\nefficiency. While several studies have recognized this advantage, most of them\nresort to full-parameter fine-tuning to make CLIP's visual encoder adapt to the\nFSAR data, which not only costs high computations but also overlooks the\npotential of the visual encoder to engage in temporal modeling and focus on\ntargeted semantics directly. To tackle these issues, we introduce MA-FSAR, a\nframework that employs the Parameter-Efficient Fine-Tuning (PEFT) technique to\nenhance the CLIP visual encoder in terms of action-related temporal and\nsemantic representations. Our solution involves a Fine-grained Multimodal\nAdaptation, which is different from the previous attempts of PEFT in regular\naction recognition. Specifically, we first insert a Global Temporal Adaptation\nthat only receives the class token to capture global motion cues efficiently.\nThen these outputs integrate with visual tokens to enhance local temporal\ndynamics by a Local Multimodal Adaptation, which incorporates text features\nunique to the FSAR support set branch to highlight fine-grained semantics\nrelated to actions. In addition to these token-level designs, we propose a\nprototype-level text-guided construction module to further enrich the temporal\nand semantic characteristics of video prototypes. Extensive experiments\ndemonstrate our superior performance in various tasks using minor trainable\nparameters.\n","authors":["Jiazheng Xing","Chao Xu","Mengmeng Wang","Guang Dai","Baigui Sun","Yong Liu","Jingdong Wang","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.01532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18516v2","updated":"2024-10-04T06:25:50Z","published":"2024-06-26T17:40:30Z","title":"Denoising as Adaptation: Noise-Space Domain Adaptation for Image\n  Restoration","summary":"  Although learning-based image restoration methods have made significant\nprogress, they still struggle with limited generalization to real-world\nscenarios due to the substantial domain gap caused by training on synthetic\ndata. Existing methods address this issue by improving data synthesis\npipelines, estimating degradation kernels, employing deep internal learning,\nand performing domain adaptation and regularization. Previous domain adaptation\nmethods have sought to bridge the domain gap by learning domain-invariant\nknowledge in either feature or pixel space. However, these techniques often\nstruggle to extend to low-level vision tasks within a stable and compact\nframework. In this paper, we show that it is possible to perform domain\nadaptation via the noise space using diffusion models. In particular, by\nleveraging the unique property of how auxiliary conditional inputs influence\nthe multi-step denoising process, we derive a meaningful diffusion loss that\nguides the restoration model in progressively aligning both restored synthetic\nand real-world outputs with a target clean distribution. We refer to this\nmethod as denoising as adaptation. To prevent shortcuts during joint training,\nwe present crucial strategies such as channel-shuffling layer and\nresidual-swapping contrastive learning in the diffusion model. They implicitly\nblur the boundaries between conditioned synthetic and real data and prevent the\nreliance of the model on easily distinguishable features. Experimental results\non three classical image restoration tasks, namely denoising, deblurring, and\nderaining, demonstrate the effectiveness of the proposed method.\n","authors":["Kang Liao","Zongsheng Yue","Zhouxia Wang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.18516v2.pdf","comment":"Project Page: https://kangliao929.github.io/projects/noise-da/"},{"id":"http://arxiv.org/abs/2410.03176v1","updated":"2024-10-04T06:24:49Z","published":"2024-10-04T06:24:49Z","title":"Investigating and Mitigating Object Hallucinations in Pretrained\n  Vision-Language (CLIP) Models","summary":"  Large Vision-Language Models (LVLMs) have achieved impressive performance,\nyet research has pointed out a serious issue with object hallucinations within\nthese models. However, there is no clear conclusion as to which part of the\nmodel these hallucinations originate from. In this paper, we present an\nin-depth investigation into the object hallucination problem specifically\nwithin the CLIP model, which serves as the backbone for many state-of-the-art\nvision-language systems. We unveil that even in isolation, the CLIP model is\nprone to object hallucinations, suggesting that the hallucination problem is\nnot solely due to the interaction between vision and language modalities. To\naddress this, we propose a counterfactual data augmentation method by creating\nnegative samples with a variety of hallucination issues. We demonstrate that\nour method can effectively mitigate object hallucinations for CLIP model, and\nwe show the the enhanced model can be employed as a visual encoder, effectively\nalleviating the object hallucination issue in LVLMs.\n","authors":["Yufang Liu","Tao Ji","Changzhi Sun","Yuanbin Wu","Aimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.03176v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03174v1","updated":"2024-10-04T06:19:29Z","published":"2024-10-04T06:19:29Z","title":"HRVMamba: High-Resolution Visual State Space Model for Dense Prediction","summary":"  Recently, State Space Models (SSMs) with efficient hardware-aware designs,\ni.e., Mamba, have demonstrated significant potential in computer vision tasks\ndue to their linear computational complexity with respect to token length and\ntheir global receptive field. However, Mamba's performance on dense prediction\ntasks, including human pose estimation and semantic segmentation, has been\nconstrained by three key challenges: insufficient inductive bias, long-range\nforgetting, and low-resolution output representation. To address these\nchallenges, we introduce the Dynamic Visual State Space (DVSS) block, which\nutilizes multi-scale convolutional kernels to extract local features across\ndifferent scales and enhance inductive bias, and employs deformable convolution\nto mitigate the long-range forgetting problem while enabling adaptive spatial\naggregation based on input and task-specific information. By leveraging the\nmulti-resolution parallel design proposed in HRNet, we introduce\nHigh-Resolution Visual State Space Model (HRVMamba) based on the DVSS block,\nwhich preserves high-resolution representations throughout the entire process\nwhile promoting effective multi-scale feature learning. Extensive experiments\nhighlight HRVMamba's impressive performance on dense prediction tasks,\nachieving competitive results against existing benchmark models without bells\nand whistles. Code is available at https://github.com/zhanghao5201/HRVMamba.\n","authors":["Hao Zhang","Yongqiang Ma","Wenqi Shao","Ping Luo","Nanning Zheng","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03171v1","updated":"2024-10-04T06:05:26Z","published":"2024-10-04T06:05:26Z","title":"Selective Transformer for Hyperspectral Image Classification","summary":"  Transformer has achieved satisfactory results in the field of hyperspectral\nimage (HSI) classification. However, existing Transformer models face two key\nchallenges when dealing with HSI scenes characterized by diverse land cover\ntypes and rich spectral information: (1) fixed receptive field representation\noverlooks effective contextual information; (2) redundant self-attention\nfeature representation. To address these limitations, we propose a novel\nSelective Transformer (SFormer) for HSI classification. The SFormer is designed\nto dynamically select receptive fields for capturing both spatial and spectral\ncontextual information, while mitigating the impact of redundant data by\nprioritizing the most relevant features. This enables a highly accurate\nclassification of the land covers of the HSI. Specifically, a Kernel Selective\nTransformer Block (KSTB) is first utilized to dynamically select an appropriate\nreceptive field range to effectively extract spatial-spectral features.\nFurthermore, to capture the most crucial tokens, a Token Selective Transformer\nBlock (TSTB) is introduced, which selects the most relevant tokens based on the\nranking of attention scores for each query. Extensive experiments on four\nbenchmark HSI datasets demonstrate that the proposed SFormer outperforms the\nstate-of-the-art HSI classification models. The codes will be released.\n","authors":["Yichu Xu","Di Wang","Lefei Zhang","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03160v1","updated":"2024-10-04T05:47:39Z","published":"2024-10-04T05:47:39Z","title":"Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep\n  Approach","summary":"  Diffusion models have revolutionized image generation, and their extension to\nvideo generation has shown promise. However, current video diffusion\nmodels~(VDMs) rely on a scalar timestep variable applied at the clip level,\nwhich limits their ability to model complex temporal dependencies needed for\nvarious tasks like image-to-video generation. To address this limitation, we\npropose a frame-aware video diffusion model~(FVDM), which introduces a novel\nvectorized timestep variable~(VTV). Unlike conventional VDMs, our approach\nallows each frame to follow an independent noise schedule, enhancing the\nmodel's capacity to capture fine-grained temporal dependencies. FVDM's\nflexibility is demonstrated across multiple tasks, including standard video\ngeneration, image-to-video generation, video interpolation, and long video\nsynthesis. Through a diverse set of VTV configurations, we achieve superior\nquality in generated videos, overcoming challenges such as catastrophic\nforgetting during fine-tuning and limited generalizability in zero-shot\nmethods.Our empirical evaluations show that FVDM outperforms state-of-the-art\nmethods in video generation quality, while also excelling in extended tasks. By\naddressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm\nin video synthesis, offering a robust framework with significant implications\nfor generative modeling and multimedia applications.\n","authors":["Yaofang Liu","Yumeng Ren","Xiaodong Cun","Aitor Artola","Yang Liu","Tieyong Zeng","Raymond H. Chan","Jean-michel Morel"],"pdf_url":"https://arxiv.org/pdf/2410.03160v1.pdf","comment":"Code at https://github.com/Yaofang-Liu/FVDM"},{"id":"http://arxiv.org/abs/2410.03146v1","updated":"2024-10-04T04:59:50Z","published":"2024-10-04T04:59:50Z","title":"Bridging the Gap between Text, Audio, Image, and Any Sequence: A Novel\n  Approach using Gloss-based Annotation","summary":"  This paper presents an innovative approach called BGTAI to simplify\nmultimodal understanding by utilizing gloss-based annotation as an intermediate\nstep in aligning Text and Audio with Images. While the dynamic temporal factors\nin textual and audio inputs contain various predicate adjectives that influence\nthe meaning of the entire sentence, images, on the other hand, present static\nscenes. By representing text and audio as gloss notations that omit complex\nsemantic nuances, a better alignment with images can potentially be achieved.\nThis study explores the feasibility of this idea, specifically, we first\npropose the first Langue2Gloss model and then integrate it into the multimodal\nmodel UniBriVL for joint training. To strengthen the adaptability of gloss with\ntext/audio and overcome the efficiency and instability issues in multimodal\ntraining, we propose a DS-Net (Data-Pair Selection Network), an Result Filter\nmodule, and a novel SP-Loss function. Our approach outperforms previous\nmultimodal models in the main experiments, demonstrating its efficacy in\nenhancing multimodal representations and improving compatibility among text,\naudio, visual, and any sequence modalities.\n","authors":["Sen Fang","Yalin Feng","Sizhou Chen","Xiaofeng Zhang","Teik Toe Teoh"],"pdf_url":"https://arxiv.org/pdf/2410.03146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13444v3","updated":"2024-10-04T04:56:35Z","published":"2024-06-19T11:09:16Z","title":"VDebugger: Harnessing Execution Feedback for Debugging Visual Programs","summary":"  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n","authors":["Xueqing Wu","Zongyu Lin","Songyan Zhao","Te-Lin Wu","Pan Lu","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2406.13444v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.00064v2","updated":"2024-10-04T04:53:00Z","published":"2024-09-30T01:43:06Z","title":"M2Distill: Multi-Modal Distillation for Lifelong Imitation Learning","summary":"  Lifelong imitation learning for manipulation tasks poses significant\nchallenges due to distribution shifts that occur in incremental learning steps.\nExisting methods often focus on unsupervised skill discovery to construct an\never-growing skill library or distillation from multiple policies, which can\nlead to scalability issues as diverse manipulation tasks are continually\nintroduced and may fail to ensure a consistent latent space throughout the\nlearning process, leading to catastrophic forgetting of previously learned\nskills. In this paper, we introduce M2Distill, a multi-modal distillation-based\nmethod for lifelong imitation learning focusing on preserving consistent latent\nspace across vision, language, and action distributions throughout the learning\nprocess. By regulating the shifts in latent representations across different\nmodalities from previous to current steps, and reducing discrepancies in\nGaussian Mixture Model (GMM) policies between consecutive learning steps, we\nensure that the learned policy retains its ability to perform previously\nlearned tasks while seamlessly integrating new skills. Extensive evaluations on\nthe LIBERO lifelong imitation learning benchmark suites, including\nLIBERO-OBJECT, LIBERO-GOAL, and LIBERO-SPATIAL, demonstrate that our method\nconsistently outperforms prior state-of-the-art methods across all evaluated\nmetrics.\n","authors":["Kaushik Roy","Akila Dissanayake","Brendan Tidd","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2410.00064v2.pdf","comment":"Submitted to ICRA2025"},{"id":"http://arxiv.org/abs/2410.03143v1","updated":"2024-10-04T04:49:56Z","published":"2024-10-04T04:49:56Z","title":"ECHOPulse: ECG controlled echocardio-grams video generation","summary":"  Echocardiography (ECHO) is essential for cardiac assessments, but its video\nquality and interpretation heavily relies on manual expertise, leading to\ninconsistent results from clinical and portable devices. ECHO video generation\noffers a solution by improving automated monitoring through synthetic data and\ngenerating high-quality videos from routine health data. However, existing\nmodels often face high computational costs, slow inference, and rely on complex\nconditional prompts that require experts' annotations. To address these\nchallenges, we propose ECHOPULSE, an ECG-conditioned ECHO video generation\nmodel. ECHOPULSE introduces two key advancements: (1) it accelerates ECHO video\ngeneration by leveraging VQ-VAE tokenization and masked visual token modeling\nfor fast decoding, and (2) it conditions on readily accessible ECG signals,\nwhich are highly coherent with ECHO videos, bypassing complex conditional\nprompts. To the best of our knowledge, this is the first work to use\ntime-series prompts like ECG signals for ECHO video generation. ECHOPULSE not\nonly enables controllable synthetic ECHO data generation but also provides\nupdated cardiac function information for disease monitoring and prediction\nbeyond ECG alone. Evaluations on three public and private datasets demonstrate\nstate-of-the-art performance in ECHO video generation across both qualitative\nand quantitative measures. Additionally, ECHOPULSE can be easily generalized to\nother modality generation tasks, such as cardiac MRI, fMRI, and 3D CT\ngeneration. Demo can seen from\n\\url{https://github.com/levyisthebest/ECHOPulse_Prelease}.\n","authors":["Yiwei Li","Sekeun Kim","Zihao Wu","Hanqi Jiang","Yi Pan","Pengfei Jin","Sifan Song","Yucheng Shi","Tianze Yang","Tianming Liu","Quanzheng Li","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.03143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02745v2","updated":"2024-10-04T04:48:57Z","published":"2024-09-20T10:50:21Z","title":"AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity","summary":"  Recently, when dealing with high-resolution images, dominant LMMs usually\ndivide them into multiple local images and one global image, which will lead to\na large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM\nthat can adaptively select the appropriate visual granularity based on the\ninput image and instruction. This approach not only reduces the number of\nvisual tokens and speeds up inference, but also improves the overall model\nperformance. Specifically, we introduce the following modules based on\nLLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling\nlayers to obtain visual tokens with different granularities; (b) a visual\ngranularity router, which includes a Transformer layer, an MLP layer, and a\nvoter layer, used to select the appropriate visual granularity based on the\nimage and instruction. Furthermore, we propose RGLF, a novel training paradigm\nthat aims at aligning the granularity predicted by the router with the\npreferences of the LMM, without the need for additional manually annotated\ndata. Extensive experiments and analysis show that AVG-LLaVA achieves superior\nperformance across 11 benchmarks, as well as significantly reduces the number\nof visual tokens and speeds up inference (e.g., an 85.3% reduction in visual\ntokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark).\n","authors":["Zhibin Lan","Liqiang Niu","Fandong Meng","Wenbo Li","Jie Zhou","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2410.02745v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.03141v1","updated":"2024-10-04T04:37:29Z","published":"2024-10-04T04:37:29Z","title":"Machine Learning for Asymptomatic Ratoon Stunting Disease Detection With\n  Freely Available Satellite Based Multispectral Imaging","summary":"  Disease detection in sugarcane, particularly the identification of\nasymptomatic infectious diseases such as Ratoon Stunting Disease (RSD), is\ncritical for effective crop management. This study employed various machine\nlearning techniques to detect the presence of RSD in different sugarcane\nvarieties, using vegetation indices derived from freely available\nsatellite-based spectral data. Our results show that the Support Vector Machine\nwith a Radial Basis Function Kernel (SVM-RBF) was the most effective algorithm,\nachieving classification accuracy between 85.64\\% and 96.55\\%, depending on the\nvariety. Gradient Boosting and Random Forest also demonstrated high performance\nachieving accuracy between 83.33\\% to 96.55\\%, while Logistic Regression and\nQuadratic Discriminant Analysis showed variable results across different\nvarieties. The inclusion of sugarcane variety and vegetation indices was\nimportant in the detection of RSD. This agreed with what was identified in the\ncurrent literature. Our study highlights the potential of satellite-based\nremote sensing as a cost-effective and efficient method for large-scale\nsugarcane disease detection alternative to traditional manual laboratory\ntesting methods.\n","authors":["Ethan Kane Waters","Carla Chia-ming Chen","Mostafa Rahimi Azghadi"],"pdf_url":"https://arxiv.org/pdf/2410.03141v1.pdf","comment":"13 pages, 1 figure and 2 tables (main text), 1 figure and 3 tables\n  (appendices). Submitted to \"Computers and Electronics in Agriculture\""},{"id":"http://arxiv.org/abs/2406.06050v3","updated":"2024-10-04T03:52:15Z","published":"2024-06-10T06:38:11Z","title":"Generalizable Human Gaussians from Single-View Image","summary":"  In this work, we tackle the task of learning 3D human Gaussians from a single\nimage, focusing on recovering detailed appearance and geometry including\nunobserved regions. We introduce a single-view generalizable Human Gaussian\nModel (HGM), which employs a novel generate-then-refine pipeline with the\nguidance from human body prior and diffusion prior. Our approach uses a\nControlNet to refine rendered back-view images from coarse predicted human\nGaussians, then uses the refined image along with the input image to\nreconstruct refined human Gaussians. To mitigate the potential generation of\nunrealistic human poses and shapes, we incorporate human priors from the SMPL-X\nmodel as a dual branch, propagating image features from the SMPL-X volume to\nthe image Gaussians using sparse convolution and attention mechanisms. Given\nthat the initial SMPL-X estimation might be inaccurate, we gradually refine it\nwith our HGM model. We validate our approach on several publicly available\ndatasets. Our method surpasses previous methods in both novel view synthesis\nand surface reconstruction. Our approach also exhibits strong generalization\nfor cross-dataset evaluation and in-the-wild images.\n","authors":["Jinnan Chen","Chen Li","Jianfeng Zhang","Lingting Zhu","Buzhen Huang","Hanlin Chen","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.06050v3.pdf","comment":"https://jinnan-chen.github.io/projects/HGM/"},{"id":"http://arxiv.org/abs/2410.03129v1","updated":"2024-10-04T03:50:10Z","published":"2024-10-04T03:50:10Z","title":"ARB-LLM: Alternating Refined Binarizations for Large Language Models","summary":"  Large Language Models (LLMs) have greatly pushed forward advancements in\nnatural language processing, yet their high memory and computational demands\nhinder practical deployment. Binarization, as an effective compression\ntechnique, can shrink model weights to just 1 bit, significantly reducing the\nhigh demands on computation and memory. However, current binarization methods\nstruggle to narrow the distribution gap between binarized and full-precision\nweights, while also overlooking the column deviation in LLM weight\ndistribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit\npost-training quantization (PTQ) technique tailored for LLMs. To narrow the\ndistribution shift between binarized and full-precision weights, we first\ndesign an alternating refined binarization (ARB) algorithm to progressively\nupdate the binarization parameters, which significantly reduces the\nquantization error. Moreover, considering the pivot role of calibration data\nand the column deviation in LLM weights, we further extend ARB to ARB-X and\nARB-RC. In addition, we refine the weight partition strategy with column-group\nbitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC\nwith CGB, we obtain ARB-LLM$_\\text{X}$ and ARB-LLM$_\\text{RC}$ respectively,\nwhich significantly outperform state-of-the-art (SOTA) binarization methods for\nLLMs. As a binary PTQ method, our ARB-LLM$_\\text{RC}$ is the first to surpass\nFP16 models of the same size. The code and models will be available at\nhttps://github.com/ZHITENGLI/ARB-LLM.\n","authors":["Zhiteng Li","Xianglong Yan","Tianao Zhang","Haotong Qin","Dong Xie","Jiang Tian","zhongchao shi","Linghe Kong","Yulun Zhang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.03129v1.pdf","comment":"The code and models will be available at\n  https://github.com/ZHITENGLI/ARB-LLM"},{"id":"http://arxiv.org/abs/2405.13675v3","updated":"2024-10-04T03:36:08Z","published":"2024-05-22T14:16:30Z","title":"Context and Geometry Aware Voxel Transformer for Semantic Scene\n  Completion","summary":"  Vision-based Semantic Scene Completion (SSC) has gained much attention due to\nits widespread applications in various 3D perception tasks. Existing\nsparse-to-dense approaches typically employ shared context-independent queries\nacross various input images, which fails to capture distinctions among them as\nthe focal regions of different inputs vary and may result in undirected feature\naggregation of cross-attention. Additionally, the absence of depth information\nmay lead to points projected onto the image plane sharing the same 2D position\nor similar sampling points in the feature map, resulting in depth ambiguity. In\nthis paper, we present a novel context and geometry aware voxel transformer. It\nutilizes a context aware query generator to initialize context-dependent\nqueries tailored to individual input images, effectively capturing their unique\ncharacteristics and aggregating information within the region of interest.\nFurthermore, it extend deformable cross-attention from 2D to 3D pixel space,\nenabling the differentiation of points with similar image coordinates based on\ntheir depth coordinates. Building upon this module, we introduce a neural\nnetwork named CGFormer to achieve semantic scene completion. Simultaneously,\nCGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost\nthe semantic and geometric representation abilities of the transformed 3D\nvolume from both local and global perspectives. Experimental results\ndemonstrate that CGFormer achieves state-of-the-art performance on the\nSemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and\n20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer\neven outperforms approaches employing temporal images as inputs or much larger\nimage backbone networks.\n","authors":["Zhu Yu","Runmin Zhang","Jiacheng Ying","Junchen Yu","Xiaohai Hu","Lun Luo","Si-Yuan Cao","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2405.13675v3.pdf","comment":"NIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2406.20095v2","updated":"2024-10-04T03:28:30Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the\ncapacity to process state information as visual-textual prompts and respond\nwith policy decisions in text. We propose LLaRA: Large Language and Robotics\nAssistant, a framework that formulates robot action policy as conversations and\nprovides improved action outputs when trained with auxiliary data that\ncomplements policy learning. We first introduce an automated pipeline to\ngenerate conversation-style instruction tuning data from existing behavior\ncloning data. Then we enrich the dataset in a self-supervised fashion by\nformulating six auxiliary tasks. A VLM finetuned with the resulting collection\nof datasets can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03107v1","updated":"2024-10-04T03:03:06Z","published":"2024-10-04T03:03:06Z","title":"MBDS: A Multi-Body Dynamics Simulation Dataset for Graph Networks\n  Simulators","summary":"  Modeling the structure and events of the physical world constitutes a\nfundamental objective of neural networks. Among the diverse approaches, Graph\nNetwork Simulators (GNS) have emerged as the leading method for modeling\nphysical phenomena, owing to their low computational cost and high accuracy.\nThe datasets employed for training and evaluating physical simulation\ntechniques are typically generated by researchers themselves, often resulting\nin limited data volume and quality. Consequently, this poses challenges in\naccurately assessing the performance of these methods. In response to this, we\nhave constructed a high-quality physical simulation dataset encompassing 1D,\n2D, and 3D scenes, along with more trajectories and time-steps compared to\nexisting datasets. Furthermore, our work distinguishes itself by developing\neight complete scenes, significantly enhancing the dataset's comprehensiveness.\nA key feature of our dataset is the inclusion of precise multi-body dynamics,\nfacilitating a more realistic simulation of the physical world. Utilizing our\nhigh-quality dataset, we conducted a systematic evaluation of various existing\nGNS methods. Our dataset is accessible for download at\nhttps://github.com/Sherlocktein/MBDS, offering a valuable resource for\nresearchers to enhance the training and evaluation of their methodologies.\n","authors":["Sheng Yang","Fengge Wu","Junsuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.03107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03105v1","updated":"2024-10-04T02:58:49Z","published":"2024-10-04T02:58:49Z","title":"Mamba in Vision: A Comprehensive Survey of Techniques and Applications","summary":"  Mamba is emerging as a novel approach to overcome the challenges faced by\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in computer\nvision. While CNNs excel at extracting local features, they often struggle to\ncapture long-range dependencies without complex architectural modifications. In\ncontrast, ViTs effectively model global relationships but suffer from high\ncomputational costs due to the quadratic complexity of their self-attention\nmechanisms. Mamba addresses these limitations by leveraging Selective\nStructured State Space Models to effectively capture long-range dependencies\nwith linear computational complexity. This survey analyzes the unique\ncontributions, computational benefits, and applications of Mamba models while\nalso identifying challenges and potential future research directions. We\nprovide a foundational resource for advancing the understanding and growth of\nMamba models in computer vision. An overview of this work is available at\nhttps://github.com/maklachur/Mamba-in-Computer-Vision.\n","authors":["Md Maklachur Rahman","Abdullah Aman Tutul","Ankur Nath","Lamyanba Laishram","Soon Ki Jung","Tracy Hammond"],"pdf_url":"https://arxiv.org/pdf/2410.03105v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.18364v2","updated":"2024-10-04T02:50:11Z","published":"2024-09-27T00:49:08Z","title":"Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human\n  Reconstruction from Occluded Images","summary":"  3D human shape reconstruction under severe occlusion due to human-object or\nhuman-human interaction is a challenging problem. Parametric models i.e.,\nSMPL(-X), which are based on the statistics across human shapes, can represent\nwhole human body shapes but are limited to minimally-clothed human shapes.\nImplicit-function-based methods extract features from the parametric models to\nemploy prior knowledge of human bodies and can capture geometric details such\nas clothing and hair. However, they often struggle to handle misaligned\nparametric models and inpaint occluded regions given a single RGB image. In\nthis work, we propose a novel pipeline, MHCDIFF, Multi-hypotheses Conditioned\nPoint Cloud Diffusion, composed of point cloud diffusion conditioned on\nprobabilistic distributions for pixel-aligned detailed 3D human reconstruction\nunder occlusion. Compared to previous implicit-function-based methods, the\npoint cloud diffusion model can capture the global consistent features to\ngenerate the occluded regions, and the denoising process corrects the\nmisaligned SMPL meshes. The core of MHCDIFF is extracting local features from\nmultiple hypothesized SMPL(-X) meshes and aggregating the set of features to\ncondition the diffusion model. In the experiments on CAPE and MultiHuman\ndatasets, the proposed method outperforms various SOTA methods based on SMPL,\nimplicit functions, point cloud diffusion, and their combined, under synthetic\nand real occlusions. Our code is publicly available at\nhttps://donghwankim0101.github.io/projects/mhcdiff/ .\n","authors":["Donghwan Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2409.18364v2.pdf","comment":"17 pages, 7 figures, accepted NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.03097v1","updated":"2024-10-04T02:46:09Z","published":"2024-10-04T02:46:09Z","title":"Combing Text-based and Drag-based Editing for Precise and Flexible Image\n  Editing","summary":"  Precise and flexible image editing remains a fundamental challenge in\ncomputer vision. Based on the modified areas, most editing methods can be\ndivided into two main types: global editing and local editing. In this paper,\nwe choose the two most common editing approaches (ie text-based editing and\ndrag-based editing) and analyze their drawbacks. Specifically, text-based\nmethods often fail to describe the desired modifications precisely, while\ndrag-based methods suffer from ambiguity. To address these issues, we proposed\n\\textbf{CLIPDrag}, a novel image editing method that is the first to combine\ntext and drag signals for precise and ambiguity-free manipulations on diffusion\nmodels. To fully leverage these two signals, we treat text signals as global\nguidance and drag points as local information. Then we introduce a novel\nglobal-local motion supervision method to integrate text signals into existing\ndrag-based methods by adapting a pre-trained language-vision model like CLIP.\nFurthermore, we also address the problem of slow convergence in CLIPDrag by\npresenting a fast point-tracking method that enforces drag points moving toward\ncorrect directions. Extensive experiments demonstrate that CLIPDrag outperforms\nexisting single drag-based methods or text-based methods.\n","authors":["Ziqi Jiang","Zhen Wang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2410.03097v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16851v3","updated":"2024-10-04T01:58:06Z","published":"2024-06-24T17:58:03Z","title":"Losing Visual Needles in Image Haystacks: Vision Language Models are\n  Easily Distracted in Short and Long Contexts","summary":"  We present LoCoVQA, a dynamic benchmark generator for evaluating long-context\nextractive reasoning in vision language models (VLMs). LoCoVQA augments test\nexamples for mathematical reasoning, VQA, and character recognition tasks with\nincreasingly long visual contexts composed of both in-distribution and\nout-of-distribution distractor images.\n  Across these tasks, a diverse set of VLMs rapidly lose performance as the\nvisual context length grows, often exhibiting a striking logarithmic decay\ntrend. This test assesses how well VLMs can ignore irrelevant information when\nanswering queries -- a task that is quite easy for language models (LMs) in the\ntext domain -- demonstrating that current state-of-the-art VLMs lack this\nessential capability for many long-context applications.\n","authors":["Aditya Sharma","Michael Saxon","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16851v3.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03080v1","updated":"2024-10-04T01:52:23Z","published":"2024-10-04T01:52:23Z","title":"Generative Edge Detection with Stable Diffusion","summary":"  Edge detection is typically viewed as a pixel-level classification problem\nmainly addressed by discriminative methods. Recently, generative edge detection\nmethods, especially diffusion model based solutions, are initialized in the\nedge detection task. Despite great potential, the retraining of task-specific\ndesigned modules and multi-step denoising inference limits their broader\napplications. Upon closer investigation, we speculate that part of the reason\nis the under-exploration of the rich discriminative information encoded in\nextensively pre-trained large models (\\eg, stable diffusion models). Thus\nmotivated, we propose a novel approach, named Generative Edge Detector (GED),\nby fully utilizing the potential of the pre-trained stable diffusion model. Our\nmodel can be trained and inferred efficiently without specific network design\ndue to the rich high-level and low-level prior knowledge empowered by the\npre-trained stable diffusion. Specifically, we propose to finetune the\ndenoising U-Net and predict latent edge maps directly, by taking the latent\nimage feature maps as input. Additionally, due to the subjectivity and\nambiguity of the edges, we also incorporate the granularity of the edges into\nthe denoising U-Net model as one of the conditions to achieve controllable and\ndiverse predictions. Furthermore, we devise a granularity regularization to\nensure the relative granularity relationship of the multiple predictions. We\nconduct extensive experiments on multiple datasets and achieve competitive\nperformance (\\eg, 0.870 and 0.880 in terms of ODS and OIS on the BSDS test\ndataset).\n","authors":["Caixia Zhou","Yaping Huang","Mochu Xiang","Jiahui Ren","Haibin Ling","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15576v2","updated":"2024-10-04T01:22:06Z","published":"2024-03-22T19:04:02Z","title":"Data-centric Prediction Explanation via Kernelized Stein Discrepancy","summary":"  Existing example-based prediction explanation methods often bridge test and\ntraining data points through the model's parameters or latent representations.\nWhile these methods offer clues to the causes of model predictions, they often\nexhibit innate shortcomings, such as incurring significant computational\noverhead or producing coarse-grained explanations. This paper presents a\nHighly-precise and Data-centric Explan}ation (HD-Explain) prediction\nexplanation method that exploits properties of Kernelized Stein Discrepancy\n(KSD). Specifically, the KSD uniquely defines a parameterized kernel function\nfor a trained model that encodes model-dependent data correlation. By\nleveraging the kernel function, one can identify training samples that provide\nthe best predictive support to a test point efficiently. We conducted thorough\nanalyses and experiments across multiple classification domains, where we show\nthat HD-Explain outperforms existing methods from various aspects, including 1)\npreciseness (fine-grained explanation), 2) consistency, and 3) computation\nefficiency, leading to a surprisingly simple, effective, and robust prediction\nexplanation solution.\n","authors":["Mahtab Sarvmaili","Hassan Sajjad","Ga Wu"],"pdf_url":"https://arxiv.org/pdf/2403.15576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02082v3","updated":"2024-10-04T01:10:29Z","published":"2024-04-02T16:28:41Z","title":"WcDT: World-centric Diffusion Transformer for Traffic Scene Generation","summary":"  In this paper, we introduce a novel approach for autonomous driving\ntrajectory generation by harnessing the complementary strengths of diffusion\nprobabilistic models (a.k.a., diffusion models) and transformers. Our proposed\nframework, termed the \"World-Centric Diffusion Transformer\"(WcDT), optimizes\nthe entire trajectory generation process, from feature extraction to model\ninference. To enhance the scene diversity and stochasticity, the historical\ntrajectory data is first preprocessed into \"Agent Move Statement\" and encoded\ninto latent space using Denoising Diffusion Probabilistic Models (DDPM)\nenhanced with Diffusion with Transformer (DiT) blocks. Then, the latent\nfeatures, historical trajectories, HD map features, and historical traffic\nsignal information are fused with various transformer-based encoders that are\nused to enhance the interaction of agents with other elements in the traffic\nscene. The encoded traffic scenes are then decoded by a trajectory decoder to\ngenerate multimodal future trajectories. Comprehensive experimental results\nshow that the proposed approach exhibits superior performance in generating\nboth realistic and diverse trajectories, showing its potential for integration\ninto automatic driving simulation systems. Our code is available at\n\\url{https://github.com/yangchen1997/WcDT}.\n","authors":["Chen Yang","Yangfan He","Aaron Xuxiang Tian","Dong Chen","Jianhui Wang","Tianyu Shi","Arsalan Heydarian"],"pdf_url":"https://arxiv.org/pdf/2404.02082v3.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03061v1","updated":"2024-10-04T00:53:32Z","published":"2024-10-04T00:53:32Z","title":"DocKD: Knowledge Distillation from LLMs for Open-World Document\n  Understanding Models","summary":"  Visual document understanding (VDU) is a challenging task that involves\nunderstanding documents across various modalities (text and image) and layouts\n(forms, tables, etc.). This study aims to enhance generalizability of small VDU\nmodels by distilling knowledge from LLMs. We identify that directly prompting\nLLMs often fails to generate informative and useful data. In response, we\npresent a new framework (called DocKD) that enriches the data generation\nprocess by integrating external document knowledge. Specifically, we provide an\nLLM with various document elements like key-value pairs, layouts, and\ndescriptions, to elicit open-ended answers. Our experiments show that DocKD\nproduces high-quality document annotations and surpasses the direct knowledge\ndistillation approach that does not leverage external document knowledge.\nMoreover, student VDU models trained with solely DocKD-generated data are not\nonly comparable to those trained with human-annotated data on in-domain tasks\nbut also significantly excel them on out-of-domain tasks.\n","authors":["Sungnyun Kim","Haofu Liao","Srikar Appalaraju","Peng Tang","Zhuowen Tu","Ravi Kumar Satzoda","R. Manmatha","Vijay Mahadevan","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2410.03061v1.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.08865v2","updated":"2024-10-04T00:44:58Z","published":"2024-07-11T20:58:38Z","title":"Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey","summary":"  Shadow removal aims at restoring the image content within shadow regions,\npursuing a uniform distribution of illumination that is consistent between\nshadow and non-shadow regions. {Comparing to other image restoration tasks,\nthere are two unique challenges in shadow removal:} 1) The patterns of shadows\nare arbitrary, varied, and often have highly complex trace structures, making\n``trace-less'' image recovery difficult. 2) The degradation caused by shadows\nis spatially non-uniform, resulting in inconsistencies in illumination and\ncolor between shadow and non-shadow areas. Recent developments in this field\nare primarily driven by deep learning-based solutions, employing a variety of\nlearning strategies, network architectures, loss functions, and training data.\nNevertheless, a thorough and insightful review of deep learning-based shadow\nremoval techniques is still lacking. In this paper, we are the first to provide\na comprehensive survey to cover various aspects ranging from technical details\nto applications. We highlight the major advancements in deep learning-based\nsingle-image shadow removal methods, thoroughly review previous research across\nvarious categories, and provide insights into the historical progression of\nthese developments. Additionally, we summarize performance comparisons both\nquantitatively and qualitatively. Beyond the technical aspects of shadow\nremoval methods, we also explore potential future directions for this field.\n","authors":["Laniqng Guo","Chong Wang","Yufei Wang","Yi Yu","Siyu Huang","Wenhan Yang","Alex C. Kot","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2407.08865v2.pdf","comment":"url: https://github.com/GuoLanqing/Awesome-Shadow-Removal"},{"id":"http://arxiv.org/abs/2402.16315v3","updated":"2024-10-04T00:40:23Z","published":"2024-02-26T05:43:51Z","title":"Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models","summary":"  Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.\n","authors":["Jeonghwan Kim","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2402.16315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03058v1","updated":"2024-10-04T00:38:29Z","published":"2024-10-04T00:38:29Z","title":"DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in\n  Dense Microscopy Images","summary":"  The proliferation of digital microscopy images, driven by advances in\nautomated whole slide scanning, presents significant opportunities for\nbiomedical research and clinical diagnostics. However, accurately annotating\ndensely packed information in these images remains a major challenge. To\naddress this, we introduce DiffKillR, a novel framework that reframes cell\nannotation as the combination of archetype matching and image registration\ntasks. DiffKillR employs two complementary neural networks: one that learns a\ndiffeomorphism-invariant feature space for robust cell matching and another\nthat computes the precise warping field between cells for annotation mapping.\nUsing a small set of annotated archetypes, DiffKillR efficiently propagates\nannotations across large microscopy images, reducing the need for extensive\nmanual labeling. More importantly, it is suitable for any type of pixel-level\nannotation. We will discuss the theoretical properties of DiffKillR and\nvalidate it on three microscopy tasks, demonstrating its advantages over\nexisting supervised, semi-supervised, and unsupervised methods.\n","authors":["Chen Liu","Danqi Liao","Alejandro Parada-Mayorga","Alejandro Ribeiro","Marcello DiStasio","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2410.03058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03054v1","updated":"2024-10-04T00:23:20Z","published":"2024-10-04T00:23:20Z","title":"CLIP-Clique: Graph-based Correspondence Matching Augmented by Vision\n  Language Models for Object-based Global Localization","summary":"  This letter proposes a method of global localization on a map with semantic\nobject landmarks. One of the most promising approaches for localization on\nobject maps is to use semantic graph matching using landmark descriptors\ncalculated from the distribution of surrounding objects. These descriptors are\nvulnerable to misclassification and partial observations. Moreover, many\nexisting methods rely on inlier extraction using RANSAC, which is stochastic\nand sensitive to a high outlier rate. To address the former issue, we augment\nthe correspondence matching using Vision Language Models (VLMs). Landmark\ndiscriminability is improved by VLM embeddings, which are independent of\nsurrounding objects. In addition, inliers are estimated deterministically using\na graph-theoretic approach. We also incorporate pose calculation using the\nweighted least squares considering correspondence similarity and observation\ncompleteness to improve the robustness. We confirmed improvements in matching\nand pose estimation accuracy through experiments on ScanNet and TUM datasets.\n","authors":["Shigemichi Matsuzaki","Kazuhito Tanaka","Kazuhiro Shintani"],"pdf_url":"https://arxiv.org/pdf/2410.03054v1.pdf","comment":"IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2410.03051v1","updated":"2024-10-04T00:13:54Z","published":"2024-10-04T00:13:54Z","title":"AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark","summary":"  Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality.\n","authors":["Wenhao Chai","Enxin Song","Yilun Du","Chenlin Meng","Vashisht Madhavan","Omer Bar-Tal","Jeng-Neng Hwang","Saining Xie","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2410.03051v1.pdf","comment":"Code, docs, weight, benchmark and training data are all avaliable at\n  \\href{https://rese1f.github.io/aurora-web/}{website}"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.03584v1","updated":"2024-10-04T16:42:13Z","published":"2024-10-04T16:42:13Z","title":"Discovering Biases in Information Retrieval Models Using Relevance\n  Thesaurus as Global Explanation","summary":"  Most efforts in interpreting neural relevance models have focused on local\nexplanations, which explain the relevance of a document to a query but are not\nuseful in predicting the model's behavior on unseen query-document pairs. We\npropose a novel method to globally explain neural relevance models by\nconstructing a \"relevance thesaurus\" containing semantically relevant query and\ndocument term pairs. This thesaurus is used to augment lexical matching models\nsuch as BM25 to approximate the neural model's predictions. Our method involves\ntraining a neural relevance model to score the relevance of partial query and\ndocument segments, which is then used to identify relevant terms across the\nvocabulary space. We evaluate the obtained thesaurus explanation based on\nranking effectiveness and fidelity to the target neural ranking model. Notably,\nour thesaurus reveals the existence of brand name bias in ranking models,\ndemonstrating one advantage of our explanation method.\n","authors":["Youngwoo Kim","Razieh Rahimi","James Allan"],"pdf_url":"https://arxiv.org/pdf/2410.03584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03432v1","updated":"2024-10-04T13:43:29Z","published":"2024-10-04T13:43:29Z","title":"EB-NeRD: A Large-Scale Dataset for News Recommendation","summary":"  Personalized content recommendations have been pivotal to the content\nexperience in digital media from video streaming to social networks. However,\nseveral domain specific challenges have held back adoption of recommender\nsystems in news publishing. To address these challenges, we introduce the\nEkstra Bladet News Recommendation Dataset (EB-NeRD). The dataset encompasses\ndata from over a million unique users and more than 37 million impression logs\nfrom Ekstra Bladet. It also includes a collection of over 125,000 Danish news\narticles, complete with titles, abstracts, bodies, and metadata, such as\ncategories. EB-NeRD served as the benchmark dataset for the RecSys '24\nChallenge, where it was demonstrated how the dataset can be used to address\nboth technical and normative challenges in designing effective and responsible\nrecommender systems for news publishing. The dataset is available at:\nhttps://recsys.eb.dk.\n","authors":["Johannes Kruse","Kasper Lindskow","Saikishore Kalloori","Marco Polignano","Claudio Pomo","Abhishek Srivastava","Anshuk Uppal","Michael Riis Andersen","Jes Frellsen"],"pdf_url":"https://arxiv.org/pdf/2410.03432v1.pdf","comment":"11 pages, 8 tables, 2 figures, RecSys '24"},{"id":"http://arxiv.org/abs/2402.15925v2","updated":"2024-10-04T13:37:12Z","published":"2024-02-24T23:01:21Z","title":"MultiContrievers: Analysis of Dense Retrieval Representations","summary":"  Dense retrievers compress source documents into (possibly lossy) vector\nrepresentations, yet there is little analysis of what information is lost\nversus preserved, and how it affects downstream tasks. We conduct the first\nanalysis of the information captured by dense retrievers compared to the\nlanguage models they are based on (e.g., BERT versus Contriever). We use 25\nMultiBert checkpoints as randomized initialisations to train MultiContrievers,\na set of 25 contriever models. We test whether specific pieces of information\n-- such as gender and occupation -- can be extracted from contriever vectors of\nwikipedia-like documents. We measure this extractability via information\ntheoretic probing. We then examine the relationship of extractability to\nperformance and gender bias, as well as the sensitivity of these results to\nmany random initialisations and data shuffles. We find that (1) contriever\nmodels have significantly increased extractability, but extractability usually\ncorrelates poorly with benchmark performance 2) gender bias is present, but is\nnot caused by the contriever representations 3) there is high sensitivity to\nboth random initialisation and to data shuffle, suggesting that future\nretrieval research should test across a wider spread of both.\n","authors":["Seraphina Goldfarb-Tarrant","Pedro Rodriguez","Jane Dwivedi-Yu","Patrick Lewis"],"pdf_url":"https://arxiv.org/pdf/2402.15925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03375v1","updated":"2024-10-04T12:40:45Z","published":"2024-10-04T12:40:45Z","title":"SoundSignature: What Type of Music Do You Like?","summary":"  SoundSignature is a music application that integrates a custom OpenAI\nAssistant to analyze users' favorite songs. The system incorporates\nstate-of-the-art Music Information Retrieval (MIR) Python packages to combine\nextracted acoustic/musical features with the assistant's extensive knowledge of\nthe artists and bands. Capitalizing on this combined knowledge, SoundSignature\nleverages semantic audio and principles from the emerging Internet of Sounds\n(IoS) ecosystem, integrating MIR with AI to provide users with personalized\ninsights into the acoustic properties of their music, akin to a musical\npreference personality report. Users can then interact with the chatbot to\nexplore deeper inquiries about the acoustic analyses performed and how they\nrelate to their musical taste. This interactivity transforms the application,\nacting not only as an informative resource about familiar and/or favorite\nsongs, but also as an educational platform that enables users to deepen their\nunderstanding of musical features, music theory, acoustic properties commonly\nused in signal processing, and the artists behind the music. Beyond general\nusability, the application also incorporates several well-established\nopen-source musician-specific tools, such as a chord recognition algorithm\n(CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter\n(basic-pitch). These features allow users without coding skills to access\nadvanced, open-source music processing algorithms simply by interacting with\nthe chatbot (e.g., can you give me the stems of this song?). In this paper, we\nhighlight the application's innovative features and educational potential, and\npresent findings from a pilot user study that evaluates its efficacy and\nusability.\n","authors":["Brandon James Carone","Pablo Ripolls"],"pdf_url":"https://arxiv.org/pdf/2410.03375v1.pdf","comment":"10 pages, 1 figure, to be published in the 2024 International\n  Symposium on the IEEE Internet of Sounds Proceedings"},{"id":"http://arxiv.org/abs/2410.03265v1","updated":"2024-10-04T09:34:07Z","published":"2024-10-04T09:34:07Z","title":"Multimodal Point-of-Interest Recommendation","summary":"  Large Language Models are applied to recommendation tasks such as items to\nbuy and news articles to read. Point of Interest is quite a new area to\nsequential recommendation based on language representations of multimodal\ndatasets. As a first step to prove our concepts, we focused on restaurant\nrecommendation based on each user's past visit history. When choosing a next\nrestaurant to visit, a user would consider genre and location of the venue and,\nif available, pictures of dishes served there. We created a pseudo restaurant\ncheck-in history dataset from the Foursquare dataset and the FoodX-251 dataset\nby converting pictures into text descriptions with a multimodal model called\nLLaVA, and used a language-based sequential recommendation framework named\nRecformer proposed in 2023. A model trained on this semi-multimodal dataset has\noutperformed another model trained on the same dataset without picture\ndescriptions. This suggests that this semi-multimodal model reflects actual\nhuman behaviours and that our path to a multimodal recommendation model is in\nthe right direction.\n","authors":["Yuta Kanzawa","Toyotaro Suzumura","Hiroki Kanezashi","Jiawei Yong","Shintaro Fukushima"],"pdf_url":"https://arxiv.org/pdf/2410.03265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03264v1","updated":"2024-10-04T09:33:34Z","published":"2024-10-04T09:33:34Z","title":"Enriching Music Descriptions with a Finetuned-LLM and Metadata for\n  Text-to-Music Retrieval","summary":"  Text-to-Music Retrieval, finding music based on a given natural language\nquery, plays a pivotal role in content discovery within extensive music\ndatabases. To address this challenge, prior research has predominantly focused\non a joint embedding of music audio and text, utilizing it to retrieve music\ntracks that exactly match descriptive queries related to musical attributes\n(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,\nusers also articulate a need to explore music that shares similarities with\ntheir favorite tracks or artists, such as \\textit{I need a similar track to\nSuperstition by Stevie Wonder}. To address these concerns, this paper proposes\nan improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes\nrich text descriptions generated with a finetuned large language model and\nmetadata. To accomplish this, we obtained various types of seed text from\nseveral existing music tag and caption datasets and a knowledge graph dataset\nof artists and tracks. The experimental results show the effectiveness of\nTTMR++ in comparison to state-of-the-art music-text joint embedding models\nthrough a comprehensive evaluation involving various musical text queries.\n","authors":["SeungHeon Doh","Minhee Lee","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2410.03264v1.pdf","comment":"Accepted for publication at the IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2410.03212v1","updated":"2024-10-04T07:58:05Z","published":"2024-10-04T07:58:05Z","title":"Data-Efficient Massive Tool Retrieval: A Reinforcement Learning Approach\n  for Query-Tool Alignment with Language Models","summary":"  Recent advancements in large language models (LLMs) integrated with external\ntools and APIs have successfully addressed complex tasks by using in-context\nlearning or fine-tuning. Despite this progress, the vast scale of tool\nretrieval remains challenging due to stringent input length constraints. In\nresponse, we propose a pre-retrieval strategy from an extensive repository,\neffectively framing the problem as the massive tool retrieval (MTR) task. We\nintroduce the MTRB (massive tool retrieval benchmark) to evaluate real-world\ntool-augmented LLM scenarios with a large number of tools. This benchmark is\ndesigned for low-resource scenarios and includes a diverse collection of tools\nwith descriptions refined for consistency and clarity. It consists of three\nsubsets, each containing 90 test samples and 10 training samples. To handle the\nlow-resource MTR task, we raise a new query-tool alignment (QTA) framework\nleverages LLMs to enhance query-tool alignment by rewriting user queries\nthrough ranking functions and the direct preference optimization (DPO) method.\nThis approach consistently outperforms existing state-of-the-art models in\ntop-5 and top-10 retrieval tasks across the MTRB benchmark, with improvements\nup to 93.28% based on the metric Sufficiency@k, which measures the adequacy of\ntool retrieval within the first k results. Furthermore, ablation studies\nvalidate the efficacy of our framework, highlighting its capacity to optimize\nperformance even with limited annotated samples. Specifically, our framework\nachieves up to 78.53% performance improvement in Sufficiency@k with just a\nsingle annotated sample. Additionally, QTA exhibits strong cross-dataset\ngeneralizability, emphasizing its potential for real-world applications.\n","authors":["Yuxiang Zhang","Xin Fan","Junjie Wang","Chongxian Chen","Fan Mo","Tetsuya Sakai","Hayato Yamana"],"pdf_url":"https://arxiv.org/pdf/2410.03212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07272v3","updated":"2024-10-04T07:26:24Z","published":"2024-09-11T13:46:52Z","title":"RePlay: a Recommendation Framework for Experimentation and Production\n  Use","summary":"  Using a single tool to build and compare recommender systems significantly\nreduces the time to market for new models. In addition, the comparison results\nwhen using such tools look more consistent. This is why many different tools\nand libraries for researchers in the field of recommendations have recently\nappeared. Unfortunately, most of these frameworks are aimed primarily at\nresearchers and require modification for use in production due to the inability\nto work on large datasets or an inappropriate architecture. In this demo, we\npresent our open-source toolkit RePlay - a framework containing an end-to-end\npipeline for building recommender systems, which is ready for production use.\nRePlay also allows you to use a suitable stack for the pipeline on each stage:\nPandas, Polars, or Spark. This allows the library to scale computations and\ndeploy to a cluster. Thus, RePlay allows data scientists to easily move from\nresearch mode to production mode using the same interfaces.\n","authors":["Alexey Vasilev","Anna Volodkevich","Denis Kulandin","Tatiana Bysheva","Anton Klenitskiy"],"pdf_url":"https://arxiv.org/pdf/2409.07272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06648v3","updated":"2024-10-04T05:33:51Z","published":"2023-12-11T18:57:35Z","title":"Dense X Retrieval: What Retrieval Granularity Should We Use?","summary":"  Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our experiments reveal that\nindexing a corpus by fine-grained units such as propositions significantly\noutperforms passage-level units in retrieval tasks. Moreover, constructing\nprompts with fine-grained retrieved units for retrieval-augmented language\nmodels improves the performance of downstream QA tasks given a specific\ncomputation budget.\n","authors":["Tong Chen","Hongwei Wang","Sihao Chen","Wenhao Yu","Kaixin Ma","Xinran Zhao","Hongming Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06648v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18443v2","updated":"2024-10-04T03:25:34Z","published":"2024-04-29T05:40:08Z","title":"BMRetriever: Tuning Large Language Models as Better Biomedical Text\n  Retrievers","summary":"  Developing effective biomedical retrieval models is important for excelling\nat knowledge-intensive biomedical tasks but still challenging due to the\ndeficiency of sufficient publicly annotated biomedical data and computational\nresources. We present BMRetriever, a series of dense retrievers for enhancing\nbiomedical retrieval via unsupervised pre-training on large biomedical corpora,\nfollowed by instruction fine-tuning on a combination of labeled datasets and\nsynthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify\nBMRetriever's efficacy on various biomedical applications. BMRetriever also\nexhibits strong parameter efficiency, with the 410M variant outperforming\nbaselines up to 11.7 times larger, and the 2B variant matching the performance\nof models with over 5B parameters. The training data and model checkpoints are\nreleased at \\url{https://huggingface.co/BMRetriever} to ensure transparency,\nreproducibility, and application to new domains.\n","authors":["Ran Xu","Wenqi Shi","Yue Yu","Yuchen Zhuang","Yanqiao Zhu","May D. Wang","Joyce C. Ho","Chao Zhang","Carl Yang"],"pdf_url":"https://arxiv.org/pdf/2404.18443v2.pdf","comment":"Accepted to EMNLP 2024. The model and data are uploaded to\n  \\url{https://github.com/ritaranx/BMRetriever}"},{"id":"http://arxiv.org/abs/2403.16504v2","updated":"2024-10-04T03:21:09Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent\n  Classification","summary":"  Multi-turn intent classification is notably challenging due to the complexity\nand evolving nature of conversational contexts. This paper introduces LARA, a\nLinguistic-Adaptive Retrieval-Augmentation framework to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating a large\nnumber of intents in chatbot interactions. LARA combines a fine-tuned smaller\nmodel with a retrieval-augmented mechanism, integrated within the architecture\nof LLMs. The integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tuning.\nComprehensive experiments demonstrate that LARA achieves state-of-the-art\nperformance on multi-turn intent classification tasks, enhancing the average\naccuracy by 3.67\\% from state-of-the-art single-turn intent classifiers.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin","Lim Kwan Hui"],"pdf_url":"https://arxiv.org/pdf/2403.16504v2.pdf","comment":"Accepted to EMNLP'24 Industry Track"},{"id":"http://arxiv.org/abs/2410.03071v1","updated":"2024-10-04T01:28:56Z","published":"2024-10-04T01:28:56Z","title":"Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion\n  and Prefix-Tuned VAEs","summary":"  Topic modeling is a powerful technique for uncovering hidden themes within a\ncollection of documents. However, the effectiveness of traditional topic models\noften relies on sufficient word co-occurrence, which is lacking in short texts.\nTherefore, existing approaches, whether probabilistic or neural, frequently\nstruggle to extract meaningful patterns from such data, resulting in incoherent\ntopics. To address this challenge, we propose a novel approach that leverages\nlarge language models (LLMs) to extend short texts into more detailed sequences\nbefore applying topic modeling. To further improve the efficiency and solve the\nproblem of semantic inconsistency from LLM-generated texts, we propose to use\nprefix tuning to train a smaller language model coupled with a variational\nautoencoder for short-text topic modeling. Our method significantly improves\nshort-text topic modeling performance, as demonstrated by extensive experiments\non real-world datasets with extreme data sparsity, outperforming current\nstate-of-the-art topic models.\n","authors":["Pritom Saha Akash","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2410.03071v1.pdf","comment":"EMNLP Findings 2024. arXiv admin note: substantial text overlap with\n  arXiv:2310.15420"},{"id":"http://arxiv.org/abs/2410.03064v1","updated":"2024-10-04T01:04:41Z","published":"2024-10-04T01:04:41Z","title":"Geometric Collaborative Filtering with Convergence","summary":"  Latent variable collaborative filtering methods have been a standard approach\nto modelling user-click interactions due to their simplicity and effectiveness.\nHowever, there is limited work on analyzing the mathematical properties of\nthese methods in particular on preventing the overfitting towards the identity,\nand such methods typically utilize loss functions that overlook the geometry\nbetween items. In this work, we introduce a notion of generalization gap in\ncollaborative filtering and analyze this with respect to latent collaborative\nfiltering models. We present a geometric upper bound that gives rise to loss\nfunctions, and a way to meaningfully utilize the geometry of item-metadata to\nimprove recommendations. We show how these losses can be minimized and gives\nthe recipe to a new latent collaborative filtering algorithm, which we refer to\nas GeoCF, due to the geometric nature of our results. We then show\nexperimentally that our proposed GeoCF algorithm can outperform other all\nexisting methods on the Movielens20M and Netflix datasets, as well as two\nlarge-scale internal datasets. In summary, our work proposes a theoretically\nsound method which paves a way to better understand generalization of\ncollaborative filtering at large.\n","authors":["Hisham Husain","Julien Monteil"],"pdf_url":"https://arxiv.org/pdf/2410.03064v1.pdf","comment":"13 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2410.03049v1","updated":"2024-10-04T00:08:46Z","published":"2024-10-04T00:08:46Z","title":"Scalable Frame-based Construction of Sociocultural NormBases for\n  Socially-Aware Dialogues","summary":"  Sociocultural norms serve as guiding principles for personal conduct in\nsocial interactions, emphasizing respect, cooperation, and appropriate\nbehavior, which is able to benefit tasks including conversational information\nretrieval, contextual information retrieval and retrieval-enhanced machine\nlearning. We propose a scalable approach for constructing a Sociocultural Norm\n(SCN) Base using Large Language Models (LLMs) for socially aware dialogues. We\nconstruct a comprehensive and publicly accessible Chinese Sociocultural\nNormBase. Our approach utilizes socially aware dialogues, enriched with\ncontextual frames, as the primary data source to constrain the generating\nprocess and reduce the hallucinations. This enables extracting of high-quality\nand nuanced natural-language norm statements, leveraging the pragmatic\nimplications of utterances with respect to the situation. As real dialogue\nannotated with gold frames are not readily available, we propose using\nsynthetic data. Our empirical results show: (i) the quality of the SCNs derived\nfrom synthetic data is comparable to that from real dialogues annotated with\ngold frames, and (ii) the quality of the SCNs extracted from real data,\nannotated with either silver (predicted) or gold frames, surpasses that without\nthe frame annotations. We further show the effectiveness of the extracted SCNs\nin a RAG-based (Retrieval-Augmented Generation) model to reason about multiple\ndownstream dialogue tasks.\n","authors":["Shilin Qu","Weiqing Wang","Xin Zhou","Haolan Zhan","Zhuang Li","Lizhen Qu","Linhao Luo","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.03049v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.03925v1","updated":"2024-10-04T21:04:39Z","published":"2024-10-04T21:04:39Z","title":"C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy\n  Policies to Enable Scalable Regulatory Compliance Audits","summary":"  The development of tools and techniques to analyze and extract organizations\ndata habits from privacy policies are critical for scalable regulatory\ncompliance audits. Unfortunately, these tools are becoming increasingly limited\nin their ability to identify compliance issues and fixes. After all, most were\ndeveloped using regulation-agnostic datasets of annotated privacy policies\nobtained from a time before the introduction of landmark privacy regulations\nsuch as EUs GDPR and Californias CCPA. In this paper, we describe the first\nopen regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA\nPrivacy Policy Provision Annotations), aimed to address this challenge. C3PA\ncontains over 48K expert-labeled privacy policy text segments associated with\nresponses to CCPA-specific disclosure mandates from 411 unique organizations.\nWe demonstrate that the C3PA dataset is uniquely suited for aiding automated\naudits of compliance with CCPA-related disclosure mandates.\n","authors":["Maaz Bin Musa","Steven M. Winston","Garrison Allen","Jacob Schiller","Kevin Moore","Sean Quick","Johnathan Melvin","Padmini Srinivasan","Mihailis E. Diamantis","Rishab Nithyanand"],"pdf_url":"https://arxiv.org/pdf/2410.03925v1.pdf","comment":"9 pages, EMNLP 2024"},{"id":"http://arxiv.org/abs/2212.06933v2","updated":"2024-10-04T18:53:45Z","published":"2022-12-13T23:06:20Z","title":"Paraphrase Identification with Deep Learning: A Review of Datasets and\n  Methods","summary":"  The rapid progress of Natural Language Processing (NLP) technologies has led\nto the widespread availability and effectiveness of text generation tools such\nas ChatGPT and Claude. While highly useful, these technologies also pose\nsignificant risks to the credibility of various media forms if they are\nemployed for paraphrased plagiarism -- one of the most subtle forms of content\nmisuse in scientific literature and general text media. Although automated\nmethods for paraphrase identification have been developed, detecting this type\nof plagiarism remains challenging due to the inconsistent nature of the\ndatasets used to train these methods. In this article, we examine traditional\nand contemporary approaches to paraphrase identification, investigating how the\nunder-representation of certain paraphrase types in popular datasets, including\nthose used to train Large Language Models (LLMs), affects the ability to detect\nplagiarism. We introduce and validate a new refined typology for paraphrases\n(ReParaphrased, REfined PARAPHRASE typology definitions) to better understand\nthe disparities in paraphrase type representation. Lastly, we propose new\ndirections for future research and dataset development to enhance AI-based\nparaphrase detection.\n","authors":["Chao Zhou","Cheng Qiu","Daniel E. Acuna"],"pdf_url":"https://arxiv.org/pdf/2212.06933v2.pdf","comment":"45 pages, 6 figures, 7 tables, 143 references"},{"id":"http://arxiv.org/abs/2409.17745v3","updated":"2024-10-04T18:35:14Z","published":"2024-09-26T11:19:09Z","title":"Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric\n  Retrieval Model","summary":"  A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline.\n","authors":["Nilanjan Sinhababu","Andrew Parry","Debasis Ganguly","Debasis Samanta","Pabitra Mitra"],"pdf_url":"https://arxiv.org/pdf/2409.17745v3.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03841v1","updated":"2024-10-04T18:14:58Z","published":"2024-10-04T18:14:58Z","title":"Explaining the (Not So) Obvious: Simple and Fast Explanation of STAN, a\n  Next Point of Interest Recommendation System","summary":"  A lot of effort in recent years have been expended to explain machine\nlearning systems. However, some machine learning methods are inherently\nexplainable, and thus are not completely black box. This enables the developers\nto make sense of the output without a developing a complex and expensive\nexplainability technique. Besides that, explainability should be tailored to\nsuit the context of the problem. In a recommendation system which relies on\ncollaborative filtering, the recommendation is based on the behaviors of\nsimilar users, therefore the explanation should tell which other users are\nsimilar to the current user. Similarly, if the recommendation system is based\non sequence prediction, the explanation should also tell which input timesteps\nare the most influential. We demonstrate this philosophy/paradigm in STAN\n(Spatio-Temporal Attention Network for Next Location Recommendation), a next\nPoint of Interest recommendation system based on collaborative filtering and\nsequence prediction. We also show that the explanation helps to \"debug\" the\noutput.\n","authors":["Fajrian Yunus","Talel Abdessalem"],"pdf_url":"https://arxiv.org/pdf/2410.03841v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.03662v1","updated":"2024-10-04T17:59:36Z","published":"2024-10-04T17:59:36Z","title":"System 2 reasoning capabilities are nigh","summary":"  In recent years, machine learning models have made strides towards human-like\nreasoning capabilities from several directions. In this work, we review the\ncurrent state of the literature and describe the remaining steps to achieve a\nneural model which can perform System 2 reasoning analogous to a human. We\nargue that if current models are insufficient to be classed as performing\nreasoning, there remains very little additional progress needed to attain that\ngoal.\n","authors":["Scott C. Lowe"],"pdf_url":"https://arxiv.org/pdf/2410.03662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03658v1","updated":"2024-10-04T17:59:00Z","published":"2024-10-04T17:59:00Z","title":"RAFT: Realistic Attacks to Fool Text Detectors","summary":"  Large language models (LLMs) have exhibited remarkable fluency across various\ntasks. However, their unethical applications, such as disseminating\ndisinformation, have become a growing concern. Although recent works have\nproposed a number of LLM detection methods, their robustness and reliability\nremain unclear. In this paper, we present RAFT: a grammar error-free black-box\nattack against existing LLM detectors. In contrast to previous attacks for\nlanguage models, our method exploits the transferability of LLM embeddings at\nthe word-level while preserving the original text quality. We leverage an\nauxiliary embedding to greedily select candidate words to perturb against the\ntarget detector. Experiments reveal that our attack effectively compromises all\ndetectors in the study across various domains by up to 99%, and are\ntransferable across source models. Manual human evaluation studies show our\nattacks are realistic and indistinguishable from original human-written text.\nWe also show that examples generated by RAFT can be used to train adversarially\nrobust detectors. Our work shows that current LLM detectors are not\nadversarially robust, underscoring the urgent need for more resilient detection\nmechanisms.\n","authors":["James Wang","Ran Li","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2410.03658v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03655v1","updated":"2024-10-04T17:57:35Z","published":"2024-10-04T17:57:35Z","title":"Geometric Representation Condition Improves Equivariant Molecule\n  Generation","summary":"  Recent advancements in molecular generative models have demonstrated\nsubstantial potential in accelerating scientific discovery, particularly in\ndrug design. However, these models often face challenges in generating\nhigh-quality molecules, especially in conditional scenarios where specific\nmolecular properties must be satisfied. In this work, we introduce GeoRCG, a\ngeneral framework to enhance the performance of molecular generative models by\nintegrating geometric representation conditions. We decompose the molecule\ngeneration process into two stages: first, generating an informative geometric\nrepresentation; second, generating a molecule conditioned on the\nrepresentation. Compared to directly generating a molecule, the relatively\neasy-to-generate representation in the first-stage guides the second-stage\ngeneration to reach a high-quality molecule in a more goal-oriented and much\nfaster way. Leveraging EDM as the base generator, we observe significant\nquality improvements in unconditional molecule generation on the widely-used\nQM9 and GEOM-DRUG datasets. More notably, in the challenging conditional\nmolecular generation task, our framework achieves an average 31\\% performance\nimprovement over state-of-the-art approaches, highlighting the superiority of\nconditioning on semantically rich geometric representations over conditioning\non individual property values as in previous approaches. Furthermore, we show\nthat, with such representation guidance, the number of diffusion steps can be\nreduced to as small as 100 while maintaining superior generation quality than\nthat achieved with 1,000 steps, thereby significantly accelerating the\ngeneration process.\n","authors":["Zian Li","Cai Zhou","Xiyuan Wang","Xingang Peng","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03654v1","updated":"2024-10-04T17:57:09Z","published":"2024-10-04T17:57:09Z","title":"Learning Humanoid Locomotion over Challenging Terrain","summary":"  Humanoid robots can, in principle, use their legs to go almost anywhere.\nDeveloping controllers capable of traversing diverse terrains, however, remains\na considerable challenge. Classical controllers are hard to generalize broadly\nwhile the learning-based methods have primarily focused on gentle terrains.\nHere, we present a learning-based approach for blind humanoid locomotion\ncapable of traversing challenging natural and man-made terrain. Our method uses\na transformer model to predict the next action based on the history of\nproprioceptive observations and actions. The model is first pre-trained on a\ndataset of flat-ground trajectories with sequence modeling, and then fine-tuned\non uneven terrain using reinforcement learning. We evaluate our model on a real\nhumanoid robot across a variety of terrains, including rough, deformable, and\nsloped surfaces. The model demonstrates robust performance, in-context\nadaptation, and emergent terrain representations. In real-world case studies,\nour humanoid robot successfully traversed over 4 miles of hiking trails in\nBerkeley and climbed some of the steepest streets in San Francisco.\n","authors":["Ilija Radosavovic","Sarthak Kamat","Trevor Darrell","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2410.03654v1.pdf","comment":"Project page: https://humanoid-challenging-terrain.github.io"},{"id":"http://arxiv.org/abs/2410.03651v1","updated":"2024-10-04T17:55:31Z","published":"2024-10-04T17:55:31Z","title":"Minimax-optimal trust-aware multi-armed bandits","summary":"  Multi-armed bandit (MAB) algorithms have achieved significant success in\nsequential decision-making applications, under the premise that humans\nperfectly implement the recommended policy. However, existing methods often\noverlook the crucial factor of human trust in learning algorithms. When trust\nis lacking, humans may deviate from the recommended policy, leading to\nundesired learning performance. Motivated by this gap, we study the trust-aware\nMAB problem by integrating a dynamic trust model into the standard MAB\nframework. Specifically, it assumes that the recommended and actually\nimplemented policy differs depending on human trust, which in turn evolves with\nthe quality of the recommended policy. We establish the minimax regret in the\npresence of the trust issue and demonstrate the suboptimality of vanilla MAB\nalgorithms such as the upper confidence bound (UCB) algorithm. To overcome this\nlimitation, we introduce a novel two-stage trust-aware procedure that provably\nattains near-optimal statistical guarantees. A simulation study is conducted to\nillustrate the benefits of our proposed algorithm when dealing with the trust\nissue.\n","authors":["Changxiao Cai","Jiacheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03645v1","updated":"2024-10-04T17:51:33Z","published":"2024-10-04T17:51:33Z","title":"GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning\n  LLMs","summary":"  Robotic simulation today remains challenging to scale up due to the human\nefforts required to create diverse simulation tasks and scenes.\nSimulation-trained policies also face scalability issues as many sim-to-real\nmethods focus on a single task. To address these challenges, this work proposes\nGenSim2, a scalable framework that leverages coding LLMs with multi-modal and\nreasoning capabilities for complex and realistic simulation task creation,\nincluding long-horizon tasks with articulated objects. To automatically\ngenerate demonstration data for these tasks at scale, we propose planning and\nRL solvers that generalize within object categories. The pipeline can generate\ndata for up to 100 articulated tasks with 200 objects and reduce the required\nhuman efforts. To utilize such data, we propose an effective multi-task\nlanguage-conditioned policy architecture, dubbed proprioceptive point-cloud\ntransformer (PPT), that learns from the generated demonstrations and exhibits\nstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and the\npolicy architecture, we show a promising usage of GenSim2 that the generated\ndata can be used for zero-shot transfer or co-train with real-world collected\ndata, which enhances the policy performance by 20% compared with training\nexclusively on limited real data.\n","authors":["Pu Hua","Minghuan Liu","Annabella Macaluso","Yunfeng Lin","Weinan Zhang","Huazhe Xu","Lirui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03645v1.pdf","comment":"CoRL 2024. Project website: https://gensim2.github.io/"},{"id":"http://arxiv.org/abs/2410.02117v2","updated":"2024-10-04T17:47:01Z","published":"2024-10-03T00:44:50Z","title":"Searching for Efficient Linear Layers over a Continuous Space of\n  Structured Matrices","summary":"  Dense linear layers are the dominant computational bottleneck in large neural\nnetworks, presenting a critical need for more efficient alternatives. Previous\nefforts focused on a small number of hand-crafted structured matrices and\nneglected to investigate whether these structures can surpass dense layers in\nterms of compute-optimal scaling laws when both the model size and training\nexamples are optimally allocated. In this work, we present a unifying framework\nthat enables searching among all linear operators expressible via an Einstein\nsummation. This framework encompasses many previously proposed structures, such\nas low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch,\nalong with many novel structures. To analyze the framework, we develop a\ntaxonomy of all such operators based on their computational and algebraic\nproperties and show that differences in the compute-optimal scaling laws are\nmostly governed by a small number of variables that we introduce. Namely, a\nsmall $\\omega$ (which measures parameter sharing) and large $\\psi$ (which\nmeasures the rank) reliably led to better scaling laws. Guided by the insight\nthat full-rank structures that maximize parameters per unit of compute perform\nthe best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture\nobtained by sparsifying computation in the BTT structure. In contrast to the\nstandard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE\nin every single linear layer of the model, including the projection matrices in\nthe attention blocks. We find BTT-MoE provides a substantial compute-efficiency\ngain over dense layers and standard MoE.\n","authors":["Andres Potapczynski","Shikai Qiu","Marc Finzi","Christopher Ferri","Zixi Chen","Micah Goldblum","Bayan Bruss","Christopher De Sa","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2410.02117v2.pdf","comment":"NeurIPS 2024. Code available at\n  https://github.com/AndPotap/einsum-search"},{"id":"http://arxiv.org/abs/2410.03640v1","updated":"2024-10-04T17:46:06Z","published":"2024-10-04T17:46:06Z","title":"Real-World Benchmarks Make Membership Inference Attacks Fail on\n  Diffusion Models","summary":"  Membership inference attacks (MIAs) on diffusion models have emerged as\npotential evidence of unauthorized data usage in training pre-trained diffusion\nmodels. These attacks aim to detect the presence of specific images in training\ndatasets of diffusion models. Our study delves into the evaluation of\nstate-of-the-art MIAs on diffusion models and reveals critical flaws and overly\noptimistic performance estimates in existing MIA evaluation. We introduce\nCopyMark, a more realistic MIA benchmark that distinguishes itself through the\nsupport for pre-trained diffusion models, unbiased datasets, and fair\nevaluation pipelines. Through extensive experiments, we demonstrate that the\neffectiveness of current MIA methods significantly degrades under these more\npractical conditions. Based on our results, we alert that MIA, in its current\nstate, is not a reliable approach for identifying unauthorized data usage in\npre-trained diffusion models. To the best of our knowledge, we are the first to\ndiscover the performance overestimation of MIAs on diffusion models and present\na unified benchmark for more realistic evaluation. Our code is available on\nGitHub: \\url{https://github.com/caradryanl/CopyMark}.\n","authors":["Chumeng Liang","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2410.03640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17210v2","updated":"2024-10-04T17:46:04Z","published":"2023-06-29T18:00:00Z","title":"Scattering Spectra Models for Physics","summary":"  Physicists routinely need probabilistic models for a number of tasks such as\nparameter inference or the generation of new realizations of a field.\nEstablishing such models for highly non-Gaussian fields is a challenge,\nespecially when the number of samples is limited. In this paper, we introduce\nscattering spectra models for stationary fields and we show that they provide\naccurate and robust statistical descriptions of a wide range of fields\nencountered in physics. These models are based on covariances of scattering\ncoefficients, i.e. wavelet decomposition of a field coupled with a point-wise\nmodulus. After introducing useful dimension reductions taking advantage of the\nregularity of a field under rotation and scaling, we validate these models on\nvarious multi-scale physical fields and demonstrate that they reproduce\nstandard statistics, including spatial moments up to 4th order. These\nscattering spectra provide us with a low-dimensional structured representation\nthat captures key properties encountered in a wide range of physical fields.\nThese generic models can be used for data exploration, classification,\nparameter inference, symmetry detection, and component separation.\n","authors":["Sihao Cheng","Rudy Morel","Erwan Allys","Brice Mnard","Stphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2306.17210v2.pdf","comment":"11 pages, 6 figures, plus appendices, updated to published version"},{"id":"http://arxiv.org/abs/2410.03634v1","updated":"2024-10-04T17:41:47Z","published":"2024-10-04T17:41:47Z","title":"Conditional Enzyme Generation Using Protein Language Models with\n  Adapters","summary":"  The conditional generation of proteins with desired functions and/or\nproperties is a key goal for generative models. Existing methods based on\nprompting of language models can generate proteins conditioned on a target\nfunctionality, such as a desired enzyme family. However, these methods are\nlimited to simple, tokenized conditioning and have not been shown to generalize\nto unseen functions. In this study, we propose ProCALM (Protein Conditionally\nAdapted Language Model), an approach for the conditional generation of proteins\nusing adapters to protein language models. Our specific implementation of\nProCALM involves finetuning ProGen2 to incorporate conditioning representations\nof enzyme function and taxonomy. ProCALM matches existing methods at\nconditionally generating sequences from target enzyme families. Impressively,\nit can also generate within the joint distribution of enzymatic function and\ntaxonomy, and it can generalize to rare and unseen enzyme families and\ntaxonomies. Overall, ProCALM is a flexible and computationally efficient\napproach, and we expect that it can be extended to a wide range of generative\nlanguage models.\n","authors":["Jason Yang","Aadyot Bhatnagar","Jeffrey A. Ruffolo","Ali Madani"],"pdf_url":"https://arxiv.org/pdf/2410.03634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03626v1","updated":"2024-10-04T17:30:54Z","published":"2024-10-04T17:30:54Z","title":"Robust Offline Imitation Learning from Diverse Auxiliary Data","summary":"  Offline imitation learning enables learning a policy solely from a set of\nexpert demonstrations, without any environment interaction. To alleviate the\nissue of distribution shift arising due to the small amount of expert data,\nrecent works incorporate large numbers of auxiliary demonstrations alongside\nthe expert data. However, the performance of these approaches rely on\nassumptions about the quality and composition of the auxiliary data. However,\nthey are rarely successful when those assumptions do not hold. To address this\nlimitation, we propose Robust Offline Imitation from Diverse Auxiliary Data\n(ROIDA). ROIDA first identifies high-quality transitions from the entire\nauxiliary dataset using a learned reward function. These high-reward samples\nare combined with the expert demonstrations for weighted behavioral cloning.\nFor lower-quality samples, ROIDA applies temporal difference learning to steer\nthe policy towards high-reward states, improving long-term returns. This\ntwo-pronged approach enables our framework to effectively leverage both high\nand low-quality data without any assumptions. Extensive experiments validate\nthat ROIDA achieves robust and consistent performance across multiple auxiliary\ndatasets with diverse ratios of expert and non-expert demonstrations. ROIDA\neffectively leverages unlabeled auxiliary data, outperforming prior methods\nreliant on specific data assumptions.\n","authors":["Udita Ghosh","Dripta S. Raychaudhuri","Jiachen Li","Konstantinos Karydis","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2410.03626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12917v2","updated":"2024-10-04T17:28:45Z","published":"2024-09-19T17:16:21Z","title":"Training Language Models to Self-Correct via Reinforcement Learning","summary":"  Self-correction is a highly desirable capability of large language models\n(LLMs), yet it has consistently been found to be largely ineffective in modern\nLLMs. Current methods for training self-correction typically depend on either\nmultiple models, a more advanced model, or additional forms of supervision. To\naddress these shortcomings, we develop a multi-turn online reinforcement\nlearning (RL) approach, SCoRe, that significantly improves an LLM's\nself-correction ability using entirely self-generated data. To build SCoRe, we\nfirst show that variants of supervised fine-tuning (SFT) on offline\nmodel-generated correction traces are often insufficient for instilling\nself-correction behavior. In particular, we observe that training via SFT falls\nprey to either a distribution mismatch between mistakes made by the\ndata-collection policy and the model's own responses, or to behavior collapse,\nwhere learning implicitly prefers only a certain mode of correction behavior\nthat is often not effective at self-correction on test problems. SCoRe\naddresses these challenges by training under the model's own distribution of\nself-generated correction traces and using appropriate regularization to steer\nthe learning process into learning a self-correction behavior that is effective\nat test time as opposed to fitting high-reward responses for a given prompt.\nThis regularization process includes an initial phase of multi-turn RL on a\nbase model to generate a policy initialization that is less susceptible to\ncollapse, followed by using a reward bonus to amplify self-correction. With\nGemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves\nstate-of-the-art self-correction performance, improving the base models'\nself-correction by 15.6% and 9.1% respectively on MATH and HumanEval.\n","authors":["Aviral Kumar","Vincent Zhuang","Rishabh Agarwal","Yi Su","John D Co-Reyes","Avi Singh","Kate Baumli","Shariq Iqbal","Colton Bishop","Rebecca Roelofs","Lei M Zhang","Kay McKinney","Disha Shrivastava","Cosmin Paduraru","George Tucker","Doina Precup","Feryal Behbahani","Aleksandra Faust"],"pdf_url":"https://arxiv.org/pdf/2409.12917v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20974v3","updated":"2024-10-04T17:23:48Z","published":"2024-05-31T16:21:16Z","title":"SaySelf: Teaching LLMs to Express Confidence with Self-Reflective\n  Rationales","summary":"  Large language models (LLMs) often generate inaccurate or fabricated\ninformation and generally fail to indicate their confidence, which limits their\nbroader applications. Previous work elicits confidence from LLMs by direct or\nself-consistency prompting, or constructing specific datasets for supervised\nfinetuning. The prompting-based approaches have inferior performance, and the\ntraining-based approaches are limited to binary or inaccurate group-level\nconfidence estimates. In this work, we present the advanced SaySelf, a training\nframework that teaches LLMs to express more accurate fine-grained confidence\nestimates. In addition, beyond the confidence scores, SaySelf initiates the\nprocess of directing LLMs to produce self-reflective rationales that clearly\nidentify gaps in their parametric knowledge and explain their uncertainty. This\nis achieved by using an LLM to automatically summarize the uncertainties in\nspecific knowledge via natural language. The summarization is based on the\nanalysis of the inconsistency in multiple sampled reasoning chains, and the\nresulting data is utilized for supervised fine-tuning. Moreover, we utilize\nreinforcement learning with a meticulously crafted reward function to calibrate\nthe confidence estimates, motivating LLMs to deliver accurate, high-confidence\npredictions and to penalize overconfidence in erroneous outputs. Experimental\nresults in both in-distribution and out-of-distribution datasets demonstrate\nthe effectiveness of SaySelf in reducing the confidence calibration error and\nmaintaining the task performance. We show that the generated self-reflective\nrationales are reasonable and can further contribute to the calibration. The\ncode is made public at https://github.com/xu1868/SaySelf.\n","authors":["Tianyang Xu","Shujin Wu","Shizhe Diao","Xiaoze Liu","Xingyao Wang","Yangyi Chen","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2405.20974v3.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2311.08644v3","updated":"2024-10-04T17:23:15Z","published":"2023-11-15T01:50:53Z","title":"Wrapper Boxes: Faithful Attribution of Model Predictions to Training\n  Data","summary":"  Can we preserve the accuracy of neural models while also providing faithful\nexplanations of model decisions to training data? We propose a \"wrapper box''\npipeline: training a neural model as usual and then using its learned feature\nrepresentation in classic, interpretable models to perform prediction. Across\nseven language models of varying sizes, including four large language models\n(LLMs), two datasets at different scales, three classic models, and four\nevaluation metrics, we first show that the predictive performance of wrapper\nclassic models is largely comparable to the original neural models.\n  Because classic models are transparent, each model decision is determined by\na known set of training examples that can be directly shown to users. Our\npipeline thus preserves the predictive performance of neural language models\nwhile faithfully attributing classic model decisions to training data. Among\nother use cases, such attribution enables model decisions to be contested based\non responsible training instances. Compared to prior work, our approach\nachieves higher coverage and correctness in identifying which training data to\nremove to change a model decision. To reproduce findings, our source code is\nonline at: https://github.com/SamSoup/WrapperBox.\n","authors":["Yiheng Su","Junyi Jessy Li","Matthew Lease"],"pdf_url":"https://arxiv.org/pdf/2311.08644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03621v1","updated":"2024-10-04T17:22:55Z","published":"2024-10-04T17:22:55Z","title":"A Global Medical Data Security and Privacy Preserving Standards\n  Identification Framework for Electronic Healthcare Consumers","summary":"  Electronic Health Records (EHR) are crucial for the success of digital\nhealthcare, with a focus on putting consumers at the center of this\ntransformation. However, the digitalization of healthcare records brings along\nsecurity and privacy risks for personal data. The major concern is that\ndifferent countries have varying standards for the security and privacy of\nmedical data. This paper proposed a novel and comprehensive framework to\nstandardize these rules globally, bringing them together on a common platform.\nTo support this proposal, the study reviews existing literature to understand\nthe research interest in this issue. It also examines six key laws and\nstandards related to security and privacy, identifying twenty concepts. The\nproposed framework utilized K-means clustering to categorize these concepts and\nidentify five key factors. Finally, an Ordinal Priority Approach is applied to\ndetermine the preferred implementation of these factors in the context of EHRs.\nThe proposed study provides a descriptive then prescriptive framework for the\nimplementation of privacy and security in the context of electronic health\nrecords. Therefore, the findings of the proposed framework are useful for\nprofessionals and policymakers in improving the security and privacy associated\nwith EHRs.\n","authors":["Vinaytosh Mishra","Kishu Gupta","Deepika Saxena","Ashutosh Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2410.03621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03618v1","updated":"2024-10-04T17:17:30Z","published":"2024-10-04T17:17:30Z","title":"Open-World Reinforcement Learning over Long Short-Term Imagination","summary":"  Training visual reinforcement learning agents in a high-dimensional open\nworld presents significant challenges. While various model-based methods have\nimproved sample efficiency by learning interactive world models, these agents\ntend to be \"short-sighted\", as they are typically trained on short snippets of\nimagined experiences. We argue that the primary obstacle in open-world\ndecision-making is improving the efficiency of off-policy exploration across an\nextensive state space. In this paper, we present LS-Imagine, which extends the\nimagination horizon within a limited number of state transition steps, enabling\nthe agent to explore behaviors that potentially lead to promising long-term\nfeedback. The foundation of our approach is to build a long short-term world\nmodel. To achieve this, we simulate goal-conditioned jumpy state transitions\nand compute corresponding affordance maps by zooming in on specific areas\nwithin single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.\n","authors":["Jiajian Li","Qi Wang","Yunbo Wang","Xin Jin","Yang Li","Wenjun Zeng","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.03618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03617v1","updated":"2024-10-04T17:17:19Z","published":"2024-10-04T17:17:19Z","title":"What Matters for Model Merging at Scale?","summary":"  Model merging aims to combine multiple expert models into a more capable\nsingle model, offering benefits such as reduced storage and serving costs,\nimproved generalization, and support for decentralized model development.\nDespite its promise, previous studies have primarily focused on merging a few\nsmall models. This leaves many unanswered questions about the effect of scaling\nmodel size and how it interplays with other key factors -- like the base model\nquality and number of expert models -- , to affect the merged model's\nperformance. This work systematically evaluates the utility of model merging at\nscale, examining the impact of these different factors. We experiment with\nmerging fully fine-tuned models using 4 popular merging methods -- Averaging,\nTask~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B\nparameters and merging up to 8 different expert models. We evaluate the merged\nmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shot\ngeneralization to unseen held-out tasks. Our experiments provide several new\ninsights about model merging at scale and the interplay between different\nfactors. First, we find that merging is more effective when experts are created\nfrom strong base models, i.e., models with good zero-shot performance. Second,\nlarger models facilitate easier merging. Third merging consistently improves\ngeneralization capabilities. Notably, when merging 8 large expert models, the\nmerged models often generalize better compared to the multitask trained models.\nFourth, we can better merge more expert models when working with larger models.\nFifth, different merging methods behave very similarly at larger scales.\nOverall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.\n","authors":["Prateek Yadav","Tu Vu","Jonathan Lai","Alexandra Chronopoulou","Manaal Faruqui","Mohit Bansal","Tsendsuren Munkhdalai"],"pdf_url":"https://arxiv.org/pdf/2410.03617v1.pdf","comment":"20 Pages, 7 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2410.03613v1","updated":"2024-10-04T17:14:59Z","published":"2024-10-04T17:14:59Z","title":"Large Language Model Performance Benchmarking on Mobile Platforms: A\n  Thorough Evaluation","summary":"  As large language models (LLMs) increasingly integrate into every aspect of\nour work and daily lives, there are growing concerns about user privacy, which\npush the trend toward local deployment of these models. There are a number of\nlightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on\nsmartphones, providing users with greater control over their personal data. As\na rapidly emerging application, we are concerned about their performance on\ncommercial-off-the-shelf mobile devices. To fully understand the current\nlandscape of LLM deployment on mobile platforms, we conduct a comprehensive\nmeasurement study on mobile devices. We evaluate both metrics that affect user\nexperience, including token throughput, latency, and battery consumption, as\nwell as factors critical to developers, such as resource utilization, DVFS\nstrategies, and inference engines. In addition, we provide a detailed analysis\nof how these hardware capabilities and system dynamics affect on-device LLM\nperformance, which may help developers identify and address bottlenecks for\nmobile LLM applications. We also provide comprehensive comparisons across the\nmobile system-on-chips (SoCs) from major vendors, highlighting their\nperformance differences in handling LLM workloads. We hope that this study can\nprovide insights for both the development of on-device LLMs and the design for\nfuture mobile system architecture.\n","authors":["Jie Xiao","Qianyi Huang","Xu Chen","Chen Tian"],"pdf_url":"https://arxiv.org/pdf/2410.03613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20592v2","updated":"2024-10-04T17:13:43Z","published":"2024-05-31T03:04:57Z","title":"LInK: Learning Joint Representations of Design and Performance Spaces\n  through Contrastive Learning for Mechanism Synthesis","summary":"  In this paper, we introduce LInK, a novel framework that integrates\ncontrastive learning of performance and design space with optimization\ntechniques for solving complex inverse problems in engineering design with\ndiscrete and continuous variables. We focus on the path synthesis problem for\nplanar linkage mechanisms. By leveraging a multimodal and\ntransformation-invariant contrastive learning framework, LInK learns a joint\nrepresentation that captures complex physics and design representations of\nmechanisms, enabling rapid retrieval from a vast dataset of over 10 million\nmechanisms. This approach improves precision through the warm start of a\nhierarchical unconstrained nonlinear optimization algorithm, combining the\nrobustness of traditional optimization with the speed and adaptability of\nmodern deep learning methods. Our results on an existing benchmark demonstrate\nthat LInK outperforms existing methods with 28 times less error compared to a\nstate of the art approach while taking 20 times less time on an existing\nbenchmark. Moreover, we introduce a significantly more challenging benchmark,\nnamed LINK ABC, which involves synthesizing linkages that trace the\ntrajectories of English capital alphabets, an inverse design benchmark task\nthat existing methods struggle with due to large nonlinearities and tiny\nfeasible space. Our results demonstrate that LInK not only advances the field\nof mechanism design but also broadens the applicability of contrastive learning\nand optimization to other areas of engineering. The code and data are publicly\navailable at https://github.com/ahnobari/LInK.\n","authors":["Amin Heyrani Nobari","Akash Srivastava","Dan Gutfreund","Kai Xu","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2405.20592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03608v1","updated":"2024-10-04T17:09:08Z","published":"2024-10-04T17:09:08Z","title":"TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and\n  Generation","summary":"  Given the widespread adoption and usage of Large Language Models (LLMs), it\nis crucial to have flexible and interpretable evaluations of their\ninstruction-following ability. Preference judgments between model outputs have\nbecome the de facto evaluation standard, despite distilling complex,\nmulti-faceted preferences into a single ranking. Furthermore, as human\nannotation is slow and costly, LLMs are increasingly used to make these\njudgments, at the expense of reliability and interpretability. In this work, we\npropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,\ninterpretable evaluation protocol that structures evaluations with\nLLM-generated, instruction-specific checklists. We first show that, given an\ninstruction, LLMs can reliably produce high-quality, tailored evaluation\nchecklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements\nand human preferences, as compared to having an LLM directly score an output.\nWe then show that STICK (Self-TICK) can be used to improve generation quality\nacross multiple benchmarks via self-refinement and Best-of-N selection. STICK\nself-refinement on LiveBench reasoning tasks leads to an absolute gain of\n$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute\nimprovement on the real-world instruction dataset, WildBench. In light of this,\nstructured, multi-faceted self-improvement is shown to be a promising way to\nfurther advance LLM capabilities. Finally, by providing LLM-generated\nchecklists to human evaluators tasked with directly scoring LLM responses to\nWildBench instructions, we notably increase inter-annotator agreement (0.194\n$\\to$ 0.256).\n","authors":["Jonathan Cook","Tim Rocktschel","Jakob Foerster","Dennis Aumiller","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14505v2","updated":"2024-10-04T17:08:17Z","published":"2024-08-24T07:59:36Z","title":"Language Model Empowered Spatio-Temporal Forecasting via Physics-Aware\n  Reprogramming","summary":"  Spatio-temporal forecasting is pivotal in numerous real-world applications,\nincluding transportation planning, energy management, and climate monitoring.\nIn this work, we aim to harness the reasoning and generalization abilities of\nPre-trained Language Models (PLMs) for more effective spatio-temporal\nforecasting, particularly in data-scarce scenarios. However, recent studies\nuncover that PLMs, which are primarily trained on textual data, often falter\nwhen tasked with modeling the intricate correlations in numerical time series,\nthereby limiting their effectiveness in comprehending spatio-temporal data. To\nbridge the gap, we propose RePST, a physics-aware PLM reprogramming framework\ntailored for spatio-temporal forecasting. Specifically, we first propose a\nphysics-aware decomposer that adaptively disentangles spatially correlated time\nseries into interpretable sub-components, which facilitates PLM to understand\nsophisticated spatio-temporal dynamics via a divide-and-conquer strategy.\nMoreover, we propose a selective discrete reprogramming scheme, which\nintroduces an expanded spatio-temporal vocabulary space to project\nspatio-temporal series into discrete representations. This scheme minimizes the\ninformation loss during reprogramming and enriches the representations derived\nby PLMs. Extensive experiments on real-world datasets show that the proposed\nRePST outperforms twelve state-of-the-art baseline methods, particularly in\ndata-scarce scenarios, highlighting the effectiveness and superior\ngeneralization capabilities of PLMs for spatio-temporal forecasting.\n","authors":["Hao Wang","Jindong Han","Wei Fan","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03602v1","updated":"2024-10-04T17:01:41Z","published":"2024-10-04T17:01:41Z","title":"Exploring gauge-fixing conditions with gradient-based optimization","summary":"  Lattice gauge fixing is required to compute gauge-variant quantities, for\nexample those used in RI-MOM renormalization schemes or as objects of\ncomparison for model calculations. Recently, gauge-variant quantities have also\nbeen found to be more amenable to signal-to-noise optimization using contour\ndeformations. These applications motivate systematic parameterization and\nexploration of gauge-fixing schemes. This work introduces a differentiable\nparameterization of gauge fixing which is broad enough to cover Landau gauge,\nCoulomb gauge, and maximal tree gauges. The adjoint state method allows\ngradient-based optimization to select gauge-fixing schemes that minimize an\narbitrary target loss function.\n","authors":["William Detmold","Gurtej Kanwar","Yin Lin","Phiala E. Shanahan","Michael L. Wagman"],"pdf_url":"https://arxiv.org/pdf/2410.03602v1.pdf","comment":"9 pages, 2 figures; Proceedings of the 41st International Symposium\n  on Lattice Field Theory (Lattice 2024)"},{"id":"http://arxiv.org/abs/2410.03601v1","updated":"2024-10-04T16:59:29Z","published":"2024-10-04T16:59:29Z","title":"How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of\n  Discrete Diffusion Models via a Stochastic Integral Framework","summary":"  Discrete diffusion models have gained increasing attention for their ability\nto model complex distributions with tractable sampling and inference. However,\nthe error analysis for discrete diffusion models remains less well-understood.\nIn this work, we propose a comprehensive framework for the error analysis of\ndiscrete diffusion models based on L\\'evy-type stochastic integrals. By\ngeneralizing the Poisson random measure to that with a time-independent and\nstate-dependent intensity, we rigorously establish a stochastic integral\nformulation of discrete diffusion models and provide the corresponding change\nof measure theorems that are intriguingly analogous to It\\^o integrals and\nGirsanov's theorem for their continuous counterparts. Our framework unifies and\nstrengthens the current theoretical results on discrete diffusion models and\nobtains the first error bound for the $\\tau$-leaping scheme in KL divergence.\nWith error sources clearly identified, our analysis gives new insight into the\nmathematical properties of discrete diffusion models and offers guidance for\nthe design of efficient and accurate algorithms for real-world discrete\ndiffusion model applications.\n","authors":["Yinuo Ren","Haoxuan Chen","Grant M. Rotskoff","Lexing Ying"],"pdf_url":"https://arxiv.org/pdf/2410.03601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15155v2","updated":"2024-10-04T16:56:06Z","published":"2024-04-22T06:30:05Z","title":"MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making","summary":"  Foundation models are becoming valuable tools in medicine. Yet despite their\npromise, the best way to leverage Large Language Models (LLMs) in complex\nmedical tasks remains an open question. We introduce a novel multi-agent\nframework, named Medical Decision-making Agents (MDAgents) that helps address\nthis gap by automatically assigning a collaboration structure to a team of\nLLMs. The assigned solo or group collaboration structure is tailored to the\nmedical task at hand, emulating real-world medical decision-making processes\nadapted to tasks of varying complexities. We evaluate our framework and\nbaseline methods using state-of-the-art LLMs across a suite of real-world\nmedical knowledge and medical diagnosis benchmarks. MDAgents achieved the best\nperformance in seven out of ten benchmarks on tasks requiring an understanding\nof medical knowledge and multi-modal reasoning, showing a significant\nimprovement of up to 6.5% (p < 0.05) compared to previous methods' best\nperformances. Ablation studies reveal that MDAgents effectively determines\nmedical complexity to optimize for efficiency and accuracy across diverse\nmedical tasks. Notably, the combination of moderator review and external\nmedical knowledge in group collaboration resulted in an average accuracy\nimprovement of 11.8%. Our code can be found at\nhttps://github.com/mitmedialab/MDAgents.\n","authors":["Yubin Kim","Chanwoo Park","Hyewon Jeong","Yik Siu Chan","Xuhai Xu","Daniel McDuff","Hyeonhoon Lee","Marzyeh Ghassemi","Cynthia Breazeal","Hae Won Park"],"pdf_url":"https://arxiv.org/pdf/2404.15155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03595v1","updated":"2024-10-04T16:55:30Z","published":"2024-10-04T16:55:30Z","title":"Understanding Reasoning in Chain-of-Thought from the Hopfieldian View","summary":"  Large Language Models have demonstrated remarkable abilities across various\ntasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to\nenhance reasoning capabilities. However, existing research primarily focuses on\nimproving performance, lacking a comprehensive framework to explain and\nunderstand the fundamental factors behind CoT's success. To bridge this gap, we\nintroduce a novel perspective grounded in the Hopfieldian view of cognition in\ncognitive neuroscience. We establish a connection between CoT reasoning and key\ncognitive elements such as stimuli, actions, neural populations, and\nrepresentation spaces. From our view, we can understand the reasoning process\nas the movement between these representation spaces. Building on this insight,\nwe develop a method for localizing reasoning errors in the response of CoTs.\nMoreover, we propose the Representation-of-Thought (RoT) framework, which\nleverages the robustness of low-dimensional representation spaces to enhance\nthe robustness of the reasoning process in CoTs. Experimental results\ndemonstrate that RoT improves the robustness and interpretability of CoT\nreasoning while offering fine-grained control over the reasoning process.\n","authors":["Lijie Hu","Liang Liu","Shu Yang","Xin Chen","Zhen Tan","Muhammad Asif Ali","Mengdi Li","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03595v1.pdf","comment":"28 pages, a new version of \"A Hopfieldian View-based Interpretation\n  for Chain-of-Thought Reasoning\""},{"id":"http://arxiv.org/abs/2407.11229v2","updated":"2024-10-04T16:52:57Z","published":"2024-07-15T20:29:24Z","title":"Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into\n  Consistency and Robustness","summary":"  Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.\n","authors":["Srija Mukhopadhyay","Adnan Qidwai","Aparna Garimella","Pritika Ramu","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.11229v2.pdf","comment":"22 pages, 9 Tables, 5 figures, 22 examples"},{"id":"http://arxiv.org/abs/2410.03588v1","updated":"2024-10-04T16:47:11Z","published":"2024-10-04T16:47:11Z","title":"Training Over a Distribution of Hyperparameters for Enhanced Performance\n  and Adaptability on Imbalanced Classification","summary":"  Although binary classification is a well-studied problem, training reliable\nclassifiers under severe class imbalance remains a challenge. Recent techniques\nmitigate the ill effects of imbalance on training by modifying the loss\nfunctions or optimization methods. We observe that different hyperparameter\nvalues on these loss functions perform better at different recall values. We\npropose to exploit this fact by training one model over a distribution of\nhyperparameter values--instead of a single value--via Loss Conditional Training\n(LCT). Experiments show that training over a distribution of hyperparameters\nnot only approximates the performance of several models but actually improves\nthe overall performance of models on both CIFAR and real medical imaging\napplications, such as melanoma and diabetic retinopathy detection. Furthermore,\ntraining models with LCT is more efficient because some hyperparameter tuning\ncan be conducted after training to meet individual needs without needing to\nretrain from scratch.\n","authors":["Kelsey Lieberman","Swarna Kamlam Ravindran","Shuai Yuan","Carlo Tomasi"],"pdf_url":"https://arxiv.org/pdf/2410.03588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18680v2","updated":"2024-10-04T16:41:32Z","published":"2024-05-29T01:07:26Z","title":"Navigable Graphs for High-Dimensional Nearest Neighbor Search:\n  Constructions and Limits","summary":"  There has been significant recent interest in graph-based nearest neighbor\nsearch methods, many of which are centered on the construction of navigable\ngraphs over high-dimensional point sets. A graph is navigable if we can\nsuccessfully move from any starting node to any target node using a greedy\nrouting strategy where we always move to the neighbor that is closest to the\ndestination according to a given distance function. The complete graph is\nnavigable for any point set, but the important question for applications is if\nsparser graphs can be constructed. While this question is fairly well\nunderstood in low-dimensions, we establish some of the first upper and lower\nbounds for high-dimensional point sets. First, we give a simple and efficient\nway to construct a navigable graph with average degree $O(\\sqrt{n \\log n })$\nfor any set of $n$ points, in any dimension, for any distance function. We\ncompliment this result with a nearly matching lower bound: even under the\nEuclidean metric in $O(\\log n)$ dimensions, a random point set has no navigable\ngraph with average degree $O(n^{\\alpha})$ for any $\\alpha < 1/2$. Our lower\nbound relies on sharp anti-concentration bounds for binomial random variables,\nwhich we use to show that the near-neighborhoods of a set of random points do\nnot overlap significantly, forcing any navigable graph to have many edges.\n","authors":["Haya Diwan","Jinrui Gou","Cameron Musco","Christopher Musco","Torsten Suel"],"pdf_url":"https://arxiv.org/pdf/2405.18680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03581v1","updated":"2024-10-04T16:40:56Z","published":"2024-10-04T16:40:56Z","title":"Nonstationary Sparse Spectral Permanental Process","summary":"  Existing permanental processes often impose constraints on kernel types or\nstationarity, limiting the model's expressiveness. To overcome these\nlimitations, we propose a novel approach utilizing the sparse spectral\nrepresentation of nonstationary kernels. This technique relaxes the constraints\non kernel types and stationarity, allowing for more flexible modeling while\nreducing computational complexity to the linear level. Additionally, we\nintroduce a deep kernel variant by hierarchically stacking multiple spectral\nfeature mappings, further enhancing the model's expressiveness to capture\ncomplex patterns in data. Experimental results on both synthetic and real-world\ndatasets demonstrate the effectiveness of our approach, particularly in\nscenarios with pronounced data nonstationarity. Additionally, ablation studies\nare conducted to provide insights into the impact of various hyperparameters on\nmodel performance.\n","authors":["Zicheng Sun","Yixuan Zhang","Zenan Ling","Xuhui Fan","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.03581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16434v3","updated":"2024-10-04T16:35:13Z","published":"2024-09-24T19:57:40Z","title":"Lessons Learned from a Unifying Empirical Study of Parameter-Efficient\n  Transfer Learning (PETL) in Visual Recognition","summary":"  Parameter-efficient transfer learning (PETL) has attracted significant\nattention lately, due to the increasing size of pre-trained models and the need\nto fine-tune (FT) them for superior downstream performance. This community-wide\nenthusiasm has sparked a plethora of approaches. Nevertheless, a systematic\nstudy to understand their performance and suitable application scenarios is\nlacking, leaving questions like when to apply PETL and which approach to use\nlargely unanswered. In this paper, we conduct a unifying empirical study of\nrepresentative PETL methods in the context of Vision Transformers. We\nsystematically tune their hyper-parameters to fairly compare their accuracy on\ndownstream tasks. Our study not only offers a valuable user guide but also\nunveils several new insights. First, if tuned carefully, different PETL methods\ncan obtain similar accuracy in the low-shot benchmark VTAB-1K. This includes\nsimple methods like FT the bias terms that were reported inferior. Second,\nthough with similar accuracy, we find that PETL methods make different mistakes\nand high-confidence predictions, likely due to their different inductive\nbiases. Such an inconsistency (or complementariness) opens up the opportunity\nfor ensemble methods, and we make preliminary attempts at this. Third, going\nbeyond the commonly used low-shot tasks, we find that PETL is also useful in\nmany-shot regimes -- it achieves comparable and sometimes better accuracy than\nfull FT, using much fewer learnable parameters. Last but not least, we\ninvestigate PETL's ability to preserve a pre-trained model's robustness to\ndistribution shifts (e.g., a CLIP backbone). Perhaps not surprisingly, PETL\nmethods outperform full FT alone. However, with weight-space ensembles, the\nfully fine-tuned model can better balance target (i.e., downstream)\ndistribution and distribution shift performance, suggesting a future research\ndirection for PETL.\n","authors":["Zheda Mai","Ping Zhang","Cheng-Hao Tu","Hong-You Chen","Li Zhang","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2409.16434v3.pdf","comment":"Code is available at https://github.com/OSU-MLB/PETL_Vision"},{"id":"http://arxiv.org/abs/2208.10570v2","updated":"2024-10-04T16:34:17Z","published":"2022-08-22T19:58:03Z","title":"Semi-Supervised Manifold Learning with Complexity Decoupled Chart\n  Autoencoders","summary":"  Autoencoding is a popular method in representation learning. Conventional\nautoencoders employ symmetric encoding-decoding procedures and a simple\nEuclidean latent space to detect hidden low-dimensional structures in an\nunsupervised way. Some modern approaches to novel data generation such as\ngenerative adversarial networks askew this symmetry, but still employ a pair of\nmassive networks--one to generate the image and another to judge the images\nquality based on priors learned from a training set. This work introduces a\nchart autoencoder with an asymmetric encoding-decoding process that can\nincorporate additional semi-supervised information such as class labels.\nBesides enhancing the capability for handling data with complicated topological\nand geometric structures, the proposed model can successfully differentiate\nnearby but disjoint manifolds and intersecting manifolds with only a small\namount of supervision. Moreover, this model only requires a low-complexity\nencoding operation, such as a locally defined linear projection. We discuss the\napproximation power of such networks and derive a bound that essentially\ndepends on the intrinsic dimension of the data manifold rather than the\ndimension of ambient space. Next we incorporate bounds for the sampling rate of\ntraining data need to faithfully represent a given data manifold. We present\nnumerical experiments that verify that the proposed model can effectively\nmanage data with multi-class nearby but disjoint manifolds of different\nclasses, overlapping manifolds, and manifolds with non-trivial topology.\nFinally, we conclude with some experiments on computer vision and molecular\ndynamics problems which showcase the efficacy of our methods on real-world\ndata.\n","authors":["Stefan C. Schonsheck","Scott Mahan","Timo Klock","Alexander Cloninger","Rongjie Lai"],"pdf_url":"https://arxiv.org/pdf/2208.10570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13213v2","updated":"2024-10-04T16:29:58Z","published":"2024-02-20T18:24:47Z","title":"Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A","summary":"  We study 14 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigororous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.\n","authors":["Benjamin Plaut","Nguyen X. Khanh","Tu Trinh"],"pdf_url":"https://arxiv.org/pdf/2402.13213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03573v1","updated":"2024-10-04T16:21:14Z","published":"2024-10-04T16:21:14Z","title":"HyResPINNs: Adaptive Hybrid Residual Networks for Learning Optimal\n  Combinations of Neural and RBF Components for Physics-Informed Modeling","summary":"  Physics-informed neural networks (PINNs) are an increasingly popular class of\ntechniques for the numerical solution of partial differential equations (PDEs),\nwhere neural networks are trained using loss functions regularized by relevant\nPDE terms to enforce physical constraints. We present a new class of PINNs\ncalled HyResPINNs, which augment traditional PINNs with adaptive hybrid\nresidual blocks that combine the outputs of a standard neural network and a\nradial basis function (RBF) network. A key feature of our method is the\ninclusion of adaptive combination parameters within each residual block, which\ndynamically learn to weigh the contributions of the neural network and RBF\nnetwork outputs. Additionally, adaptive connections between residual blocks\nallow for flexible information flow throughout the network. We show that\nHyResPINNs are more robust to training point locations and neural network\narchitectures than traditional PINNs. Moreover, HyResPINNs offer orders of\nmagnitude greater accuracy than competing methods on certain problems, with\nonly modest increases in training costs. We demonstrate the strengths of our\napproach on challenging PDEs, including the Allen-Cahn equation and the\nDarcy-Flow equation. Our results suggest that HyResPINNs effectively bridge the\ngap between traditional numerical methods and modern machine learning-based\nsolvers.\n","authors":["Madison Cooley","Robert M. Kirby","Shandian Zhe","Varun Shankar"],"pdf_url":"https://arxiv.org/pdf/2410.03573v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.03569v1","updated":"2024-10-04T16:19:33Z","published":"2024-10-04T16:19:33Z","title":"Teaching Transformers Modular Arithmetic at Scale","summary":"  Modular addition is, on its face, a simple operation: given $N$ elements in\n$\\mathbb{Z}_q$, compute their sum modulo $q$. Yet, scalable machine learning\nsolutions to this problem remain elusive: prior work trains ML models that sum\n$N \\le 6$ elements mod $q \\le 1000$. Promising applications of ML models for\ncryptanalysis-which often involve modular arithmetic with large $N$ and\n$q$-motivate reconsideration of this problem. This work proposes three changes\nto the modular addition model training pipeline: more diverse training data, an\nangular embedding, and a custom loss function. With these changes, we\ndemonstrate success with our approach for $N = 256, q = 3329$, a case which is\ninteresting for cryptographic applications, and a significant increase in $N$\nand $q$ over prior work. These techniques also generalize to other modular\narithmetic problems, motivating future work.\n","authors":["Eshika Saxena","Alberto Alfarano","Emily Wenger","Kristin Lauter"],"pdf_url":"https://arxiv.org/pdf/2410.03569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03568v1","updated":"2024-10-04T16:18:29Z","published":"2024-10-04T16:18:29Z","title":"Towards Linguistically-Aware and Language-Independent Tokenization for\n  Large Language Models (LLMs)","summary":"  This paper presents a comprehensive study on the tokenization techniques\nemployed by state-of-the-art large language models (LLMs) and their\nimplications on the cost and availability of services across different\nlanguages, especially low resource languages. The analysis considers multiple\nLLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base\nembeddings), and DaVinci (employing r50k_base embeddings), as well as the\nwidely used BERT base tokenizer. The study evaluates the tokenization\nvariability observed across these models and investigates the challenges of\nlinguistic representation in subword tokenization. The research underscores the\nimportance of fostering linguistically-aware development practices, especially\nfor languages that are traditionally under-resourced. Moreover, this paper\nintroduces case studies that highlight the real-world implications of\ntokenization choices, particularly in the context of electronic health record\n(EHR) systems. This research aims to promote generalizable Internationalization\n(I18N) practices in the development of AI services in this domain and beyond,\nwith a strong emphasis on inclusivity, particularly for languages traditionally\nunderrepresented in AI applications.\n","authors":["Abrar Rahman","Garry Bowlin","Binit Mohanty","Sean McGunigal"],"pdf_url":"https://arxiv.org/pdf/2410.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03565v1","updated":"2024-10-04T16:15:31Z","published":"2024-10-04T16:15:31Z","title":"Training on more Reachable Tasks for Generalisation in Reinforcement\n  Learning","summary":"  In multi-task reinforcement learning, agents train on a fixed set of tasks\nand have to generalise to new ones. Recent work has shown that increased\nexploration improves this generalisation, but it remains unclear why exactly\nthat is. In this paper, we introduce the concept of reachability in multi-task\nreinforcement learning and show that an initial exploration phase increases the\nnumber of reachable tasks the agent is trained on. This, and not the increased\nexploration, is responsible for the improved generalisation, even to\nunreachable tasks. Inspired by this, we propose a novel method Explore-Go that\nimplements such an exploration phase at the beginning of each episode.\nExplore-Go only modifies the way experience is collected and can be used with\nmost existing on-policy or off-policy reinforcement learning algorithms. We\ndemonstrate the effectiveness of our method when combined with some popular\nalgorithms and show an increase in generalisation performance across several\nenvironments.\n","authors":["Max Weltevrede","Caroline Horsch","Matthijs T. J. Spaan","Wendelin Bhmer"],"pdf_url":"https://arxiv.org/pdf/2410.03565v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.08069"},{"id":"http://arxiv.org/abs/2404.02986v2","updated":"2024-10-04T16:13:39Z","published":"2024-04-03T18:14:23Z","title":"Universal Functional Regression with Neural Operator Flows","summary":"  Regression on function spaces is typically limited to models with Gaussian\nprocess priors. We introduce the notion of universal functional regression, in\nwhich we aim to learn a prior distribution over non-Gaussian function spaces\nthat remains mathematically tractable for functional regression. To do this, we\ndevelop Neural Operator Flows (OpFlow), an infinite-dimensional extension of\nnormalizing flows. OpFlow is an invertible operator that maps the (potentially\nunknown) data function space into a Gaussian process, allowing for exact\nlikelihood estimation of functional point evaluations. OpFlow enables robust\nand accurate uncertainty quantification via drawing posterior samples of the\nGaussian process and subsequently mapping them into the data function space. We\nempirically study the performance of OpFlow on regression and generation tasks\nwith data generated from Gaussian processes with known posterior forms and\nnon-Gaussian processes, as well as real-world earthquake seismograms with an\nunknown closed-form distribution.\n","authors":["Yaozhong Shi","Angela F. Gao","Zachary E. Ross","Kamyar Azizzadenesheli"],"pdf_url":"https://arxiv.org/pdf/2404.02986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12821v3","updated":"2024-10-04T16:07:29Z","published":"2024-02-20T08:41:23Z","title":"Identifying Factual Inconsistencies in Summaries: Grounding LLM\n  Inference via Task Taxonomy","summary":"  Factual inconsistencies pose a significant hurdle for the faithful\nsummarization by generative models. While a major direction to enhance\ninconsistency detection is to derive stronger Natural Language Inference (NLI)\nmodels, we propose an orthogonal aspect that underscores the importance of\nincorporating task-specific taxonomy into the inference. To this end, we\nconsolidate key error types of inconsistent facts in summaries, and incorporate\nthem to facilitate both the zero-shot and supervised paradigms of LLMs.\nExtensive experiments on ten datasets of five distinct domains suggest that,\nzero-shot LLM inference could benefit from the explicit solution space depicted\nby the error type taxonomy, and achieves state-of-the-art performance overall,\nsurpassing specialized non-LLM baselines, as well as recent LLM baselines. We\nfurther distill models that fuse the taxonomy into parameters through our\ndesigned prompt completions and supervised training strategies, efficiently\nsubstituting state-of-the-art zero-shot inference with much larger LLMs.\n","authors":["Liyan Xu","Zhenlin Su","Mo Yu","Jin Xu","Jinho D. Choi","Jie Zhou","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2402.12821v3.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03537v1","updated":"2024-10-04T15:54:49Z","published":"2024-10-04T15:54:49Z","title":"Ward: Provable RAG Dataset Inference via LLM Watermarks","summary":"  Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to\nincorporate external data during generation. This raises concerns for data\nowners regarding unauthorized use of their content in RAG systems. Despite its\nimportance, the challenge of detecting such unauthorized usage remains\nunderexplored, with existing datasets and methodologies from adjacent fields\nbeing ill-suited for its study. In this work, we take several steps to bridge\nthis gap. First, we formalize this problem as (black-box) RAG Dataset Inference\n(RAG-DI). To facilitate research on this challenge, we further introduce a\nnovel dataset specifically designed for benchmarking RAG-DI methods under\nrealistic conditions, and propose a set of baseline approaches. Building on\nthis foundation, we introduce Ward, a RAG-DI method based on LLM watermarks\nthat enables data owners to obtain rigorous statistical guarantees regarding\nthe usage of their dataset in a RAG system. In our experimental evaluation, we\nshow that Ward consistently outperforms all baselines across many challenging\nsettings, achieving higher accuracy, superior query efficiency and robustness.\nOur work provides a foundation for future studies of RAG-DI and highlights LLM\nwatermarks as a promising approach to this problem.\n","authors":["Nikola Jovanovi","Robin Staab","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.03537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01770v2","updated":"2024-10-04T15:54:24Z","published":"2024-10-02T17:27:13Z","title":"Explainable Earth Surface Forecasting under Extreme Events","summary":"  With climate change-related extreme events on the rise, high dimensional\nEarth observation data presents a unique opportunity for forecasting and\nunderstanding impacts on ecosystems. This is, however, impeded by the\ncomplexity of processing, visualizing, modeling, and explaining this data. To\nshowcase how this challenge can be met, here we train a convolutional long\nshort-term memory-based architecture on the novel DeepExtremeCubes dataset.\nDeepExtremeCubes includes around 40,000 long-term Sentinel-2 minicubes (January\n2016-October 2022) worldwide, along with labeled extreme events, meteorological\ndata, vegetation land cover, and topography map, sampled from locations\naffected by extreme climate events and surrounding areas. When predicting\nfuture reflectances and vegetation impacts through kernel normalized difference\nvegetation index, the model achieved an R$^2$ score of 0.9055 in the test set.\nExplainable artificial intelligence was used to analyze the model's predictions\nduring the October 2020 Central South America compound heatwave and drought\nevent. We chose the same area exactly one year before the event as\ncounterfactual, finding that the average temperature and surface pressure are\ngenerally the best predictors under normal conditions. In contrast, minimum\nanomalies of evaporation and surface latent heat flux take the lead during the\nevent. A change of regime is also observed in the attributions before the\nevent, which might help assess how long the event was brewing before happening.\nThe code to replicate all experiments and figures in this paper is publicly\navailable at https://github.com/DeepExtremes/txyXAI\n","authors":["Oscar J. Pellicer-Valero","Miguel-ngel Fernndez-Torres","Chaonan Ji","Miguel D. Mahecha","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2410.01770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03535v1","updated":"2024-10-04T15:54:02Z","published":"2024-10-04T15:54:02Z","title":"NRGBoost: Energy-Based Generative Boosted Trees","summary":"  Despite the rise to dominance of deep learning in unstructured data domains,\ntree-based methods such as Random Forests (RF) and Gradient Boosted Decision\nTrees (GBDT) are still the workhorses for handling discriminative tasks on\ntabular data. We explore generative extensions of these popular algorithms with\na focus on explicitly modeling the data density (up to a normalization\nconstant), thus enabling other applications besides sampling. As our main\ncontribution we propose an energy-based generative boosting algorithm that is\nanalogous to the second order boosting implemented in popular packages like\nXGBoost. We show that, despite producing a generative model capable of handling\ninference tasks over any input variable, our proposed algorithm can achieve\nsimilar discriminative performance to GBDT on a number of real world tabular\ndatasets, outperforming alternative generative approaches. At the same time, we\nshow that it is also competitive with neural network based models for sampling.\n","authors":["Joo Bravo"],"pdf_url":"https://arxiv.org/pdf/2410.03535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03529v1","updated":"2024-10-04T15:50:10Z","published":"2024-10-04T15:50:10Z","title":"No Need to Talk: Asynchronous Mixture of Language Models","summary":"  We introduce SmallTalk LM, an innovative method for training a mixture of\nlanguage models in an almost asynchronous manner. Each model of the mixture\nspecializes in distinct parts of the data distribution, without the need of\nhigh-bandwidth communication between the nodes training each model. At\ninference, a lightweight router directs a given sequence to a single expert,\naccording to a short prefix. This inference scheme naturally uses a fraction of\nthe parameters from the overall mixture model. Our experiments on language\nmodeling demonstrate tha SmallTalk LM achieves significantly lower perplexity\nthan dense model baselines for the same total training FLOPs and an almost\nidentical inference cost. Finally, in our downstream evaluations we outperform\nthe dense baseline on $75\\%$ of the tasks.\n","authors":["Anastasiia Filippova","Angelos Katharopoulos","David Grangier","Ronan Collobert"],"pdf_url":"https://arxiv.org/pdf/2410.03529v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2406.02175v3","updated":"2024-10-04T15:44:53Z","published":"2024-06-04T10:11:46Z","title":"Branches: A Fast Dynamic Programming and Branch & Bound Algorithm for\n  Optimal Decision Trees","summary":"  Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine\nLearning, yet it poses a formidable optimisation challenge. Despite numerous\nefforts dating back to the early 1990's, practical algorithms have only\nrecently emerged, primarily leveraging Dynamic Programming (DP) and Branch &\nBound (B&B) techniques. These methods fall into two categories: algorithms like\nDL8.5, MurTree and STreeD utilise an efficient DP strategy but lack effective\nbounds for pruning the search space; while algorithms like OSDT and GOSDT\nemploy more efficient pruning bounds but at the expense of a less refined DP\nstrategy. We introduce Branches, a new algorithm that combines the strengths of\nboth approaches. Using DP and B&B with a novel analytical bound for efficient\npruning, Branches offers both speed and sparsity optimisation. Unlike other\nmethods, it also handles non-binary features. Theoretical analysis shows its\nlower complexity compared to existing methods, and empirical results confirm\nthat Branches outperforms the state-of-the-art in speed, iterations, and\noptimality.\n","authors":["Ayman Chaouki","Jesse Read","Albert Bifet"],"pdf_url":"https://arxiv.org/pdf/2406.02175v3.pdf","comment":"This preprint is currently under review"},{"id":"http://arxiv.org/abs/2410.03523v1","updated":"2024-10-04T15:44:23Z","published":"2024-10-04T15:44:23Z","title":"A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models","summary":"  Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs.\nhttps://github.com/yascho/probabilistic-unlearning\n","authors":["Yan Scholten","Stephan Gnnemann","Leo Schwinn"],"pdf_url":"https://arxiv.org/pdf/2410.03523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04732v2","updated":"2024-10-04T15:39:36Z","published":"2024-02-07T10:33:09Z","title":"Graph Cuts with Arbitrary Size Constraints Through Optimal Transport","summary":"  A common way of partitioning graphs is through minimum cuts. One drawback of\nclassical minimum cut methods is that they tend to produce small groups, which\nis why more balanced variants such as normalized and ratio cuts have seen more\nsuccess. However, we believe that with these variants, the balance constraints\ncan be too restrictive for some applications like for clustering of imbalanced\ndatasets, while not being restrictive enough for when searching for perfectly\nbalanced partitions. Here, we propose a new graph cut algorithm for\npartitioning graphs under arbitrary size constraints. We formulate the graph\ncut problem as a Gromov-Wasserstein with a concave regularizer problem. We then\npropose to solve it using an accelerated proximal GD algorithm which guarantees\nglobal convergence to a critical point, results in sparse solutions and only\nincurs an additional ratio of $\\mathcal{O}(\\log(n))$ compared to the classical\nspectral clustering algorithm but was seen to be more efficient.\n","authors":["Chakib Fettal","Lazhar Labiod","Mohamed Nadif"],"pdf_url":"https://arxiv.org/pdf/2402.04732v2.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2410.03519v1","updated":"2024-10-04T15:38:37Z","published":"2024-10-04T15:38:37Z","title":"Improving Online Bagging for Complex Imbalanced Data Stream","summary":"  Learning classifiers from imbalanced and concept drifting data streams is\nstill a challenge. Most of the current proposals focus on taking into account\nchanges in the global imbalance ratio only and ignore the local difficulty\nfactors, such as the minority class decomposition into sub-concepts and the\npresence of unsafe types of examples (borderline or rare ones). As the above\nfactors present in the stream may deteriorate the performance of popular online\nclassifiers, we propose extensions of resampling online bagging, namely\nNeighbourhood Undersampling or Oversampling Online Bagging to take better\naccount of the presence of unsafe minority examples. The performed\ncomputational experiments with synthetic complex imbalanced data streams have\nshown their advantage over earlier variants of online bagging resampling\nensembles.\n","authors":["Bartosz Przybyl","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2410.03519v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.03517v1","updated":"2024-10-04T15:36:48Z","published":"2024-10-04T15:36:48Z","title":"Fine-Grained Expressive Power of Weisfeiler-Leman: A Homomorphism\n  Counting Perspective","summary":"  The ability of graph neural networks (GNNs) to count homomorphisms has\nrecently been proposed as a practical and fine-grained measure of their\nexpressive power. Although several existing works have investigated the\nhomomorphism counting power of certain GNN families, a simple and unified\nframework for analyzing the problem is absent. In this paper, we first propose\n\\emph{generalized folklore Weisfeiler-Leman (GFWL)} algorithms as a flexible\ndesign basis for expressive GNNs, and then provide a theoretical framework to\nalgorithmically determine the homomorphism counting power of an arbitrary class\nof GNN within the GFWL design space. As the considered design space is large\nenough to accommodate almost all known powerful GNNs, our result greatly\nextends all existing works, and may find its application in the automation of\nGNN model design.\n","authors":["Junru Zhou","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10716v2","updated":"2024-10-04T15:36:08Z","published":"2023-05-18T05:27:46Z","title":"A Survey on Time-Series Pre-Trained Models","summary":"  Time-Series Mining (TSM) is an important research area since it shows great\npotential in practical applications. Deep learning models that rely on massive\nlabeled data have been utilized for TSM successfully. However, constructing a\nlarge-scale well-labeled dataset is difficult due to data annotation costs.\nRecently, pre-trained models have gradually attracted attention in the time\nseries domain due to their remarkable performance in computer vision and\nnatural language processing. In this survey, we provide a comprehensive review\nof Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding,\napplying, and studying TS-PTMs. Specifically, we first briefly introduce the\ntypical deep learning models employed in TSM. Then, we give an overview of\nTS-PTMs according to the pre-training techniques. The main categories we\nexplore include supervised, unsupervised, and self-supervised TS-PTMs. Further,\nextensive experiments involving 27 methods, 434 datasets, and 679 transfer\nlearning scenarios are conducted to analyze the advantages and disadvantages of\ntransfer learning strategies, Transformer-based models, and representative\nTS-PTMs. Finally, we point out some potential directions of TS-PTMs for future\nwork.\n","authors":["Qianli Ma","Zhen Liu","Zhenjing Zheng","Ziyang Huang","Siying Zhu","Zhongzhong Yu","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2305.10716v2.pdf","comment":"Accepted in the IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2408.11901v2","updated":"2024-10-04T15:33:39Z","published":"2024-08-21T18:00:08Z","title":"A Unified Theory of Quantum Neural Network Loss Landscapes","summary":"  Classical neural networks with random initialization famously behave as\nGaussian processes in the limit of many neurons, which allows one to completely\ncharacterize their training and generalization behavior. No such general\nunderstanding exists for quantum neural networks (QNNs), which -- outside of\ncertain special cases -- are known to not behave as Gaussian processes when\nrandomly initialized. We here prove that QNNs and their first two derivatives\ninstead generally form what we call \"Wishart processes,\" where certain\nalgebraic properties of the network determine the hyperparameters of the\nprocess. This Wishart process description allows us to, for the first time:\ngive necessary and sufficient conditions for a QNN architecture to have a\nGaussian process limit; calculate the full gradient distribution, generalizing\npreviously known barren plateau results; and calculate the local minima\ndistribution of algebraically constrained QNNs. Our unified framework suggests\na certain simple operational definition for the \"trainability\" of a given QNN\nmodel using a newly introduced, experimentally accessible quantity we call the\n\"degrees of freedom\" of the network architecture.\n","authors":["Eric R. Anschuetz"],"pdf_url":"https://arxiv.org/pdf/2408.11901v2.pdf","comment":"57 pages, 4 figures, added references and fixed minor bugs"},{"id":"http://arxiv.org/abs/2408.08558v2","updated":"2024-10-04T15:32:38Z","published":"2024-08-16T06:43:58Z","title":"Linear combinations of Gaussian latents in generative models:\n  interpolation and beyond","summary":"  Sampling from generative models has become a crucial tool for applications\nlike data synthesis and augmentation. Diffusion, Flow Matching and Continuous\nNormalizing Flows have shown effectiveness across various modalities, and rely\non Gaussian latent variables for generation. For search-based or creative\napplications that require additional control over the generation process, it\nhas become common to manipulate the latent variable directly. However, existing\napproaches for performing such manipulations (e.g. interpolation or forming\nlow-dimensional representations) only work well in special cases or are network\nor data-modality specific. We propose Combination of Gaussian variables (COG)\nas a general purpose interpolation method that is easy to implement yet\noutperforms recent sophisticated methods. Moreover, COG naturally addresses the\nbroader task of forming general linear combinations of latent variables,\nallowing the construction of subspaces of the latent space, dramatically\nsimplifying the creation of expressive low-dimensional spaces of\nhigh-dimensional objects.\n","authors":["Erik Bodin","Carl Henrik Ek","Henry Moss"],"pdf_url":"https://arxiv.org/pdf/2408.08558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03514v1","updated":"2024-10-04T15:29:11Z","published":"2024-10-04T15:29:11Z","title":"Stabilized Neural Prediction of Potential Outcomes in Continuous Time","summary":"  Patient trajectories from electronic health records are widely used to\npredict potential outcomes of treatments over time, which then allows to\npersonalize care. Yet, existing neural methods for this purpose have a key\nlimitation: while some adjust for time-varying confounding, these methods\nassume that the time series are recorded in discrete time. In other words, they\nare constrained to settings where measurements and treatments are conducted at\nfixed time steps, even though this is unrealistic in medical practice. In this\nwork, we aim to predict potential outcomes in continuous time. The latter is of\ndirect practical relevance because it allows for modeling patient trajectories\nwhere measurements and treatments take place at arbitrary, irregular\ntimestamps. We thus propose a new method called stabilized continuous time\ninverse propensity network (SCIP-Net). For this, we further derive stabilized\ninverse propensity weights for robust prediction of the potential outcomes. To\nthe best of our knowledge, our SCIP-Net is the first neural method that\nperforms proper adjustments for time-varying confounding in continuous time.\n","authors":["Konstantin Hess","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2410.03514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15712v2","updated":"2024-10-04T15:29:09Z","published":"2024-05-24T17:01:37Z","title":"Infinite Limits of Multi-head Transformer Dynamics","summary":"  In this work, we analyze various scaling limits of the training dynamics of\ntransformer models in the feature learning regime. We identify the set of\nparameterizations that admit well-defined infinite width and depth limits,\nallowing the attention layers to update throughout training--a relevant notion\nof feature learning in these models. We then use tools from dynamical mean\nfield theory (DMFT) to analyze various infinite limits (infinite key/query\ndimension, infinite heads, and infinite depth) which have different statistical\ndescriptions depending on which infinite limit is taken and how attention\nlayers are scaled. We provide numerical evidence of convergence to the limits\nand discuss how the parameterization qualitatively influences learned features.\n","authors":["Blake Bordelon","Hamza Tahir Chaudhry","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2405.15712v2.pdf","comment":"Updating for Neurips 2024"},{"id":"http://arxiv.org/abs/2311.17801v2","updated":"2024-10-04T15:26:45Z","published":"2023-11-29T16:51:21Z","title":"Towards Efficient Hyperdimensional Computing Using Photonics","summary":"  Over the past few years, silicon photonics-based computing has emerged as a\npromising alternative to CMOS-based computing for Deep Neural Networks (DNN).\nUnfortunately, the non-linear operations and the high-precision requirements of\nDNNs make it extremely challenging to design efficient silicon photonics-based\nsystems for DNN inference and training. Hyperdimensional Computing (HDC) is an\nemerging, brain-inspired machine learning technique that enjoys several\nadvantages over existing DNNs, including being lightweight, requiring\nlow-precision operands, and being robust to noise introduced by the\nnonidealities in the hardware. For HDC, computing in-memory (CiM) approaches\nhave been widely used, as CiM reduces the data transfer cost if the operands\ncan fit into the memory. However, inefficient multi-bit operations, high write\nlatency, and low endurance make CiM ill-suited for HDC. On the other hand, the\nexisting electro-photonic DNN accelerators are inefficient for HDC because they\nare specifically optimized for matrix multiplication in DNNs and consume a lot\nof power with high-precision data converters.\n  In this paper, we argue that photonic computing and HDC complement each other\nbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,\nthe first-ever electro-photonic accelerator for HDC training and inference,\nsupporting the basic, record-based, and graph encoding schemes. Evaluating with\npopular datasets, we show that our accelerator can achieve two to five orders\nof magnitude lower EDP than the state-of-the-art electro-photonic DNN\naccelerators for implementing HDC training and inference. PhotoHDC also\nachieves four orders of magnitude lower energy-delay product than CiM-based\naccelerators for both HDC training and inference.\n","authors":["Farbin Fayza","Cansu Demirkiran","Hanning Chen","Che-Kai Liu","Avi Mohan","Hamza Errahmouni","Sanggeon Yun","Mohsen Imani","David Zhang","Darius Bunandar","Ajay Joshi"],"pdf_url":"https://arxiv.org/pdf/2311.17801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03511v1","updated":"2024-10-04T15:26:01Z","published":"2024-10-04T15:26:01Z","title":"Authentication by Location Tracking in Underwater Acoustic Networks","summary":"  Physical layer message authentication in underwater acoustic networks (UWANs)\nleverages the characteristics of the underwater acoustic channel (UWAC) as a\nfingerprint of the transmitting device. However, as the device moves its UWAC\nchanges, and the authentication mechanism must track such variations. In this\npaper, we propose a context-based authentication mechanism operating in two\nsteps: first, we estimate the position of the underwater device, then we\npredict its future position based on the previously estimated ones. To check\nthe authenticity of the transmission, we compare the estimated and the\npredicted position. The location is estimated using a convolutional neural\nnetwork taking as input the sample covariance matrix of the estimated UWACs.\nThe prediction uses either a Kalman filter or a recurrent neural network (RNN).\nThe authentication check is performed on the squared error between the\npredicted and estimated positions. The solution based on the Kalman filter\noutperforms that built on the RNN when the device moves according to a\ncorrelated Gauss-Markov mobility model, which reproduces a typical underwater\nmotion.\n","authors":["Gianmaria Ventura","Francesco Ardizzon","Stefano Tomasin"],"pdf_url":"https://arxiv.org/pdf/2410.03511v1.pdf","comment":"Article submitted to IEEE Transaction on Wireless Communications"},{"id":"http://arxiv.org/abs/2402.08062v3","updated":"2024-10-04T15:23:43Z","published":"2024-02-12T21:12:11Z","title":"Avoiding Catastrophe in Online Learning by Asking for Help","summary":"  Most learning algorithms with formal regret guarantees assume that no mistake\nis irreparable and essentially rely on trying all possible behaviors. This\napproach is problematic when some mistakes are \\emph{catastrophic}, i.e.,\nirreparable. We propose an online learning problem where the goal is to\nminimize the chance of catastrophe. Specifically, we assume that the payoff in\neach round represents the chance of avoiding catastrophe that round and aim to\nmaximize the product of payoffs (the overall chance of avoiding catastrophe)\nwhile allowing a limited number of queries to a mentor. We first show that in\ngeneral, any algorithm either constantly queries the mentor or is nearly\nguaranteed to cause catastrophe. However, in settings where the mentor policy\nclass is learnable in the standard online learning model, we provide an\nalgorithm whose regret and rate of querying the mentor both approach 0 as the\ntime horizon grows. Conceptually, if a policy class is learnable in the absence\nof catastrophic risk, it is learnable in the presence of catastrophic risk if\nthe agent can ask for help.\n","authors":["Benjamin Plaut","Hanlin Zhu","Stuart Russell"],"pdf_url":"https://arxiv.org/pdf/2402.08062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03505v1","updated":"2024-10-04T15:20:57Z","published":"2024-10-04T15:20:57Z","title":"Classification-Denoising Networks","summary":"  Image classification and denoising suffer from complementary issues of lack\nof robustness or partially ignoring conditioning information. We argue that\nthey can be alleviated by unifying both tasks through a model of the joint\nprobability of (noisy) images and class labels. Classification is performed\nwith a forward pass followed by conditioning. Using the Tweedie-Miyasawa\nformula, we evaluate the denoising function with the score, which can be\ncomputed by marginalization and back-propagation. The training objective is\nthen a combination of cross-entropy loss and denoising score matching loss\nintegrated over noise levels. Numerical experiments on CIFAR-10 and ImageNet\nshow competitive classification and denoising performance compared to reference\ndeep convolutional classifiers/denoisers, and significantly improves efficiency\ncompared to previous joint approaches. Our model shows an increased robustness\nto adversarial perturbations compared to a standard discriminative classifier,\nand allows for a novel interpretation of adversarial gradients as a difference\nof denoisers.\n","authors":["Louis Thiry","Florentin Guth"],"pdf_url":"https://arxiv.org/pdf/2410.03505v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03499v1","updated":"2024-10-04T15:13:31Z","published":"2024-10-04T15:13:31Z","title":"FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein\n  Estimator","summary":"  Federated Learning (FL) facilitates data privacy by enabling collaborative\nin-situ training across decentralized clients. Despite its inherent advantages,\nFL faces significant challenges of performance and convergence when dealing\nwith data that is not independently and identically distributed (non-i.i.d.).\nWhile previous research has primarily addressed the issue of skewed label\ndistribution across clients, this study focuses on the less explored challenge\nof multi-domain FL, where client data originates from distinct domains with\nvarying feature distributions. We introduce a novel method designed to address\nthese challenges FedStein: Enhancing Multi-Domain Federated Learning Through\nthe James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)\nestimates of batch normalization (BN) statistics across clients, while\nmaintaining local BN parameters. The non-BN layer parameters are exchanged via\nstandard FL techniques. Extensive experiments conducted across three datasets\nand multiple models demonstrate that FedStein surpasses existing methods such\nas FedAvg and FedBN, with accuracy improvements exceeding 14% in certain\ndomains leading to enhanced domain generalization. The code is available at\nhttps://github.com/sunnyinAI/FedStein\n","authors":["Sunny Gupta","Nikita Jangid","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2410.03499v1.pdf","comment":"12 pages, 2 figures. Accepted at International Workshop on Federated\n  Foundation Models In Conjunction with NeurIPS 2024 (FL@FM-NeurIPS'24)"},{"id":"http://arxiv.org/abs/2410.03497v1","updated":"2024-10-04T15:11:15Z","published":"2024-10-04T15:11:15Z","title":"Collaborative and Efficient Personalization with Mixtures of Adaptors","summary":"  Non-iid data is prevalent in real-world federated learning problems. Data\nheterogeneity can come in different types in terms of distribution shifts. In\nthis work, we are interested in the heterogeneity that comes from concept\nshifts, i.e., shifts in the prediction across clients. In particular, we\nconsider multi-task learning, where we want the model to adapt to the task of\nthe client. We propose a parameter-efficient framework to tackle this issue,\nwhere each client learns to mix between parameter-efficient adaptors according\nto its task. We use Low-Rank Adaptors (LoRAs) as the backbone and extend its\nconcept to other types of layers. We call our framework Federated Low-Rank\nAdaptive Learning (FLoRAL). This framework is not an algorithm but rather a\nmodel parameterization for a multi-task learning objective, so it can work on\ntop of any algorithm that optimizes this objective, which includes many\nalgorithms from the literature. FLoRAL is memory-efficient, and clients are\npersonalized with small states (e.g., one number per adaptor) as the adaptors\nthemselves are federated. Hence, personalization is--in this sense--federated\nas well. Even though clients can personalize more freely by training an adaptor\nlocally, we show that collaborative and efficient training of adaptors is\npossible and performs better. We also show that FLoRAL can outperform an\nensemble of full models with optimal cluster assignment, which demonstrates the\nbenefits of federated personalization and the robustness of FLoRAL to\noverfitting. We show promising experimental results on synthetic datasets,\nreal-world federated multi-task problems such as MNIST, CIFAR-10, and\nCIFAR-100. We also provide a theoretical analysis of local SGD on a relaxed\nobjective and discuss the effects of aggregation mismatch on convergence.\n","authors":["Abdulla Jasem Almansoori","Samuel Horvth","Martin Tak"],"pdf_url":"https://arxiv.org/pdf/2410.03497v1.pdf","comment":"36 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.14393v3","updated":"2024-10-04T15:10:36Z","published":"2024-06-20T15:12:27Z","title":"Jailbreaking as a Reward Misspecification Problem","summary":"  The widespread adoption of large language models (LLMs) has raised concerns\nabout their safety and reliability, particularly regarding their vulnerability\nto adversarial attacks. In this paper, we propose a novel perspective that\nattributes this vulnerability to reward misspecification during the alignment\nprocess. This misspecification occurs when the reward function fails to\naccurately capture the intended behavior, leading to misaligned model outputs.\nWe introduce a metric ReGap to quantify the extent of reward misspecification\nand demonstrate its effectiveness and robustness in detecting harmful backdoor\nprompts. Building upon these insights, we present ReMiss, a system for\nautomated red teaming that generates adversarial prompts in a\nreward-misspecified space. ReMiss achieves state-of-the-art attack success\nrates on the AdvBench benchmark against various target aligned LLMs while\npreserving the human readability of the generated prompts. Furthermore, these\nattacks on open-source models demonstrate high transferability to closed-source\nmodels like GPT-4o and out-of-distribution tasks from HarmBench. Detailed\nanalysis highlights the unique advantages of the proposed reward\nmisspecification objective compared to previous methods, offering new insights\nfor improving LLM safety and robustness.\n","authors":["Zhihui Xie","Jiahui Gao","Lei Li","Zhenguo Li","Qi Liu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2406.14393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03496v1","updated":"2024-10-04T15:10:22Z","published":"2024-10-04T15:10:22Z","title":"Fourier PINNs: From Strong Boundary Conditions to Adaptive Fourier Bases","summary":"  Interest is rising in Physics-Informed Neural Networks (PINNs) as a mesh-free\nalternative to traditional numerical solvers for partial differential equations\n(PDEs). However, PINNs often struggle to learn high-frequency and multi-scale\ntarget solutions. To tackle this problem, we first study a strong Boundary\nCondition (BC) version of PINNs for Dirichlet BCs and observe a consistent\ndecline in relative error compared to the standard PINNs. We then perform a\ntheoretical analysis based on the Fourier transform and convolution theorem. We\nfind that strong BC PINNs can better learn the amplitudes of high-frequency\ncomponents of the target solutions. However, constructing the architecture for\nstrong BC PINNs is difficult for many BCs and domain geometries. Enlightened by\nour theoretical analysis, we propose Fourier PINNs -- a simple, general, yet\npowerful method that augments PINNs with pre-specified, dense Fourier bases.\nOur proposed architecture likewise learns high-frequency components better but\nplaces no restrictions on the particular BCs or problem domains. We develop an\nadaptive learning and basis selection algorithm via alternating neural net\nbasis optimization, Fourier and neural net basis coefficient estimation, and\ncoefficient truncation. This scheme can flexibly identify the significant\nfrequencies while weakening the nominal frequencies to better capture the\ntarget solution's power spectrum. We show the advantage of our approach through\na set of systematic experiments.\n","authors":["Madison Cooley","Varun Shankar","Robert M. Kirby","Shandian Zhe"],"pdf_url":"https://arxiv.org/pdf/2410.03496v1.pdf","comment":"24 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.03494v1","updated":"2024-10-04T15:09:05Z","published":"2024-10-04T15:09:05Z","title":"Generative Artificial Intelligence for Navigating Synthesizable Chemical\n  Space","summary":"  We introduce SynFormer, a generative modeling framework designed to\nefficiently explore and navigate synthesizable chemical space. Unlike\ntraditional molecular generation approaches, we generate synthetic pathways for\nmolecules to ensure that designs are synthetically tractable. By incorporating\na scalable transformer architecture and a diffusion module for building block\nselection, SynFormer surpasses existing models in synthesizable molecular\ndesign. We demonstrate SynFormer's effectiveness in two key applications: (1)\nlocal chemical space exploration, where the model generates synthesizable\nanalogs of a reference molecule, and (2) global chemical space exploration,\nwhere the model aims to identify optimal molecules according to a black-box\nproperty prediction oracle. Additionally, we demonstrate the scalability of our\napproach via the improvement in performance as more computational resources\nbecome available. With our code and trained models openly available, we hope\nthat SynFormer will find use across applications in drug discovery and\nmaterials science.\n","authors":["Wenhao Gao","Shitong Luo","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2410.03494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15506v2","updated":"2024-10-04T15:02:35Z","published":"2024-05-24T12:51:23Z","title":"Learning to Discretize Denoising Diffusion ODEs","summary":"  Diffusion Probabilistic Models (DPMs) are generative models showing\ncompetitive performance in various domains, including image synthesis and 3D\npoint cloud generation. Sampling from pre-trained DPMs involves multiple neural\nfunction evaluations (NFE) to transform Gaussian noise samples into images,\nresulting in higher computational costs compared to single-step generative\nmodels such as GANs or VAEs. Therefore, reducing the number of NFEs while\npreserving generation quality is crucial. To address this, we propose LD3, a\nlightweight framework designed to learn the optimal time discretization for\nsampling. LD3 can be combined with various samplers and consistently improves\ngeneration quality without having to retrain resource-intensive neural\nnetworks. We demonstrate analytically and empirically that LD3 improves\nsampling efficiency with much less computational overhead. We evaluate our\nmethod with extensive experiments on 7 pre-trained models, covering\nunconditional and conditional sampling in both pixel-space and latent-space\nDPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional\nCIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient\napproach to sampling from pre-trained diffusion models. Code is available at\nhttps://github.com/vinhsuhi/LD3/tree/main.\n","authors":["Vinh Tong","Trung-Dung Hoang","Anji Liu","Guy Van den Broeck","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2405.15506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05753v4","updated":"2024-10-04T15:00:24Z","published":"2024-06-09T12:16:30Z","title":"Grounding Continuous Representations in Geometry: Equivariant Neural\n  Fields","summary":"  Conditional Neural Fields (CNFs) are increasingly being leveraged as\ncontinuous signal representations, by associating each data-sample with a\nlatent variable that conditions a shared backbone Neural Field (NeF) to\nreconstruct the sample. However, existing CNF architectures face limitations\nwhen using this latent downstream in tasks requiring fine grained geometric\nreasoning, such as classification and segmentation. We posit that this results\nfrom lack of explicit modelling of geometric information (e.g. locality in the\nsignal or the orientation of a feature) in the latent space of CNFs. As such,\nwe propose Equivariant Neural Fields (ENFs), a novel CNF architecture which\nuses a geometry-informed cross-attention to condition the NeF on a geometric\nvariable, a latent point cloud of features, that enables an equivariant\ndecoding from latent to field. We show that this approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws: if the field transforms, the latent\nrepresentation transforms accordingly - and vice versa. Crucially, this\nequivariance relation ensures that the latent is capable of (1) representing\ngeometric patterns faitfhully, allowing for geometric reasoning in latent\nspace, (2) weight-sharing over similar local patterns, allowing for efficient\nlearning of datasets of fields. We validate these main properties in a range of\ntasks including classification, segmentation, forecasting and reconstruction,\nshowing clear improvement over baselines with a geometry-free latent space.\n","authors":["David R Wessels","David M Knigge","Samuele Papa","Riccardo Valperga","Sharvaree Vadgama","Efstratios Gavves","Erik J Bekkers"],"pdf_url":"https://arxiv.org/pdf/2406.05753v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03487v1","updated":"2024-10-04T14:59:10Z","published":"2024-10-04T14:59:10Z","title":"A Multimodal Framework for Deepfake Detection","summary":"  The rapid advancement of deepfake technology poses a significant threat to\ndigital media integrity. Deepfakes, synthetic media created using AI, can\nconvincingly alter videos and audio to misrepresent reality. This creates risks\nof misinformation, fraud, and severe implications for personal privacy and\nsecurity. Our research addresses the critical issue of deepfakes through an\ninnovative multimodal approach, targeting both visual and auditory elements.\nThis comprehensive strategy recognizes that human perception integrates\nmultiple sensory inputs, particularly visual and auditory information, to form\na complete understanding of media content. For visual analysis, a model that\nemploys advanced feature extraction techniques was developed, extracting nine\ndistinct facial characteristics and then applying various machine learning and\ndeep learning models. For auditory analysis, our model leverages\nmel-spectrogram analysis for feature extraction and then applies various\nmachine learning and deep learningmodels. To achieve a combined analysis, real\nand deepfake audio in the original dataset were swapped for testing purposes\nand ensured balanced samples. Using our proposed models for video and audio\nclassification i.e. Artificial Neural Network and VGG19, the overall sample is\nclassified as deepfake if either component is identified as such. Our\nmultimodal framework combines visual and auditory analyses, yielding an\naccuracy of 94%.\n","authors":["Kashish Gandhi","Prutha Kulkarni","Taran Shah","Piyush Chaudhari","Meera Narvekar","Kranti Ghag"],"pdf_url":"https://arxiv.org/pdf/2410.03487v1.pdf","comment":"22 pages, 14 figures, Accepted in Journal of Electrical Systems"},{"id":"http://arxiv.org/abs/2409.06366v2","updated":"2024-10-04T14:56:10Z","published":"2024-09-10T09:44:15Z","title":"One Policy to Run Them All: an End-to-end Learning Approach to\n  Multi-Embodiment Locomotion","summary":"  Deep Reinforcement Learning techniques are achieving state-of-the-art results\nin robust legged locomotion. While there exists a wide variety of legged\nplatforms such as quadruped, humanoids, and hexapods, the field is still\nmissing a single learning framework that can control all these different\nembodiments easily and effectively and possibly transfer, zero or few-shot, to\nunseen robot embodiments. We introduce URMA, the Unified Robot Morphology\nArchitecture, to close this gap. Our framework brings the end-to-end Multi-Task\nReinforcement Learning approach to the realm of legged robots, enabling the\nlearned policy to control any type of robot morphology. The key idea of our\nmethod is to allow the network to learn an abstract locomotion controller that\ncan be seamlessly shared between embodiments thanks to our morphology-agnostic\nencoders and decoders. This flexible architecture can be seen as a potential\nfirst step in building a foundation model for legged robot locomotion. Our\nexperiments show that URMA can learn a locomotion policy on multiple\nembodiments that can be easily transferred to unseen robot platforms in\nsimulation and the real world.\n","authors":["Nico Bohlinger","Grzegorz Czechmanowski","Maciej Krupka","Piotr Kicki","Krzysztof Walas","Jan Peters","Davide Tateo"],"pdf_url":"https://arxiv.org/pdf/2409.06366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03478v1","updated":"2024-10-04T14:52:09Z","published":"2024-10-04T14:52:09Z","title":"VEDIT: Latent Prediction Architecture For Procedural Video\n  Representation Learning","summary":"  Procedural video representation learning is an active research area where the\nobjective is to learn an agent which can anticipate and forecast the future\ngiven the present video input, typically in conjunction with textual\nannotations. Prior works often rely on large-scale pretraining of visual\nencoders and prediction models with language supervision. However, the\nnecessity and effectiveness of extending compute intensive pretraining to learn\nvideo clip sequences with noisy text supervision have not yet been fully\nvalidated by previous works. In this work, we show that a strong off-the-shelf\nfrozen pretrained visual encoder, along with a well designed prediction model,\ncan achieve state-of-the-art (SoTA) performance in forecasting and procedural\nplanning without the need for pretraining the prediction model, nor requiring\nadditional supervision from language or ASR. Instead of learning\nrepresentations from pixel space, our method utilizes the latent embedding\nspace of publicly available vision encoders. By conditioning on frozen\nclip-level embeddings from observed steps to predict the actions of unseen\nsteps, our prediction model is able to learn robust representations for\nforecasting through iterative denoising - leveraging the recent advances in\ndiffusion transformers (Peebles & Xie, 2023). Empirical studies over a total of\nfive procedural learning tasks across four datasets (NIV, CrossTask, COIN and\nEgo4D-v2) show that our model advances the strong baselines in long-horizon\naction anticipation (+2.6% in Verb ED@20, +3.1% in Noun ED@20), and\nsignificantly improves the SoTA in step forecasting (+5.0%), task\nclassification (+3.8%), and procedure planning tasks (up to +2.28% in success\nrate, +3.39% in mAcc, and +0.90% in mIoU).\n","authors":["Han Lin","Tushar Nagarajan","Nicolas Ballas","Mido Assran","Mojtaba Komeili","Mohit Bansal","Koustuv Sinha"],"pdf_url":"https://arxiv.org/pdf/2410.03478v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.03477v1","updated":"2024-10-04T14:48:13Z","published":"2024-10-04T14:48:13Z","title":"On the Hardness of Learning One Hidden Layer Neural Networks","summary":"  In this work, we consider the problem of learning one hidden layer ReLU\nneural networks with inputs from $\\mathbb{R}^d$. We show that this learning\nproblem is hard under standard cryptographic assumptions even when: (1) the\nsize of the neural network is polynomial in $d$, (2) its input distribution is\na standard Gaussian, and (3) the noise is Gaussian and polynomially small in\n$d$. Our hardness result is based on the hardness of the Continuous Learning\nwith Errors (CLWE) problem, and in particular, is based on the largely believed\nworst-case hardness of approximately solving the shortest vector problem up to\na multiplicative polynomial factor.\n","authors":["Shuchen Li","Ilias Zadik","Manolis Zampetakis"],"pdf_url":"https://arxiv.org/pdf/2410.03477v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2410.03470v1","updated":"2024-10-04T14:40:11Z","published":"2024-10-04T14:40:11Z","title":"Vulnerability Detection via Topological Analysis of Attention Maps","summary":"  Recently, deep learning (DL) approaches to vulnerability detection have\ngained significant traction. These methods demonstrate promising results, often\nsurpassing traditional static code analysis tools in effectiveness.\n  In this study, we explore a novel approach to vulnerability detection\nutilizing the tools from topological data analysis (TDA) on the attention\nmatrices of the BERT model. Our findings reveal that traditional machine\nlearning (ML) techniques, when trained on the topological features extracted\nfrom these attention matrices, can perform competitively with pre-trained\nlanguage models (LLMs) such as CodeBERTa. This suggests that TDA tools,\nincluding persistent homology, are capable of effectively capturing semantic\ninformation critical for identifying vulnerabilities.\n","authors":["Pavel Snopov","Andrey Nikolaevich Golubinskiy"],"pdf_url":"https://arxiv.org/pdf/2410.03470v1.pdf","comment":"Accepted to ITaS2024. Contains 8 pages"},{"id":"http://arxiv.org/abs/2407.17125v3","updated":"2024-10-04T14:36:36Z","published":"2024-07-24T09:48:48Z","title":"To Know or Not To Know? Analyzing Self-Consistency of Large Language\n  Models under Ambiguity","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. This paper focuses\non entity type ambiguity, analyzing the proficiency and consistency of\nstate-of-the-art LLMs in applying factual knowledge when prompted with\nambiguous entities. To do so, we propose an evaluation protocol that\ndisentangles knowing from applying knowledge, and test state-of-the-art LLMs on\n49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing\nthe correct entity reading, achieving an average accuracy of only 85%, and as\nlow as 75% with underspecified prompts. The results also reveal systematic\ndiscrepancies in LLM behavior, showing that while the models may possess\nknowledge, they struggle to apply it consistently, exhibit biases toward\npreferred readings, and display self-inconsistencies. This highlights the need\nto address entity ambiguity in the future for more trustworthy LLMs.\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v3.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.03464v1","updated":"2024-10-04T14:27:43Z","published":"2024-10-04T14:27:43Z","title":"S7: Selective and Simplified State Space Layers for Sequence Modeling","summary":"  A central challenge in sequence modeling is efficiently handling tasks with\nextended contexts. While recent state-space models (SSMs) have made significant\nprogress in this area, they often lack input-dependent filtering or require\nsubstantial increases in model complexity to handle input variability. We\naddress this gap by introducing S7, a simplified yet powerful SSM that can\nhandle input dependence while incorporating stable reparameterization and\nspecific design choices to dynamically adjust state transitions based on input\ncontent, maintaining efficiency and performance. We prove that this\nreparameterization ensures stability in long-sequence modeling by keeping state\ntransitions well-behaved over time. Additionally, it controls the gradient\nnorm, enabling efficient training and preventing issues like exploding or\nvanishing gradients. S7 significantly outperforms baselines across various\nsequence modeling tasks, including neuromorphic event-based datasets, Long\nRange Arena benchmarks, and various physical and biological time series.\nOverall, S7 offers a more straightforward approach to sequence modeling without\nrelying on complex, domain-specific inductive biases, achieving significant\nimprovements across key benchmarks.\n","authors":["Taylan Soydan","Nikola Zubi","Nico Messikommer","Siddhartha Mishra","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2410.03464v1.pdf","comment":"23 pages, 3 figures, 11 tables. Equal contribution by Taylan Soydan\n  and Nikola Zubi\\'c"},{"id":"http://arxiv.org/abs/2410.03463v1","updated":"2024-10-04T14:26:54Z","published":"2024-10-04T14:26:54Z","title":"Diffusion State-Guided Projected Gradient for Inverse Problems","summary":"  Recent advancements in diffusion models have been effective in learning data\npriors for solving inverse problems. They leverage diffusion sampling steps for\ninducing a data prior while using a measurement guidance gradient at each step\nto impose data consistency. For general inverse problems, approximations are\nneeded when an unconditionally trained diffusion model is used since the\nmeasurement likelihood is intractable, leading to inaccurate posterior\nsampling. In other words, due to their approximations, these methods fail to\npreserve the generation process on the data manifold defined by the diffusion\nprior, leading to artifacts in applications such as image restoration. To\nenhance the performance and robustness of diffusion models in solving inverse\nproblems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad),\nwhich projects the measurement gradient onto a subspace that is a low-rank\napproximation of an intermediate state of the diffusion process. DiffStateGrad,\nas a module, can be added to a wide range of diffusion-based inverse solvers to\nimprove the preservation of the diffusion process on the prior manifold and\nfilter out artifact-inducing components. We highlight that DiffStateGrad\nimproves the robustness of diffusion models in terms of the choice of\nmeasurement guidance step size and noise while improving the worst-case\nperformance. Finally, we demonstrate that DiffStateGrad improves upon the\nstate-of-the-art on linear and nonlinear image restoration inverse problems.\n","authors":["Rayhan Zirvi","Bahareh Tolooshams","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2410.03463v1.pdf","comment":"preprint. under review. RZ and BT have equal contributions"},{"id":"http://arxiv.org/abs/2410.03462v1","updated":"2024-10-04T14:24:06Z","published":"2024-10-04T14:24:06Z","title":"Linear Transformer Topological Masking with Graph Random Features","summary":"  When training transformers on graph-structured data, incorporating\ninformation about the underlying topology is crucial for good performance.\nTopological masking, a type of relative position encoding, achieves this by\nupweighting or downweighting attention depending on the relationship between\nthe query and keys in a graph. In this paper, we propose to parameterise\ntopological masks as a learnable function of a weighted adjacency matrix -- a\nnovel, flexible approach which incorporates a strong structural inductive bias.\nBy approximating this mask with graph random features (for which we prove the\nfirst known concentration bounds), we show how this can be made fully\ncompatible with linear attention, preserving $\\mathcal{O}(N)$ time and space\ncomplexity with respect to the number of input tokens. The fastest previous\nalternative was $\\mathcal{O}(N \\log N)$ and only suitable for specific graphs.\nOur efficient masking algorithms provide strong performance gains for tasks on\nimage and point cloud data, including with $>30$k nodes.\n","authors":["Isaac Reid","Kumar Avinava Dubey","Deepali Jain","Will Whitney","Amr Ahmed","Joshua Ainslie","Alex Bewley","Mithun Jacob","Aranyak Mehta","David Rendleman","Connor Schenck","Richard E. Turner","Ren Wagner","Adrian Weller","Krzysztof Choromanski"],"pdf_url":"https://arxiv.org/pdf/2410.03462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01651v2","updated":"2024-10-04T14:23:12Z","published":"2024-06-03T14:48:54Z","title":"FusionDTI: Fine-grained Binding Discovery with Token-level Fusion for\n  Drug-Target Interaction","summary":"  Predicting drug-target interaction (DTI) is critical in the drug discovery\nprocess. Despite remarkable advances in recent DTI models through the\nintegration of representations from diverse drug and target encoders, such\nmodels often struggle to capture the fine-grained interactions between drugs\nand protein, i.e. the binding of specific drug atoms (or substructures) and key\namino acids of proteins, which is crucial for understanding the binding\nmechanisms and optimising drug design. To address this issue, this paper\nintroduces a novel model, called FusionDTI, which uses a token-level Fusion\nmodule to effectively learn fine-grained information for Drug-Target\nInteraction. In particular, our FusionDTI model uses the SELFIES representation\nof drugs to mitigate sequence fragment invalidation and incorporates the\nstructure-aware (SA) vocabulary of target proteins to address the limitation of\namino acid sequences in structural information, additionally leveraging\npre-trained language models extensively trained on large-scale biomedical\ndatasets as encoders to capture the complex information of drugs and targets.\nExperiments on three well-known benchmark datasets show that our proposed\nFusionDTI model achieves the best performance in DTI prediction compared with\nseven existing state-of-the-art baselines. Furthermore, our case study\nindicates that FusionDTI could highlight the potential binding sites, enhancing\nthe explainability of the DTI prediction.\n","authors":["Zhaohan Meng","Zaiqiao Meng","Ke Yuan","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2406.01651v2.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.03461v1","updated":"2024-10-04T14:21:27Z","published":"2024-10-04T14:21:27Z","title":"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval Augmented Generation","summary":"  While retrieval augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. One common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10 % of their computational cost.\n","authors":["Tobias Leemann","Periklis Petridis","Giuseppe Vietri","Dionysis Manousakas","Aaron Roth","Sergul Aydore"],"pdf_url":"https://arxiv.org/pdf/2410.03461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19992v2","updated":"2024-10-04T14:20:31Z","published":"2024-09-30T06:30:33Z","title":"A large-scale operational study of fingerprint quality and demographics","summary":"  Even though a few initial works have shown on small sets of data some level\nof bias in the performance of fingerprint recognition technology with respect\nto certain demographic groups, there is still not sufficient evidence to\nunderstand the impact that certain factors such as gender, age or finger-type\nmay have on fingerprint quality and, in turn, also on fingerprint matching\naccuracy. The present work addresses this still under researched topic, on a\nlarge-scale database of operational data containing 10-print impressions of\nalmost 16,000 subjects. The results reached provide further insight into the\ndependency of fingerprint quality and demographics, and show that there in fact\nexists a certain degree of performance variability in fingerprint-based\nrecognition systems for different segments of the population. Based on the\nexperimental evaluation, the work points out new observations based on\ndata-driven evidence, provides plausible hypotheses to explain such\nobservations, and concludes with potential follow-up actions that can help to\nreduce the observed fingerprint quality differences. This way, the current\npaper can be considered as a contribution to further increase the algorithmic\nfairness and equality of biometric technology.\n","authors":["Javier Galbally","Aleksandrs Cepilovs","Ramon Blanco-Gonzalo","Gillian Ormiston","Oscar Miguel-Hurtado","Istvan Sz. Racz"],"pdf_url":"https://arxiv.org/pdf/2409.19992v2.pdf","comment":"Extended journal version submitted to IET Biometrics. 10 pages, 5\n  figures Reference conference paper: J. Galbally, A. Cepilovs, R.\n  Blanco-Gonzalo, G. Ormiston, O. Miguel-Hurtado, and I. S. Racz, 'Fingerprint\n  quality per individual finger type: A large-scale study on real operational\n  data' in Proc. IEEE Intl. Workshop on Biometrics and Forensics 2023 (IWBF\n  2023)"},{"id":"http://arxiv.org/abs/2409.11684v2","updated":"2024-10-04T14:18:49Z","published":"2024-09-18T03:52:48Z","title":"Recurrent Interpolants for Probabilistic Time Series Prediction","summary":"  Sequential models like recurrent neural networks and transformers have become\nstandard for probabilistic multivariate time series forecasting across various\ndomains. Despite their strengths, they struggle with capturing high-dimensional\ndistributions and cross-feature dependencies. Recent work explores generative\napproaches using diffusion or flow-based models, extending to time series\nimputation and forecasting. However, scalability remains a challenge. This work\nproposes a novel method combining recurrent neural networks' efficiency with\ndiffusion models' probabilistic modeling, based on stochastic interpolants and\nconditional generation with control features, offering insights for future\ndevelopments in this dynamic field.\n","authors":["Yu Chen","Marin Bilo","Sarthak Mittal","Wei Deng","Kashif Rasul","Anderson Schneider"],"pdf_url":"https://arxiv.org/pdf/2409.11684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03459v1","updated":"2024-10-04T14:18:31Z","published":"2024-10-04T14:18:31Z","title":"Generative Semantic Communication for Text-to-Speech Synthesis","summary":"  Semantic communication is a promising technology to improve communication\nefficiency by transmitting only the semantic information of the source data.\nHowever, traditional semantic communication methods primarily focus on data\nreconstruction tasks, which may not be efficient for emerging generative tasks\nsuch as text-to-speech (TTS) synthesis. To address this limitation, this paper\ndevelops a novel generative semantic communication framework for TTS synthesis,\nleveraging generative artificial intelligence technologies. Firstly, we utilize\na pre-trained large speech model called WavLM and the residual vector\nquantization method to construct two semantic knowledge bases (KBs) at the\ntransmitter and receiver, respectively. The KB at the transmitter enables\neffective semantic extraction, while the KB at the receiver facilitates\nlifelike speech synthesis. Then, we employ a transformer encoder and a\ndiffusion model to achieve efficient semantic coding without introducing\nsignificant communication overhead. Finally, numerical results demonstrate that\nour framework achieves much higher fidelity for the generated speech than four\nbaselines, in both cases with additive white Gaussian noise channel and\nRayleigh fading channel.\n","authors":["Jiahao Zheng","Jinke Ren","Peng Xu","Zhihao Yuan","Jie Xu","Fangxin Wang","Gui Gui","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2410.03459v1.pdf","comment":"The paper has been accepted by IEEE Globecom Workshop"},{"id":"http://arxiv.org/abs/2209.13762v2","updated":"2024-10-04T14:13:24Z","published":"2022-09-28T01:19:38Z","title":"Consensus Knowledge Graph Learning via Multi-view Sparse Low Rank Block\n  Model","summary":"  Network analysis has been a powerful tool to unveil relationships and\ninteractions among a large number of objects. Yet its effectiveness in\naccurately identifying important node-node interactions is challenged by the\nrapidly growing network size, with data being collected at an unprecedented\ngranularity and scale. Common wisdom to overcome such high dimensionality is\ncollapsing nodes into smaller groups and conducting connectivity analysis on\nthe group level. Dividing efforts into two phases inevitably opens a gap in\nconsistency and drives down efficiency. Consensus learning emerges as a new\nnormal for common knowledge discovery with multiple data sources available. In\nthis paper, we propose a unified multi-view sparse low-rank block model (msLBM)\nframework, which enables simultaneous grouping and connectivity analysis by\ncombining multiple data sources. The msLBM framework efficiently represents\noverlapping information across large scale concepts and accommodates different\ntypes of heterogeneity across sources. Both features are desirable when\nanalyzing high dimensional electronic health record (EHR) datasets from\nmultiple health systems. An estimating procedure based on the alternating\nminimization algorithm is proposed. Our theoretical results demonstrate that a\nconsensus knowledge graph can be more accurately learned by leveraging\nmulti-source datasets, and statistically optimal rates can be achieved under\nmild conditions. Applications to the real world EHR data suggest that our\nproposed msLBM algorithm can more reliably reveal network structure among\nclinical concepts by effectively combining summary level EHR data from multiple\nhealth systems.\n","authors":["Tianxi Cai","Dong Xia","Luwan Zhang","Doudou Zhou"],"pdf_url":"https://arxiv.org/pdf/2209.13762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03450v1","updated":"2024-10-04T14:10:39Z","published":"2024-10-04T14:10:39Z","title":"MLLM as Retriever: Interactively Learning Multimodal Retrieval for\n  Embodied Agents","summary":"  MLLM agents demonstrate potential for complex embodied tasks by retrieving\nmultimodal task-relevant trajectory data. However, current retrieval methods\nprimarily focus on surface-level similarities of textual or visual cues in\ntrajectories, neglecting their effectiveness for the specific task at hand. To\naddress this issue, we propose a novel method, MLLM as ReTriever (MART), which\nenhances the performance of embodied agents by utilizing interaction data to\nfine-tune an MLLM retriever based on preference learning, such that the\nretriever fully considers the effectiveness of trajectories and prioritize them\nfor unseen tasks. We also introduce Trajectory Abstraction, a mechanism that\nleverages MLLMs' summarization capabilities to represent trajectories with\nfewer tokens while preserving key information, enabling agents to better\ncomprehend milestones in the trajectory. Experimental results across various\nenvironments demonstrate our method significantly improves task success rates\nin unseen scenes compared to baseline methods. This work presents a new\nparadigm for multimodal retrieval in embodied agents, by fine-tuning a\ngeneral-purpose MLLM as the retriever to assess trajectory effectiveness. All\nbenchmark task sets and simulator code modifications for action and observation\nspaces will be released.\n","authors":["Junpeng Yue","Xinru Xu","Brje F. Karlsson","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.03450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13455v4","updated":"2024-10-04T13:59:30Z","published":"2022-10-21T09:59:15Z","title":"Epistemic Monte Carlo Tree Search","summary":"  The AlphaZero/MuZero (A/MZ) family of algorithms has achieved remarkable\nsuccess across various challenging domains by integrating Monte Carlo Tree\nSearch (MCTS) with learned models. Learned models introduce epistemic\nuncertainty, which is caused by learning from limited data and is useful for\nexploration in sparse reward environments. MCTS does not account for the\npropagation of this uncertainty however. To address this, we introduce\nEpistemic MCTS (EMCTS): a theoretically motivated approach to account for the\nepistemic uncertainty in search and harness the search for deep exploration. In\nthe challenging sparse-reward task of writing code in the Assembly language\nSUBLEQ, AZ paired with our method achieves significantly higher sample\nefficiency over baseline AZ. Search with EMCTS solves variations of the\ncommonly used hard-exploration benchmark Deep Sea - which baseline A/MZ are\npractically unable to solve - much faster than an otherwise equivalent method\nthat does not use search for uncertainty estimation, demonstrating significant\nbenefits from search for epistemic uncertainty estimation.\n","authors":["Yaniv Oren","Villiam Vadocz","Matthijs T. J. Spaan","Wendelin Bhmer"],"pdf_url":"https://arxiv.org/pdf/2210.13455v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17451v2","updated":"2024-10-04T13:56:14Z","published":"2024-04-26T14:43:19Z","title":"Any-Quantile Probabilistic Forecasting of Short-Term Electricity Demand","summary":"  Power systems operate under uncertainty originating from multiple factors\nthat are impossible to account for deterministically. Distributional\nforecasting is used to control and mitigate risks associated with this\nuncertainty. Recent progress in deep learning has helped to significantly\nimprove the accuracy of point forecasts, while accurate distributional\nforecasting still presents a significant challenge. In this paper, we propose a\nnovel general approach for distributional forecasting capable of predicting\narbitrary quantiles. We show that our general approach can be seamlessly\napplied to two distinct neural architectures leading to the state-of-the-art\ndistributional forecasting results in the context of short-term electricity\ndemand forecasting task. We empirically validate our method on 35 hourly\nelectricity demand time-series for European countries. Our code is available\nhere: https://github.com/boreshkinai/any-quantile.\n","authors":["Slawek Smyl","Boris N. Oreshkin","Pawe Peka","Grzegorz Dudek"],"pdf_url":"https://arxiv.org/pdf/2404.17451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05879v3","updated":"2024-10-04T13:54:25Z","published":"2024-04-08T21:26:04Z","title":"Rapid and Precise Topological Comparison with Merge Tree Neural Networks","summary":"  Merge trees are a valuable tool in the scientific visualization of scalar\nfields; however, current methods for merge tree comparisons are computationally\nexpensive, primarily due to the exhaustive matching between tree nodes. To\naddress this challenge, we introduce the Merge Tree Neural Network (MTNN), a\nlearned neural network model designed for merge tree comparison. The MTNN\nenables rapid and high-quality similarity computation. We first demonstrate how\nto train graph neural networks, which emerged as effective encoders for graphs,\nin order to produce embeddings of merge trees in vector spaces for efficient\nsimilarity comparison. Next, we formulate the novel MTNN model that further\nimproves the similarity comparisons by integrating the tree and node embeddings\nwith a new topological attention mechanism. We demonstrate the effectiveness of\nour model on real-world data in different domains and examine our model's\ngeneralizability across various datasets. Our experimental analysis\ndemonstrates our approach's superiority in accuracy and efficiency. In\nparticular, we speed up the prior state-of-the-art by more than $100\\times$ on\nthe benchmark datasets while maintaining an error rate below $0.1\\%$.\n","authors":["Yu Qin","Brittany Terese Fasy","Carola Wenk","Brian Summa"],"pdf_url":"https://arxiv.org/pdf/2404.05879v3.pdf","comment":"Published on IEEE VIS 2024 with Best Paper Award"},{"id":"http://arxiv.org/abs/2405.15476v2","updated":"2024-10-04T13:52:33Z","published":"2024-05-24T11:55:46Z","title":"Editable Concept Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) have garnered much attention for their\nability to elucidate the prediction process through a human-understandable\nconcept layer. However, most previous studies focused on cases where the data,\nincluding concepts, are clean. In many scenarios, we always need to\nremove/insert some training data or new concepts from trained CBMs due to\ndifferent reasons, such as privacy concerns, data mislabelling, spurious\nconcepts, and concept annotation errors. Thus, the challenge of deriving\nefficient editable CBMs without retraining from scratch persists, particularly\nin large-scale applications. To address these challenges, we propose Editable\nConcept Bottleneck Models (ECBMs). Specifically, ECBMs support three different\nlevels of data removal: concept-label-level, concept-level, and data-level.\nECBMs enjoy mathematically rigorous closed-form approximations derived from\ninfluence functions that obviate the need for re-training. Experimental results\ndemonstrate the efficiency and effectiveness of our ECBMs, affirming their\nadaptability within the realm of CBMs.\n","authors":["Lijie Hu","Chenyang Ren","Zhengyu Hu","Hongbin Lin","Cheng-Long Wang","Hui Xiong","Jingfeng Zhang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2405.15476v2.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.03437v1","updated":"2024-10-04T13:52:02Z","published":"2024-10-04T13:52:02Z","title":"Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs","summary":"  Solving time-dependent parametric partial differential equations (PDEs) is\nchallenging, as models must adapt to variations in parameters such as\ncoefficients, forcing terms, and boundary conditions. Data-driven neural\nsolvers either train on data sampled from the PDE parameters distribution in\nthe hope that the model generalizes to new instances or rely on gradient-based\nadaptation and meta-learning to implicitly encode the dynamics from\nobservations. This often comes with increased inference complexity. Inspired by\nthe in-context learning capabilities of large language models (LLMs), we\nintroduce Zebra, a novel generative auto-regressive transformer designed to\nsolve parametric PDEs without requiring gradient adaptation at inference. By\nleveraging in-context information during both pre-training and inference, Zebra\ndynamically adapts to new tasks by conditioning on input sequences that\nincorporate context trajectories or preceding states. This approach enables\nZebra to flexibly handle arbitrarily sized context inputs and supports\nuncertainty quantification through the sampling of multiple solution\ntrajectories. We evaluate Zebra across a variety of challenging PDE scenarios,\ndemonstrating its adaptability, robustness, and superior performance compared\nto existing approaches.\n","authors":["Louis Serrano","Armand Kassa Koupa","Thomas X Wang","Pierre Erbacher","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2410.03437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03435v1","updated":"2024-10-04T13:51:19Z","published":"2024-10-04T13:51:19Z","title":"A General Framework for Producing Interpretable Semantic Text Embeddings","summary":"  Semantic text embedding is essential to many tasks in Natural Language\nProcessing (NLP). While black-box models are capable of generating high-quality\nembeddings, their lack of interpretability limits their use in tasks that\ndemand transparency. Recent approaches have improved interpretability by\nleveraging domain-expert-crafted or LLM-generated questions, but these methods\nrely heavily on expert input or well-prompt design, which restricts their\ngeneralizability and ability to generate discriminative questions across a wide\nrange of tasks. To address these challenges, we introduce \\algo{CQG-MBQA}\n(Contrastive Question Generation - Multi-task Binary Question Answering), a\ngeneral framework for producing interpretable semantic text embeddings across\ndiverse tasks. Our framework systematically generates highly discriminative,\nlow cognitive load yes/no questions through the \\algo{CQG} method and answers\nthem efficiently with the \\algo{MBQA} model, resulting in interpretable\nembeddings in a cost-effective manner. We validate the effectiveness and\ninterpretability of \\algo{CQG-MBQA} through extensive experiments and ablation\nstudies, demonstrating that it delivers embedding quality comparable to many\nadvanced black-box models while maintaining inherently interpretability.\nAdditionally, \\algo{CQG-MBQA} outperforms other interpretable text embedding\nmethods across various downstream tasks.\n","authors":["Yiqun Sun","Qiang Huang","Yixuan Tang","Anthony K. H. Tung","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.03435v1.pdf","comment":"19 pages, 5 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2410.01570v2","updated":"2024-10-04T13:51:16Z","published":"2024-10-02T14:09:51Z","title":"Truncated Kernel Stochastic Gradient Descent on Spheres","summary":"  Inspired by the structure of spherical harmonics, we propose the truncated\nkernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square\nloss function for spherical data fitting. T-kernel SGD employs a \"truncation\"\noperation, enabling the application of series-based kernels function in\nstochastic gradient descent, thereby avoiding the difficulties of finding\nsuitable closed-form kernel functions in high-dimensional spaces.\n  In contrast to traditional kernel SGD, T-kernel SGD is more effective in\nbalancing bias and variance by dynamically adjusting the hypothesis space\nduring iterations. The most significant advantage of the proposed algorithm is\nthat it can achieve theoretically optimal convergence rates using a constant\nstep size (independent of the sample size) while overcoming the inherent\nsaturation problem of kernel SGD. Additionally, we leverage the structure of\nspherical polynomials to derive an equivalent T-kernel SGD, significantly\nreducing storage and computational costs compared to kernel SGD. Typically,\nT-kernel SGD requires only $\\mathcal{O}(n^{1+\\frac{d}{d-1}\\epsilon})$\ncomputational complexity and $\\mathcal{O}(n^{\\frac{d}{d-1}\\epsilon})$ storage\nto achieve optimal rates for the d-dimensional sphere, where\n$0<\\epsilon<\\frac{1}{2}$ can be arbitrarily small if the optimal fitting or the\nunderlying space possesses sufficient regularity. This regularity is determined\nby the smoothness parameter of the objective function and the decaying rate of\nthe eigenvalues of the integral operator associated with the kernel function,\nboth of which reflect the difficulty of the estimation problem. Our main\nresults quantitatively characterize how this prior information influences the\nconvergence of T-kernel SGD. The numerical experiments further validate the\ntheoretical findings presented in this paper.\n","authors":["JinHui Bai","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.01570v2.pdf","comment":"57 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.04390v3","updated":"2024-10-04T13:50:58Z","published":"2024-02-06T20:45:31Z","title":"Densely Multiplied Physics Informed Neural Networks","summary":"  Although physics-informed neural networks (PINNs) have shown great potential\nin dealing with nonlinear partial differential equations (PDEs), it is common\nthat PINNs will suffer from the problem of insufficient precision or obtaining\nincorrect outcomes. Unlike most of the existing solutions trying to enhance the\nability of PINN by optimizing the training process, this paper improved the\nneural network architecture to improve the performance of PINN. We propose a\ndensely multiply PINN (DM-PINN) architecture, which multiplies the output of a\nhidden layer with the outputs of all the behind hidden layers. Without\nintroducing more trainable parameters, this effective mechanism can\nsignificantly improve the accuracy of PINNs. The proposed architecture is\nevaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation,\nBurgers equation and 1D convection equation). Comparisons between the proposed\narchitecture and different PINN structures demonstrate the superior performance\nof the DM-PINN in both accuracy and efficiency.\n","authors":["Feilong Jiang","Xiaonan Hou","Min Xia"],"pdf_url":"https://arxiv.org/pdf/2402.04390v3.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16232v2","updated":"2024-10-04T13:48:21Z","published":"2024-06-23T22:06:25Z","title":"Jacobian Descent for Multi-Objective Optimization","summary":"  Many optimization problems require balancing multiple conflicting objectives.\nAs gradient descent is limited to single-objective optimization, we introduce\nits direct generalization: Jacobian descent (JD). This algorithm iteratively\nupdates parameters using the Jacobian matrix of a vector-valued objective\nfunction, in which each row is the gradient of an individual objective. While\nseveral methods to combine gradients already exist in the literature, they are\ngenerally hindered when the objectives conflict. In contrast, we propose\nprojecting gradients to fully resolve conflict while ensuring that they\npreserve an influence proportional to their norm. We prove significantly\nstronger convergence guarantees with this approach, supported by our empirical\nresults. Our method also enables instance-wise risk minimization (IWRM), a\nnovel learning paradigm in which the loss of each training example is\nconsidered a separate objective. Applied to simple image classification tasks,\nIWRM exhibits promising results compared to the direct minimization of the\naverage loss. Additionally, we outline an efficient implementation of JD using\nthe Gramian of the Jacobian matrix to reduce time and memory requirements.\n","authors":["Pierre Quinton","Valrian Rey"],"pdf_url":"https://arxiv.org/pdf/2406.16232v2.pdf","comment":"39 pages, 10 figures, conference"},{"id":"http://arxiv.org/abs/2410.03432v1","updated":"2024-10-04T13:43:29Z","published":"2024-10-04T13:43:29Z","title":"EB-NeRD: A Large-Scale Dataset for News Recommendation","summary":"  Personalized content recommendations have been pivotal to the content\nexperience in digital media from video streaming to social networks. However,\nseveral domain specific challenges have held back adoption of recommender\nsystems in news publishing. To address these challenges, we introduce the\nEkstra Bladet News Recommendation Dataset (EB-NeRD). The dataset encompasses\ndata from over a million unique users and more than 37 million impression logs\nfrom Ekstra Bladet. It also includes a collection of over 125,000 Danish news\narticles, complete with titles, abstracts, bodies, and metadata, such as\ncategories. EB-NeRD served as the benchmark dataset for the RecSys '24\nChallenge, where it was demonstrated how the dataset can be used to address\nboth technical and normative challenges in designing effective and responsible\nrecommender systems for news publishing. The dataset is available at:\nhttps://recsys.eb.dk.\n","authors":["Johannes Kruse","Kasper Lindskow","Saikishore Kalloori","Marco Polignano","Claudio Pomo","Abhishek Srivastava","Anshuk Uppal","Michael Riis Andersen","Jes Frellsen"],"pdf_url":"https://arxiv.org/pdf/2410.03432v1.pdf","comment":"11 pages, 8 tables, 2 figures, RecSys '24"},{"id":"http://arxiv.org/abs/2410.03424v1","updated":"2024-10-04T13:32:34Z","published":"2024-10-04T13:32:34Z","title":"Cayley Graph Propagation","summary":"  In spite of the plethora of success stories with graph neural networks (GNNs)\non modelling graph-structured data, they are notoriously vulnerable to\nover-squashing, whereby tasks necessitate the mixing of information between\ndistance pairs of nodes. To address this problem, prior work suggests rewiring\nthe graph structure to improve information flow. Alternatively, a significant\nbody of research has dedicated itself to discovering and precomputing\nbottleneck-free graph structures to ameliorate over-squashing. One well\nregarded family of bottleneck-free graphs within the mathematical community are\nexpander graphs, with prior work$\\unicode{x2014}$Expander Graph Propagation\n(EGP)$\\unicode{x2014}$proposing the use of a well-known expander graph\nfamily$\\unicode{x2014}$the Cayley graphs of the $\\mathrm{SL}(2,\\mathbb{Z}_n)$\nspecial linear group$\\unicode{x2014}$as a computational template for GNNs.\nHowever, in EGP the computational graphs used are truncated to align with a\ngiven input graph. In this work, we show that truncation is detrimental to the\ncoveted expansion properties. Instead, we propose CGP, a method to propagate\ninformation over a complete Cayley graph structure, thereby ensuring it is\nbottleneck-free to better alleviate over-squashing. Our empirical evidence\nacross several real-world datasets not only shows that CGP recovers significant\nimprovements as compared to EGP, but it is also akin to or outperforms\ncomputationally complex graph rewiring techniques.\n","authors":["JJ Wilson","Maya Bechler-Speicher","Petar Velikovi"],"pdf_url":"https://arxiv.org/pdf/2410.03424v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.03423v1","updated":"2024-10-04T13:32:05Z","published":"2024-10-04T13:32:05Z","title":"Aircraft Radar Altimeter Interference Mitigation Through a CNN-Layer\n  Only Denoising Autoencoder Architecture","summary":"  Denoising autoencoders for signal processing applications have been shown to\nexperience significant difficulty in learning to reconstruct radio frequency\ncommunication signals, particularly in the large sample regime. In\ncommunication systems, this challenge is primarily due to the need to\nreconstruct the modulated data stream which is generally highly stochastic in\nnature. In this work, we take advantage of this limitation by using the\ndenoising autoencoder to instead remove interfering radio frequency\ncommunication signals while reconstructing highly structured FMCW radar\nsignals. More specifically, in this work we show that a CNN-layer only\nautoencoder architecture can be utilized to improve the accuracy of a radar\naltimeter's ranging estimate even in severe interference environments\nconsisting of a multitude of interference signals. This is demonstrated through\ncomprehensive performance analysis of an end-to-end FMCW radar altimeter\nsimulation with and without the convolutional layer-only autoencoder. The\nproposed approach significantly improves interference mitigation in the\npresence of both narrow-band tone interference as well as wideband QPSK\ninterference in terms of range RMS error, number of false altitude reports, and\nthe peak-to-sidelobe ratio of the resulting range profile. FMCW radar signals\nof up to 40,000 IQ samples can be reliably reconstructed.\n","authors":["Samuel B. Brown","Stephen Young","Adam Wagenknecht","Daniel Jakubisin","Charles E. Thornton","Aaron Orndorff","William C. Headley"],"pdf_url":"https://arxiv.org/pdf/2410.03423v1.pdf","comment":"To be presented at MILCOM 2024, Washington DC"},{"id":"http://arxiv.org/abs/2408.03414v2","updated":"2024-10-04T13:24:59Z","published":"2024-08-06T19:23:42Z","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","summary":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","authors":["Marcus Buckmann","Edward Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03414v2.pdf","comment":"48 pages, 24 figures"},{"id":"http://arxiv.org/abs/2410.03411v1","updated":"2024-10-04T13:23:45Z","published":"2024-10-04T13:23:45Z","title":"Benchmarking the Fidelity and Utility of Synthetic Relational Data","summary":"  Synthesizing relational data has started to receive more attention from\nresearchers, practitioners, and industry. The task is more difficult than\nsynthesizing a single table due to the added complexity of relationships\nbetween tables. For the same reason, benchmarking methods for synthesizing\nrelational data introduces new challenges. Our work is motivated by a lack of\nan empirical evaluation of state-of-the-art methods and by gaps in the\nunderstanding of how such an evaluation should be done. We review related work\non relational data synthesis, common benchmarking datasets, and approaches to\nmeasuring the fidelity and utility of synthetic data. We combine the best\npractices and a novel robust detection approach into a benchmarking tool and\nuse it to compare six methods, including two commercial tools. While some\nmethods are better than others, no method is able to synthesize a dataset that\nis indistinguishable from original data. For utility, we typically observe\nmoderate correlation between real and synthetic data for both model predictive\nperformance and feature importance.\n","authors":["Valter Hudovernik","Martin Jurkovi","Erik trumbelj"],"pdf_url":"https://arxiv.org/pdf/2410.03411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03408v1","updated":"2024-10-04T13:17:34Z","published":"2024-10-04T13:17:34Z","title":"Predictive Coding for Decision Transformer","summary":"  Recent work in offline reinforcement learning (RL) has demonstrated the\neffectiveness of formulating decision-making as return-conditioned supervised\nlearning. Notably, the decision transformer (DT) architecture has shown promise\nacross various domains. However, despite its initial success, DTs have\nunderperformed on several challenging datasets in goal-conditioned RL. This\nlimitation stems from the inefficiency of return conditioning for guiding\npolicy learning, particularly in unstructured and suboptimal datasets,\nresulting in DTs failing to effectively learn temporal compositionality.\nMoreover, this problem might be further exacerbated in long-horizon\nsparse-reward tasks. To address this challenge, we propose the Predictive\nCoding for Decision Transformer (PCDT) framework, which leverages generalized\nfuture conditioning to enhance DT methods. PCDT utilizes an architecture that\nextends the DT framework, conditioned on predictive codings, enabling\ndecision-making based on both past and future factors, thereby improving\ngeneralization. Through extensive experiments on eight datasets from the\nAntMaze and FrankaKitchen environments, our proposed method achieves\nperformance on par with or surpassing existing popular value-based and\ntransformer-based methods in offline goal-conditioned RL. Furthermore, we also\nevaluate our method on a goal-reaching task with a physical robot.\n","authors":["Tung M. Luu","Donghoon Lee","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.03408v1.pdf","comment":"8 pages, IROS 2024 (Code: https://github.com/tunglm2203/pcdt)"},{"id":"http://arxiv.org/abs/2410.03406v1","updated":"2024-10-04T13:12:25Z","published":"2024-10-04T13:12:25Z","title":"Conformal confidence sets for biomedical image segmentation","summary":"  We develop confidence sets which provide spatial uncertainty guarantees for\nthe output of a black-box machine learning model designed for image\nsegmentation. To do so we adapt conformal inference to the imaging setting,\nobtaining thresholds on a calibration dataset based on the distribution of the\nmaximum of the transformed logit scores within and outside of the ground truth\nmasks. We prove that these confidence sets, when applied to new predictions of\nthe model, are guaranteed to contain the true unknown segmented mask with\ndesired probability. We show that learning appropriate score transformations on\na learning dataset before performing calibration is crucial for optimizing\nperformance. We illustrate and validate our approach on a polpys tumor dataset.\nTo do so we obtain the logit scores from a deep neural network trained for\npolpys segmentation and show that using distance transformed scores to obtain\nouter confidence sets and the original scores for inner confidence sets enables\ntight bounds on tumor location whilst controlling the false coverage rate.\n","authors":["Samuel Davenport"],"pdf_url":"https://arxiv.org/pdf/2410.03406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03403v1","updated":"2024-10-04T13:10:31Z","published":"2024-10-04T13:10:31Z","title":"Distributed Networked Multi-task Learning","summary":"  We consider a distributed multi-task learning scheme that accounts for\nmultiple linear model estimation tasks with heterogeneous and/or correlated\ndata streams. We assume that nodes can be partitioned into groups corresponding\nto different learning tasks and communicate according to a directed network\ntopology. Each node estimates a linear model asynchronously and is subject to\nlocal (within-group) regularization and global (across groups) regularization\nterms targeting noise reduction and generalization performance improvement\nrespectively. We provide a finite-time characterization of convergence of the\nestimators and task relation and illustrate the scheme's general applicability\nin two examples: random field temperature estimation and modeling student\nperformance from different academic districts.\n","authors":["Lingzhou Hong","Alfredo Garcia"],"pdf_url":"https://arxiv.org/pdf/2410.03403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03399v1","updated":"2024-10-04T13:03:43Z","published":"2024-10-04T13:03:43Z","title":"EBES: Easy Benchmarking for Event Sequences","summary":"  Event sequences, characterized by irregular sampling intervals and a mix of\ncategorical and numerical features, are common data structures in various\nreal-world domains such as healthcare, finance, and user interaction logs.\nDespite advances in temporal data modeling techniques, there is no standardized\nbenchmarks for evaluating their performance on event sequences. This\ncomplicates result comparison across different papers due to varying evaluation\nprotocols, potentially misleading progress in this field. We introduce EBES, a\ncomprehensive benchmarking tool with standardized evaluation scenarios and\nprotocols, focusing on regression and classification problems with\nsequence-level targets. Our library simplifies benchmarking, dataset addition,\nand method integration through a unified interface. It includes a novel\nsynthetic dataset and provides preprocessed real-world datasets, including the\nlargest publicly available banking dataset. Our results provide an in-depth\nanalysis of datasets, identifying some as unsuitable for model comparison. We\ninvestigate the importance of modeling temporal and sequential components, as\nwell as the robustness and scaling properties of the models. These findings\nhighlight potential directions for future research. Our benchmark aim is to\nfacilitate reproducible research, expediting progress and increasing real-world\nimpacts.\n","authors":["Dmitry Osin","Igor Udovichenko","Viktor Moskvoretskii","Egor Shvetsov","Evgeny Burnaev"],"pdf_url":"https://arxiv.org/pdf/2410.03399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03396v1","updated":"2024-10-04T12:59:45Z","published":"2024-10-04T12:59:45Z","title":"GraphCroc: Cross-Correlation Autoencoder for Graph Structural\n  Reconstruction","summary":"  Graph-structured data is integral to many applications, prompting the\ndevelopment of various graph representation methods. Graph autoencoders (GAEs),\nin particular, reconstruct graph structures from node embeddings. Current GAE\nmodels primarily utilize self-correlation to represent graph structures and\nfocus on node-level tasks, often overlooking multi-graph scenarios. Our\ntheoretical analysis indicates that self-correlation generally falls short in\naccurately representing specific graph features such as islands, symmetrical\nstructures, and directional edges, particularly in smaller or multiple graph\ncontexts. To address these limitations, we introduce a cross-correlation\nmechanism that significantly enhances the GAE representational capabilities.\nAdditionally, we propose GraphCroc, a new GAE that supports flexible encoder\narchitectures tailored for various downstream tasks and ensures robust\nstructural reconstruction, through a mirrored encoding-decoding process. This\nmodel also tackles the challenge of representation bias during optimization by\nimplementing a loss-balancing strategy. Both theoretical analysis and numerical\nevaluations demonstrate that our methodology significantly outperforms existing\nself-correlation-based GAEs in graph structure reconstruction.\n","authors":["Shijin Duan","Ruyi Ding","Jiaxing He","Aidong Adam Ding","Yunsi Fei","Xiaolin Xu"],"pdf_url":"https://arxiv.org/pdf/2410.03396v1.pdf","comment":"22 pages, 16 figures. Accepted in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.09206v2","updated":"2024-10-04T12:55:45Z","published":"2024-06-13T15:06:11Z","title":"Self-Training for Sample-Efficient Active Learning for Text\n  Classification with Pre-Trained Language Models","summary":"  Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. In\nthis work, we investigate how self-training, a semi-supervised approach that\nuses a model to obtain pseudo-labels for unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Building on a\ncomprehensive reproduction of four previous self-training approaches, some of\nwhich are evaluated for the first time in the context of active learning or\nnatural language processing, we introduce HAST, a new and effective\nself-training strategy, which is evaluated on four text classification\nbenchmarks. Our results show that it outperforms the reproduced self-training\napproaches and reaches classification results comparable to previous\nexperiments for three out of four datasets, using as little as 25% of the data.\nThe code is publicly available at\nhttps://github.com/chschroeder/self-training-for-sample-efficient-active-learning .\n","authors":["Christopher Schrder","Gerhard Heyer"],"pdf_url":"https://arxiv.org/pdf/2406.09206v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.03390v1","updated":"2024-10-04T12:54:21Z","published":"2024-10-04T12:54:21Z","title":"Lightning UQ Box: A Comprehensive Framework for Uncertainty\n  Quantification in Deep Learning","summary":"  Uncertainty quantification (UQ) is an essential tool for applying deep neural\nnetworks (DNNs) to real world tasks, as it attaches a degree of confidence to\nDNN outputs. However, despite its benefits, UQ is often left out of the\nstandard DNN workflow due to the additional technical knowledge required to\napply and evaluate existing UQ procedures. Hence there is a need for a\ncomprehensive toolbox that allows the user to integrate UQ into their modelling\nworkflow, without significant overhead. We introduce \\texttt{Lightning UQ Box}:\na unified interface for applying and evaluating various approaches to UQ. In\nthis paper, we provide a theoretical and quantitative comparison of the wide\nrange of state-of-the-art UQ methods implemented in our toolbox. We focus on\ntwo challenging vision tasks: (i) estimating tropical cyclone wind speeds from\ninfrared satellite imagery and (ii) estimating the power output of solar panels\nfrom RGB images of the sky. By highlighting the differences between methods our\nresults demonstrate the need for a broad and approachable experimental\nframework for UQ, that can be used for benchmarking UQ methods. The toolbox,\nexample implementations, and further information are available at:\nhttps://github.com/lightning-uq-box/lightning-uq-box\n","authors":["Nils Lehmann","Jakob Gawlikowski","Adam J. Stewart","Vytautas Jancauskas","Stefan Depeweg","Eric Nalisnick","Nina Maria Gottschling"],"pdf_url":"https://arxiv.org/pdf/2410.03390v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.03385v1","updated":"2024-10-04T12:52:37Z","published":"2024-10-04T12:52:37Z","title":"From Epilepsy Seizures Classification to Detection: A Deep\n  Learning-based Approach for Raw EEG Signals","summary":"  Epilepsy represents the most prevalent neurological disease in the world.\nOne-third of people suffering from mesial temporal lobe epilepsy (MTLE) exhibit\ndrug resistance, urging the need to develop new treatments. A key part in\nanti-seizure medication (ASM) development is the capability of detecting and\nquantifying epileptic seizures occurring in electroencephalogram (EEG) signals,\nwhich is crucial for treatment efficacy evaluation. In this study, we\nintroduced a seizure detection pipeline based on deep learning models applied\nto raw EEG signals. This pipeline integrates: a new pre-processing technique\nwhich segments continuous raw EEG signals without prior distinction between\nseizure and seizure-free activities; a post-processing algorithm developed to\nreassemble EEG segments and allow the identification of seizures start/end; and\nfinally, a new evaluation procedure based on a strict seizure events comparison\nbetween predicted and real labels. Models training have been performed using a\ndata splitting strategy which addresses the potential for data leakage. We\ndemonstrated the fundamental differences between a seizure classification and a\nseizure detection task and showed the differences in performance between the\ntwo tasks. Finally, we demonstrated the generalization capabilities across\nspecies of our best architecture, combining a Convolutional Neural Network and\na Transformer encoder. The model was trained on animal EEGs and tested on human\nEEGs with a F1-score of 93% on a balanced Bonn dataset.\n","authors":["Davy Darankoum","Manon Villalba","Clelia Allioux","Baptiste Caraballo","Carine Dumont","Eloise Gronlier","Corinne Roucard","Yann Roche","Chloe Habermacher","Sergei Grudinin","Julien Volle"],"pdf_url":"https://arxiv.org/pdf/2410.03385v1.pdf","comment":"25 pages, 7 tables, 4 figures"},{"id":"http://arxiv.org/abs/2405.21012v2","updated":"2024-10-04T12:50:11Z","published":"2024-05-31T16:52:51Z","title":"G-Transformer for Conditional Average Potential Outcome Estimation over\n  Time","summary":"  Estimating potential outcomes for treatments over time based on observational\ndata is important for personalized decision-making in medicine. Yet, existing\nneural methods for this task either (1) do not perform proper adjustments for\ntime-varying confounders, or (2) suffer from large estimation variance. In\norder to address both limitations, we introduce the G-transformer (GT). Our GT\nis a novel, neural end-to-end model which adjusts for time-varying confounders,\nand provides low-variance estimation of conditional average potential outcomes\n(CAPOs) over time. Specifically, our GT is the first neural model to perform\nregression-based iterative G-computation for CAPOs in the time-varying setting.\nWe evaluate the effectiveness of our GT across various experiments. In sum,\nthis work represents a significant step towards personalized decision-making\nfrom electronic health records.\n","authors":["Konstantin Hess","Dennis Frauen","Valentyn Melnychuk","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2405.21012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03380v1","updated":"2024-10-04T12:48:21Z","published":"2024-10-04T12:48:21Z","title":"Predicting perturbation targets with causal differential networks","summary":"  Rationally identifying variables responsible for changes to a biological\nsystem can enable myriad applications in disease understanding and cell\nengineering. From a causality perspective, we are given two datasets generated\nby the same causal model, one observational (control) and one interventional\n(perturbed). The goal is to isolate the subset of measured variables (e.g.\ngenes) that were the targets of the intervention, i.e. those whose conditional\nindependencies have changed. Knowing the causal graph would limit the search\nspace, allowing us to efficiently pinpoint these variables. However, current\nalgorithms that infer causal graphs in the presence of unknown intervention\ntargets scale poorly to the hundreds or thousands of variables in biological\ndata, as they must jointly search the combinatorial spaces of graphs and\nconsistent intervention targets. In this work, we propose a causality-inspired\napproach for predicting perturbation targets that decouples the two search\nsteps. First, we use an amortized causal discovery model to separately infer\ncausal graphs from the observational and interventional datasets. Then, we\nlearn to map these paired graphs to the sets of variables that were intervened\nupon, in a supervised learning framework. This approach consistently\noutperforms baselines for perturbation modeling on seven single-cell\ntranscriptomics datasets, each with thousands of measured variables. We also\ndemonstrate significant improvements over six causal discovery algorithms in\npredicting intervention targets across a variety of tractable, synthetic\ndatasets.\n","authors":["Menghua Wu","Umesh Padia","Sean H. Murphy","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2410.03380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14629v2","updated":"2024-10-04T12:47:03Z","published":"2024-05-23T14:35:56Z","title":"Which Experiences Are Influential for RL Agents? Efficiently Estimating\n  The Influence of Experiences","summary":"  In reinforcement learning (RL) with experience replay, experiences stored in\na replay buffer influence the RL agent's performance. Information about how\nthese experiences influence the agent's performance is valuable for various\npurposes, such as identifying experiences that negatively influence\nunderperforming agents. One method for estimating the influence of experiences\nis the leave-one-out (LOO) method. However, this method is usually\ncomputationally prohibitive. In this paper, we present Policy Iteration with\nTurn-over Dropout (PIToD), which efficiently estimates the influence of\nexperiences. We evaluate how accurately PIToD estimates the influence of\nexperiences and its efficiency compared to LOO. We then apply PIToD to amend\nunderperforming RL agents, i.e., we use PIToD to estimate negatively\ninfluential experiences for the RL agents and to delete the influence of these\nexperiences. We show that RL agents' performance is significantly improved via\namendments with PIToD.\n","authors":["Takuya Hiraoka","Guanquan Wang","Takashi Onishi","Yoshimasa Tsuruoka"],"pdf_url":"https://arxiv.org/pdf/2405.14629v2.pdf","comment":"Source code:\n  https://github.com/TakuyaHiraoka/Which-Experiences-Are-Influential-for-RL-Agents"},{"id":"http://arxiv.org/abs/2410.03376v1","updated":"2024-10-04T12:41:54Z","published":"2024-10-04T12:41:54Z","title":"Mitigating Adversarial Perturbations for Deep Reinforcement Learning via\n  Vector Quantization","summary":"  Recent studies reveal that well-performing reinforcement learning (RL) agents\nin training often lack resilience against adversarial perturbations during\ndeployment. This highlights the importance of building a robust agent before\ndeploying it in the real world. Most prior works focus on developing robust\ntraining-based procedures to tackle this problem, including enhancing the\nrobustness of the deep neural network component itself or adversarially\ntraining the agent on strong attacks. In this work, we instead study an input\ntransformation-based defense for RL. Specifically, we propose using a variant\nof vector quantization (VQ) as a transformation for input observations, which\nis then used to reduce the space of adversarial attacks during testing,\nresulting in the transformed observations being less affected by attacks. Our\nmethod is computationally efficient and seamlessly integrates with adversarial\ntraining, further enhancing the robustness of RL agents against adversarial\nattacks. Through extensive experiments in multiple environments, we demonstrate\nthat using VQ as the input transformation effectively defends against\nadversarial attacks on the agent's observations.\n","authors":["Tung M. Luu","Thanh Nguyen","Tee Joshua Tian Jin","Sungwoon Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.03376v1.pdf","comment":"8 pages, IROS 2024 (Code: https://github.com/tunglm2203/vq_robust_rl)"},{"id":"http://arxiv.org/abs/2410.03373v1","updated":"2024-10-04T12:39:46Z","published":"2024-10-04T12:39:46Z","title":"Make Interval Bound Propagation great again","summary":"  In various scenarios motivated by real life, such as medical data analysis,\nautonomous driving, and adversarial training, we are interested in robust deep\nnetworks. A network is robust when a relatively small perturbation of the input\ncannot lead to drastic changes in output (like change of class, etc.). This\nfalls under the broader scope field of Neural Network Certification (NNC). Two\ncrucial problems in NNC are of profound interest to the scientific community:\nhow to calculate the robustness of a given pre-trained network and how to\nconstruct robust networks. The common approach to constructing robust networks\nis Interval Bound Propagation (IBP). This paper demonstrates that IBP is\nsub-optimal in the first case due to its susceptibility to the wrapping effect.\nEven for linear activation, IBP gives strongly sub-optimal bounds.\nConsequently, one should use strategies immune to the wrapping effect to obtain\nbounds close to optimal ones. We adapt two classical approaches dedicated to\nstrict computations -- Dubleton Arithmetic and Affine Arithmetic -- to mitigate\nthe wrapping effect in neural networks. These techniques yield precise results\nfor networks with linear activation functions, thus resisting the wrapping\neffect. As a result, we achieve bounds significantly closer to the optimal\nlevel than IBPs.\n","authors":["Patryk Krukowski","Daniel Wilczak","Jacek Tabor","Anna Bielawska","Przemysaw Spurek"],"pdf_url":"https://arxiv.org/pdf/2410.03373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18338v2","updated":"2024-10-04T12:39:37Z","published":"2024-09-26T23:23:27Z","title":"AQMLator -- An Auto Quantum Machine Learning E-Platform","summary":"  A successful Machine Learning (ML) model implementation requires three main\ncomponents: training dataset, suitable model architecture and training\nprocedure. Given dataset and task, finding an appropriate model might be\nchallenging. AutoML, a branch of ML, focuses on automatic architecture search\n-- a meta method that aims at moving human from ML system design process. The\nsuccess of ML and the development of quantum computing (QC) in recent years led\nto a birth of new fascinating field called Quantum Machine Learning (QML) that,\namongst others, incorporates quantum computers into ML models. In this paper we\npresent AQMLator, an Auto Quantum Machine Learning platform that aims to\nautomatically propose and train the quantum layers of an ML model with minimal\ninput from the user. This way, data scientists can bypass the entry barrier for\nQC and use QML. AQMLator uses standard ML libraries, making it easy to\nintroduce into existing ML pipelines.\n","authors":["Tomasz Rybotycki","Piotr Gawron"],"pdf_url":"https://arxiv.org/pdf/2409.18338v2.pdf","comment":"15 pages, 3 figures, links to software in the text"},{"id":"http://arxiv.org/abs/2405.19874v2","updated":"2024-10-04T12:39:20Z","published":"2024-05-30T09:28:56Z","title":"Is In-Context Learning Sufficient for Instruction Following in LLMs?","summary":"  In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.\n","authors":["Hao Zhao","Maksym Andriushchenko","Francesco Croce","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2405.19874v2.pdf","comment":"Preprint. Code at https://github.com/tml-epfl/icl-alignment"},{"id":"http://arxiv.org/abs/2410.03368v1","updated":"2024-10-04T12:34:24Z","published":"2024-10-04T12:34:24Z","title":"Latent Abstractions in Generative Diffusion Models","summary":"  In this work we study how diffusion-based generative models produce\nhigh-dimensional data, such as an image, by implicitly relying on a\nmanifestation of a low-dimensional set of latent abstractions, that guide the\ngenerative process. We present a novel theoretical framework that extends NLF,\nand that offers a unique perspective on SDE-based generative models. The\ndevelopment of our theory relies on a novel formulation of the joint (state and\nmeasurement) dynamics, and an information-theoretic measure of the influence of\nthe system state on the measurement process. According to our theory, diffusion\nmodels can be cast as a system of SDE, describing a non-linear filter in which\nthe evolution of unobservable latent abstractions steers the dynamics of an\nobservable measurement process (corresponding to the generative pathways). In\naddition, we present an empirical study to validate our theory and previous\nempirical results on the emergence of latent abstractions at different stages\nof the generative process.\n","authors":["Giulio Franzese","Mattia Martini","Giulio Corallo","Paolo Papotti","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2410.03368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03364v1","updated":"2024-10-04T12:30:42Z","published":"2024-10-04T12:30:42Z","title":"Error Correction Code Transformer: From Non-Unified to Unified","summary":"  Channel coding is vital for reliable data transmission in modern wireless\nsystems, and its significance will increase with the emergence of\nsixth-generation (6G) networks, which will need to support various error\ncorrection codes. However, traditional decoders were typically designed as\nfixed hardware circuits tailored to specific decoding algorithms, leading to\ninefficiencies and limited flexibility. To address these challenges, this paper\nproposes a unified, code-agnostic Transformer-based decoding architecture\ncapable of handling multiple linear block codes, including Polar, Low-Density\nParity-Check (LDPC), and Bose-Chaudhuri-Hocquenghem (BCH), within a single\nframework. To achieve this, standardized units are employed to harmonize\nparameters across different code types, while the redesigned unified attention\nmodule compresses the structural information of various codewords.\nAdditionally, a sparse mask, derived from the sparsity of the parity-check\nmatrix, is introduced to enhance the model's ability to capture inherent\nconstraints between information and parity-check bits, resulting in improved\ndecoding accuracy and robustness. Extensive experimental results demonstrate\nthat the proposed unified Transformer-based decoder not only outperforms\nexisting methods but also provides a flexible, efficient, and high-performance\nsolution for next-generation wireless communication systems.\n","authors":["Yongli Yan","Jieao Zhu","Tianyue Zheng","Jiaqi He","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2410.03364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14557v3","updated":"2024-10-04T12:16:14Z","published":"2024-01-25T22:54:39Z","title":"Comparison of Reservoir Computing topologies using the Recurrent Kernel\n  approach","summary":"  Reservoir Computing (RC) has become popular in recent years thanks to its\nfast and efficient computational capabilities. Standard RC has been shown to be\nequivalent in the asymptotic limit to Recurrent Kernels, which helps in\nanalyzing its expressive power. However, many well-established RC paradigms,\nsuch as Leaky RC, Sparse RC, and Deep RC, are yet to be systematically analyzed\nin such a way. We define the Recurrent Kernel limit of all these RC topologies\nand conduct a convergence study for a wide range of activation functions and\nhyperparameters. Our findings provide new insights into various aspects of\nReservoir Computing. First, we demonstrate that there is an optimal sparsity\nlevel which grows with the reservoir size. Furthermore, our analysis suggests\nthat Deep RC should use reservoir layers of decreasing sizes. Finally, we\nperform a benchmark demonstrating the efficiency of Structured Reservoir\nComputing compared to vanilla and Sparse Reservoir Computing.\n","authors":["Giuseppe Alessio D'Inverno","Jonathan Dong"],"pdf_url":"https://arxiv.org/pdf/2401.14557v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14836v3","updated":"2024-10-04T12:15:02Z","published":"2024-04-23T08:42:35Z","title":"Probabilistic forecasting of power system imbalance using neural\n  network-based ensembles","summary":"  Keeping the balance between electricity generation and consumption is\nbecoming increasingly challenging and costly, mainly due to the rising share of\nrenewables, electric vehicles and heat pumps and electrification of industrial\nprocesses. Accurate imbalance forecasts, along with reliable uncertainty\nestimations, enable transmission system operators (TSOs) to dispatch\nappropriate reserve volumes, reducing balancing costs. Further, market parties\ncan use these probabilistic forecasts to design strategies that exploit asset\nflexibility to help balance the grid, generating revenue with known risks.\nDespite its importance, literature regarding system imbalance (SI) forecasting\nis limited. Further, existing methods do not focus on situations with high\nimbalance magnitude, which are crucial to forecast accurately for both TSOs and\nmarket parties. Hence, we propose an ensemble of C-VSNs, which are our\nadaptation of variable selection networks (VSNs). Each minute, our model\npredicts the imbalance of the current and upcoming two quarter-hours, along\nwith uncertainty estimations on these forecasts. We evaluate our approach by\nforecasting the imbalance of Belgium, where high imbalance magnitude is defined\nas $|$SI$| > 500\\,$MW (occurs 1.3% of the time in Belgium). For high imbalance\nmagnitude situations, our model outperforms the state-of-the-art by 23.4% (in\nterms of continuous ranked probability score (CRPS), which evaluates\nprobabilistic forecasts), while also attaining a 6.5% improvement in overall\nCRPS. Similar improvements are achieved in terms of root-mean-squared error.\nAdditionally, we developed a fine-tuning methodology to effectively include new\ninputs with limited history in our model. This work was performed in\ncollaboration with Elia (the Belgian TSO) to further improve their imbalance\nforecasts, demonstrating the relevance of our work.\n","authors":["Jonas Van Gompel","Bert Claessens","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2404.14836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03348v1","updated":"2024-10-04T12:12:36Z","published":"2024-10-04T12:12:36Z","title":"Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning","summary":"  Neurosymbolic learning has emerged as a promising paradigm to incorporate\nsymbolic reasoning into deep learning models. However, existing frameworks are\nlimited in scalability with respect to both the training data and the\ncomplexity of symbolic programs. We propose Dolphin, a framework to scale\nneurosymbolic learning at a fundamental level by mapping both forward chaining\nand backward gradient propagation in symbolic programs to vectorized\ncomputations. For this purpose, Dolphin introduces a set of abstractions and\nprimitives built directly on top of a high-performance deep learning framework\nlike PyTorch, effectively enabling symbolic programs to be written as PyTorch\nmodules. It thereby enables neurosymbolic programs to be written in a language\nlike Python that is familiar to developers and compile them to computation\ngraphs that are amenable to end-to-end differentiation on GPUs. We evaluate\nDolphin on a suite of 13 benchmarks across 5 neurosymbolic tasks that combine\ndeep learning models for text, image, or video processing with symbolic\nprograms that involve multi-hop reasoning, recursion, and even black-box\nfunctions like Python eval(). Dolphin only takes 0.33%-37.17% of the time (and\n2.77% on average) to train these models on the largest input per task compared\nto baselines Scallop, ISED, and IndeCateR+, which time out on most of these\ninputs. Models written in Dolphin also achieve state-of-the-art accuracies even\non the largest benchmarks.\n","authors":["Aaditya Naik","Jason Liu","Claire Wang","Saikat Dutta","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2410.03348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14432v3","updated":"2024-10-04T11:58:58Z","published":"2024-05-23T11:00:31Z","title":"The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed\n  Learning","summary":"  Byzantine-resilient distributed machine learning seeks to achieve robust\nlearning performance in the presence of misbehaving or adversarial workers.\nWhile state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD)\nmethods were proven theoretically optimal, their empirical success has often\nrelied on pre-aggregation gradient clipping. However, the currently considered\nstatic clipping strategy exhibits mixed results: improving robustness against\nsome attacks while being ineffective or detrimental against others. We address\nthis gap by proposing a principled adaptive clipping strategy, termed Adaptive\nRobust Clipping (ARC). We show that ARC consistently enhances the empirical\nrobustness of SOTA Robust-DGD methods, while preserving the theoretical\nrobustness guarantees. Our analysis shows that ARC provably improves the\nasymptotic convergence guarantee of Robust-DGD in the case when the model is\nwell-initialized. We validate this theoretical insight through an exhaustive\nset of experiments on benchmark image classification tasks. We observe that the\nimprovement induced by ARC is more pronounced in highly heterogeneous and\nadversarial settings.\n","authors":["Youssef Allouah","Rachid Guerraoui","Nirupam Gupta","Ahmed Jellouli","Geovani Rizk","John Stephan"],"pdf_url":"https://arxiv.org/pdf/2405.14432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19000v2","updated":"2024-10-04T11:49:30Z","published":"2024-07-26T17:55:23Z","title":"Reinforcement learning for anisotropic p-adaptation and error estimation\n  in high-order solvers","summary":"  We present a novel approach to automate and optimize anisotropic p-adaptation\nin high-order h/p solvers using Reinforcement Learning (RL). The dynamic RL\nadaptation uses the evolving solution to adjust the high-order polynomials. We\ndevelop an offline training approach, decoupled from the main solver, which\nshows minimal overcost when performing simulations. In addition, we derive an\ninexpensive RL-based error estimation approach that enables the quantification\nof local discretization errors. The proposed methodology is agnostic to both\nthe computational mesh and the partial differential equation to be solved.\n  The application of RL to mesh adaptation offers several benefits. It enables\nautomated and adaptive mesh refinement, reducing the need for manual\nintervention. It optimizes computational resources by dynamically allocating\nhigh-order polynomials where necessary and minimizing refinement in stable\nregions. This leads to computational cost savings while maintaining the\naccuracy of the solution. Furthermore, RL allows for the exploration of\nunconventional mesh adaptations, potentially enhancing the accuracy and\nrobustness of simulations. This work extends our original research, offering a\nmore robust, reproducible, and generalizable approach applicable to complex\nthree-dimensional problems. We provide validation for laminar and turbulent\ncases: circular cylinders, Taylor Green Vortex and a 10MW wind turbine to\nillustrate the flexibility of the proposed approach.\n","authors":["David Huergo","Martn de Frutos","Eduardo Jan","Oscar A. Marino","Gonzalo Rubio","Esteban Ferrer"],"pdf_url":"https://arxiv.org/pdf/2407.19000v2.pdf","comment":"38 pages, 18 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.03335v1","updated":"2024-10-04T11:40:53Z","published":"2024-10-04T11:40:53Z","title":"Audio-Agent: Leveraging LLMs For Audio Generation, Editing and\n  Composition","summary":"  We introduce Audio-Agent, a multimodal framework for audio generation,\nediting and composition based on text or video inputs. Conventional approaches\nfor text-to-audio (TTA) tasks often make single-pass inferences from text\ndescriptions. While straightforward, this design struggles to produce\nhigh-quality audio when given complex text conditions. In our method, we\nutilize a pre-trained TTA diffusion network as the audio generation agent to\nwork in tandem with GPT-4, which decomposes the text condition into atomic,\nspecific instructions, and calls the agent for audio generation. Consequently,\nAudio-Agent generates high-quality audio that is closely aligned with the\nprovided text or video while also supporting variable-length generation. For\nvideo-to-audio (VTA) tasks, most existing methods require training a timestamp\ndetector to synchronize video events with generated audio, a process that can\nbe tedious and time-consuming. We propose a simpler approach by fine-tuning a\npre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both\nsemantic and temporal conditions to bridge video and audio modality. Thus our\nframework provides a comprehensive solution for both TTA and VTA tasks without\nsubstantial computational overhead in training.\n","authors":["Zixuan Wang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2410.03335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12915v2","updated":"2024-10-04T11:36:48Z","published":"2024-06-13T17:54:09Z","title":"GROD: Enhancing Generalization of Transformer with Out-of-Distribution\n  Detection","summary":"  Transformer networks excel in natural language processing (NLP) and computer\nvision (CV) tasks. However, they face challenges in generalizing to\nOut-of-Distribution (OOD) datasets, that is, data whose distribution differs\nfrom that seen during training. The OOD detection aims to distinguish data that\ndeviates from the expected distribution, while maintaining optimal performance\non in-distribution (ID) data. This paper introduces a novel approach based on\nOOD detection, termed the Generate Rounded OOD Data (GROD) algorithm, which\nsignificantly bolsters the generalization performance of transformer networks\nacross various tasks. GROD is motivated by our new OOD detection Probably\nApproximately Correct (PAC) Theory for transformer. The transformer has\nlearnability in terms of OOD detection that is, when the data is sufficient the\noutlier can be well represented. By penalizing the misclassification of OOD\ndata within the loss function and generating synthetic outliers, GROD\nguarantees learnability and refines the decision boundaries between inlier and\noutlier. This strategy demonstrates robust adaptability and general\napplicability across different data types. Evaluated across diverse OOD\ndetection tasks in NLP and CV, GROD achieves SOTA regardless of data format.\nThe code is available at\nhttps://anonymous.4open.science/r/GROD-OOD-Detection-with-transformers-B70F.\n","authors":["Yijin Zhou","Yuguang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00524v2","updated":"2024-10-04T11:32:31Z","published":"2024-10-01T09:07:24Z","title":"Deep Model Interpretation with Limited Data : A Coreset-based Approach","summary":"  Model Interpretation aims at the extraction of insights from the internals of\na trained model. A common approach to address this task is the characterization\nof relevant features internally encoded in the model that are critical for its\nproper operation. Despite recent progress of these methods, they come with the\nweakness of being computationally expensive due to the dense evaluation of\ndatasets that they require. As a consequence, research on the design of these\nmethods have focused on smaller data subsets which may led to reduced insights.\nTo address these computational costs, we propose a coreset-based interpretation\nframework that utilizes coreset selection methods to sample a representative\nsubset of the large dataset for the interpretation task. Towards this goal, we\npropose a similarity-based evaluation protocol to assess the robustness of\nmodel interpretation methods towards the amount data they take as input.\nExperiments considering several interpretation methods, DNN models, and coreset\nselection methods show the effectiveness of the proposed framework.\n","authors":["Hamed Behzadi-Khormouji","Jos Oramas"],"pdf_url":"https://arxiv.org/pdf/2410.00524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06577v3","updated":"2024-10-04T11:25:21Z","published":"2023-09-11T08:05:09Z","title":"Efficient Finite Initialization for Tensorized Neural Networks","summary":"  We present a novel method for initializing layers of tensorized neural\nnetworks in a way that avoids the explosion of the parameters of the matrix it\nemulates. The method is intended for layers with a high number of nodes in\nwhich there is a connection to the input or output of all or most of the nodes,\nwe cannot or do not want to store/calculate all the elements of the represented\nlayer and they follow a smooth distribution. This method is equally applicable\nto normalize general tensor networks in which we want to avoid overflows.\n  The core of this method is the use of the Frobenius norm and the partial\nlineal entrywise norm of reduced forms of the layer in an iterative partial\nform, so that it has to be finite and within a certain range. These norms are\nefficient to compute, fully or partially for most cases of interest. In\naddition, the method benefits from the reuse of intermediate calculations. We\napply the method to different layers and check its performance. We create a\nPython function to run it on an arbitrary layer, available in a Jupyter\nNotebook in the i3BQuantum repository:\nhttps://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/TN_Normalizer.ipynb\n","authors":["Alejandro Mata Ali","Iigo Perez Delgado","Marina Ristol Roura","Aitor Moreno Fdez. de Leceta"],"pdf_url":"https://arxiv.org/pdf/2309.06577v3.pdf","comment":"8 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.02069v2","updated":"2024-10-04T11:15:49Z","published":"2024-10-02T22:36:12Z","title":"Semi-Supervised Fine-Tuning of Vision Foundation Models with\n  Content-Style Decomposition","summary":"  In this paper, we present a semi-supervised fine-tuning approach designed to\nimprove the performance of pre-trained foundation models on downstream tasks\nwith limited labeled data. By leveraging content-style decomposition within an\ninformation-theoretic framework, our method enhances the latent representations\nof pre-trained vision foundation models, aligning them more effectively with\nspecific task objectives and addressing the problem of distribution shift. We\nevaluate our approach on multiple datasets, including MNIST, its augmented\nvariations (with yellow and white stripes), CIFAR-10, SVHN, and GalaxyMNIST.\nThe experiments show improvements over supervised finetuning baseline of\npre-trained models, particularly in low-labeled data regimes, across both\nfrozen and trainable backbones for the majority of the tested datasets.\n","authors":["Mariia Drozdova","Vitaliy Kinakh","Yury Belousov","Erica Lastufka","Slava Voloshynovskiy"],"pdf_url":"https://arxiv.org/pdf/2410.02069v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2409.17591v2","updated":"2024-10-04T11:08:32Z","published":"2024-09-26T07:16:38Z","title":"Conjugate Bayesian Two-step Change Point Detection for Hawkes Process","summary":"  The Bayesian two-step change point detection method is popular for the Hawkes\nprocess due to its simplicity and intuitiveness. However, the non-conjugacy\nbetween the point process likelihood and the prior requires most existing\nBayesian two-step change point detection methods to rely on non-conjugate\ninference methods. These methods lack analytical expressions, leading to low\ncomputational efficiency and impeding timely change point detection. To address\nthis issue, this work employs data augmentation to propose a conjugate Bayesian\ntwo-step change point detection method for the Hawkes process, which proves to\nbe more accurate and efficient. Extensive experiments on both synthetic and\nreal data demonstrate the superior effectiveness and efficiency of our method\ncompared to baseline methods. Additionally, we conduct ablation studies to\nexplore the robustness of our method concerning various hyperparameters. Our\ncode is publicly available at https://github.com/Aurora2050/CoBay-CPD.\n","authors":["Zeyue Zhang","Xiaoling Lu","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.17591v2.pdf","comment":"10 pages, accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.14654v2","updated":"2024-10-04T11:08:06Z","published":"2024-06-20T18:17:58Z","title":"Major Entity Identification: A Generalizable Alternative to Coreference\n  Resolution","summary":"  The limited generalization of coreference resolution (CR) models has been a\nmajor bottleneck in the task's broad application. Prior work has identified\nannotation differences, especially for mention detection, as one of the main\nreasons for the generalization gap and proposed using additional annotated\ntarget domain data. Rather than relying on this additional annotation, we\npropose an alternative referential task, Major Entity Identification (MEI),\nwhere we: (a) assume the target entities to be specified in the input, and (b)\nlimit the task to only the frequent entities. Through extensive experiments, we\ndemonstrate that MEI models generalize well across domains on multiple datasets\nwith supervised models and LLM-based few-shot prompting. Additionally, MEI fits\nthe classification framework, which enables the use of robust and intuitive\nclassification-based metrics. Finally, MEI is also of practical use as it\nallows a user to search for all mentions of a particular entity or a group of\nentities of interest.\n","authors":["Kawshik Manikantan","Shubham Toshniwal","Makarand Tapaswi","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2406.14654v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.02131v2","updated":"2024-10-04T11:05:18Z","published":"2024-10-03T01:24:09Z","title":"C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language\n  Pre-Training","summary":"  Accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with their\naccompanying textual reports holds immense potential to enhance clinical\ndiagnostics through the combination of physiological data and qualitative\ninsights. However, this integration faces significant challenges due to\ninherent modality disparities and the scarcity of labeled data for robust\ncross-modal learning. To address these obstacles, we propose C-MELT, a novel\nframework that pre-trains ECG and text data using a contrastive masked\nauto-encoder architecture. C-MELT uniquely combines the strengths of generative\nwith enhanced discriminative capabilities to achieve robust cross-modal\nrepresentations. This is accomplished through masked modality modeling,\nspecialized loss functions, and an improved negative sampling strategy tailored\nfor cross-modal alignment. Extensive experiments on five public datasets across\ndiverse downstream tasks demonstrate that C-MELT significantly outperforms\nexisting methods, achieving 15% and 2% increases in linear probing and\nzero-shot performance over state-of-the-art models, respectively. These results\nhighlight the effectiveness of C-MELT, underscoring its potential to advance\nautomated clinical diagnostics through multi-modal representations.\n","authors":["Manh Pham","Aaqib Saeed","Dong Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02131v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03315v1","updated":"2024-10-04T11:00:17Z","published":"2024-10-04T11:00:17Z","title":"Influence-oriented Personalized Federated Learning","summary":"  Traditional federated learning (FL) methods often rely on fixed weighting for\nparameter aggregation, neglecting the mutual influence by others. Hence, their\neffectiveness in heterogeneous data contexts is limited. To address this\nproblem, we propose an influence-oriented federated learning framework, namely\nFedC^2I, which quantitatively measures Client-level and Class-level Influence\nto realize adaptive parameter aggregation for each client. Our core idea is to\nexplicitly model the inter-client influence within an FL system via the\nwell-crafted influence vector and influence matrix. The influence vector\nquantifies client-level influence, enables clients to selectively acquire\nknowledge from others, and guides the aggregation of feature representation\nlayers. Meanwhile, the influence matrix captures class-level influence in a\nmore fine-grained manner to achieve personalized classifier aggregation. We\nevaluate the performance of FedC^2I against existing federated learning methods\nunder non-IID settings and the results demonstrate the superiority of our\nmethod.\n","authors":["Yue Tan","Guodong Long","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03311v1","updated":"2024-10-04T10:48:54Z","published":"2024-10-04T10:48:54Z","title":"Quo Vadis, Motion Generation? From Large Language Models to Large Motion\n  Models","summary":"  Inspired by the recent success of LLMs, the field of human motion\nunderstanding has increasingly shifted towards the development of large motion\nmodels. Despite some progress, current state-of-the-art works remain far from\nachieving truly generalist models, largely due to the lack of large-scale,\nhigh-quality motion data. To address this, we present MotionBase, the first\nmillion-level motion generation benchmark, offering 15 times the data volume of\nthe previous largest dataset, and featuring multimodal data with hierarchically\ndetailed text descriptions. By leveraging this vast dataset, our large motion\nmodel demonstrates strong performance across a broad range of motions,\nincluding unseen ones. Through systematic investigation, we underscore the\nimportance of scaling both data and model size, with synthetic data and pseudo\nlabels playing a crucial role in mitigating data acquisition costs. Moreover,\nour research reveals the limitations of existing evaluation metrics,\nparticularly in handling out-of-domain text instructions -- an issue that has\nlong been overlooked. In addition to these, we introduce a novel 2D lookup-free\napproach for motion tokenization, which preserves motion information and\nexpands codebook capacity, further enhancing the representative ability of\nlarge motion models. The release of MotionBase and the insights gained from\nthis study are expected to pave the way for the development of more powerful\nand versatile motion generation models.\n","authors":["Ye Wang","Sipeng Zheng","Bin Cao","Qianshan Wei","Qin Jin","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.03311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03306v1","updated":"2024-10-04T10:43:34Z","published":"2024-10-04T10:43:34Z","title":"Selective Test-Time Adaptation for Unsupervised Anomaly Detection using\n  Neural Implicit Representations","summary":"  Deep learning models in medical imaging often encounter challenges when\nadapting to new clinical settings unseen during training. Test-time adaptation\noffers a promising approach to optimize models for these unseen domains, yet\nits application in anomaly detection (AD) remains largely unexplored. AD aims\nto efficiently identify deviations from normative distributions; however, full\nadaptation, including pathological shifts, may inadvertently learn the\nanomalies it intends to detect. We introduce a novel concept of\n\\emph{selective} test-time adaptation that utilizes the inherent\ncharacteristics of deep pre-trained features to adapt \\emph{selectively} in a\nzero-shot manner to any test image from an unseen domain. This approach employs\na model-agnostic, lightweight multi-layer perceptron for neural implicit\nrepresentations, enabling the adaptation of outputs from any\nreconstruction-based AD method without altering the source-trained model.\nRigorous validation in brain AD demonstrated that our strategy substantially\nenhances detection accuracy for multiple conditions and different target\ndistributions. Specifically, our method improves the detection rates by up to\n78\\% for enlarged ventricles and 24\\% for edemas.\n","authors":["Sameer Ambekar","Julia A. Schnabel","Cosmin Bereca"],"pdf_url":"https://arxiv.org/pdf/2410.03306v1.pdf","comment":"Accepted at MICCAIw ADSMI"},{"id":"http://arxiv.org/abs/2402.07594v2","updated":"2024-10-04T10:41:18Z","published":"2024-02-12T11:48:54Z","title":"Foundational Inference Models for Dynamical Systems","summary":"  Dynamical systems governed by ordinary differential equations (ODEs) serve as\nmodels for a vast number of natural and social phenomena. In this work, we\noffer a fresh perspective on the classical problem of imputing missing time\nseries data, whose underlying dynamics are assumed to be determined by ODEs.\nSpecifically, we revisit ideas from amortized inference and neural operators,\nand propose a novel supervised learning framework for zero-shot time series\nimputation, through parametric functions satisfying some (hidden) ODEs. Our\nproposal consists of two components. First, a broad probability distribution\nover the space of ODE solutions, observation times and noise mechanisms, with\nwhich we generate a large, synthetic dataset of (hidden) ODE solutions, along\nwith their noisy and sparse observations. Second, a neural recognition model\nthat is trained offline, to map the generated time series onto the spaces of\ninitial conditions and time derivatives of the (hidden) ODE solutions, which we\nthen integrate to impute the missing data. We empirically demonstrate that one\nand the same (pretrained) recognition model can perform zero-shot imputation\nacross 63 distinct time series with missing values, each sampled from widely\ndifferent dynamical systems. Likewise, we demonstrate that it can perform\nzero-shot imputation of missing high-dimensional data in 10 vastly different\nsettings, spanning human motion, air quality, traffic and electricity studies,\nas well as Navier-Stokes simulations -- without requiring any fine-tuning. What\nis more, our proposal often outperforms state-of-the-art methods, which are\ntrained on the target datasets.\n  Our pretrained model will be available online soon.\n","authors":["Patrick Seifner","Kostadin Cvejoski","Antonia Krner","Ramss J. Snchez"],"pdf_url":"https://arxiv.org/pdf/2402.07594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03303v1","updated":"2024-10-04T10:40:11Z","published":"2024-10-04T10:40:11Z","title":"SELU: Self-Learning Embodied MLLMs in Unknown Environments","summary":"  Recently, multimodal large language models (MLLMs) have demonstrated strong\nvisual understanding and decision-making capabilities, enabling the exploration\nof autonomously improving MLLMs in unknown environments. However, external\nfeedback like human or environmental feedback is not always available. To\naddress this challenge, existing methods primarily focus on enhancing the\ndecision-making capabilities of MLLMs through voting and scoring mechanisms,\nwhile little effort has been paid to improving the environmental comprehension\nof MLLMs in unknown environments. To fully unleash the self-learning potential\nof MLLMs, we propose a novel actor-critic self-learning paradigm, dubbed SELU,\ninspired by the actor-critic paradigm in reinforcement learning. The critic\nemploys self-asking and hindsight relabeling to extract knowledge from\ninteraction trajectories collected by the actor, thereby augmenting its\nenvironmental comprehension. Simultaneously, the actor is improved by the\nself-feedback provided by the critic, enhancing its decision-making. We\nevaluate our method in the AI2-THOR and VirtualHome environments, and SELU\nachieves critic improvements of approximately 28% and 30%, and actor\nimprovements of about 20% and 24% via self-learning.\n","authors":["Boyu Li","Haobin Jiang","Ziluo Ding","Xinrun Xu","Haoran Li","Dongbin Zhao","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2410.03303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11041v4","updated":"2024-10-04T10:15:10Z","published":"2024-07-06T15:03:40Z","title":"Integer-only Quantized Transformers for Embedded FPGA-based Time-series\n  Forecasting in AIoT","summary":"  This paper presents the design of a hardware accelerator for Transformers,\noptimized for on-device time-series forecasting in AIoT systems. It integrates\ninteger-only quantization and Quantization-Aware Training with optimized\nhardware designs to realize 6-bit and 4-bit quantized Transformer models, which\nachieved precision comparable to 8-bit quantized models from related research.\nUtilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7\nXC7S15), we examine the feasibility of deploying Transformer models on embedded\nIoT devices. This includes a thorough analysis of achievable precision,\nresource utilization, timing, power, and energy consumption for on-device\ninference. Our results indicate that while sufficient performance can be\nattained, the optimization process is not trivial. For instance, reducing the\nquantization bitwidth does not consistently result in decreased latency or\nenergy consumption, underscoring the necessity of systematically exploring\nvarious optimization combinations. Compared to an 8-bit quantized Transformer\nmodel in related studies, our 4-bit quantized Transformer model increases test\nloss by only 0.63%, operates up to 132.33x faster, and consumes 48.19x less\nenergy.\n","authors":["Tianheng Ling","Chao Qian","Gregor Schiele"],"pdf_url":"https://arxiv.org/pdf/2407.11041v4.pdf","comment":"Accepted by 2024 IEEE Annual Congress on Artificial Intelligence of\n  Things (IEEE AIoT) and got best paper award. 7 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.03294v1","updated":"2024-10-04T10:12:24Z","published":"2024-10-04T10:12:24Z","title":"Resource-aware Mixed-precision Quantization for Enhancing Deployability\n  of Transformers for Time-series Forecasting on Embedded FPGAs","summary":"  This study addresses the deployment challenges of integer-only quantized\nTransformers on resource-constrained embedded FPGAs (Xilinx Spartan-7 XC7S15).\nWe enhanced the flexibility of our VHDL template by introducing a selectable\nresource type for storing intermediate results across model layers, thereby\nbreaking the deployment bottleneck by utilizing BRAM efficiently. Moreover, we\ndeveloped a resource-aware mixed-precision quantization approach that enables\nresearchers to explore hardware-level quantization strategies without requiring\nextensive expertise in Neural Architecture Search. This method provides\naccurate resource utilization estimates with a precision discrepancy as low as\n3%, compared to actual deployment metrics. Compared to previous work, our\napproach has successfully facilitated the deployment of model configurations\nutilizing mixed-precision quantization, thus overcoming the limitations\ninherent in five previously non-deployable configurations with uniform\nquantization bitwidths. Consequently, this research enhances the applicability\nof Transformers in embedded systems, facilitating a broader range of\nTransformer-powered applications on edge devices.\n","authors":["Tianheng Ling","Chao Qian","Gregor Schiele"],"pdf_url":"https://arxiv.org/pdf/2410.03294v1.pdf","comment":"Accepted by the 21st EAI International Conference on Mobile and\n  Ubiquitous Systems: Computing, Networking and Services (MobiQuitous2024). 20\n  pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.03293v1","updated":"2024-10-04T10:06:55Z","published":"2024-10-04T10:06:55Z","title":"Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram\n  Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis","summary":"  The work presented in this paper makes three scientific contributions with a\nspecific focus on mining and analysis of COVID-19-related posts on Instagram.\nFirst, it presents a multilingual dataset of 500,153 Instagram posts about\nCOVID-19 published between January 2020 and September 2024. This dataset,\navailable at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in\n161 different languages as well as 535,021 distinct hashtags. After the\ndevelopment of this dataset, multilingual sentiment analysis was performed,\nwhich involved classifying each post as positive, negative, or neutral. The\nresults of sentiment analysis are presented as a separate attribute in this\ndataset. Second, it presents the results of performing sentiment analysis per\nyear from 2020 to 2024. The findings revealed the trends in sentiment related\nto COVID-19 on Instagram since the beginning of the pandemic. For instance,\nbetween 2020 and 2024, the sentiment trends show a notable shift, with positive\nsentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from\n44.19% to 58.34%. Finally, the paper also presents findings of\nlanguage-specific sentiment analysis. This analysis highlighted similar and\ncontrasting trends of sentiment across posts published in different languages\non Instagram. For instance, out of all English posts, 49.68% were positive,\n14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,\n4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting\ndistinct differences in the sentiment distribution between these two languages.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.03293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03292v1","updated":"2024-10-04T10:06:17Z","published":"2024-10-04T10:06:17Z","title":"Demystifying the Token Dynamics of Deep Selective State Space Models","summary":"  Selective state space models (SSM), such as Mamba, have gained prominence for\ntheir effectiveness in modeling sequential data. Despite their outstanding\nempirical performance, a comprehensive theoretical understanding of deep\nselective SSM remains elusive, hindering their further development and adoption\nfor applications that need high fidelity. In this paper, we investigate the\ndynamical properties of tokens in a pre-trained Mamba model. In particular, we\nderive the dynamical system governing the continuous-time limit of the Mamba\nmodel and characterize the asymptotic behavior of its solutions. In the\none-dimensional case, we prove that only one of the following two scenarios\nhappens: either all tokens converge to zero, or all tokens diverge to infinity.\nWe provide criteria based on model parameters to determine when each scenario\noccurs. For the convergent scenario, we empirically verify that this scenario\nnegatively impacts the model's performance. For the divergent scenario, we\nprove that different tokens will diverge to infinity at different rates,\nthereby contributing unequally to the updates during model training. Based on\nthese investigations, we propose two refinements for the model: excluding the\nconvergent scenario and reordering tokens based on their importance scores,\nboth aimed at improving practical performance. Our experimental results\nvalidate these refinements, offering insights into enhancing Mamba's\neffectiveness in real-world applications.\n","authors":["Thieu N Vo","Tung D. Pham","Xin T. Tong","Tan Minh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.03292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03291v1","updated":"2024-10-04T10:05:15Z","published":"2024-10-04T10:05:15Z","title":"Enhanced Transformer architecture for in-context learning of dynamical\n  systems","summary":"  Recently introduced by some of the authors, the in-context identification\nparadigm aims at estimating, offline and based on synthetic data, a meta-model\nthat describes the behavior of a whole class of systems. Once trained, this\nmeta-model is fed with an observed input/output sequence (context) generated by\na real system to predict its behavior in a zero-shot learning fashion. In this\npaper, we enhance the original meta-modeling framework through three key\ninnovations: by formulating the learning task within a probabilistic framework;\nby managing non-contiguous context and query windows; and by adopting recurrent\npatching to effectively handle long context sequences. The efficacy of these\nmodifications is demonstrated through a numerical example focusing on the\nWiener-Hammerstein system class, highlighting the model's enhanced performance\nand scalability.\n","authors":["Matteo Rufolo","Dario Piga","Gabriele Maroni","Marco Forgione"],"pdf_url":"https://arxiv.org/pdf/2410.03291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03284v1","updated":"2024-10-04T09:55:44Z","published":"2024-10-04T09:55:44Z","title":"uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed\n  MABs","summary":"  In this paper, we present a novel algorithm, uniINF, for the Heavy-Tailed\nMulti-Armed Bandits (HTMAB) problem, demonstrating robustness and adaptability\nin both stochastic and adversarial environments. Unlike the stochastic MAB\nsetting where loss distributions are stationary with time, our study extends to\nthe adversarial setup, where losses are generated from heavy-tailed\ndistributions that depend on both arms and time. Our novel algorithm `uniINF`\nenjoys the so-called Best-of-Both-Worlds (BoBW) property, performing optimally\nin both stochastic and adversarial environments without knowing the exact\nenvironment type. Moreover, our algorithm also possesses a Parameter-Free\nfeature, i.e., it operates without the need of knowing the heavy-tail\nparameters $(\\sigma, \\alpha)$ a-priori. To be precise, uniINF ensures\nnearly-optimal regret in both stochastic and adversarial environments, matching\nthe corresponding lower bounds when $(\\sigma, \\alpha)$ is known (up to\nlogarithmic factors). To our knowledge, uniINF is the first parameter-free\nalgorithm to achieve the BoBW property for the heavy-tailed MAB problem.\nTechnically, we develop innovative techniques to achieve BoBW guarantees for\nParameter-Free HTMABs, including a refined analysis for the dynamics of\nlog-barrier, an auto-balancing learning rate scheduling scheme, an adaptive\nskipping-clipping loss tuning technique, and a stopping-time analysis for\nlogarithmic regret.\n","authors":["Yu Chen","Jiatai Huang","Yan Dai","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03282v1","updated":"2024-10-04T09:54:11Z","published":"2024-10-04T09:54:11Z","title":"Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the\n  Wasserstein Geometry","summary":"  We deal with the task of sampling from an unnormalized Boltzmann density\n$\\rho_D$ by learning a Boltzmann curve given by energies $f_t$ starting in a\nsimple density $\\rho_Z$. First, we examine conditions under which Fisher-Rao\nflows are absolutely continuous in the Wasserstein geometry. Second, we address\nspecific interpolations $f_t$ and the learning of the related density/velocity\npairs $(\\rho_t,v_t)$. It was numerically observed that the linear\ninterpolation, which requires only a parametrization of the velocity field\n$v_t$, suffers from a \"teleportation-of-mass\" issue. Using tools from the\nWasserstein geometry, we give an analytical example, where we can precisely\nmeasure the explosion of the velocity field. Inspired by M\\'at\\'e and Fleuret,\nwho parametrize both $f_t$ and $v_t$, we propose an interpolation which\nparametrizes only $f_t$ and fixes an appropriate $v_t$. This corresponds to the\nWasserstein gradient flow of the Kullback-Leibler divergence related to\nLangevin dynamics. We demonstrate by numerical examples that our model provides\na well-behaved flow field which successfully solves the above sampling task.\n","authors":["Jannis Chemseddine","Christian Wald","Richard Duong","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2410.03282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03281v1","updated":"2024-10-04T09:53:20Z","published":"2024-10-04T09:53:20Z","title":"BN-SCAFFOLD: controlling the drift of Batch Normalization statistics in\n  Federated Learning","summary":"  Federated Learning (FL) is gaining traction as a learning paradigm for\ntraining Machine Learning (ML) models in a decentralized way. Batch\nNormalization (BN) is ubiquitous in Deep Neural Networks (DNN), as it improves\nconvergence and generalization. However, BN has been reported to hinder\nperformance of DNNs in heterogeneous FL. Recently, the FedTAN algorithm has\nbeen proposed to mitigate the effect of heterogeneity on BN, by aggregating BN\nstatistics and gradients from all the clients. However, it has a high\ncommunication cost, that increases linearly with the depth of the DNN. SCAFFOLD\nis a variance reduction algorithm, that estimates and corrects the client drift\nin a communication-efficient manner. Despite its promising results in\nheterogeneous FL settings, it has been reported to underperform for models with\nBN. In this work, we seek to revive SCAFFOLD, and more generally variance\nreduction, as an efficient way of training DNN with BN in heterogeneous FL. We\nintroduce a unified theoretical framework for analyzing the convergence of\nvariance reduction algorithms in the BN-DNN setting, inspired of by the work of\nWang et al. 2023, and show that SCAFFOLD is unable to remove the bias\nintroduced by BN. We thus propose the BN-SCAFFOLD algorithm, which extends the\nclient drift correction of SCAFFOLD to BN statistics. We prove convergence\nusing the aforementioned framework and validate the theoretical results with\nexperiments on MNIST and CIFAR-10. BN-SCAFFOLD equals the performance of\nFedTAN, without its high communication cost, outperforming Federated Averaging\n(FedAvg), SCAFFOLD, and other FL algorithms designed to mitigate BN\nheterogeneity.\n","authors":["Gonzalo Iaki Quintana","Laurence Vancamberg","Vincent Jugnon","Mathilde Mougeot","Agns Desolneux"],"pdf_url":"https://arxiv.org/pdf/2410.03281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03280v1","updated":"2024-10-04T09:53:16Z","published":"2024-10-04T09:53:16Z","title":"Manikin-Recorded Cardiopulmonary Sounds Dataset Using Digital\n  Stethoscope","summary":"  Heart and lung sounds are crucial for healthcare monitoring. Recent\nimprovements in stethoscope technology have made it possible to capture patient\nsounds with enhanced precision. In this dataset, we used a digital stethoscope\nto capture both heart and lung sounds, including individual and mixed\nrecordings. To our knowledge, this is the first dataset to offer both separate\nand mixed cardiorespiratory sounds. The recordings were collected from a\nclinical manikin, a patient simulator designed to replicate human physiological\nconditions, generating clean heart and lung sounds at different body locations.\nThis dataset includes both normal sounds and various abnormalities (i.e.,\nmurmur, atrial fibrillation, tachycardia, atrioventricular block, third and\nfourth heart sound, wheezing, crackles, rhonchi, pleural rub, and gurgling\nsounds). The dataset includes audio recordings of chest examinations performed\nat different anatomical locations, as determined by specialist nurses. Each\nrecording has been enhanced using frequency filters to highlight specific sound\ntypes. This dataset is useful for applications in artificial intelligence, such\nas automated cardiopulmonary disease detection, sound classification,\nunsupervised separation techniques, and deep learning algorithms related to\naudio signal processing.\n","authors":["Yasaman Torabi","Shahram Shirani","James P. Reilly"],"pdf_url":"https://arxiv.org/pdf/2410.03280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03276v1","updated":"2024-10-04T09:49:28Z","published":"2024-10-04T09:49:28Z","title":"Sm: enhanced localization in Multiple Instance Learning for medical\n  imaging classification","summary":"  Multiple Instance Learning (MIL) is widely used in medical imaging\nclassification to reduce the labeling effort. While only bag labels are\navailable for training, one typically seeks predictions at both bag and\ninstance levels (classification and localization tasks, respectively). Early\nMIL methods treated the instances in a bag independently. Recent methods\naccount for global and local dependencies among instances. Although they have\nyielded excellent results in classification, their performance in terms of\nlocalization is comparatively limited. We argue that these models have been\ndesigned to target the classification task, while implications at the instance\nlevel have not been deeply investigated. Motivated by a simple observation --\nthat neighboring instances are likely to have the same label -- we propose a\nnovel, principled, and flexible mechanism to model local dependencies. It can\nbe used alone or combined with any mechanism to model global dependencies\n(e.g., transformers). A thorough empirical validation shows that our module\nleads to state-of-the-art performance in localization while being competitive\nor superior in classification. Our code is at\nhttps://github.com/Franblueee/SmMIL.\n","authors":["Francisco M. Castro-Macas","Pablo Morales-lvarez","Yunan Wu","Rafael Molina","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2410.03276v1.pdf","comment":"24 pages, 14 figures, 2024 Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2409.16956v2","updated":"2024-10-04T09:46:24Z","published":"2024-09-25T14:12:50Z","title":"Informed deep hierarchical classification: a non-standard analysis\n  inspired approach","summary":"  This work proposes a novel approach to the deep hierarchical classification\ntask, i.e., the problem of classifying data according to multiple labels\norganized in a rigid parent-child structure. It consists in a multi-output deep\nneural network equipped with specific projection operators placed before each\noutput layer. The design of such an architecture, called lexicographic hybrid\ndeep neural network (LH-DNN), has been possible by combining tools from\ndifferent and quite distant research fields: lexicographic multi-objective\noptimization, non-standard analysis, and deep learning. To assess the efficacy\nof the approach, the resulting network is compared against the B-CNN, a\nconvolutional neural network tailored for hierarchical classification tasks, on\nthe CIFAR10, CIFAR100 (where it has been originally and recently proposed\nbefore being adopted and tuned for multiple real-world applications) and\nFashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable\nif not superior performance, especially in the learning of the hierarchical\nrelations, in the face of a drastic reduction of the learning parameters,\ntraining epochs, and computational time, without the need for ad-hoc loss\nfunctions weighting values.\n","authors":["Lorenzo Fiaschi","Marco Cococcioni"],"pdf_url":"https://arxiv.org/pdf/2409.16956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06449v2","updated":"2024-10-04T09:45:39Z","published":"2024-06-10T16:39:39Z","title":"Cometh: A continuous-time discrete-state graph diffusion model","summary":"  Discrete-state denoising diffusion models led to state-of-the-art performance\nin graph generation, especially in the molecular domain. Recently, they have\nbeen transposed to continuous time, allowing more flexibility in the reverse\nprocess and a better trade-off between sampling efficiency and quality. Here,\nto leverage the benefits of both approaches, we propose Cometh, a\ncontinuous-time discrete-state graph diffusion model, tailored to the\nspecificities of graph data. In addition, we also successfully replaced the set\nof structural encodings previously used in the discrete graph diffusion model\nwith a single random-walk-based encoding, providing a simple and principled way\nto boost the model's expressive power. Empirically, we show that integrating\ncontinuous time leads to significant improvements across various metrics over\nstate-of-the-art discrete-state diffusion models on a large set of molecular\nand non-molecular benchmark datasets. In terms of VUN samples, Cometh obtains a\nnear-perfect performance of 99.5% on the planar graph dataset and outperforms\nDiGress by 12.6% on the large GuacaMol dataset.\n","authors":["Antoine Siraudin","Fragkiskos D. Malliaros","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2406.06449v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2408.04391v2","updated":"2024-10-04T09:41:36Z","published":"2024-08-08T11:51:34Z","title":"Robustness investigation of cross-validation based quality measures for\n  model assessment","summary":"  In this paper the accuracy and robustness of quality measures for the\nassessment of machine learning models are investigated. The prediction quality\nof a machine learning model is evaluated model-independent based on a\ncross-validation approach, where the approximation error is estimated for\nunknown data. The presented measures quantify the amount of explained variation\nin the model prediction. The reliability of these measures is assessed by means\nof several numerical examples, where an additional data set for the\nverification of the estimated prediction error is available. Furthermore, the\nconfidence bounds of the presented quality measures are estimated and local\nquality measures are derived from the prediction residuals obtained by the\ncross-validation approach.\n","authors":["Thomas Most","Lars Grning","Sebastian Wolff"],"pdf_url":"https://arxiv.org/pdf/2408.04391v2.pdf","comment":"accepted for publication in Engineering Modelling, Analysis &\n  Simulation (EMAS)"},{"id":"http://arxiv.org/abs/2405.03958v3","updated":"2024-10-04T09:40:05Z","published":"2024-05-07T02:45:28Z","title":"Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your\n  Diffusion Model","summary":"  Current state-of-the-art diffusion models employ U-Net architectures\ncontaining convolutional and (qkv) self-attention layers. The U-Net processes\nimages while being conditioned on the time embedding input for each sampling\nstep and the class or caption embedding input corresponding to the desired\nconditional generation. Such conditioning involves scale-and-shift operations\nto the convolutional layers but does not directly affect the attention layers.\nWhile these standard architectural choices are certainly effective, not\nconditioning the attention layers feels arbitrary and potentially suboptimal.\nIn this work, we show that simply adding LoRA conditioning to the attention\nlayers without changing or tuning the other parts of the U-Net architecture\nimproves the image generation quality. For example, a drop-in addition of LoRA\nconditioning to EDM diffusion model yields FID scores of 1.91/1.75 for\nunconditional and class-conditional CIFAR-10 generation, improving upon the\nbaseline of 1.97/1.79.\n","authors":["Joo Young Choi","Jaesung R. Park","Inkyu Park","Jaewoong Cho","Albert No","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2405.03958v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03267v1","updated":"2024-10-04T09:34:52Z","published":"2024-10-04T09:34:52Z","title":"Optimal Transport for $$-Contaminated Credal Sets","summary":"  We provide a version for lower probabilities of Monge's and Kantorovich's\noptimal transport problems. We show that, when the lower probabilities are the\nlower envelopes of $\\epsilon$-contaminated sets, then our version of Monge's,\nand a restricted version of our Kantorovich's problems, coincide with their\nrespective classical versions. We also give sufficient conditions for the\nexistence of our version of Kantorovich's optimal plan, and for the two\nproblems to be equivalent. As a byproduct, we show that for\n$\\epsilon$-contaminations the lower probability versions of Monge's and\nKantorovich's optimal transport problems need not coincide. The applications of\nour results to Machine Learning and Artificial Intelligence are also discussed.\n","authors":["Michele Caprio"],"pdf_url":"https://arxiv.org/pdf/2410.03267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03263v1","updated":"2024-10-04T09:31:10Z","published":"2024-10-04T09:31:10Z","title":"Test-time Adaptation for Regression by Subspace Alignment","summary":"  This paper investigates test-time adaptation (TTA) for regression, where a\nregression model pre-trained in a source domain is adapted to an unknown target\ndistribution with unlabeled target data. Although regression is one of the\nfundamental tasks in machine learning, most of the existing TTA methods have\nclassification-specific designs, which assume that models output\nclass-categorical predictions, whereas regression models typically output only\nsingle scalar values. To enable TTA for regression, we adopt a feature\nalignment approach, which aligns the feature distributions between the source\nand target domains to mitigate the domain gap. However, we found that naive\nfeature alignment employed in existing TTA methods for classification is\nineffective or even worse for regression because the features are distributed\nin a small subspace and many of the raw feature dimensions have little\nsignificance to the output. For an effective feature alignment in TTA for\nregression, we propose Significant-subspace Alignment (SSA). SSA consists of\ntwo components: subspace detection and dimension weighting. Subspace detection\nfinds the feature subspace that is representative and significant to the\noutput. Then, the feature alignment is performed in the subspace during TTA.\nMeanwhile, dimension weighting raises the importance of the dimensions of the\nfeature subspace that have greater significance to the output. We\nexperimentally show that SSA outperforms various baselines on real-world\ndatasets.\n","authors":["Kazuki Adachi","Shin'ya Yamaguchi","Atsutoshi Kumagai","Tomoki Hamagami"],"pdf_url":"https://arxiv.org/pdf/2410.03263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01524v2","updated":"2024-10-04T09:25:27Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03249v1","updated":"2024-10-04T09:14:11Z","published":"2024-10-04T09:14:11Z","title":"How much can we forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we use experimental evidence and theoretical estimates to\nchallenge the common assumption that small-scale contamination renders\nbenchmark evaluations invalid. First, we experimentally quantify the magnitude\nof benchmark overfitting based on scaling along three dimensions: The number of\nmodel parameters (up to 1.6B), the number of times an example is seen (up to\n144), and the number of training tokens (up to 40B). We find that if model and\ndata follow the Chinchilla scaling laws, minor contamination indeed leads to\noverfitting. At the same time, even 144 times of contamination can be forgotten\nif the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. We then derive a simple theory of example\nforgetting via cumulative weight decay. It allows us to bound the number of\ngradient steps required to forget past data for any training run where we know\nthe hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,\nhave forgotten the data seen at the beginning of training. Experimentally, we\ndemonstrate that forgetting occurs faster than what is predicted by our bounds.\nTaken together, our results suggest that moderate amounts of contamination can\nbe forgotten at the end of realistically scaled training runs.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04743v2","updated":"2024-10-04T09:05:45Z","published":"2024-09-07T07:18:08Z","title":"GRVFL-MV: Graph Random Vector Functional Link Based on Multi-View\n  Learning","summary":"  The classification performance of the random vector functional link (RVFL), a\nrandomized neural network, has been widely acknowledged. However, due to its\nshallow learning nature, RVFL often fails to consider all the relevant\ninformation available in a dataset. Additionally, it overlooks the geometrical\nproperties of the dataset. To address these limitations, a novel graph random\nvector functional link based on multi-view learning (GRVFL-MV) model is\nproposed. The proposed model is trained on multiple views, incorporating the\nconcept of multiview learning (MVL), and it also incorporates the geometrical\nproperties of all the views using the graph embedding (GE) framework. The\nfusion of RVFL networks, MVL, and GE framework enables our proposed model to\nachieve the following: i) efficient learning: by leveraging the topology of\nRVFL, our proposed model can efficiently capture nonlinear relationships within\nthe multi-view data, facilitating efficient and accurate predictions; ii)\ncomprehensive representation: fusing information from diverse perspectives\nenhance the proposed model's ability to capture complex patterns and\nrelationships within the data, thereby improving the model's overall\ngeneralization performance; and iii) structural awareness: by employing the GE\nframework, our proposed model leverages the original data distribution of the\ndataset by naturally exploiting both intrinsic and penalty subspace learning\ncriteria. The evaluation of the proposed GRVFL-MV model on various datasets,\nincluding 27 UCI and KEEL datasets, 50 datasets from Corel5k, and 45 datasets\nfrom AwA, demonstrates its superior performance compared to baseline models.\nThese results highlight the enhanced generalization capabilities of the\nproposed GRVFL-MV model across a diverse range of datasets.\n","authors":["M. Tanveer","R. K. Sharma","M. Sajid","A. Quadir"],"pdf_url":"https://arxiv.org/pdf/2409.04743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05815v2","updated":"2024-10-04T08:46:03Z","published":"2024-06-09T15:03:36Z","title":"What Can We Learn from State Space Models for Machine Learning on\n  Graphs?","summary":"  Machine learning on graphs has recently found extensive applications across\ndomains. However, the commonly used Message Passing Neural Networks (MPNNs)\nsuffer from limited expressive power and struggle to capture long-range\ndependencies. Graph transformers offer a strong alternative due to their global\nattention mechanism, but they come with great computational overheads,\nespecially for large graphs. In recent years, State Space Models (SSMs) have\nemerged as a compelling approach to replace full attention in transformers to\nmodel sequential data. It blends the strengths of RNNs and CNNs, offering a)\nefficient computation, b) the ability to capture long-range dependencies, and\nc) good generalization across sequences of various lengths. However, extending\nSSMs to graph-structured data presents unique challenges due to the lack of\ncanonical node ordering in graphs. In this work, we propose Graph State Space\nConvolution (GSSC) as a principled extension of SSMs to graph-structured data.\nBy leveraging global permutation-equivariant set aggregation and factorizable\ngraph kernels that rely on relative node distances as the convolution kernels,\nGSSC preserves all three advantages of SSMs. We demonstrate the provably\nstronger expressiveness of GSSC than MPNNs in counting graph substructures and\nshow its effectiveness across 11 real-world, widely used benchmark datasets.\nGSSC achieves the best results on 6 out of 11 datasets with all significant\nimprovements compared to the state-of-the-art baselines and second-best results\non the other 5 datasets. Our findings highlight the potential of GSSC as a\npowerful and scalable model for graph machine learning. Our code is available\nat https://github.com/Graph-COM/GSSC.\n","authors":["Yinan Huang","Siqi Miao","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2406.05815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09493v6","updated":"2024-10-04T08:44:54Z","published":"2024-01-17T01:17:12Z","title":"Identifying Three-Dimensional Radiative Patterns Associated with Early\n  Tropical Cyclone Intensification","summary":"  Cloud radiative feedback impacts early tropical cyclone (TC) intensification,\nbut limitations in existing diagnostic frameworks make them unsuitable for\nstudying asymmetric or transient radiative heating. We propose a linear\nVariational Encoder-Decoder (VED) to learn the hidden relationship between\nradiation and the surface intensification of realistic simulated TCs. Limiting\nVED model inputs enables using its uncertainty to identify periods when\nradiation has more importance for intensification. A close examination of the\nextracted 3D radiative structures suggests that longwave radiative forcing from\ninner core deep convection and shallow clouds both contribute to\nintensification, with the deep convection having the most impact overall. We\nfind that deep convection downwind of the shallow clouds is critical to the\nintensification of Haiyan. Our work demonstrates that machine learning can\ndiscover thermodynamic-kinematic relationships without relying on axisymmetric\nor deterministic assumptions, paving the way towards the objective discovery of\nprocesses leading to TC intensification in realistic conditions.\n","authors":["Frederick Iat-Hin Tam","Tom Beucler","James H. Ruppert Jr"],"pdf_url":"https://arxiv.org/pdf/2401.09493v6.pdf","comment":"15 pages, 6 figures (main text)"},{"id":"http://arxiv.org/abs/2410.03229v1","updated":"2024-10-04T08:34:14Z","published":"2024-10-04T08:34:14Z","title":"Elucidating the Design Choice of Probability Paths in Flow Matching for\n  Forecasting","summary":"  Flow matching has recently emerged as a powerful paradigm for generative\nmodeling and has been extended to probabilistic time series forecasting in\nlatent spaces. However, the impact of the specific choice of probability path\nmodel on forecasting performance remains under-explored. In this work, we\ndemonstrate that forecasting spatio-temporal data with flow matching is highly\nsensitive to the selection of the probability path model. Motivated by this\ninsight, we propose a novel probability path model designed to improve\nforecasting performance. Our empirical results across various dynamical system\nbenchmarks show that our model achieves faster convergence during training and\nimproved predictive performance compared to existing probability path models.\nImportantly, our approach is efficient during inference, requiring only a few\nsampling steps. This makes our proposed model practical for real-world\napplications and opens new avenues for probabilistic forecasting.\n","authors":["Soon Hoe Lim","Yijin Wang","Annan Yu","Emma Hart","Michael W. Mahoney","Xiaoye S. Li","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2410.03229v1.pdf","comment":"30 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.00741v2","updated":"2024-10-04T16:10:38Z","published":"2024-10-01T14:33:22Z","title":"VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP\n  Models","summary":"  Contrastive Language-Image Pre-training (CLIP) has been widely studied and\napplied in numerous applications. However, the emphasis on brief summary texts\nduring pre-training prevents CLIP from understanding long descriptions. This\nissue is particularly acute regarding videos given that videos often contain\nabundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra\nLength) model, which aims to unleash the long-description understanding\ncapability of video CLIP models. Firstly, we establish an automatic data\ncollection system and gather a large-scale VILD pre-training dataset with VIdeo\nand Long-Description pairs. Then, we propose Text-similarity-guided Primary\nComponent Matching (TPCM) to better learn the distribution of feature space\nwhile expanding the long description capability. We also introduce two new\ntasks namely Detail-aware Description Ranking (DDR) and Hallucination-aware\nDescription Ranking (HDR) for further understanding improvement. Finally, we\nconstruct a Long Video Description Ranking (LVDR) benchmark for evaluating the\nlong-description capability more comprehensively. Extensive experimental\nresults on widely-used text-video retrieval benchmarks with both short and long\ndescriptions and our LVDR benchmark can fully demonstrate the effectiveness of\nour method.\n","authors":["Jiapeng Wang","Chengyu Wang","Kunzhe Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2410.00741v2.pdf","comment":"EMNLP 2024 Main conference"},{"id":"http://arxiv.org/abs/2410.03323v1","updated":"2024-10-04T11:20:04Z","published":"2024-10-04T11:20:04Z","title":"Does SpatioTemporal information benefit Two video summarization\n  benchmarks?","summary":"  An important aspect of summarizing videos is understanding the temporal\ncontext behind each part of the video to grasp what is and is not important.\nVideo summarization models have in recent years modeled spatio-temporal\nrelationships to represent this information. These models achieved\nstate-of-the-art correlation scores on important benchmark datasets. However,\nwhat has not been reviewed is whether spatio-temporal relationships are even\nrequired to achieve state-of-the-art results. Previous work in activity\nrecognition has found biases, by prioritizing static cues such as scenes or\nobjects, over motion information. In this paper we inquire if similar spurious\nrelationships might influence the task of video summarization. To do so, we\nanalyse the role that temporal information plays on existing benchmark\ndatasets. We first estimate a baseline with temporally invariant models to see\nhow well such models rank on benchmark datasets (TVSum and SumMe). We then\ndisrupt the temporal order of the videos to investigate the impact it has on\nexisting state-of-the-art models. One of our findings is that the temporally\ninvariant models achieve competitive correlation scores that are close to the\nhuman baselines on the TVSum dataset. We also demonstrate that existing models\nare not affected by temporal perturbations. Furthermore, with certain\ndisruption strategies that shuffle fixed time segments, we can actually improve\ntheir correlation scores. With these results, we find that spatio-temporal\nrelationship play a minor role and we raise the question whether these\nbenchmarks adequately model the task of video summarization. Code available at:\nhttps://github.com/AashGan/TemporalPerturbSum\n","authors":["Aashutosh Ganesh","Mirela Popa","Daan Odijk","Nava Tintarev"],"pdf_url":"https://arxiv.org/pdf/2410.03323v1.pdf","comment":"Accepted for presentation at AEQUITAS workshop, Co-located with ECAI\n  2024"},{"id":"http://arxiv.org/abs/2407.11566v2","updated":"2024-10-04T09:39:44Z","published":"2024-07-16T10:19:14Z","title":"TGIF: Text-Guided Inpainting Forgery Dataset","summary":"  Digital image manipulation has become increasingly accessible and realistic\nwith the advent of generative AI technologies. Recent developments allow for\ntext-guided inpainting, making sophisticated image edits possible with minimal\neffort. This poses new challenges for digital media forensics. For example,\ndiffusion model-based approaches could either splice the inpainted region into\nthe original image, or regenerate the entire image. In the latter case,\ntraditional image forgery localization (IFL) methods typically fail. This paper\nintroduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive\ncollection of images designed to support the training and evaluation of image\nforgery localization and synthetic image detection (SID) methods. The TGIF\ndataset includes approximately 75k forged images, originating from popular\nopen-source and commercial methods, namely SD2, SDXL, and Adobe Firefly. We\nbenchmark several state-of-the-art IFL and SID methods on TGIF. Whereas\ntraditional IFL methods can detect spliced images, they fail to detect\nregenerated inpainted images. Moreover, traditional SID may detect the\nregenerated inpainted images to be fake, but cannot localize the inpainted\narea. Finally, both IFL and SID methods fail when exposed to stronger\ncompression, while they are less robust to modern compression algorithms, such\nas WEBP. In conclusion, this work demonstrates the inefficiency of\nstate-of-the-art detectors on local manipulations performed by modern\ngenerative approaches, and aspires to help with the development of more capable\nIFL and SID methods. The dataset and code can be downloaded at\nhttps://github.com/IDLabMedia/tgif-dataset.\n","authors":["Hannes Mareen","Dimitrios Karageorgiou","Glenn Van Wallendael","Peter Lambert","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.11566v2.pdf","comment":"6 pages, accepted at IEEE WIFS 2024"},{"id":"http://arxiv.org/abs/2410.03264v1","updated":"2024-10-04T09:33:34Z","published":"2024-10-04T09:33:34Z","title":"Enriching Music Descriptions with a Finetuned-LLM and Metadata for\n  Text-to-Music Retrieval","summary":"  Text-to-Music Retrieval, finding music based on a given natural language\nquery, plays a pivotal role in content discovery within extensive music\ndatabases. To address this challenge, prior research has predominantly focused\non a joint embedding of music audio and text, utilizing it to retrieve music\ntracks that exactly match descriptive queries related to musical attributes\n(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,\nusers also articulate a need to explore music that shares similarities with\ntheir favorite tracks or artists, such as \\textit{I need a similar track to\nSuperstition by Stevie Wonder}. To address these concerns, this paper proposes\nan improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes\nrich text descriptions generated with a finetuned large language model and\nmetadata. To accomplish this, we obtained various types of seed text from\nseveral existing music tag and caption datasets and a knowledge graph dataset\nof artists and tracks. The experimental results show the effectiveness of\nTTMR++ in comparison to state-of-the-art music-text joint embedding models\nthrough a comprehensive evaluation involving various musical text queries.\n","authors":["SeungHeon Doh","Minhee Lee","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2410.03264v1.pdf","comment":"Accepted for publication at the IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2211.10881v4","updated":"2024-10-04T07:53:15Z","published":"2022-11-20T06:31:23Z","title":"Deepfake Detection: A Comprehensive Survey from the Reliability\n  Perspective","summary":"  The mushroomed Deepfake synthetic materials circulated on the internet have\nraised a profound social impact on politicians, celebrities, and individuals\nworldwide. In this survey, we provide a thorough review of the existing\nDeepfake detection studies from the reliability perspective. We identify three\nreliability-oriented research challenges in the current Deepfake detection\ndomain: transferability, interpretability, and robustness. Moreover, while\nsolutions have been frequently addressed regarding the three challenges, the\ngeneral reliability of a detection model has been barely considered, leading to\nthe lack of reliable evidence in real-life usages and even for prosecutions on\nDeepfake-related cases in court. We, therefore, introduce a model reliability\nstudy metric using statistical random sampling knowledge and the publicly\navailable benchmark datasets to review the reliability of the existing\ndetection models on arbitrary Deepfake candidate suspects. Case studies are\nfurther executed to justify the real-life Deepfake cases including different\ngroups of victims with the help of the reliably qualified detection models as\nreviewed in this survey. Reviews and experiments on the existing approaches\nprovide informative discussions and future research directions for Deepfake\ndetection.\n","authors":["Tianyi Wang","Xin Liao","Kam Pui Chow","Xiaodong Lin","Yinglong Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10881v4.pdf","comment":"Accepted to ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2410.03070v1","updated":"2024-10-04T01:24:02Z","published":"2024-10-04T01:24:02Z","title":"FedMAC: Tackling Partial-Modality Missing in Federated Learning with\n  Cross-Modal Aggregation and Contrastive Regularization","summary":"  Federated Learning (FL) is a method for training machine learning models\nusing distributed data sources. It ensures privacy by allowing clients to\ncollaboratively learn a shared global model while storing their data locally.\nHowever, a significant challenge arises when dealing with missing modalities in\nclients' datasets, where certain features or modalities are unavailable or\nincomplete, leading to heterogeneous data distribution. While previous studies\nhave addressed the issue of complete-modality missing, they fail to tackle\npartial-modality missing on account of severe heterogeneity among clients at an\ninstance level, where the pattern of missing data can vary significantly from\none sample to another. To tackle this challenge, this study proposes a novel\nframework named FedMAC, designed to address multi-modality missing under\nconditions of partial-modality missing in FL. Additionally, to avoid trivial\naggregation of multi-modal features, we introduce contrastive-based\nregularization to impose additional constraints on the latent representation\nspace. The experimental results demonstrate the effectiveness of FedMAC across\nvarious client configurations with statistical heterogeneity, outperforming\nbaseline methods by up to 26% in severe missing scenarios, highlighting its\npotential as a solution for the challenge of partially missing modalities in\nfederated systems.\n","authors":["Manh Duong Nguyen","Trung Thanh Nguyen","Huy Hieu Pham","Trong Nghia Hoang","Phi Le Nguyen","Thanh Trung Huynh"],"pdf_url":"https://arxiv.org/pdf/2410.03070v1.pdf","comment":"The 22nd International Symposium on Network Computing and\n  Applications (NCA 2024)"},{"id":"http://arxiv.org/abs/2410.03879v1","updated":"2024-10-04T19:22:35Z","published":"2024-10-04T19:22:35Z","title":"SONIQUE: Video Background Music Generation Using Unpaired Audio-Visual\n  Data","summary":"  We present SONIQUE, a model for generating background music tailored to video\ncontent. Unlike traditional video-to-music generation approaches, which rely\nheavily on paired audio-visual datasets, SONIQUE leverages unpaired data,\ncombining royalty-free music and independent video sources. By utilizing large\nlanguage models (LLMs) for video understanding and converting visual\ndescriptions into musical tags, alongside a U-Net-based conditional diffusion\nmodel, SONIQUE enables customizable music generation. Users can control\nspecific aspects of the music, such as instruments, genres, tempo, and\nmelodies, ensuring the generated output fits their creative vision. SONIQUE is\nopen-source, with a demo available online.\n","authors":["Liqian Zhang","Magdalena Fuentes"],"pdf_url":"https://arxiv.org/pdf/2410.03879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03869v1","updated":"2024-10-04T19:04:43Z","published":"2024-10-04T19:04:43Z","title":"Chain-of-Jailbreak Attack for Image Generation Models via Editing Step\n  by Step","summary":"  Text-based image generation models, such as Stable Diffusion and DALL-E 3,\nhold significant potential in content creation and publishing workflows, making\nthem the focus in recent years. Despite their remarkable capability to generate\ndiverse and vivid images, considerable efforts are being made to prevent the\ngeneration of harmful content, such as abusive, violent, or pornographic\nmaterial. To assess the safety of existing models, we introduce a novel\njailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises\nimage generation models through a step-by-step editing process. Specifically,\nfor malicious queries that cannot bypass the safeguards with a single prompt,\nwe intentionally decompose the query into multiple sub-queries. The image\ngeneration models are then prompted to generate and iteratively edit images\nbased on these sub-queries. To evaluate the effectiveness of our CoJ attack\nmethod, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine\nsafety scenarios, three types of editing operations, and three editing\nelements. Experiments on four widely-used image generation services provided by\nGPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack\nmethod can successfully bypass the safeguards of models for over 60% cases,\nwhich significantly outperforms other jailbreaking methods (i.e., 14%).\nFurther, to enhance these models' safety against our CoJ attack method, we also\npropose an effective prompting-based method, Think Twice Prompting, that can\nsuccessfully defend over 95% of CoJ attack. We release our dataset and code to\nfacilitate the AI safety research.\n","authors":["Wenxuan Wang","Kuiyi Gao","Zihan Jia","Youliang Yuan","Jen-tse Huang","Qiuzhi Liu","Shuai Wang","Wenxiang Jiao","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2410.03869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03800v1","updated":"2024-10-04T07:52:46Z","published":"2024-10-04T07:52:46Z","title":"M2AR: A Web-based Modeling Environment for the Augmented Reality\n  Workflow Modeling Language","summary":"  This paper introduces M2AR, a new web-based, two- and three-dimensional\nmodeling environment that enables the modeling and execution of augmented\nreality applications without requiring programming knowledge. The platform is\nbased on a 3D JavaScript library and the mixed reality immersive web standard\nWebXR. For a first demonstration of its feasibility, the previously introduced\nAugmented Reality Workflow Modeling Language (ARWFML) has been successfully\nimplemented using this environment. The usefulness of the new modeling\nenvironment is demonstrated by showing use cases of the ARWFML on M2AR.\n","authors":["Fabian Muff","Hans-Georg Fill"],"pdf_url":"https://arxiv.org/pdf/2410.03800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05302v1","updated":"2024-10-04T12:39:29Z","published":"2024-10-04T12:39:29Z","title":"Episodic fine-tuning prototypical networks for optimization-based\n  few-shot learning: Application to audio classification","summary":"  The Prototypical Network (ProtoNet) has emerged as a popular choice in\nFew-shot Learning (FSL) scenarios due to its remarkable performance and\nstraightforward implementation. Building upon such success, we first propose a\nsimple (yet novel) method to fine-tune a ProtoNet on the (labeled) support set\nof the test episode of a C-way-K-shot test episode (without using the query set\nwhich is only used for evaluation). We then propose an algorithmic framework\nthat combines ProtoNet with optimization-based FSL algorithms (MAML and\nMeta-Curvature) to work with such a fine-tuning method. Since\noptimization-based algorithms endow the target learner model with the ability\nto fast adaption to only a few samples, we utilize ProtoNet as the target model\nto enhance its fine-tuning performance with the help of a specifically designed\nepisodic fine-tuning strategy. The experimental results confirm that our\nproposed models, MAML-Proto and MC-Proto, combined with our unique fine-tuning\nmethod, outperform regular ProtoNet by a large margin in few-shot audio\nclassification tasks on the ESC-50 and Speech Commands v2 datasets. We note\nthat although we have only applied our model to the audio domain, it is a\ngeneral method and can be easily extended to other domains.\n","authors":["Xuanyu Zhuang","Geoffroy Peeters","Gal Richard"],"pdf_url":"https://arxiv.org/pdf/2410.05302v1.pdf","comment":"Accepted at MLSP 2024"}]},"2024-10-07T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.05269v1","updated":"2024-10-07T17:59:58Z","published":"2024-10-07T17:59:58Z","title":"Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models","summary":"  Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.\n","authors":["Fei Wang","Ninareh Mehrabi","Palash Goyal","Rahul Gupta","Kai-Wei Chang","Aram Galstyan"],"pdf_url":"https://arxiv.org/pdf/2410.05269v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/DataAdvisor/"},{"id":"http://arxiv.org/abs/2410.05267v1","updated":"2024-10-07T17:59:48Z","published":"2024-10-07T17:59:48Z","title":"Grounding Partially-Defined Events in Multimodal Data","summary":"  How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.\n","authors":["Kate Sanders","Reno Kriz","David Etter","Hannah Recknor","Alexander Martin","Cameron Carpenter","Jingyang Lin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2410.05267v1.pdf","comment":"Preprint; 9 pages; 2024 EMNLP Findings"},{"id":"http://arxiv.org/abs/2406.11839v2","updated":"2024-10-07T17:59:42Z","published":"2024-06-17T17:59:58Z","title":"mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models","summary":"  Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.\n","authors":["Fei Wang","Wenxuan Zhou","James Y. Huang","Nan Xu","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11839v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO"},{"id":"http://arxiv.org/abs/2410.05265v1","updated":"2024-10-07T17:59:35Z","published":"2024-10-07T17:59:35Z","title":"PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs","summary":"  Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.\n","authors":["Mengzhao Chen","Yi Liu","Jiahao Wang","Yi Bin","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.05265v1.pdf","comment":"A PTQ method to significantly boost the performance of static\n  activation quantization"},{"id":"http://arxiv.org/abs/2410.05262v1","updated":"2024-10-07T17:58:47Z","published":"2024-10-07T17:58:47Z","title":"TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles","summary":"  As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\"\n","authors":["Qingchen Yu","Shichao Song","Ke Fang","Yunfeng Shi","Zifan Zheng","Hanyu Wang","Simin Niu","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.05262v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2410.05258v1","updated":"2024-10-07T17:57:38Z","published":"2024-10-07T17:57:38Z","title":"Differential Transformer","summary":"  Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.\n","authors":["Tianzhu Ye","Li Dong","Yuqing Xia","Yutao Sun","Yi Zhu","Gao Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.05258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05254v1","updated":"2024-10-07T17:55:35Z","published":"2024-10-07T17:55:35Z","title":"GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments","summary":"  Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.\n","authors":["Eilam Shapira","Omer Madmon","Itamar Reinman","Samuel Joseph Amouyal","Roi Reichart","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2410.05254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05252v1","updated":"2024-10-07T17:55:10Z","published":"2024-10-07T17:55:10Z","title":"Causal Micro-Narratives","summary":"  We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.\n","authors":["Mourad Heddaya","Qingcheng Zeng","Chenhao Tan","Rob Voigt","Alexander Zentefis"],"pdf_url":"https://arxiv.org/pdf/2410.05252v1.pdf","comment":"Accepted to EMNLP 2024 Workshop on Narrative Understanding"},{"id":"http://arxiv.org/abs/2410.05248v1","updated":"2024-10-07T17:52:21Z","published":"2024-10-07T17:52:21Z","title":"SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe","summary":"  To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.\n","authors":["Yuxin Xiao","Shujian Zhang","Wenxuan Zhou","Marzyeh Ghassemi","Sanqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.05248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17975v2","updated":"2024-10-07T17:49:13Z","published":"2024-06-25T23:12:07Z","title":"SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)","summary":"  Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs.\n","authors":["Matthieu Meeus","Igor Shilov","Shubham Jain","Manuel Faysse","Marek Rei","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2406.17975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05243v1","updated":"2024-10-07T17:47:50Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly take\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01574v5","updated":"2024-10-07T17:46:08Z","published":"2024-06-03T17:53:00Z","title":"MMLU-Pro: A More Robust and Challenging Multi-Task Language\n  Understanding Benchmark (Published at NeurIPS 2024 Track Datasets and\n  Benchmarks)","summary":"  In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.\n","authors":["Yubo Wang","Xueguang Ma","Ge Zhang","Yuansheng Ni","Abhranil Chandra","Shiguang Guo","Weiming Ren","Aaran Arulraj","Xuan He","Ziyan Jiang","Tianle Li","Max Ku","Kai Wang","Alex Zhuang","Rongqi Fan","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01574v5.pdf","comment":"This version has been accepted and published at NeurIPS 2024 Track\n  Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2410.05239v1","updated":"2024-10-07T17:42:53Z","published":"2024-10-07T17:42:53Z","title":"TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation\n  Models","summary":"  Vision-Language Models (VLMs) have shown impressive performance in vision\ntasks, but adapting them to new domains often requires expensive fine-tuning.\nPrompt tuning techniques, including textual, visual, and multimodal prompting,\noffer efficient alternatives by leveraging learnable prompts. However, their\napplication to Vision-Language Segmentation Models (VLSMs) and evaluation under\nsignificant domain shifts remain unexplored. This work presents an open-source\nbenchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal\nprompt tuning techniques into VLSMs, making prompt tuning usable for downstream\nsegmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt\ntuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$\ndifferent combinations. We test various prompt tuning on $8$ diverse medical\ndatasets, including $3$ radiology datasets (breast tumor, echocardiograph,\nchest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin\ncancer), and two natural domain segmentation datasets. Our study found that\ntextual prompt tuning struggles under significant domain shifts, from\nnatural-domain images to medical data. Furthermore, visual prompt tuning, with\nfewer hyperparameters than multimodal prompt tuning, often achieves performance\ncompetitive to multimodal approaches, making it a valuable first attempt. Our\nwork advances the understanding and applicability of different prompt-tuning\ntechniques for robust domain-specific segmentation. The source code is\navailable at https://github.com/naamiinepal/tunevlseg.\n","authors":["Rabin Adhikari","Safal Thapaliya","Manish Dhakal","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2410.05239v1.pdf","comment":"Accepted at ACCV 2024 (oral presentation)"},{"id":"http://arxiv.org/abs/2410.05235v1","updated":"2024-10-07T17:41:45Z","published":"2024-10-07T17:41:45Z","title":"CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with\n  Explanatory Argumentative Structures","summary":"  Explaining Artificial Intelligence (AI) decisions is a major challenge\nnowadays in AI, in particular when applied to sensitive scenarios like medicine\nand law. However, the need to explain the rationale behind decisions is a main\nissue also for human-based deliberation as it is important to justify\n\\textit{why} a certain decision has been taken. Resident medical doctors for\ninstance are required not only to provide a (possibly correct) diagnosis, but\nalso to explain how they reached a certain conclusion. Developing new tools to\naid residents to train their explanation skills is therefore a central\nobjective of AI in education. In this paper, we follow this direction, and we\npresent, to the best of our knowledge, the first multilingual dataset for\nMedical Question Answering where correct and incorrect diagnoses for a clinical\ncase are enriched with a natural language explanation written by doctors. These\nexplanations have been manually annotated with argument components (i.e.,\npremise, claim) and argument relations (i.e., attack, support), resulting in\nthe Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases\nin four languages (English, Spanish, French, Italian) with explanations, where\nwe annotated 5021 claims, 2313 premises, 2431 support relations, and 1106\nattack relations. We conclude by showing how competitive baselines perform over\nthis challenging dataset for the argument mining task.\n","authors":["katerina Sviridova","Anar Yeginbergen","Ainara Estarrona","Elena Cabrio","Serena Villata","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2410.05235v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.05224v1","updated":"2024-10-07T17:29:40Z","published":"2024-10-07T17:29:40Z","title":"Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates","summary":"  Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.\n","authors":["Avanika Narayan","Mayee F. Chen","Kush Bhatia","Christopher R"],"pdf_url":"https://arxiv.org/pdf/2410.05224v1.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2410.05222v1","updated":"2024-10-07T17:26:31Z","published":"2024-10-07T17:26:31Z","title":"Precise Model Benchmarking with Only a Few Observations","summary":"  How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.\n","authors":["Riccardo Fogliato","Pratik Patil","Nil-Jana Akpinar","Mathew Monfort"],"pdf_url":"https://arxiv.org/pdf/2410.05222v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.15877v3","updated":"2024-10-07T17:23:30Z","published":"2024-06-22T15:52:04Z","title":"BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions","summary":"  Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical requires the capability of utilizing diverse function calls as tools\nto efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.\n","authors":["Terry Yue Zhuo","Minh Chien Vu","Jenny Chim","Han Hu","Wenhao Yu","Ratnadira Widyasari","Imam Nur Bani Yusuf","Haolan Zhan","Junda He","Indraneil Paul","Simon Brunner","Chen Gong","Thong Hoang","Armel Randy Zebaze","Xiaoheng Hong","Wen-Ding Li","Jean Kaddour","Ming Xu","Zhihan Zhang","Prateek Yadav","Naman Jain","Alex Gu","Zhoujun Cheng","Jiawei Liu","Qian Liu","Zijian Wang","David Lo","Binyuan Hui","Niklas Muennighoff","Daniel Fried","Xiaoning Du","Harm de Vries","Leandro Von Werra"],"pdf_url":"https://arxiv.org/pdf/2406.15877v3.pdf","comment":"44 pages, 14 figures, 7 tables, built with love by the BigCode\n  community :)"},{"id":"http://arxiv.org/abs/2410.05218v1","updated":"2024-10-07T17:22:56Z","published":"2024-10-07T17:22:56Z","title":"Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories","summary":"  Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs.\n","authors":["Toni J. B. Liu","Nicolas Boull","Raphal Sarfati","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2410.05218v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2309.02233v3","updated":"2024-10-07T17:21:45Z","published":"2023-09-05T13:39:38Z","title":"Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question\n  Answering (Published in Findings of EMNLP 2024)","summary":"  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.\n","authors":["Yubo Wang","Xueguang Ma","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2309.02233v3.pdf","comment":"This version has been accepted and published at EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2410.05210v1","updated":"2024-10-07T17:16:20Z","published":"2024-10-07T17:16:20Z","title":"Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving\n  Vision-Linguistic Compositionality","summary":"  In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.\n","authors":["Youngtaek Oh","Jae Won Cho","Dong-Jin Kim","In So Kweon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.05210v1.pdf","comment":"EMNLP 2024 (Long, Main). Project page:\n  https://ytaek-oh.github.io/fsc-clip"},{"id":"http://arxiv.org/abs/2406.06369v4","updated":"2024-10-07T17:13:45Z","published":"2024-06-10T15:30:13Z","title":"Annotation alignment: Comparing LLM and human annotations of\n  conversational safety","summary":"  Do LLMs align with human perceptions of safety? We study this question via\nannotation alignment, the extent to which LLMs and humans agree when annotating\nthe safety of user-chatbot conversations. We leverage the recent DICES dataset\n(Aroyo et al., 2023), in which 350 conversations are each rated for safety by\n112 annotators spanning 10 race-gender groups. GPT-4 achieves a Pearson\ncorrelation of $r = 0.59$ with the average annotator rating, \\textit{higher}\nthan the median annotator's correlation with the average ($r=0.51$). We show\nthat larger datasets are needed to resolve whether LLMs exhibit disparities in\nhow well they correlate with different demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation within groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.\n","authors":["Rajiv Movva","Pang Wei Koh","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2406.06369v4.pdf","comment":"EMNLP 2024 (Main). Main text contains 6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.05206v1","updated":"2024-10-07T17:09:03Z","published":"2024-10-07T17:09:03Z","title":"Studying and Mitigating Biases in Sign Language Understanding Models","summary":"  Ensuring that the benefits of sign language technologies are distributed\nequitably among all community members is crucial. Thus, it is important to\naddress potential biases and inequities that may arise from the design or use\nof these resources. Crowd-sourced sign language datasets, such as the ASL\nCitizen dataset, are great resources for improving accessibility and preserving\nlinguistic diversity, but they must be used thoughtfully to avoid reinforcing\nexisting biases.\n  In this work, we utilize the rich information about participant demographics\nand lexical features present in the ASL Citizen dataset to study and document\nthe biases that may result from models trained on crowd-sourced sign datasets.\nFurther, we apply several bias mitigation techniques during model training, and\nfind that these techniques reduce performance disparities without decreasing\naccuracy. With the publication of this work, we release the demographic\ninformation about the participants in the ASL Citizen dataset to encourage\nfuture bias mitigation work in this space.\n","authors":["Katherine Atwell","Danielle Bragg","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.05206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05193v1","updated":"2024-10-07T16:50:47Z","published":"2024-10-07T16:50:47Z","title":"RevisEval: Improving LLM-as-a-Judge via Response-Adapted References","summary":"  With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.\n","authors":["Qiyuan Zhang","Yufei Wang","Tiezheng YU","Yuxin Jiang","Chuhan Wu","Liangyou Li","Yasheng Wang","Xin Jiang","Lifeng Shang","Ruiming Tang","Fuyuan Lyu","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05192v1","updated":"2024-10-07T16:49:39Z","published":"2024-10-07T16:49:39Z","title":"Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss\n  Landscape Perspective","summary":"  Training language models currently requires pre-determining a fixed compute\nbudget because the typical cosine learning rate schedule depends on the total\nnumber of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a\nconstant learning rate to produce a main branch of iterates that can in\nprinciple continue indefinitely without a pre-specified compute budget. Then,\ngiven any compute budget, one can branch out from the main branch at a proper\nat any time with a rapidly decaying learning rate to produce a strong model.\nEmpirically, WSD generates a non-traditional loss curve: the loss remains\nelevated during the stable phase but sharply declines during the decay phase.\nTowards explaining this phenomenon, we conjecture that pretraining loss\nexhibits a river valley landscape, which resembles a deep valley with a river\nat its bottom. Under this assumption, we show that during the stable phase, the\niterate undergoes large oscillations due to the high learning rate, yet it\nprogresses swiftly along the river. During the decay phase, the rapidly\ndropping learning rate minimizes the iterate's oscillations, moving it closer\nto the river and revealing true optimization progress. Therefore, the sustained\nhigh learning rate phase and fast decaying phase are responsible for progress\nin the river and the mountain directions respectively, and are both critical.\nOur analysis predicts phenomenons consistent with empirical observations and\nshows that this landscape can emerge from pretraining on a simple bi-gram\ndataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that\nreuses previous checkpoints' decay phases and keeps only one main branch, where\nwe resume from a decayed checkpoint. WSD-S empirically outperforms WSD and\nCyclic-Cosine in obtaining multiple language model checkpoints across various\ncompute budgets in a single run for parameters scaling from 0.1B to 1.2B.\n","authors":["Kaiyue Wen","Zhiyuan Li","Jason Wang","David Hall","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05192v1.pdf","comment":"45 pages,13 figures"},{"id":"http://arxiv.org/abs/2410.02525v2","updated":"2024-10-07T16:46:05Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00099v4","updated":"2024-10-07T16:45:42Z","published":"2024-04-30T18:00:02Z","title":"Creative Beam Search: LLM-as-a-Judge For Improving Response Generation","summary":"  Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2405.00099v4.pdf","comment":"Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)"},{"id":"http://arxiv.org/abs/2410.05183v1","updated":"2024-10-07T16:42:10Z","published":"2024-10-07T16:42:10Z","title":"Beyond Correlation: Interpretable Evaluation of Machine Translation\n  Metrics","summary":"  Machine Translation (MT) evaluation metrics assess translation quality\nautomatically. Recently, researchers have employed MT metrics for various new\nuse cases, such as data filtering and translation re-ranking. However, most MT\nmetrics return assessments as scalar scores that are difficult to interpret,\nposing a challenge to making informed design choices. Moreover, MT metrics'\ncapabilities have historically been evaluated using correlation with human\njudgment, which, despite its efficacy, falls short of providing intuitive\ninsights into metric performance, especially in terms of new metric use cases.\nTo address these issues, we introduce an interpretable evaluation framework for\nMT metrics. Within this framework, we evaluate metrics in two scenarios that\nserve as proxies for the data filtering and translation re-ranking use cases.\nFurthermore, by measuring the performance of MT metrics using Precision,\nRecall, and F-score, we offer clearer insights into their capabilities than\ncorrelation with human judgments. Finally, we raise concerns regarding the\nreliability of manually curated data following the Direct Assessments+Scalar\nQuality Metrics (DA+SQM) guidelines, reporting a notably low agreement with\nMultidimensional Quality Metrics (MQM) annotations.\n","authors":["Stefano Perrella","Lorenzo Proietti","Pere-Llus Huguet Cabot","Edoardo Barba","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2410.05183v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference. 26 pages"},{"id":"http://arxiv.org/abs/2410.05180v1","updated":"2024-10-07T16:40:21Z","published":"2024-10-07T16:40:21Z","title":"Enhancing Equity in Large Language Models for Medical Applications","summary":"  Recent advancements have highlighted the potential of large language models\n(LLMs) in medical applications, notably in automating Clinical Trial Matching\nfor translational research and providing medical question-answering for\nclinical decision support. However, our study reveals significant inequities in\nthe use of LLMs, particularly for individuals from specific racial, gender, and\nunderrepresented groups influenced by social determinants of health. These\ndisparities could worsen existing health inequities if LLMs are broadly adopted\nin healthcare. To address this, we propose and evaluate a novel framework,\nEquityGuard, designed to detect and mitigate biases in LLM-based medical\napplications. EquityGuard incorporates a Bias Detection Mechanism capable of\nidentifying and correcting unfair predictions, thus enhancing outcomes and\npromoting equity across diverse population groups.\n","authors":["Yuelyu Ji","Wenhe Ma","Sonish Sivarajkumar","Hang Zhang","Eugene Mathew Sadhu","Zhuochun Li","Xizhi Wu","Shyam Visweswaran","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02381v2","updated":"2024-10-07T16:39:24Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.14901v2","updated":"2024-10-07T16:38:35Z","published":"2024-02-22T18:09:33Z","title":"A Usage-centric Take on Intent Understanding in E-Commerce","summary":"  Identifying and understanding user intents is a pivotal task for E-Commerce.\nDespite its essential role in product recommendation and business user\nprofiling analysis, intent understanding has not been consistently defined or\naccurately benchmarked. In this paper, we focus on predicative user intents as\n\"how a customer uses a product\", and pose intent understanding as a natural\nlanguage reasoning task, independent of product ontologies. We identify two\nweaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:\ncategory-rigidity and property-ambiguity. They limit its ability to strongly\nalign user intents with products having the most desirable property, and to\nrecommend useful products across diverse categories. Following these\nobservations, we introduce a Product Recovery Benchmark featuring a novel\nevaluation framework and an example dataset. We further validate the above\nFolkScope weaknesses on this benchmark. Our code and dataset are available at\nhttps://github.com/stayones/Usgae-Centric-Intent-Understanding.\n","authors":["Wendi Zhou","Tianyi Li","Pavlos Vougiouklis","Mark Steedman","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2402.14901v2.pdf","comment":"Acepted by EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2310.09675v2","updated":"2024-10-07T16:28:52Z","published":"2023-10-14T22:24:26Z","title":"Efficient Model-Agnostic Multi-Group Equivariant Networks","summary":"  Constructing model-agnostic group equivariant networks, such as equitune\n(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be\ncomputationally expensive for large product groups. We address this problem by\nproviding efficient model-agnostic equivariant designs for two related\nproblems: one where the network has multiple inputs each with potentially\ndifferent groups acting on them, and another where there is a single input but\nthe group acting on it is a large product group. For the first design, we\ninitially consider a linear model and characterize the entire equivariant space\nthat satisfies this constraint. This characterization gives rise to a novel\nfusion layer between different channels that satisfies an invariance-symmetry\n(IS) constraint, which we call an IS layer. We then extend this design beyond\nlinear models, similar to equitune, consisting of equivariant and IS layers. We\nalso show that the IS layer is a universal approximator of invariant-symmetric\nfunctions. Inspired by the first design, we use the notion of the IS property\nto design a second efficient model-agnostic equivariant design for large\nproduct groups acting on a single input. For the first design, we provide\nexperiments on multi-image classification where each view is transformed\nindependently with transformations such as rotations. We find equivariant\nmodels are robust to such transformations and perform competitively otherwise.\nFor the second design, we consider three applications: language\ncompositionality on the SCAN dataset to product groups; fairness in natural\nlanguage generation from GPT-2 to address intersectionality; and robust\nzero-shot image classification with CLIP. Overall, our methods are simple and\ngeneral, competitive with equitune and its variants, while also being\ncomputationally more efficient.\n","authors":["Razan Baltaji","Sourya Basu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.09675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v2","updated":"2024-10-07T16:26:00Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05168v1","updated":"2024-10-07T16:25:39Z","published":"2024-10-07T16:25:39Z","title":"ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation","summary":"  Reranking documents based on their relevance to a given query is critical in\ninformation retrieval. Traditional reranking methods often focus on improving\nthe initial rankings but lack transparency, failing to explain why one document\nis ranked higher. In this paper, we introduce ReasoningRank, a novel reranking\napproach that enhances clarity by generating two types of reasoning: explicit\nreasoning, which explains how a document addresses the query, and comparison\nreasoning, which justifies the relevance of one document over another. We\nleverage large language models (LLMs) as teacher models to generate these\nexplanations and distill this knowledge into smaller, more resource-efficient\nstudent models. While the student models may not outperform LLMs in speed, they\nsignificantly reduce the computational burden by requiring fewer resources,\nmaking them more suitable for large-scale or resource-constrained settings.\nThese student models are trained to both generate meaningful reasoning and\nrerank documents, achieving competitive performance across multiple datasets,\nincluding MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank\nimproves reranking accuracy and provides valuable insights into the\ndecision-making process, offering a structured and interpretable solution for\nreranking tasks.\n","authors":["Yuelyu Ji","Zhuochun Li","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.05168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02902v2","updated":"2024-10-07T16:25:04Z","published":"2024-10-03T18:48:38Z","title":"Better Instruction-Following Through Minimum Bayes Risk","summary":"  General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.\n","authors":["Ian Wu","Patrick Fernandes","Amanda Bertsch","Seungone Kim","Sina Pakazad","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.02902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05165v1","updated":"2024-10-07T16:23:36Z","published":"2024-10-07T16:23:36Z","title":"Efficient Inference for Large Language Model-based Generative\n  Recommendation","summary":"  Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.\n","authors":["Xinyu Lin","Chaoqun Yang","Wenjie Wang","Yongqi Li","Cunxiao Du","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.05165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05162v1","updated":"2024-10-07T16:14:47Z","published":"2024-10-07T16:14:47Z","title":"Deciphering the Interplay of Parametric and Non-parametric Memory in\n  Retrieval-augmented Language Models","summary":"  Generative language models often struggle with specialized or less-discussed\nknowledge. A potential solution is found in Retrieval-Augmented Generation\n(RAG) models which act like retrieving information before generating responses.\nIn this study, we explore how the \\textsc{Atlas} approach, a RAG model, decides\nbetween what it already knows (parametric) and what it retrieves\n(non-parametric). We use causal mediation analysis and controlled experiments\nto examine how internal representations influence information processing. Our\nfindings disentangle the effects of parametric knowledge and the retrieved\ncontext. They indicate that in cases where the model can choose between both\ntypes of information (parametric and non-parametric), it relies more on the\ncontext than the parametric knowledge. Furthermore, the analysis investigates\nthe computations involved in \\emph{how} the model uses the information from the\ncontext. We find that multiple mechanisms are active within the model and can\nbe detected with mediation analysis: first, the decision of \\emph{whether the\ncontext is relevant}, and second, how the encoder computes output\nrepresentations to support copying when relevant.\n","authors":["Mehrdad Farahani","Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2410.05162v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05160v1","updated":"2024-10-07T16:14:05Z","published":"2024-10-07T16:14:05Z","title":"VLM2Vec: Training Vision-Language Models for Massive Multimodal\n  Embedding Tasks","summary":"  Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.\n","authors":["Ziyan Jiang","Rui Meng","Xinyi Yang","Semih Yavuz","Yingbo Zhou","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05160v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2405.14577v2","updated":"2024-10-07T16:01:49Z","published":"2024-05-23T13:51:55Z","title":"Representation noising effectively prevents harmful fine-tuning on LLMs","summary":"  Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","ukasz Bartoszcze","David Atanasov","Robie Gonzales","Subhabrata Majumdar","Carsten Maple","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2405.14577v2.pdf","comment":"Published in NeurIPs 2024"},{"id":"http://arxiv.org/abs/2311.09090v4","updated":"2024-10-07T16:01:06Z","published":"2023-11-15T16:35:59Z","title":"Social Bias Probing: Fairness Benchmarking for Language Models","summary":"  While the impact of social biases in language models has been recognized,\nprior methods for bias evaluation have been limited to binary association tests\non small datasets, limiting our understanding of bias complexities. This paper\nproposes a novel framework for probing language models for social biases by\nassessing disparate treatment, which involves treating individuals differently\naccording to their affiliation with a sensitive demographic group. We curate\nSoFa, a large-scale benchmark designed to address the limitations of existing\nfairness collections. SoFa expands the analysis beyond the binary comparison of\nstereotypical versus anti-stereotypical identities to include a diverse range\nof identities and stereotypes. Comparing our methodology with existing\nbenchmarks, we reveal that biases within language models are more nuanced than\nacknowledged, indicating a broader scope of encoded biases than previously\nrecognized. Benchmarking LMs on SoFa, we expose how identities expressing\ndifferent religions lead to the most pronounced disparate treatments across all\nmodels. Finally, our findings indicate that real-life adversities faced by\nvarious groups such as women and people with disabilities are mirrored in the\nbehavior of these models.\n","authors":["Marta Marchiori Manerba","Karolina Staczak","Riccardo Guidotti","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2311.09090v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05146v1","updated":"2024-10-07T15:58:03Z","published":"2024-10-07T15:58:03Z","title":"CTC-GMM: CTC guided modality matching for fast and accurate streaming\n  speech translation","summary":"  Models for streaming speech translation (ST) can achieve high accuracy and\nlow latency if they're developed with vast amounts of paired audio in the\nsource language and written text in the target language. Yet, these text labels\nfor the target language are often pseudo labels due to the prohibitive cost of\nmanual ST data labeling. In this paper, we introduce a methodology named\nConnectionist Temporal Classification guided modality matching (CTC-GMM) that\nenhances the streaming ST model by leveraging extensive machine translation\n(MT) text data. This technique employs CTC to compress the speech sequence into\na compact embedding sequence that matches the corresponding text sequence,\nallowing us to utilize matched {source-target} language text pairs from the MT\ncorpora to refine the streaming ST model further. Our evaluations with FLEURS\nand CoVoST2 show that the CTC-GMM approach can increase translation accuracy\nrelatively by 13.9% and 6.4% respectively, while also boosting decoding speed\nby 59.7% on GPU.\n","authors":["Rui Zhao","Jinyu Li","Ruchao Fan","Matt Post"],"pdf_url":"https://arxiv.org/pdf/2410.05146v1.pdf","comment":"Accepted by IEEE Spoken Language Technology Workshop (SLT 2024)"},{"id":"http://arxiv.org/abs/2407.10930v2","updated":"2024-10-07T15:52:48Z","published":"2024-07-15T17:30:31Z","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","summary":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai\n","authors":["Dilara Soylu","Christopher Potts","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2407.10930v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.00126v2","updated":"2024-10-07T15:44:45Z","published":"2024-02-29T21:05:37Z","title":"FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition","summary":"  Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.\n","authors":["Xiaoqiang Wang","Lingfei Wu","Tengfei Ma","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.00126v2.pdf","comment":"Accepted at EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2404.12132v2","updated":"2024-10-07T15:28:18Z","published":"2024-04-18T12:33:57Z","title":"Non-Invasive Suicide Risk Prediction Through Speech Analysis","summary":"  The delayed access to specialized psychiatric assessments and care for\npatients at risk of suicidal tendencies in emergency departments creates a\nnotable gap in timely intervention, hindering the provision of adequate mental\nhealth support during critical situations. To address this, we present a\nnon-invasive, speech-based approach for automatic suicide risk assessment. For\nour study, we collected a novel speech recording dataset from $20$ patients. We\nextract three sets of features, including wav2vec, interpretable speech and\nacoustic features, and deep learning-based spectral representations. We proceed\nby conducting a binary classification to assess suicide risk in a\nleave-one-subject-out fashion. Our most effective speech model achieves a\nbalanced accuracy of $66.2\\,\\%$. Moreover, we show that integrating our speech\nmodel with a series of patients' metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The metadata\nintegration yields a balanced accuracy of $94.4\\,\\%$, marking an absolute\nimprovement of $28.2\\,\\%$, demonstrating the efficacy of our proposed\napproaches for automatic suicide risk assessment in emergency medicine.\n","authors":["Shahin Amiriparian","Maurice Gerczuk","Justina Lutz","Wolfgang Strube","Irina Papazova","Alkomiet Hasan","Alexander Kathan","Bjrn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2404.12132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17385v2","updated":"2024-10-07T15:15:18Z","published":"2024-06-25T09:04:21Z","title":"Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance","summary":"  Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.\n","authors":["Manon Reusens","Philipp Borchert","Jochen De Weerdt","Bart Baesens"],"pdf_url":"https://arxiv.org/pdf/2406.17385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05257v2","updated":"2024-10-07T15:14:26Z","published":"2024-09-09T00:40:47Z","title":"UPCS: Unbiased Persona Construction for Dialogue Generation","summary":"  Narrative systems, such as dialogue and storytelling systems, often utilize\npersona profiles to enhance personalized interactions. Existing persona\nprofiles frequently exhibit biases, posing risks to system integrity and\nfairness. To address this, we introduce the UPCS framework, which categorizes\ncharacter descriptions into eight dimensions, including bias mitigation\nstrategies. Experimental results demonstrate UPCS's superiority in accuracy,\ndiversity, bias elimination, and user satisfaction, marking a significant\nadvancement in persona construction for reliable narrative systems.\n","authors":["Kuiyun Chen","Yanbin Wei"],"pdf_url":"https://arxiv.org/pdf/2409.05257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11472v3","updated":"2024-10-07T15:11:12Z","published":"2022-05-23T17:14:32Z","title":"Diversity Over Size: On the Effect of Sample and Topic Sizes for\n  Topic-Dependent Argument Mining Datasets","summary":"  The task of Argument Mining, that is extracting and classifying argument\ncomponents for a specific topic from large document sources, is an inherently\ndifficult task for machine learning models and humans alike, as large Argument\nMining datasets are rare and recognition of argument components requires expert\nknowledge. The task becomes even more difficult if it also involves stance\ndetection of retrieved arguments. In this work, we investigate the effect of\nArgument Mining dataset composition in few- and zero-shot settings. Our\nfindings show that, while fine-tuning is mandatory to achieve acceptable model\nperformance, using carefully composed training samples and reducing the\ntraining sample size by up to almost 90% can still yield 95% of the maximum\nperformance. This gain is consistent across three Argument Mining tasks on\nthree different datasets. We also publish a new dataset for future\nbenchmarking.\n","authors":["Benjamin Schiller","Johannes Daxenberger","Andreas Waldis","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2205.11472v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00161v2","updated":"2024-10-07T15:07:09Z","published":"2024-09-30T19:09:13Z","title":"KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head","summary":"  Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.\n","authors":["Isaac Rehg"],"pdf_url":"https://arxiv.org/pdf/2410.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15929v2","updated":"2024-10-07T15:01:48Z","published":"2024-02-24T23:16:57Z","title":"Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs","summary":"  Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.\n","authors":["Isha Chaudhary","Vedaant V. Jain","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2402.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05102v1","updated":"2024-10-07T15:01:29Z","published":"2024-10-07T15:01:29Z","title":"SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks","summary":"  Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.\n","authors":["Fenia Christopoulou","Ronald Cardenas","Gerasimos Lampouras","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05102v1.pdf","comment":"20 papges, 9 figures, 5 tables. Under Review"},{"id":"http://arxiv.org/abs/2406.16078v2","updated":"2024-10-07T15:01:26Z","published":"2024-06-23T11:11:46Z","title":"First Heuristic Then Rational: Dynamic Use of Heuristics in Language\n  Model Reasoning","summary":"  Multi-step reasoning instruction, such as chain-of-thought prompting, is\nwidely adopted to explore better language models (LMs) performance. We report\non the systematic strategy that LMs employ in such a multi-step reasoning\nprocess. Our controlled experiments reveal that LMs rely more heavily on\nheuristics, such as lexical overlap, in the earlier stages of reasoning, where\nmore reasoning steps remain to reach a goal. Conversely, their reliance on\nheuristics decreases as LMs progress closer to the final answer through\nmultiple reasoning steps. This suggests that LMs can backtrack only a limited\nnumber of future steps and dynamically combine heuristic strategies with\nrationale ones in tasks involving multi-step reasoning.\n","authors":["Yoichi Aoki","Keito Kudo","Tatsuki Kuribayashi","Shusaku Sone","Masaya Taniguchi","Keisuke Sakaguchi","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2406.16078v2.pdf","comment":"This paper is accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05099v1","updated":"2024-10-07T14:55:20Z","published":"2024-10-07T14:55:20Z","title":"Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances","summary":"  Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them.\n","authors":["Alina Wrblewska"],"pdf_url":"https://arxiv.org/pdf/2410.05099v1.pdf","comment":"Accepted at CoNLL 2024"},{"id":"http://arxiv.org/abs/2410.02707v2","updated":"2024-10-07T14:46:11Z","published":"2024-10-03T17:31:31Z","title":"LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations","summary":"  Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.\n","authors":["Hadas Orgad","Michael Toker","Zorik Gekhman","Roi Reichart","Idan Szpektor","Hadas Kotek","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2410.02707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16671v7","updated":"2024-10-07T14:44:44Z","published":"2024-02-26T15:47:01Z","title":"StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding","summary":"  Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Mistral and the CodeLlama model family, ranging from 7B to 34B\nparameters. Our StructLM series surpasses task-specific models on 16 out of 18\nevaluated datasets and establishes new SoTA performance on 8 SKG tasks.\nFurthermore, StructLM demonstrates strong generalization across 6 novel\nheld-out SKG tasks, outperforming TableLlama by an average of 35\\% and Flan-UL2\n20B by an average of 10\\%. Contrary to expectations, we observe that scaling\nmodel size offers marginal benefits, with StructLM-34B showing only slight\nimprovements over StructLM-7B. This suggests that structured knowledge\ngrounding is still a challenging task and requires more innovative design to\npush to a new level.\n","authors":["Alex Zhuang","Ge Zhang","Tianyu Zheng","Xinrun Du","Junjie Wang","Weiming Ren","Stephen W. Huang","Jie Fu","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16671v7.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05085v1","updated":"2024-10-07T14:39:45Z","published":"2024-10-07T14:39:45Z","title":"Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification","summary":"  Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness.\n","authors":["Jeremie Bogaert","Marie-Catherine de Marneffe","Antonin Descampe","Louis Escouflaire","Cedrick Fairon","Francois-Xavier Standaert"],"pdf_url":"https://arxiv.org/pdf/2410.05085v1.pdf","comment":"This paper is a faithful translation of a paper which was\n  peer-reviewed and published in the French journal Traitement Automatique des\n  Langues, n. 64"},{"id":"http://arxiv.org/abs/2405.14768v2","updated":"2024-10-07T14:35:14Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.05080v1","updated":"2024-10-07T14:33:50Z","published":"2024-10-07T14:33:50Z","title":"ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery","summary":"  The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.\n","authors":["Ziru Chen","Shijie Chen","Yuting Ning","Qianheng Zhang","Boshi Wang","Botao Yu","Yifei Li","Zeyi Liao","Chen Wei","Zitong Lu","Vishal Dey","Mingyi Xue","Frazier N. Baker","Benjamin Burns","Daniel Adu-Ampratwum","Xuhui Huang","Xia Ning","Song Gao","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05080v1.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2410.05077v1","updated":"2024-10-07T14:31:43Z","published":"2024-10-07T14:31:43Z","title":"ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering","summary":"  Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.\n","authors":["Francesco Maria Molfese","Simone Conia","Riccardo Orlando","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2410.05077v1.pdf","comment":"Accepted at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.05076v1","updated":"2024-10-07T14:30:27Z","published":"2024-10-07T14:30:27Z","title":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention","summary":"  Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.\n","authors":["Lijie Yang","Zhihao Zhang","Zhuofu Chen","Zikun Li","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2410.05076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12034v2","updated":"2024-10-07T14:27:56Z","published":"2024-06-17T19:06:54Z","title":"Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts","summary":"  We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.\n","authors":["Junmo Kang","Leonid Karlinsky","Hongyin Luo","Zhen Wang","Jacob Hansen","James Glass","David Cox","Rameswar Panda","Rogerio Feris","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2406.12034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19745v2","updated":"2024-10-07T14:17:44Z","published":"2024-09-29T15:40:54Z","title":"PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances\n  Retrieval-Augmented Generation with Zero Inference Overhead","summary":"  Large language models (LLMs) enhanced with retrieval-augmented generation\n(RAG) have introduced a new paradigm for web search. However, the limited\ncontext awareness of LLMs degrades their performance on RAG tasks. Existing\nmethods to enhance context awareness are often inefficient, incurring time or\nmemory overhead during inference, and many are tailored to specific position\nembeddings. In this paper, we propose Position-Embedding-Agnostic attention\nRe-weighting (PEAR), which enhances the context awareness of LLMs with zero\ninference overhead. Specifically, on a proxy task focused on context copying,\nwe first detect heads which suppress the models' context awareness thereby\ndiminishing RAG performance. To weaken the impact of these heads, we re-weight\ntheir outputs with learnable coefficients. The LLM (with frozen parameters) is\noptimized by adjusting these coefficients to minimize loss on the proxy task.\nAs a result, the coefficients are optimized to values less than one, thereby\nreducing their tendency to suppress RAG performance. During inference, the\noptimized coefficients are fixed to re-weight these heads, regardless of the\nspecific task at hand. Our proposed PEAR offers two major advantages over\nprevious approaches: (1) It introduces zero additional inference overhead in\nterms of memory usage or inference time, while outperforming competitive\nbaselines in accuracy and efficiency across various RAG tasks. (2) It is\nindependent of position embedding algorithms, ensuring broader applicability.\n","authors":["Tao Tan","Yining Qian","Ang Lv","Hongzhan Lin","Songhao Wu","Yongbo Wang","Feng Wang","Jingtong Wu","Xin Lu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19745v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.05052v1","updated":"2024-10-07T14:09:58Z","published":"2024-10-07T14:09:58Z","title":"Initialization of Large Language Models via Reparameterization to\n  Mitigate Loss Spikes","summary":"  Loss spikes, a phenomenon in which the loss value diverges suddenly, is a\nfundamental issue in the pre-training of large language models. This paper\nsupposes that the non-uniformity of the norm of the parameters is one of the\ncauses of loss spikes. Here, in training of neural networks, the scale of the\ngradients is required to be kept constant throughout the layers to avoid the\nvanishing and exploding gradients problem. However, to meet these requirements\nin the Transformer model, the norm of the model parameters must be non-uniform,\nand thus, parameters whose norm is smaller are more sensitive to the parameter\nupdate. To address this issue, we propose a novel technique, weight scaling as\nreparameterization (WeSaR). WeSaR introduces a gate parameter per parameter\nmatrix and adjusts it to the value satisfying the requirements. Because of the\ngate parameter, WeSaR sets the norm of the original parameters uniformly, which\nresults in stable training. Experimental results with the Transformer decoders\nconsisting of 130 million, 1.3 billion, and 13 billion parameters showed that\nWeSaR stabilizes and accelerates training and that it outperformed compared\nmethods including popular initialization methods.\n","authors":["Kosuke Nishida","Kyosuke Nishida","Kuniko Saito"],"pdf_url":"https://arxiv.org/pdf/2410.05052v1.pdf","comment":"EMNLP2024 accepted"},{"id":"http://arxiv.org/abs/2406.12058v4","updated":"2024-10-07T14:08:13Z","published":"2024-06-17T19:50:40Z","title":"WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions","summary":"  Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WDs). We focus on\ntwo existing mental health and well-being datasets: (a) Multi-label\nClassification-based MultiWD, and (b) WellXplain for evaluating attention\nmechanism veracity against expert-labeled explanations. The labels are based on\nHalbert Dunn's theory of wellness, which gives grounding to our evaluation. We\nreveal four surprising results about LMs/LLMs: (1) Despite their human-like\ncapabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on\nWellXplain fails to deliver any remarkable improvements in performance or\nexplanations. (2) Re-examining LMs' predictions based on a confidence-oriented\nloss function reveals a significant performance drop. (3) Across all LMs/LLMs,\nthe alignment between attention and explanations remains low, with LLMs scoring\na dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific\nknowledge and undervalue explanations, causing these discrepancies. This study\nhighlights the need for further research into their consistency and\nexplanations in mental health and well-being.\n","authors":["Seyedali Mohammadi","Edward Raff","Jinendra Malekar","Vedant Palit","Francis Ferraro","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2406.12058v4.pdf","comment":"Accepted in BlackboxNLP @ EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05047v1","updated":"2024-10-07T14:01:20Z","published":"2024-10-07T14:01:20Z","title":"A test suite of prompt injection attacks for LLM-based machine\n  translation","summary":"  LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied.\n","authors":["Antonio Valerio Miceli-Barone","Zhifan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05046v1","updated":"2024-10-07T14:00:18Z","published":"2024-10-07T14:00:18Z","title":"Named Clinical Entity Recognition Benchmark","summary":"  This technical report introduces a Named Clinical Entity Recognition\nBenchmark for evaluating language models in healthcare, addressing the crucial\nnatural language processing (NLP) task of extracting structured information\nfrom clinical narratives to support applications like automated coding,\nclinical trial cohort identification, and clinical decision support.\n  The leaderboard provides a standardized platform for assessing diverse\nlanguage models, including encoder and decoder architectures, on their ability\nto identify and classify clinical entities across multiple medical domains. A\ncurated collection of openly available clinical datasets is utilized,\nencompassing entities such as diseases, symptoms, medications, procedures, and\nlaboratory measurements. Importantly, these entities are standardized according\nto the Observational Medical Outcomes Partnership (OMOP) Common Data Model,\nensuring consistency and interoperability across different healthcare systems\nand datasets, and a comprehensive evaluation of model performance. Performance\nof models is primarily assessed using the F1-score, and it is complemented by\nvarious assessment modes to provide comprehensive insights into model\nperformance. The report also includes a brief analysis of models evaluated to\ndate, highlighting observed trends and limitations.\n  By establishing this benchmarking framework, the leaderboard aims to promote\ntransparency, facilitate comparative analyses, and drive innovation in clinical\nentity recognition tasks, addressing the need for robust evaluation methods in\nhealthcare NLP.\n","authors":["Wadood M Abdul","Marco AF Pimentel","Muhammad Umar Salman","Tathagata Raha","Clment Christophe","Praveen K Kanithi","Nasir Hayat","Ronnie Rajan","Shadab Khan"],"pdf_url":"https://arxiv.org/pdf/2410.05046v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05045v1","updated":"2024-10-07T14:00:08Z","published":"2024-10-07T14:00:08Z","title":"Can LLMs plan paths with extra hints from solvers?","summary":"  Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs.\n","authors":["Erik Wu","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2410.05045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05021v1","updated":"2024-10-07T13:24:24Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language Model pre-training benefits from a broader data mixture to enhance\nperformance across domains and languages. However, training on such\nheterogeneous text corpora is complex, requiring extensive and cost-intensive\nefforts. Since these data sources vary in lexical, syntactic, and semantic\naspects, they cause negative interference or the \"curse of multilinguality\". We\npropose a novel pre-training framework to alleviate this curse. Our method,\nDEPT, decouples the embedding layers from the transformer body while\nsimultaneously training the latter in multiple contexts. DEPT enables the model\nto train without being bound to a shared global vocabulary. DEPT: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces the\nparameter count of the token embeddings by up to 80% and the communication\ncosts by 675x for billion-scale models (3) enhances model generalization and\nplasticity in adapting to new languages and domains, and (4) allows training\nwith custom optimized vocabulary per data source. We prove DEPT's potential by\nperforming the first vocabulary-agnostic federated multilingual pre-training of\na 1.3 billion-parameter model across high and low-resource languages, reducing\nits parameter count by 409 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15160v2","updated":"2024-10-07T13:19:53Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"  Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.\n","authors":["Gilad Yehudai","Haim Kaplan","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2407.15160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05018v1","updated":"2024-10-07T13:19:08Z","published":"2024-10-07T13:19:08Z","title":"On the Biased Assessment of Expert Finding Systems","summary":"  In large organisations, identifying experts on a given topic is crucial in\nleveraging the internal knowledge spread across teams and departments.\nSo-called enterprise expert retrieval systems automatically discover and\nstructure employees' expertise based on the vast amount of heterogeneous data\navailable about them and the work they perform. Evaluating these systems\nrequires comprehensive ground truth expert annotations, which are hard to\nobtain. Therefore, the annotation process typically relies on automated\nrecommendations of knowledge areas to validate. This case study provides an\nanalysis of how these recommendations can impact the evaluation of expert\nfinding systems. We demonstrate on a popular benchmark that system-validated\nannotations lead to overestimated performance of traditional term-based\nretrieval models and even invalidate comparisons with more recent neural\nmethods. We also augment knowledge areas with synonyms to uncover a strong bias\ntowards literal mentions of their constituent words. Finally, we propose\nconstraints to the annotation process to prevent these biased evaluations, and\nshow that this still allows annotation suggestions of high utility. These\nfindings should inform benchmark creation or selection for expert finding, to\nguarantee meaningful comparison of methods.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Chris Develder","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2410.05018v1.pdf","comment":"Accepted to the 4th Workshop on Recommender Systems for Human\n  Resources (RecSys in HR 2024) as part of RecSys 2024"},{"id":"http://arxiv.org/abs/2402.18376v2","updated":"2024-10-07T13:17:03Z","published":"2024-02-28T14:52:15Z","title":"Tokenization Is More Than Compression","summary":"  Tokenization is a foundational step in natural language processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n","authors":["Craig W. Schmidt","Varshini Reddy","Haoran Zhang","Alec Alameddine","Omri Uzan","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2402.18376v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05006v1","updated":"2024-10-07T13:05:26Z","published":"2024-10-07T13:05:26Z","title":"SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness","summary":"  Accurately modeling the relationships between skills is a crucial part of\nhuman resources processes such as recruitment and employee development. Yet, no\nbenchmarks exist to evaluate such methods directly. We construct and release\nSkillMatch, a benchmark for the task of skill relatedness, based on expert\nknowledge mining from millions of job ads. Additionally, we propose a scalable\nself-supervised learning technique to adapt a Sentence-BERT model based on\nskill co-occurrence in job ads. This new method greatly surpasses traditional\nmodels for skill relatedness as measured on SkillMatch. By releasing SkillMatch\npublicly, we aim to contribute a foundation for research towards increased\naccuracy and transparency of skill-based recommendation systems.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Thomas Demeester","Chris Develder"],"pdf_url":"https://arxiv.org/pdf/2410.05006v1.pdf","comment":"Accepted to the International workshop on AI for Human Resources and\n  Public Employment Services (AI4HR&PES) as part of ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2406.04866v2","updated":"2024-10-07T12:32:24Z","published":"2024-06-07T12:01:59Z","title":"ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question\n  Answering","summary":"  We introduce ComplexTempQA, a large-scale dataset consisting of over 100\nmillion question-answer pairs designed to tackle the challenges in temporal\nquestion answering. ComplexTempQA significantly surpasses existing benchmarks\nlike HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from\nWikipedia and Wikidata, the dataset covers questions spanning over two decades\nand offers an unmatched breadth of topics. We introduce a unique taxonomy that\ncategorizes questions as attributes, comparisons, and counting questions, each\nrevolving around events, entities, and time periods. One standout feature of\nComplexTempQA is the high complexity of its questions, which demand effective\ncapabilities for answering such as across-time comparison, temporal\naggregation, and multi-hop reasoning involving temporal event ordering and\nentity recognition. Additionally, each question is accompanied by detailed\nmetadata, including specific time scopes, allowing for comprehensive evaluation\nand enhancement of the temporal reasoning abilities of large language models.\nComplexTempQA serves both as a testing ground for developing sophisticated AI\nmodels and as a foundation for advancing research in question answering,\ninformation retrieval, and language understanding.\n","authors":["Raphael Gruber","Abdelrahman Abdallah","Michael Frber","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2406.04866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04981v1","updated":"2024-10-07T12:22:06Z","published":"2024-10-07T12:22:06Z","title":"On the Rigour of Scientific Writing: Criteria, Analysis, and Insights","summary":"  Rigour is crucial for scientific research as it ensures the reproducibility\nand validity of results and findings. Despite its importance, little work\nexists on modelling rigour computationally, and there is a lack of analysis on\nwhether these criteria can effectively signal or measure the rigour of\nscientific papers in practice. In this paper, we introduce a bottom-up,\ndata-driven framework to automatically identify and define rigour criteria and\nassess their relevance in scientific writing. Our framework includes rigour\nkeyword extraction, detailed rigour definition generation, and salient criteria\nidentification. Furthermore, our framework is domain-agnostic and can be\ntailored to the evaluation of scientific rigour for different areas,\naccommodating the distinct salient criteria across fields. We conducted\ncomprehensive experiments based on datasets collected from two high impact\nvenues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the\neffectiveness of our framework in modelling rigour. In addition, we analyse\nlinguistic patterns of rigour, revealing that framing certainty is crucial for\nenhancing the perception of scientific rigour, while suggestion certainty and\nprobability uncertainty diminish it.\n","authors":["Joseph James","Chenghao Xiao","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04981v1.pdf","comment":"Accepted Findings at EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.02987v2","updated":"2024-10-07T12:11:58Z","published":"2024-02-05T13:18:42Z","title":"Reconstruct Your Previous Conversations! Comprehensively Investigating\n  Privacy Leakage Risks in Conversations with GPT Models","summary":"  Significant advancements have recently been made in large language models\nrepresented by GPT models. Users frequently have multi-round private\nconversations with cloud-hosted GPT models for task optimization. Yet, this\noperational paradigm introduces additional attack surfaces, particularly in\ncustom GPTs and hijacked chat sessions. In this paper, we introduce a\nstraightforward yet potent Conversation Reconstruction Attack. This attack\ntargets the contents of previous conversations between GPT models and benign\nusers, i.e., the benign users' input contents during their interaction with GPT\nmodels. The adversary could induce GPT models to leak such contents by querying\nthem with designed malicious prompts. Our comprehensive examination of privacy\nrisks during the interactions with GPT models under this attack reveals GPT-4's\nconsiderable resilience. We present two advanced attacks targeting improved\nreconstruction of past conversations, demonstrating significant privacy leakage\nacross all models under these advanced techniques. Evaluating various defense\nmechanisms, we find them ineffective against these attacks. Our findings\nhighlight the ease with which privacy can be compromised in interactions with\nGPT models, urging the community to safeguard against potential abuses of these\nmodels' capabilities.\n","authors":["Junjie Chu","Zeyang Sha","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02987v2.pdf","comment":"Accepted in EMNLP 2024. 14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.19339v2","updated":"2024-10-07T12:05:55Z","published":"2024-09-28T12:49:16Z","title":"Visual Question Decomposition on Multimodal Large Language Models","summary":"  Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.\n","authors":["Haowei Zhang","Jianzhe Liu","Zhen Han","Shuo Chen","Bailan He","Volker Tresp","Zhiqiang Xu","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2409.19339v2.pdf","comment":"Accepted to EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.04962v1","updated":"2024-10-07T12:01:32Z","published":"2024-10-07T12:01:32Z","title":"Activation Scaling for Steering and Interpreting Language Models","summary":"  Given the prompt \"Rome is in\", can we steer a language model to flip its\nprediction of an incorrect token \"France\" to a correct token \"Italy\" by only\nmultiplying a few relevant activation vectors with scalars? We argue that\nsuccessfully intervening on a model is a prerequisite for interpreting its\ninternal workings. Concretely, we establish a three-term objective: a\nsuccessful intervention should flip the correct with the wrong token and vice\nversa (effectiveness), and leave other tokens unaffected (faithfulness), all\nwhile being sparse (minimality). Using gradient-based optimization, this\nobjective lets us learn (and later evaluate) a specific kind of efficient and\ninterpretable intervention: activation scaling only modifies the signed\nmagnitude of activation vectors to strengthen, weaken, or reverse the steering\ndirections already encoded in the model. On synthetic tasks, this intervention\nperforms comparably with steering vectors in terms of effectiveness and\nfaithfulness, but is much more minimal allowing us to pinpoint interpretable\nmodel components. We evaluate activation scaling from different angles, compare\nperformance on different datasets, and make activation scalars a learnable\nfunction of the activation vectors themselves to generalize to varying-length\nprompts.\n","authors":["Niklas Stoehr","Kevin Du","Vsteinn Snbjarnarson","Robert West","Ryan Cotterell","Aaron Schein"],"pdf_url":"https://arxiv.org/pdf/2410.04962v1.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.17023v2","updated":"2024-10-07T11:59:37Z","published":"2024-07-24T06:06:07Z","title":"DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models","summary":"  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.\n","authors":["Sara Vera Marjanovi","Haeun Yu","Pepa Atanasova","Maria Maistro","Christina Lioma","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2407.17023v2.pdf","comment":"15 pages, 6 figures, Accepted to Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.04185v2","updated":"2024-10-07T11:54:11Z","published":"2024-09-06T11:01:55Z","title":"Residual Stream Analysis with Multi-Layer SAEs","summary":"  Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, SAEs are\nusually trained separately on each transformer layer, making it difficult to\nuse them to study how information flows across layers. To solve this problem,\nwe introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer. Given that the residual\nstream is understood to preserve information across layers, we expected MLSAE\nlatents to `switch on' at a token position and remain active at later layers.\nInterestingly, we find that individual latents are often active at a single\nlayer for a given token or prompt, but this layer may differ for different\ntokens or prompts. We quantify these phenomena by defining a distribution over\nlayers and considering its variance. We find that the variance of the\ndistributions of latent activations over layers is about two orders of\nmagnitude greater when aggregating over tokens compared with a single token.\nFor larger underlying models, the degree to which latents are active at\nmultiple layers increases, which is consistent with the fact that the residual\nstream activation vectors at adjacent layers become more similar. Finally, we\nrelax the assumption that the residual stream basis is the same at every layer\nby applying pre-trained tuned-lens transformations, but our findings remain\nqualitatively similar. Our results represent a new approach to understanding\nhow representations change as they flow through transformers. We release our\ncode to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.\n","authors":["Tim Lawson","Lucy Farnik","Conor Houghton","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2409.04185v2.pdf","comment":"34 pages, 26 figures"},{"id":"http://arxiv.org/abs/2407.10805v4","updated":"2024-10-07T11:52:50Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has enhanced large language models\n(LLMs) by using knowledge retrieval to address knowledge gaps. However,\nexisting RAG approaches often fail to ensure the depth and completeness of the\ninformation retrieved, which is essential for complex reasoning tasks. In this\nwork, we present Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that\niteratively retrieves information from both unstructured and structured\nknowledge sources in a tightly integrated manner. Specifically, ToG-2 leverages\nknowledge graphs (KGs) to connect documents via entities, facilitating deep and\nknowledge-guided context retrieval. Simultaneously, it uses documents as entity\ncontexts to enable precise and efficient graph retrieval.\n  ToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate accurate\nanswers. We conduct a series of experiments to demonstrate the following\nadvantages of ToG-2: (1) ToG-2 tightly integrates context retrieval and graph\nretrieval, enhancing context retrieval through the KG while enabling reliable\ngraph retrieval based on contexts; (2) it achieves deep and faithful reasoning\nin LLMs through an iterative knowledge retrieval process that integrates\ncontexts and the KG; and (3) ToG-2 is training-free and compatible with various\nLLMs as a plug-and-play solution. Extensive experiments show that ToG-2\nachieves state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive\ndatasets with GPT-3.5, and can elevate the performance of smaller models (e.g.,\nLLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Cehao Yang","Jiaxin Mao","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14883v2","updated":"2024-10-07T11:37:44Z","published":"2024-04-23T10:09:46Z","title":"Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans","summary":"  Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.\n","authors":["Vittoria Dentella","Fritz Guenther","Evelina Leivada"],"pdf_url":"https://arxiv.org/pdf/2404.14883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04925v1","updated":"2024-10-07T11:17:05Z","published":"2024-10-07T11:17:05Z","title":"Intent Classification for Bank Chatbots through LLM Fine-Tuning","summary":"  This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication.\n","authors":["Bibina Lajinov","Patrik Valbek","Michal Spiiak"],"pdf_url":"https://arxiv.org/pdf/2410.04925v1.pdf","comment":"7 pages, no figures"},{"id":"http://arxiv.org/abs/2401.05930v4","updated":"2024-10-07T09:58:48Z","published":"2024-01-11T14:09:09Z","title":"SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully","summary":"  Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple\nhallucination tasks.\n","authors":["Jushi Kai","Tianhang Zhang","Hai Hu","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2401.05930v4.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.04878v1","updated":"2024-10-07T09:57:59Z","published":"2024-10-07T09:57:59Z","title":"Leveraging Grammar Induction for Language Understanding and Generation","summary":"  Grammar induction has made significant progress in recent years. However, it\nis not clear how the application of induced grammar could enhance practical\nperformance in downstream tasks. In this work, we introduce an unsupervised\ngrammar induction method for language understanding and generation. We\nconstruct a grammar parser to induce constituency structures and dependency\nrelations, which is simultaneously trained on downstream tasks without\nadditional syntax annotations. The induced grammar features are subsequently\nincorporated into Transformer as a syntactic mask to guide self-attention. We\nevaluate and apply our method to multiple machine translation tasks and natural\nlanguage understanding tasks. Our method demonstrates superior performance\ncompared to the original Transformer and other models enhanced with external\nparsers. Experimental results indicate that our method is effective in both\nfrom-scratch and pre-trained scenarios. Additionally, our research highlights\nthe contribution of explicitly modeling the grammatical structure of texts to\nneural network models.\n","authors":["Jushi Kai","Shengyuan Hou","Yusheng Huang","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04878v1.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2408.15625v2","updated":"2024-10-07T09:49:08Z","published":"2024-08-28T08:25:22Z","title":"CBF-LLM: Safe Control for LLM Alignment","summary":"  This paper proposes a control-based framework for aligning large language\nmodels (LLMs) by leveraging a control barrier function (CBF) to ensure\nuser-desirable text generation. The presented framework applies the safety\nfilter, designed based on the CBF, to the output generation of the baseline\nLLM, i.e., the sequence of the token, with the aim of intervening in the\ngenerated text. The overall text-generation system is implemented with Llama 3\nand a RoBERTa model, and the source code is available at\nhttps://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control\nability and effectiveness in reducing the number of interventions needed for\nuser-specified alignment tasks.\n","authors":["Yuya Miyaoka","Masaki Inoue"],"pdf_url":"https://arxiv.org/pdf/2408.15625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15489v2","updated":"2024-10-07T08:55:15Z","published":"2024-07-22T09:16:30Z","title":"A Comparison of Language Modeling and Translation as Multilingual\n  Pretraining Objectives","summary":"  Pretrained language models (PLMs) display impressive performances and have\ncaptured the attention of the NLP community. Establishing best practices in\npretraining has, therefore, become a major focus of NLP research, especially\nsince insights gained from monolingual English models may not necessarily apply\nto more complex multilingual models. One significant caveat of the current\nstate of the art is that different works are rarely comparable: they often\ndiscuss different parameter counts, training data, and evaluation methodology.\n  This paper proposes a comparison of multilingual pretraining objectives in a\ncontrolled methodological environment. We ensure that training data and model\narchitectures are comparable, and discuss the downstream performances across 6\nlanguages that we observe in probing and fine-tuning scenarios. We make two key\nobservations: (1) the architecture dictates which pretraining objective is\noptimal; (2) multilingual translation is a very effective pretraining objective\nunder the right conditions. We make our code, data, and model weights available\nat \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.\n","authors":["Zihao Li","Shaoxiong Ji","Timothee Mickus","Vincent Segonne","Jrg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2407.15489v2.pdf","comment":"Proceedings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.04838v1","updated":"2024-10-07T08:53:00Z","published":"2024-10-07T08:53:00Z","title":"Rationale-Aware Answer Verification by Pairwise Self-Evaluation","summary":"  Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.\n","authors":["Akira Kawabata","Saku Sugawara"],"pdf_url":"https://arxiv.org/pdf/2410.04838v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.00545v2","updated":"2024-10-07T08:52:39Z","published":"2024-10-01T09:38:34Z","title":"What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine\n  Translation with a Human-centered Study","summary":"  Gender bias in machine translation (MT) is recognized as an issue that can\nharm people and society. And yet, advancements in the field rarely involve\npeople, the final MT users, or inform how they might be impacted by biased\ntechnologies. Current evaluations are often restricted to automatic methods,\nwhich offer an opaque estimate of what the downstream impact of gender\ndisparities might be. We conduct an extensive human-centered study to examine\nif and to what extent bias in MT brings harms with tangible costs, such as\nquality of service gaps across women and men. To this aim, we collect\nbehavioral data from 90 participants, who post-edited MT outputs to ensure\ncorrect gender translation. Across multiple datasets, languages, and types of\nusers, our study shows that feminine post-editing demands significantly more\ntechnical and temporal effort, also corresponding to higher financial costs.\nExisting bias measurements, however, fail to reflect the found disparities. Our\nfindings advocate for human-centered approaches that can inform the societal\nimpact of bias.\n","authors":["Beatrice Savoldi","Sara Papi","Matteo Negri","Ana Guerberof","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2410.00545v2.pdf","comment":"Accepted ad EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.06551v2","updated":"2024-10-07T08:49:49Z","published":"2024-07-09T05:16:22Z","title":"OffsetBias: Leveraging Debiased Data for Tuning Evaluators","summary":"  Employing Large Language Models (LLMs) to assess the quality of generated\nresponses, such as prompting instruct-tuned models or fine-tuning judge models,\nhas become a widely adopted evaluation method. It is also known that such\nevaluators are vulnerable to biases, such as favoring longer responses. While\nit is important to overcome this problem, the specifics of these biases remain\nunder-explored. In this work, we qualitatively identify six types of biases\ninherent in various judge models. We propose EvalBiasBench as a meta-evaluation\ncollection of hand-crafted test cases for each bias type. Additionally, we\npresent de-biasing dataset construction methods and the associated preference\ndataset OffsetBias. Experimental results demonstrate that fine-tuning on our\ndataset significantly enhances the robustness of judge models against biases\nand improves performance across most evaluation scenarios. We release our\ndatasets and the fine-tuned judge model to public.\n","authors":["Junsoo Park","Seungyeon Jwa","Meiying Ren","Daeyoung Kim","Sanghyuk Choi"],"pdf_url":"https://arxiv.org/pdf/2407.06551v2.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2408.08313v2","updated":"2024-10-07T08:44:35Z","published":"2024-08-15T17:59:57Z","title":"Can Large Language Models Understand Symbolic Graphics Programs?","summary":"  Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n","authors":["Zeju Qiu","Weiyang Liu","Haiwen Feng","Zhen Liu","Tim Z. Xiao","Katherine M. Collins","Joshua B. Tenenbaum","Adrian Weller","Michael J. Black","Bernhard Schlkopf"],"pdf_url":"https://arxiv.org/pdf/2408.08313v2.pdf","comment":"Technical Report v2 (46 pages, 24 figures, project page:\n  https://sgp-bench.github.io/, substantial update from v1)"},{"id":"http://arxiv.org/abs/2410.04834v1","updated":"2024-10-07T08:44:04Z","published":"2024-10-07T08:44:04Z","title":"As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss","summary":"  Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.\n","authors":["Xin Mao","Feng-Lin Li","Huimin Xu","Wei Zhang","Wang Chen","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2410.04834v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.02298v2","updated":"2024-10-07T08:40:35Z","published":"2024-10-03T08:34:17Z","title":"Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models","summary":"  As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.\n","authors":["Guobin Shen","Dongcheng Zhao","Yiting Dong","Xiang He","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.02298v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.04819v1","updated":"2024-10-07T08:13:16Z","published":"2024-10-07T08:13:16Z","title":"MINER: Mining the Underlying Pattern of Modality-Specific Neurons in\n  Multimodal Large Language Models","summary":"  In recent years, multimodal large language models (MLLMs) have significantly\nadvanced, integrating more modalities into diverse applications. However, the\nlack of explainability remains a major barrier to their use in scenarios\nrequiring decision transparency. Current neuron-level explanation paradigms\nmainly focus on knowledge localization or language- and domain-specific\nanalyses, leaving the exploration of multimodality largely unaddressed. To\ntackle these challenges, we propose MINER, a transferable framework for mining\nmodality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)\nmodality separation, (2) importance score calculation, (3) importance score\naggregation, (4) modality-specific neuron selection. Extensive experiments\nacross six benchmarks and two representative MLLMs show that (I) deactivating\nONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for\nQwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly\nconverge in the lower layers, (III) MSNs influence how key information from\nvarious modalities converges to the last token, (IV) two intriguing phenomena\nworth further investigation, i.e., semantic probing and semantic telomeres. The\nsource code is available at this URL.\n","authors":["Kaichen Huang","Jiahao Huo","Yibo Yan","Kun Wang","Yutao Yue","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00016v3","updated":"2024-10-07T07:54:37Z","published":"2024-07-28T15:45:08Z","title":"Towards a Universal Method for Meaningful Signal Detection","summary":"  It is known that human speech and certain animal vocalizations can convey\nmeaningful content because we can decipher the content that a given utterance\ndoes convey. This paper explores an alternative approach to determining whether\na signal is meaningful, one that analyzes only the signal itself and is\nindependent of what the conveyed meaning might be. We devise a method that\ntakes a waveform as input and outputs a score indicating its degree of\n`meaningfulness`. We cluster contiguous portions of the input to minimize the\ntotal description length, and then take the length of the code of the assigned\ncluster labels as meaningfulness score. We evaluate our method empirically,\nagainst several baselines, and show that it is the only one to give a high\nscore to human speech in various languages and with various speakers, a\nmoderate score to animal vocalizations from birds and orcas, and a low score to\nambient noise from various sources.\n","authors":["Louis Mahon"],"pdf_url":"https://arxiv.org/pdf/2408.00016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14399v2","updated":"2024-10-07T07:49:27Z","published":"2024-09-22T11:35:59Z","title":"Beyond Persuasion: Towards Conversational Recommender System with\n  Credible Explanations","summary":"  With the aid of large language models, current conversational recommender\nsystem (CRS) has gaining strong abilities to persuade users to accept\nrecommended items. While these CRSs are highly persuasive, they can mislead\nusers by incorporating incredible information in their explanations, ultimately\ndamaging the long-term trust between users and the CRS. To address this, we\npropose a simple yet effective method, called PC-CRS, to enhance the\ncredibility of CRS's explanations during persuasion. It guides the explanation\ngeneration through our proposed credibility-aware persuasive strategies and\nthen gradually refines explanations via post-hoc self-reflection. Experimental\nresults demonstrate the efficacy of PC-CRS in promoting persuasive and credible\nexplanations. Further analysis reveals the reason behind current methods\nproducing incredible explanations and the potential of credible explanations to\nimprove recommendation accuracy.\n","authors":["Peixin Qin","Chen Huang","Yang Deng","Wenqiang Lei","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14399v2.pdf","comment":"Findings of EMNLP 2024. Our code is available at\n  https://github.com/mumen798/PC-CRS"},{"id":"http://arxiv.org/abs/2407.01449v3","updated":"2024-10-07T07:46:00Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Cline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.04808v1","updated":"2024-10-07T07:41:48Z","published":"2024-10-07T07:41:48Z","title":"LPZero: Language Model Zero-cost Proxy Search from Zero","summary":"  In spite of the outstanding performance, Neural Architecture Search (NAS) is\ncriticized for massive computation. Recently, Zero-shot NAS has emerged as a\npromising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce\ncomputational demands. Despite this, existing ZC proxies heavily rely on expert\nknowledge and incur significant trial-and-error costs. Particularly in NLP\ntasks, most existing ZC proxies fail to surpass the performance of the naive\nbaseline. To address these challenges, we introduce a novel framework,\n\\textbf{LPZero}, which is the first to automatically design ZC proxies for\nvarious tasks, achieving higher ranking consistency than human-designed\nproxies. Specifically, we model the ZC proxy as a symbolic equation and\nincorporate a unified proxy search space that encompasses existing ZC proxies,\nwhich are composed of a predefined set of mathematical symbols. To\nheuristically search for the best ZC proxy, LPZero incorporates genetic\nprogramming to find the optimal symbolic composition. We propose a\n\\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates\nunpromising proxies, thereby mitigating the risk of proxy degradation.\nExtensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's\nsuperior ranking ability and performance on downstream tasks compared to\ncurrent approaches.\n","authors":["Peijie Dong","Lujun Li","Xiang Liu","Zhenheng Tang","Xuebo Liu","Qiang Wang","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2410.04808v1.pdf","comment":"8 pages, 7 figures, 10 appendix"},{"id":"http://arxiv.org/abs/2410.04798v1","updated":"2024-10-07T07:21:49Z","published":"2024-10-07T07:21:49Z","title":"DAPE V2: Process Attention Score as Feature Map for Length Extrapolation","summary":"  The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Jing Xiong","Jiankai Sun","Jingyao Li","Minbin Huang","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.04798v1.pdf","comment":"Tech Report. arXiv admin note: text overlap with arXiv:2405.14722"},{"id":"http://arxiv.org/abs/2410.04795v1","updated":"2024-10-07T07:14:37Z","published":"2024-10-07T07:14:37Z","title":"Representing the Under-Represented: Cultural and Core Capability\n  Benchmarks for Developing Thai Large Language Models","summary":"  The rapid advancement of large language models (LLMs) has highlighted the\nneed for robust evaluation frameworks that assess their core capabilities, such\nas reasoning, knowledge, and commonsense, leading to the inception of certain\nwidely-used benchmark suites such as the H6 benchmark. However, these benchmark\nsuites are primarily built for the English language, and there exists a lack\nthereof for under-represented languages, in terms of LLM development, such as\nThai. On the other hand, developing LLMs for Thai should also include enhancing\nthe cultural understanding as well as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai\nCultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough\nevaluation of various LLMs with multi-lingual capabilities, we provide a\ncomprehensive analysis of the proposed benchmarks and how they contribute to\nThai LLM development. Furthermore, we will make both the datasets and\nevaluation code publicly available to encourage further research and\ndevelopment for Thai LLMs.\n","authors":["Dahyun Kim","Sukyung Lee","Yungi Kim","Attapol Rutherford","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2410.04795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v3","updated":"2024-10-07T07:13:24Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v3.pdf","comment":"accepted at EMNLP2024 - system demonstration track"},{"id":"http://arxiv.org/abs/2410.04790v1","updated":"2024-10-07T07:02:09Z","published":"2024-10-07T07:02:09Z","title":"GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted\n  Graph for Long Document QA","summary":"  In the past, Retrieval-Augmented Generation (RAG) methods split text into\nchunks to enable language models to handle long documents. Recent tree-based\nRAG methods are able to retrieve detailed information while preserving global\ncontext. However, with the advent of more powerful LLMs, such as Llama 3.1,\nwhich offer better comprehension and support for longer inputs, we found that\neven recent tree-based RAG methods perform worse than directly feeding the\nentire document into Llama 3.1, although RAG methods still hold an advantage in\nreducing computational costs. In this paper, we propose a new retrieval method,\ncalled LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph\n(GARLIC), which outperforms previous state-of-the-art baselines, including\nLlama 3.1, while retaining the computational efficiency of RAG methods. Our\nmethod introduces several improvements: (1) Rather than using a tree structure,\nwe construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many\nsummarization, where the graph edges are derived from attention mechanisms, and\neach node focuses on a single event or very few events. (2) We introduce a\nnovel retrieval method that leverages the attention weights of LLMs rather than\ndense embedding similarity. Our method allows for searching the graph along\nmultiple paths and can terminate at any depth. (3) We use the LLM to control\nthe retrieval process, enabling it to dynamically adjust the amount and depth\nof information retrieved for different queries. Experimental results show that\nour method outperforms previous state-of-the-art baselines, including Llama\n3.1, on two single-document and two multi-document QA datasets, while\nmaintaining similar computational complexity to traditional RAG methods.\n","authors":["Xinyu Wang","Yanzheng Xiang","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2410.04790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07284v4","updated":"2024-10-07T06:54:30Z","published":"2023-10-11T08:17:54Z","title":"Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction","summary":"  Humans can easily isolate a single speaker from a complex acoustic\nenvironment, a capability referred to as the \"Cocktail Party Effect.\" However,\nreplicating this ability has been a significant challenge in the field of\ntarget speaker extraction (TSE). Traditional TSE approaches predominantly rely\non voiceprints, which raise privacy concerns and face issues related to the\nquality and availability of enrollment samples, as well as intra-speaker\nvariability. To address these issues, this work introduces a novel text-guided\nTSE paradigm named LLM-TSE. In this paradigm, a state-of-the-art large language\nmodel, LLaMA 2, processes typed text input from users to extract semantic cues.\nWe demonstrate that textual descriptions alone can effectively serve as cues\nfor extraction, thus addressing privacy concerns and reducing dependency on\nvoiceprints. Furthermore, our approach offers flexibility by allowing the user\nto specify the extraction or suppression of a speaker and enhances robustness\nagainst intra-speaker variability by incorporating context-dependent textual\ninformation. Experimental results show competitive performance with text-based\ncues alone and demonstrate the effectiveness of using text as a task selector.\nAdditionally, they achieve a new state-of-the-art when combining text-based\ncues with pre-registered cues. This work represents the first integration of\nLLMs with TSE, potentially establishing a new benchmark in solving the cocktail\nparty problem and expanding the scope of TSE applications by providing a\nversatile, privacy-conscious solution.\n","authors":["Xiang Hao","Jibin Wu","Jianwei Yu","Chenglin Xu","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07284v4.pdf","comment":"Under review, https://github.com/haoxiangsnr/llm-tse"},{"id":"http://arxiv.org/abs/2410.04784v1","updated":"2024-10-07T06:49:41Z","published":"2024-10-07T06:49:41Z","title":"Formality is Favored: Unraveling the Learning Preferences of Large\n  Language Models on Data with Conflicting Knowledge","summary":"  Having been trained on massive pretraining data, large language models have\nshown excellent performance on many knowledge-intensive tasks. However,\npretraining data tends to contain misleading and even conflicting information,\nand it is intriguing to understand how LLMs handle these noisy data during\ntraining. In this study, we systematically analyze LLMs' learning preferences\nfor data with conflicting knowledge. We find that pretrained LLMs establish\nlearning preferences similar to humans, i.e., preferences towards formal texts\nand texts with fewer spelling errors, resulting in faster learning and more\nfavorable treatment of knowledge in data with such features when facing\nconflicts. This finding is generalizable across models and languages and is\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\ntrust data with features that signify consistency with the majority of data,\nand it is possible to instill new preferences and erase old ones by\nmanipulating the degree of consistency with the majority data.\n","authors":["Jiahuan Li","Yiqing Cao","Shujian Huang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04784v1.pdf","comment":"accepted by EMNLP 2024, main conference"},{"id":"http://arxiv.org/abs/2408.01084v2","updated":"2024-10-07T06:11:46Z","published":"2024-08-02T08:03:38Z","title":"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for\n  Handling Noisy Contexts","summary":"  When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge the gap between\nexternal knowledge and the LLMs' parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLMs\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.\n","authors":["Youna Kim","Hyuhng Joon Kim","Cheonbok Park","Choonghyun Park","Hyunsoo Cho","Junyeob Kim","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01084v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.05356v2","updated":"2024-10-07T05:29:01Z","published":"2024-09-09T06:28:47Z","title":"IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech\n  Corpus for Scaling Indian TTS","summary":"  Recent advancements in text-to-speech (TTS) synthesis show that large-scale\nmodels trained with extensive web data produce highly natural-sounding output.\nHowever, such data is scarce for Indian languages due to the lack of\nhigh-quality, manually subtitled data on platforms like LibriVox or YouTube. To\naddress this gap, we enhance existing large-scale ASR datasets containing\nnatural conversations collected in low-quality environments to generate\nhigh-quality TTS training data. Our pipeline leverages the cross-lingual\ngeneralization of denoising and speech enhancement models trained on English\nand applied to Indian languages. This results in IndicVoices-R (IV-R), the\nlargest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704\nhours of high-quality speech from 10,496 speakers across 22 Indian languages.\nIV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,\nand IndicTTS. We also introduce the IV-R Benchmark, the first to assess\nzero-shot, few-shot, and many-shot speaker generalization capabilities of TTS\nmodels on Indian voices, ensuring diversity in age, gender, and style. We\ndemonstrate that fine-tuning an English pre-trained model on a combined dataset\nof high-quality IndicTTS and our IV-R dataset results in better zero-shot\nspeaker generalization compared to fine-tuning on the IndicTTS dataset alone.\nFurther, our evaluation reveals limited zero-shot generalization for Indian\nvoices in TTS models trained on prior datasets, which we improve by fine-tuning\nthe model on our data containing diverse set of speakers across language\nfamilies. We open-source all data and code, releasing the first TTS model for\nall 22 official Indian languages.\n","authors":["Ashwin Sankar","Srija Anand","Praveen Srinivasa Varadhan","Sherry Thomas","Mehak Singal","Shridhar Kumar","Deovrat Mehendale","Aditi Krishana","Giri Raju","Mitesh Khapra"],"pdf_url":"https://arxiv.org/pdf/2409.05356v2.pdf","comment":"Accepted to NeurIPS 2024 Datasets and Benchmarks track"},{"id":"http://arxiv.org/abs/2407.17467v2","updated":"2024-10-07T05:16:25Z","published":"2024-07-24T17:59:02Z","title":"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models","summary":"  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Considering the balance between efficiency and\neffectiveness, CMR can be regarded as the optimal mixture ratio. Through\nextensive experiments, we ascertain the predictability of CMR, propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n","authors":["Jiawei Gu","Zacc Yang","Chuanghao Ding","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17467v2.pdf","comment":"EMNLP 2024 main conference"},{"id":"http://arxiv.org/abs/2410.04753v1","updated":"2024-10-07T05:14:18Z","published":"2024-10-07T05:14:18Z","title":"ImProver: Agent-Based Automated Proof Optimization","summary":"  Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.\n","authors":["Riyaz Ahuja","Jeremy Avigad","Prasad Tetali","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2410.04753v1.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.04752v1","updated":"2024-10-07T05:07:48Z","published":"2024-10-07T05:07:48Z","title":"Document-level Causal Relation Extraction with Knowledge-guided Binary\n  Question Answering","summary":"  As an essential task in information extraction (IE), Event-Event Causal\nRelation Extraction (ECRE) aims to identify and classify the causal\nrelationships between event mentions in natural language texts. However,\nexisting research on ECRE has highlighted two critical challenges, including\nthe lack of document-level modeling and causal hallucinations. In this paper,\nwe propose a Knowledge-guided binary Question Answering (KnowQA) method with\nevent structures for ECRE, consisting of two stages: Event Structure\nConstruction and Binary Question Answering. We conduct extensive experiments\nunder both zero-shot and fine-tuning settings with large language models (LLMs)\non the MECI and MAVEN-ERE datasets. Experimental results demonstrate the\nusefulness of event structures on document-level ECRE and the effectiveness of\nKnowQA by achieving state-of-the-art on the MECI dataset. We observe not only\nthe effectiveness but also the high generalizability and low inconsistency of\nour method, particularly when with complete event structures after fine-tuning\nthe models.\n","authors":["Zimu Wang","Lei Xia","Wei Wang","Xinya Du"],"pdf_url":"https://arxiv.org/pdf/2410.04752v1.pdf","comment":"Accepted at Findings of EMNLP 2024. Camera-ready version"},{"id":"http://arxiv.org/abs/2410.04751v1","updated":"2024-10-07T05:07:01Z","published":"2024-10-07T05:07:01Z","title":"Intriguing Properties of Large Language and Vision Models","summary":"  Recently, large language and vision models (LLVMs) have received significant\nattention and development efforts due to their remarkable generalization\nperformance across a wide range of tasks requiring perception and cognitive\nabilities. A key factor behind their success is their simple architecture,\nwhich consists of a vision encoder, a projector, and a large language model\n(LLM). Despite their achievements in advanced reasoning tasks, their\nperformance on fundamental perception-related tasks (e.g., MMVP) remains\nsurprisingly low. This discrepancy raises the question of how LLVMs truly\nperceive images and exploit the advantages of the vision encoder. To address\nthis, we systematically investigate this question regarding several aspects:\npermutation invariance, robustness, math reasoning, alignment preserving and\nimportance, by evaluating the most common LLVM's families (i.e., LLaVA) across\n10 evaluation benchmarks. Our extensive experiments reveal several intriguing\nproperties of current LLVMs: (1) they internally process the image in a global\nmanner, even when the order of visual patch sequences is randomly permuted; (2)\nthey are sometimes able to solve math problems without fully perceiving\ndetailed numerical information; (3) the cross-modal alignment is overfitted to\ncomplex reasoning tasks, thereby, causing them to lose some of the original\nperceptual capabilities of their vision encoder; (4) the representation space\nin the lower layers (<25%) plays a crucial role in determining performance and\nenhancing visual understanding. Lastly, based on the above observations, we\nsuggest potential future directions for building better LLVMs and constructing\nmore challenging evaluation benchmarks.\n","authors":["Young-Jun Lee","Byungsoo Ko","Han-Gyu Kim","Yechan Hwang","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.04751v1.pdf","comment":"Code is available in https://github.com/passing2961/IP-LLVM"},{"id":"http://arxiv.org/abs/2405.14722v3","updated":"2024-10-07T04:35:58Z","published":"2024-05-23T15:51:24Z","title":"DAPE: Data-Adaptive Positional Encoding for Length Extrapolation","summary":"  Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Minbin Huang","Jingyao Li","Jing Xiong","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2405.14722v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2304.09797v6","updated":"2024-10-07T04:28:04Z","published":"2023-04-19T16:29:48Z","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models","summary":"  The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted extensive and comprehensive experiments on seven benchmarks. The\nresults show that PHP significantly improves accuracy while remaining highly\nefficient. For instance, with text-davinci-003, we observed a 4.2% improvement\non GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction\nin sample paths with self-consistency. With GPT-4 and PHP, we achieve\nstate-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%),\nAQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).\n","authors":["Chuanyang Zheng","Zhengying Liu","Enze Xie","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2304.09797v6.pdf","comment":"Accepted to ICML AI4MATH 2024"},{"id":"http://arxiv.org/abs/2403.19270v2","updated":"2024-10-07T04:21:15Z","published":"2024-03-28T09:56:04Z","title":"sDPO: Don't Use Your Data All at Once","summary":"  As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.\n","authors":["Dahyun Kim","Yungi Kim","Wonho Song","Hyeonwoo Kim","Yunsu Kim","Sanghoon Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2403.19270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13803v3","updated":"2024-10-07T04:18:40Z","published":"2024-05-22T16:30:24Z","title":"\"I Like Sunnie More Than I Expected!\": Exploring User Expectation and\n  Perception of an Anthropomorphic LLM-based Conversational Agent for\n  Well-Being Support","summary":"  The human-computer interaction (HCI) research community has a longstanding\ninterest in exploring the mismatch between users' actual experiences and\nexpectation toward new technologies, for instance, large language models\n(LLMs). In this study, we compared users' (N = 38) initial expectations against\ntheir post-interaction perceptions of two LLM-powered mental well-being\nintervention activity recommendation systems. Both systems have a built-in LLM\nto recommend a personalized well-being intervention activity, but one system\n(Sunnie) has an anthropomorphic conversational interaction design via elements\nsuch as appearance, persona, and natural conversation. Results showed that user\nengagement was high with both systems, and both systems exceeded users'\nexpectations along the utility dimension, highlighting AI's potential to offer\nuseful intervention activity recommendations. In addition, Sunnie further\noutperformed the non-anthropomorphic baseline system in relational warmth.\nThese findings suggest that anthropomorphic conversational interaction design\nmay be particularly effective in fostering warmth in mental health support\ncontexts.\n","authors":["Siyi Wu","Julie Y. A. Cachia","Feixue Han","Bingsheng Yao","Tianyi Xie","Xuan Zhao","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13803v3.pdf","comment":"In Submission"},{"id":"http://arxiv.org/abs/2408.07930v3","updated":"2024-10-07T04:17:18Z","published":"2024-08-15T04:57:55Z","title":"MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and\n  Iterative Sub-SQL Refinement for Text-to-SQL","summary":"  Recent In-Context Learning based methods have achieved remarkable success in\nText-to-SQL task. However, there is still a large gap between the performance\nof these models and human performance on datasets with complex database schema\nand difficult questions, such as BIRD. Besides, existing work has neglected to\nsupervise intermediate steps when solving questions iteratively with question\ndecomposition methods, and the schema linking methods used in these works are\nvery rudimentary. To address these issues, we propose MAG-SQL, a multi-agent\ngenerative approach with soft schema linking and iterative Sub-SQL refinement.\nIn our framework, an entity-based method with tables' summary is used to select\nthe columns in database, and a novel targets-conditions decomposition method is\nintroduced to decompose those complex questions. Additionally, we build a\niterative generating module which includes a Sub-SQL Generator and Sub-SQL\nRefiner, introducing external oversight for each step of generation. Through a\nseries of ablation studies, the effectiveness of each agent in our framework\nhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL\nachieves an execution accuracy of 61.08%, compared to the baseline accuracy of\n46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.\nBesides, our approach makes similar progress on Spider.\n","authors":["Wenxuan Xie","Gaochen Wu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07930v3.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.04739v1","updated":"2024-10-07T04:15:02Z","published":"2024-10-07T04:15:02Z","title":"TableRAG: Million-Token Table Understanding with Language Models","summary":"  Recent advancements in language models (LMs) have notably enhanced their\nability to reason with tabular data, primarily through program-aided mechanisms\nthat manipulate and analyze tables. However, these methods often require the\nentire table as input, leading to scalability challenges due to the positional\nbias or context length constraints. In response to these challenges, we\nintroduce TableRAG, a Retrieval-Augmented Generation (RAG) framework\nspecifically designed for LM-based table understanding. TableRAG leverages\nquery expansion combined with schema and cell retrieval to pinpoint crucial\ninformation before providing it to the LMs. This enables more efficient data\nencoding and precise retrieval, significantly reducing prompt lengths and\nmitigating information loss. We have developed two new million-token benchmarks\nfrom the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's\neffectiveness at scale. Our results demonstrate that TableRAG's retrieval\ndesign achieves the highest retrieval quality, leading to the new\nstate-of-the-art performance on large-scale table understanding.\n","authors":["Si-An Chen","Lesly Miculicich","Julian Martin Eisenschlos","Zifeng Wang","Zilong Wang","Yanfei Chen","Yasuhisa Fujii","Hsuan-Tien Lin","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04739v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.04734v1","updated":"2024-10-07T04:00:22Z","published":"2024-10-07T04:00:22Z","title":"TLDR: Token-Level Detective Reward Model for Large Vision Language\n  Models","summary":"  Although reward models have been successful in improving multimodal large\nlanguage models, the reward models themselves remain brutal and contain minimal\ninformation. Notably, existing reward models only mimic human annotations by\nassigning only one binary feedback to any text, no matter how long the text is.\nIn the realm of multimodal language models, where models are required to\nprocess both images and texts, a naive reward model may learn implicit biases\ntoward texts and become less grounded in images. In this paper, we propose a\n$\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\n($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We\nfirst introduce a perturbation-based method to generate synthetic hard\nnegatives and their token-level labels to train TLDR models. Then we show the\nrich usefulness of TLDR models both in assisting off-the-shelf models to\nself-correct their generations, and in serving as a hallucination evaluation\ntool. Finally, we show that TLDR models can significantly speed up human\nannotation by 3 times to acquire a broader range of high-quality vision\nlanguage data.\n","authors":["Deqing Fu","Tong Xiao","Rui Wang","Wang Zhu","Pengchuan Zhang","Guan Pang","Robin Jia","Lawrence Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04734v1.pdf","comment":"Work done at Meta"},{"id":"http://arxiv.org/abs/2409.06927v2","updated":"2024-10-07T03:56:35Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v2.pdf","comment":"9 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.17169v3","updated":"2024-10-07T03:48:18Z","published":"2024-06-24T23:02:56Z","title":"Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models","summary":"  As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.\n","authors":["Nisarg Patel","Mohith Kulkarni","Mihir Parmar","Aashna Budhiraja","Mutsumi Nakamura","Neeraj Varshney","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2406.17169v3.pdf","comment":"Accepted at EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.04731v1","updated":"2024-10-07T03:47:34Z","published":"2024-10-07T03:47:34Z","title":"Efficient transformer with reinforced position embedding for language\n  models","summary":"  In this paper, we propose an efficient transformer architecture that uses\nreinforced positional embedding to obtain superior performance with half the\nnumber of encoder decoder layers. We demonstrate that concatenating positional\nencoding with trainable token embeddings, normalizing columns in the token\nembedding matrix, and using the normalized token embedding matrix as the value\nof the attention layer improve the training and validation loss and the\ntraining time in an encoder-decoder Transformer model for a Portuguese-English\ntranslation task with 10 epochs or 12 hours of training across 10 trials. Our\nmethod, with roughly a threefold parameter reduction compared to the baseline\nmodel, yields a mean training loss of 1.21, a mean validation loss of 1.51, and\nan average training time of 1352.27 seconds per epoch, surpassing the baseline\nmodel with the same embedding dimension that employs addition of positional\nencoding and token embeddings, which achieves a mean training loss of 1.96, a\nvalidation loss of 2.18, and an average training time of 4297.79 seconds per\nepoch. Additionally, we evaluated our proposed architecture and the baseline\nacross 14 diverse translation datasets from TensorFlow. The results indicate\nthat our method consistently achieves lower or comparable training and\nvalidation losses, suggesting enhanced learning efficiency.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2410.04731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16997v2","updated":"2024-10-07T03:41:57Z","published":"2024-07-24T04:39:24Z","title":"Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective","summary":"  This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.\n","authors":["Yujian Liu","Yang Zhang","Tommi Jaakkola","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2407.16997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05621v2","updated":"2024-10-07T03:41:27Z","published":"2023-12-09T17:38:39Z","title":"PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching","summary":"  Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.\n","authors":["Zhenting Qi","Xiaoyu Tan","Shaojie Shi","Chao Qu","Yinghui Xu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2312.05621v2.pdf","comment":"Accepted by EMNLP 2023 (Industry Track), Oral Presentation"},{"id":"http://arxiv.org/abs/2410.04727v1","updated":"2024-10-07T03:38:27Z","published":"2024-10-07T03:38:27Z","title":"Forgetting Curve: A Reliable Method for Evaluating Memorization\n  Capability for Long-context Models","summary":"  Numerous recent works target to extend effective context length for language\nmodels and various methods, tasks and benchmarks exist to measure model's\neffective memorization length. However, through thorough investigations, we\nfind limitations for currently existing evaluations on model's memorization\ncapability. We provide an extensive survey for limitations in this work and\npropose a new method called forgetting curve to measure the memorization\ncapability of long-context models. We show that forgetting curve has the\nadvantage of being robust to the tested corpus and the experimental settings,\nof not relying on prompts and can be applied to any model size.\n  We apply our forgetting curve to a large variety of models involving both\ntransformer and RNN/SSM based architectures. Our measurement provides empirical\nevidence for the effectiveness of transformer extension techniques while raises\nquestions for the effective length of RNN/SSM based models. We also examine the\ndifference between our measurement and existing benchmarks as well as popular\nmetrics for various models. Our code and results can be found at\nhttps://github.com/1azybug/ForgettingCurve.\n","authors":["Xinyu Liu","Runsong Zhao","Pengcheng Huang","Chunyang Xiao","Bei Li","Jingang Wang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.04727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17141v3","updated":"2024-10-07T03:19:16Z","published":"2024-03-25T19:28:10Z","title":"MetaAligner: Towards Generalizable Multi-Objective Alignment of Language\n  Models","summary":"  Recent advancements in large language models (LLMs) focus on aligning to\nheterogeneous human expectations and values via multi-objective preference\nalignment. However, existing methods are dependent on the policy model\nparameters, which require high-cost repetition of their alignment algorithms\nfor each new policy model, and they cannot expand to unseen objectives due to\ntheir static alignment objectives. In this work, we propose Meta-Objective\nAligner (MetaAligner), the first policy-agnostic and generalizable method for\nmulti-objective preference alignment. MetaAligner models multi-objective\nalignment into three stages: (1) dynamic objectives reformulation algorithm\nreorganizes traditional alignment datasets to supervise the model on performing\nflexible alignment across different objectives; (2) conditional weak-to-strong\ncorrection paradigm aligns the weak outputs of fixed policy models to approach\nstrong outputs with higher preferences in the corresponding alignment\nobjectives, enabling plug-and-play inferences on any policy models, which\nsignificantly reduces training costs and facilitates alignment on close-source\npolicy models; (3) generalizable inference method flexibly adjusts target\nobjectives by updating their text descriptions in the prompts, facilitating\ngeneralizable alignment to unseen objectives. Experimental results show that\nMetaAligner achieves significant and balanced improvements in multi-objective\nalignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU\ntraining hours compared to previous alignment methods. The model also\neffectively aligns unseen objectives, marking the first step towards\ngeneralizable multi-objective preference alignment.\n","authors":["Kailai Yang","Zhiwei Liu","Qianqian Xie","Jimin Huang","Tianlin Zhang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2403.17141v3.pdf","comment":"Accepted by NeurIPS 2024 main track"},{"id":"http://arxiv.org/abs/2410.04717v1","updated":"2024-10-07T03:15:11Z","published":"2024-10-07T03:15:11Z","title":"$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization","summary":"  Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.\n","authors":["Dylan Zhang","Justin Wang","Francois Charton"],"pdf_url":"https://arxiv.org/pdf/2410.04717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04715v1","updated":"2024-10-07T03:13:06Z","published":"2024-10-07T03:13:06Z","title":"Rule-based Data Selection for Large Language Models","summary":"  The quality of training data significantly impacts the performance of large\nlanguage models (LLMs). There are increasing studies using LLMs to rate and\nselect data based on several human-crafted metrics (rules). However, these\nconventional rule-based approaches often depend too heavily on human\nheuristics, lack effective metrics for assessing rules, and exhibit limited\nadaptability to new tasks. In our study, we introduce an innovative rule-based\nframework that utilizes the orthogonality of score vectors associated with\nrules as a novel metric for rule evaluations. Our approach includes an\nautomated pipeline that first uses LLMs to generate a diverse set of rules,\nencompassing various rating dimensions to evaluate data quality. Then it rates\na batch of data based on these rules and uses the determinantal point process\n(DPP) from random matrix theory to select the most orthogonal score vectors,\nthereby identifying a set of independent rules. These rules are subsequently\nused to evaluate all data, selecting samples with the highest average scores\nfor downstream tasks such as LLM training. We verify the effectiveness of our\nmethod through two experimental setups: 1) comparisons with ground truth\nratings and 2) benchmarking LLMs trained with the chosen data. Our\ncomprehensive experiments cover a range of scenarios, including general\npre-training and domain-specific fine-tuning in areas such as IMDB, Medical,\nMath, and Code. The outcomes demonstrate that our DPP-based rule rating method\nconsistently outperforms other approaches, including rule-free rating, uniform\nsampling, importance resampling, and QuRating, in terms of both rating\nprecision and model performance.\n","authors":["Xiaomin Li","Mingye Gao","Zhiwei Zhang","Chang Yue","Hong Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12327v3","updated":"2024-10-07T03:08:12Z","published":"2024-07-17T05:53:20Z","title":"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language\n  Models","summary":"  Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\n\\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.\n","authors":["Ayush Kaushal","Tejas Vaidhya","Arnab Kumar Mondal","Tejas Pandey","Aaryan Bhagat","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2407.12327v3.pdf","comment":"42 pages, 21 figures, and 13 tables"},{"id":"http://arxiv.org/abs/2405.15984v3","updated":"2024-10-07T03:07:58Z","published":"2024-05-24T23:56:36Z","title":"Evaluating and Safeguarding the Adversarial Robustness of\n  Retrieval-Based In-Context Learning","summary":"  With the emergence of large language models, such as LLaMA and OpenAI GPT-3,\nIn-Context Learning (ICL) gained significant attention due to its effectiveness\nand efficiency. However, ICL is very sensitive to the choice, order, and\nverbaliser used to encode the demonstrations in the prompt. Retrieval-Augmented\nICL methods try to address this problem by leveraging retrievers to extract\nsemantically related examples as demonstrations. While this approach yields\nmore accurate results, its robustness against various types of adversarial\nattacks, including perturbations on test samples, demonstrations, and retrieved\ndata, remains under-explored. Our study reveals that retrieval-augmented models\ncan enhance robustness against test sample attacks, outperforming vanilla ICL\nwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibit\noverconfidence in the demonstrations, leading to a 2% increase in ASR for\ndemonstration attacks. Adversarial training can help improve the robustness of\nICL methods to adversarial attacks; however, such a training scheme can be too\ncostly in the context of LLMs. As an alternative, we introduce an effective\ntraining-free adversarial defence method, DARD, which enriches the example pool\nwith those attacked samples. We show that DARD yields improvements in\nperformance and robustness, achieving a 15% reduction in ASR over the\nbaselines. Code and data are released to encourage further research:\nhttps://github.com/simonucl/adv-retreival-icl\n","authors":["Simon Yu","Jie He","Pasquale Minervini","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2405.15984v3.pdf","comment":"COLM 2024, 30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05328v2","updated":"2024-10-07T03:06:34Z","published":"2024-06-08T02:59:52Z","title":"FacLens: Transferable Probe for Foreseeing Non-Factuality in Large\n  Language Models","summary":"  Despite advancements in large language models (LLMs), non-factual responses\nremain prevalent. Unlike extensive studies on post-hoc detection of such\nresponses, this work studies non-factuality prediction (NFP), aiming to predict\nwhether an LLM will generate a non-factual response to a question before the\ngeneration process. Previous efforts on NFP have demonstrated LLMs' awareness\nof their internal knowledge, but they still face challenges in efficiency and\ntransferability. In this work, we propose a lightweight NFP model named\nFactuality Lens (FacLens), which effectively probes hidden representations of\nquestions for the NFP task. Besides, we discover that hidden question\nrepresentations sourced from different LLMs exhibit similar NFP patterns, which\nenables the transferability of FacLens across LLMs to reduce development costs.\nExtensive experiments highlight FacLens's superiority in both effectiveness and\nefficiency.\n","authors":["Yanling Wang","Haoyang Li","Hao Zou","Jing Zhang","Xinlei He","Qi Li","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2406.05328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v2","updated":"2024-10-07T03:01:01Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xiaolei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2405.20267v4","updated":"2024-10-07T02:53:44Z","published":"2024-05-30T17:19:19Z","title":"Auto-Arena: Automating LLM Evaluations with Agent Peer Battles and\n  Committee Discussions","summary":"  As LLMs continuously evolve, there is an urgent need for a reliable\nevaluation method that delivers trustworthy results promptly. Currently, static\nbenchmarks suffer from inflexibility and unreliability, leading users to prefer\nhuman voting platforms like Chatbot Arena. However, human evaluations require\nsignificant manual effort. To address this, we propose the Auto-Arena, an\ninnovative framework that automates the entire evaluation process using\nLLM-powered agents. Firstly, an LLM examiner generates questions. Then, two LLM\ncandidates engage in a multi-round peer battle based on individual questions,\naiming at revealing their true performance differences. Finally, a committee of\nLLM judges collaboratively discusses and decides the winner, reducing bias and\nenhancing fairness. During the peer battles, we observe intriguing scenarios\nwhere the LLM candidates display competitive behaviors and even learn from the\nopponents. In our extensive experiments involving 15 recent LLMs, Auto-Arena\nshows a 92.14% correlation with human preferences, surpassing all previous\nexpert-annotated benchmarks without any manual efforts. As a result, Auto-Arena\noffers a promising alternative to current human evaluation platforms for\nevaluating LLMs automatically.\n","authors":["Ruochen Zhao","Wenxuan Zhang","Yew Ken Chia","Weiwen Xu","Deli Zhao","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2405.20267v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04707v1","updated":"2024-10-07T02:52:30Z","published":"2024-10-07T02:52:30Z","title":"Learning How Hard to Think: Input-Adaptive Allocation of LM Computation","summary":"  Computationally intensive decoding procedures--including search, reranking,\nand self-critique--can improve the quality of language model (LM) outputs in\nproblems spanning code generation, numerical reasoning, and dialog. Existing\nwork typically applies the same decoding procedure for every input to an LM.\nBut not all inputs require the same amount of computation to process. Can we\nallocate decoding computation adaptively, using more resources to answer\nquestions whose answers will be harder to compute? We present an approach that\npredicts the distribution of rewards given an input and computation budget,\nthen allocates additional computation to inputs for which it is predicted to be\nmost useful. We apply this approach in two decoding procedures: first, an\nadaptive best-of-k procedure that dynamically selects the number of samples to\ngenerate as input to a reranker; second, a routing procedure that dynamically\nresponds to a query using a decoding procedure that is expensive but accurate,\nor one that is cheaper but less capable. Across a suite of programming,\nmathematics, and dialog tasks, we show that accurate computation-allocation\nprocedures can be learned, and reduce computation by up to 50% at no cost to\nresponse quality, or improve quality by up to 10% at a fixed computational\nbudget.\n","authors":["Mehul Damani","Idan Shenfeld","Andi Peng","Andreea Bobu","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.04707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04704v1","updated":"2024-10-07T02:48:18Z","published":"2024-10-07T02:48:18Z","title":"Modeling and Estimation of Vocal Tract and Glottal Source Parameters\n  Using ARMAX-LF Model","summary":"  Modeling and estimation of the vocal tract and glottal source parameters of\nvowels from raw speech can be typically done by using the Auto-Regressive with\neXogenous input (ARX) model and Liljencrants-Fant (LF) model with an\niteration-based estimation approach. However, the all-pole autoregressive model\nin the modeling of vocal tract filters cannot provide the locations of\nanti-formants (zeros), which increases the estimation errors in certain classes\nof speech sounds, such as nasal, fricative, and stop consonants. In this paper,\nwe propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)\nmodel to extend the ARX-LF model to a wider variety of speech sounds, including\nvowels and nasalized consonants. The LF model represents the glottal source\nderivative as a parametrized time-domain model, and the ARMAX model represents\nthe vocal tract as a pole-zero filter with an additional exogenous LF\nexcitation as input. To estimate multiple parameters with fewer errors, we\nfirst utilize the powerful nonlinear fitting ability of deep neural networks\n(DNNs) to build a mapping from extracted glottal source derivatives or speech\nwaveforms to corresponding LF parameters. Then, glottal source and vocal tract\nparameters can be estimated with fewer estimation errors and without any\niterations as in the analysis-by-synthesis strategy. Experimental results with\nsynthesized speech using the linear source-filter model, synthesized speech\nusing the physical model, and real speech signals showed that the proposed\nARMAX-LF model with a DNN-based estimation method can estimate the parameters\nof both vowels and nasalized sounds with fewer errors and estimation time.\n","authors":["Kai Lia","Masato Akagia","Yongwei Lib","Masashi Unokia"],"pdf_url":"https://arxiv.org/pdf/2410.04704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00943v2","updated":"2024-10-07T02:47:36Z","published":"2024-04-01T06:03:39Z","title":"Evalverse: Unified and Accessible Library for Large Language Model\n  Evaluation","summary":"  This paper introduces Evalverse, a novel library that streamlines the\nevaluation of Large Language Models (LLMs) by unifying disparate evaluation\ntools into a single, user-friendly framework. Evalverse enables individuals\nwith limited knowledge of artificial intelligence to easily request LLM\nevaluations and receive detailed reports, facilitated by an integration with\ncommunication platforms like Slack. Thus, Evalverse serves as a powerful tool\nfor the comprehensive assessment of LLMs, offering both researchers and\npractitioners a centralized and easily accessible evaluation framework.\nFinally, we also provide a demo video for Evalverse, showcasing its\ncapabilities and implementation in a two-minute format.\n","authors":["Jihoo Kim","Wonho Song","Dahyun Kim","Yunsu Kim","Yungi Kim","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2404.00943v2.pdf","comment":"Accepted to EMNLP 2024 Demo Track"},{"id":"http://arxiv.org/abs/2410.04699v1","updated":"2024-10-07T02:30:18Z","published":"2024-10-07T02:30:18Z","title":"The LLM Effect: Are Humans Truly Using LLMs, or Are They Being\n  Influenced By Them Instead?","summary":"  Large Language Models (LLMs) have shown capabilities close to human\nperformance in various analytical tasks, leading researchers to use them for\ntime and labor-intensive analyses. However, their capability to handle highly\nspecialized and open-ended tasks in domains like policy studies remains in\nquestion. This paper investigates the efficiency and accuracy of LLMs in\nspecialized tasks through a structured user study focusing on Human-LLM\npartnership. The study, conducted in two stages-Topic Discovery and Topic\nAssignment-integrates LLMs with expert annotators to observe the impact of LLM\nsuggestions on what is usually human-only analysis. Results indicate that\nLLM-generated topic lists have significant overlap with human generated topic\nlists, with minor hiccups in missing document-specific topics. However, LLM\nsuggestions may significantly improve task completion speed, but at the same\ntime introduce anchoring bias, potentially affecting the depth and nuance of\nthe analysis, raising a critical question about the trade-off between increased\nefficiency and the risk of biased analysis.\n","authors":["Alexander S. Choi","Syeda Sabrina Akter","JP Singh","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.04699v1.pdf","comment":"Accepted to EMNLP Main 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2410.04698v1","updated":"2024-10-07T02:30:07Z","published":"2024-10-07T02:30:07Z","title":"MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning\n  in LLMs","summary":"  Recent large language models (LLMs) have demonstrated versatile capabilities\nin long-context scenarios. Although some recent benchmarks have been developed\nto evaluate the long-context capabilities of LLMs, there is a lack of\nbenchmarks evaluating the mathematical reasoning abilities of LLMs over long\ncontexts, which is crucial for LLMs' application in real-world scenarios. In\nthis paper, we introduce MathHay, an automated benchmark designed to assess the\nlong-context mathematical reasoning capabilities of LLMs. Unlike previous\nbenchmarks like Needle in a Haystack, which focus primarily on information\nretrieval within long texts, MathHay demands models with both\ninformation-seeking and complex mathematical reasoning abilities. We conduct\nextensive experiments on MathHay to assess the long-context mathematical\nreasoning abilities of eight top-performing LLMs. Even the best-performing\nmodel, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over\nlong contexts, achieving only 51.26% accuracy at 128K tokens. This highlights\nthe significant room for improvement on the MathHay benchmark.\n","authors":["Lei Wang","Shan Dong","Yuhui Xu","Hanze Dong","Yalu Wang","Amrita Saha","Ee-Peng Lim","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.04698v1.pdf","comment":"Work-in-Progress"},{"id":"http://arxiv.org/abs/2401.15884v3","updated":"2024-10-07T02:19:21Z","published":"2024-01-29T04:36:39Z","title":"Corrective Retrieval Augmented Generation","summary":"  Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.\n","authors":["Shi-Qi Yan","Jia-Chen Gu","Yun Zhu","Zhen-Hua Ling"],"pdf_url":"https://arxiv.org/pdf/2401.15884v3.pdf","comment":"Update results, add more analysis, and fix typos"},{"id":"http://arxiv.org/abs/2410.04691v1","updated":"2024-10-07T02:12:22Z","published":"2024-10-07T02:12:22Z","title":"Deeper Insights Without Updates: The Power of In-Context Learning Over\n  Fine-Tuning","summary":"  Fine-tuning and in-context learning (ICL) are two prevalent methods in\nimbuing large language models with task-specific knowledge. It is commonly\nbelieved that fine-tuning can surpass ICL given sufficient training samples as\nit allows the model to adjust its internal parameters based on the data.\nHowever, this paper presents a counterintuitive finding: For tasks with\nimplicit patterns, ICL captures these patterns significantly better than\nfine-tuning. We developed several datasets featuring implicit patterns, such as\nsequences determining answers through parity or identifying reducible terms in\ncalculations. We then evaluated the models' understanding of these patterns\nunder both fine-tuning and ICL across models ranging from 0.5B to 7B\nparameters. The results indicate that models employing ICL can quickly grasp\ndeep patterns and significantly improve accuracy. In contrast, fine-tuning,\ndespite utilizing thousands of times more training samples than ICL, achieved\nonly limited improvements. We also proposed circuit shift theory from a\nmechanistic interpretability's view to explain why ICL wins.\n","authors":["Qingyu Yin","Xuzheng He","Luoao Deng","Chak Tou Leong","Fan Wang","Yanzhao Yan","Xiaoyu Shen","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.04691v1.pdf","comment":"EMNLP'24 Findings"},{"id":"http://arxiv.org/abs/2406.08464v2","updated":"2024-10-07T01:45:38Z","published":"2024-06-12T17:52:30Z","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\n  with Nothing","summary":"  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n","authors":["Zhangchen Xu","Fengqing Jiang","Luyao Niu","Yuntian Deng","Radha Poovendran","Yejin Choi","Bill Yuchen Lin"],"pdf_url":"https://arxiv.org/pdf/2406.08464v2.pdf","comment":"Link: https://magpie-align.github.io/"},{"id":"http://arxiv.org/abs/2405.16406v3","updated":"2024-10-07T01:27:59Z","published":"2024-05-26T02:15:49Z","title":"SpinQuant: LLM quantization with learned rotations","summary":"  Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\n","authors":["Zechun Liu","Changsheng Zhao","Igor Fedorov","Bilge Soran","Dhruv Choudhary","Raghuraman Krishnamoorthi","Vikas Chandra","Yuandong Tian","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2405.16406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12832v3","updated":"2024-10-07T01:26:23Z","published":"2024-09-19T15:07:35Z","title":"FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists","summary":"  Flavor development in the food industry is increasingly challenged by the\nneed for rapid innovation and precise flavor profile creation. Traditional\nflavor research methods typically rely on iterative, subjective testing, which\nlacks the efficiency and scalability required for modern demands. This paper\npresents three contributions to address the challenges. Firstly, we define a\nnew problem domain for scientific agents in flavor science, conceptualized as\nthe generation of hypotheses for flavor profile sourcing and understanding. To\nfacilitate research in this area, we introduce the FoodPuzzle, a challenging\nbenchmark consisting of 978 food items and 1,766 flavor molecules profiles. We\npropose a novel Scientific Agent approach, integrating in-context learning and\nretrieval augmented techniques to generate grounded hypotheses in the domain of\nfood science. Experimental results indicate that our model significantly\nsurpasses traditional methods in flavor profile prediction tasks, demonstrating\nits potential to transform flavor development practices.\n","authors":["Tenghao Huang","Donghee Lee","John Sweeney","Jiatong Shi","Emily Steliotes","Matthew Lange","Jonathan May","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12832v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04663v1","updated":"2024-10-07T00:22:07Z","published":"2024-10-07T00:22:07Z","title":"Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates","summary":"  This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions.\n","authors":["Chaithanya Bandi","Hari Bandi","Abir Harrasse"],"pdf_url":"https://arxiv.org/pdf/2410.04663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08760v3","updated":"2024-10-07T00:16:54Z","published":"2024-04-12T18:36:20Z","title":"The Generation Gap: Exploring Age Bias in the Value Systems of Large\n  Language Models","summary":"  We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}\n","authors":["Siyang Liu","Trish Maturi","Bowen Yi","Siqi Shen","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2404.08760v3.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2410.04657v1","updated":"2024-10-07T00:09:50Z","published":"2024-10-07T00:09:50Z","title":"Contrastive Learning to Improve Retrieval for Real-world Fact Checking","summary":"  Recent work on fact-checking addresses a realistic setting where models\nincorporate evidence retrieved from the web to decide the veracity of claims. A\nbottleneck in this pipeline is in retrieving relevant evidence: traditional\nmethods may surface documents directly related to a claim, but fact-checking\ncomplex claims requires more inferences. For instance, a document about how a\nvaccine was developed is relevant to addressing claims about what it might\ncontain, even if it does not address them directly. We present Contrastive\nFact-Checking Reranker (CFR), an improved retriever for this setting. By\nleveraging the AVeriTeC dataset, which annotates subquestions for claims with\nhuman written answers from evidence documents, we fine-tune Contriever with a\ncontrastive objective based on multiple training signals, including\ndistillation from GPT-4, evaluating subquestion answers, and gold labels in the\ndataset. We evaluate our model on both retrieval and end-to-end veracity\njudgments about claims. On the AVeriTeC dataset, we find a 6\\% improvement in\nveracity classification accuracy. We also show our gains can be transferred to\nFEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to\nmake inferences.\n","authors":["Aniruddh Sriram","Fangyuan Xu","Eunsol Choi","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2410.04657v1.pdf","comment":"EMNLP 2024 FEVER Workshop"},{"id":"http://arxiv.org/abs/2112.02325v2","updated":"2024-10-07T14:26:18Z","published":"2021-12-04T13:18:12Z","title":"A Russian Jeopardy! Data Set for Question-Answering Systems","summary":"  Question answering (QA) is one of the most common NLP tasks that relates to\nnamed entity recognition, fact extraction, semantic search and some other\nfields. In industry, it is much appreciated in chatbots and corporate\ninformation systems. It is also a challenging task that attracted the attention\nof a very general audience at the quiz show Jeopardy! In this article we\ndescribe a Jeopardy!-like Russian QA data set collected from the official\nRussian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like\nquestions with 29,375 from the Russian analogue of Jeopardy! - \"Own Game\". We\nobserve its linguistic features and the related QA-task. We conclude about\nperspectives of a QA competition based on the data set collected from this\ndatabase.\n","authors":["Elena Mikhalkova"],"pdf_url":"https://arxiv.org/pdf/2112.02325v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.05270v1","updated":"2024-10-07T17:59:59Z","published":"2024-10-07T17:59:59Z","title":"Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia","summary":"  We consider the problem of adapting a contrastively pretrained\nvision-language model like CLIP (Radford et al., 2021) for few-shot\nclassification. The existing literature addresses this problem by learning a\nlinear classifier of the frozen visual features, optimizing word embeddings, or\nlearning external feature adapters. This paper introduces an alternative way\nfor CLIP adaptation without adding 'external' parameters to optimize. We find\nthat simply fine-tuning the last projection matrix of the vision encoder leads\nto strong performance compared to the existing baselines. Furthermore, we show\nthat regularizing training with the distance between the fine-tuned and\npretrained matrices adds reliability for adapting CLIP through this layer.\nPerhaps surprisingly, this approach, coined ProLIP, yields performances on par\nor better than state of the art on 11 few-shot classification benchmarks,\nfew-shot domain generalization, cross-dataset transfer and test-time\nadaptation. Code will be made available at\nhttps://github.com/astra-vision/ProLIP .\n","authors":["Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Patrick Prez","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2410.05270v1.pdf","comment":"Preprint,under review"},{"id":"http://arxiv.org/abs/2410.05267v1","updated":"2024-10-07T17:59:48Z","published":"2024-10-07T17:59:48Z","title":"Grounding Partially-Defined Events in Multimodal Data","summary":"  How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.\n","authors":["Kate Sanders","Reno Kriz","David Etter","Hannah Recknor","Alexander Martin","Cameron Carpenter","Jingyang Lin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2410.05267v1.pdf","comment":"Preprint; 9 pages; 2024 EMNLP Findings"},{"id":"http://arxiv.org/abs/2410.05266v1","updated":"2024-10-07T17:59:45Z","published":"2024-10-07T17:59:45Z","title":"Brain Mapping with Dense Features: Grounding Cortical Semantic\n  Selectivity in Natural Images With Vision Transformers","summary":"  Advances in large-scale artificial neural networks have facilitated novel\ninsights into the functional topology of the brain. Here, we leverage this\napproach to study how semantic categories are organized in the human visual\ncortex. To overcome the challenge presented by the co-occurrence of multiple\ncategories in natural images, we introduce BrainSAIL (Semantic Attribution and\nImage Localization), a method for isolating specific neurally-activating visual\nconcepts in images. BrainSAIL exploits semantically consistent, dense spatial\nfeatures from pre-trained vision models, building upon their demonstrated\nability to robustly predict neural activity. This method derives clean,\nspatially dense embeddings without requiring any additional training, and\nemploys a novel denoising process that leverages the semantic consistency of\nimages under random augmentations. By unifying the space of whole-image\nembeddings and dense visual features and then applying voxel-wise encoding\nmodels to these features, we enable the identification of specific subregions\nof each image which drive selectivity patterns in different areas of the higher\nvisual cortex. We validate BrainSAIL on cortical regions with known category\nselectivity, demonstrating its ability to accurately localize and disentangle\nselectivity to diverse visual concepts. Next, we demonstrate BrainSAIL's\nability to characterize high-level visual selectivity to scene properties and\nlow-level visual features such as depth, luminance, and saturation, providing\ninsights into the encoding of complex visual information. Finally, we use\nBrainSAIL to directly compare the feature selectivity of different brain\nencoding models across different regions of interest in visual cortex. Our\ninnovative method paves the way for significant advances in mapping and\ndecomposing high-level visual representations in the human brain.\n","authors":["Andrew F. Luo","Jacob Yeung","Rushikesh Zawar","Shaurya Dewan","Margaret M. Henderson","Leila Wehbe","Michael J. Tarr"],"pdf_url":"https://arxiv.org/pdf/2410.05266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11839v2","updated":"2024-10-07T17:59:42Z","published":"2024-06-17T17:59:58Z","title":"mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models","summary":"  Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.\n","authors":["Fei Wang","Wenxuan Zhou","James Y. Huang","Nan Xu","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11839v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO"},{"id":"http://arxiv.org/abs/2410.05261v1","updated":"2024-10-07T17:58:35Z","published":"2024-10-07T17:58:35Z","title":"TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and\n  Grounding with 16x Fewer Tokens","summary":"  Reading dense text and locating objects within images are fundamental\nabilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.\nPrevious LVLMs, including superior proprietary models like GPT-4o, have\nstruggled to excel in both tasks simultaneously. Moreover, previous LVLMs with\nfine-grained perception cost thousands of tokens per image, making them\nresource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient\nfine-grained perception and demonstrating cutting-edge performance across\ngeneral-purpose, OCR, and grounding tasks with 16 times fewer image tokens.\nCritical improvements include: (1) Token Compression: Building on the efficient\narchitecture of its predecessor, TextHawk2 significantly reduces the number of\ntokens per image by 16 times, facilitating training and deployment of the\nTextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We\nenhance the visual encoder through LVLM co-training, unlocking its potential\nfor previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:\nWe maintain a comparable scale of 100 million samples while diversifying the\nsources of pre-training data. We assess TextHawk2 across multiple benchmarks,\nwhere it consistently delivers superior performance and outperforms\nclosed-source models of similar scale, such as achieving 78.4% accuracy on\nOCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%\naccuracy@0.5 on RefCOCOg-test.\n","authors":["Ya-Qi Yu","Minghui Liao","Jiwen Zhang","Jihao Wu"],"pdf_url":"https://arxiv.org/pdf/2410.05261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05260v1","updated":"2024-10-07T17:58:22Z","published":"2024-10-07T17:58:22Z","title":"DART: A Diffusion-Based Autoregressive Motion Model for Real-Time\n  Text-Driven Motion Control","summary":"  Text-conditioned human motion generation, which allows for user interaction\nthrough natural language, has become increasingly popular. Existing methods\ntypically generate short, isolated motions based on a single input sentence.\nHowever, human motions are continuous and can extend over long periods,\ncarrying rich semantics. Creating long, complex motions that precisely respond\nto streams of text descriptions, particularly in an online and real-time\nsetting, remains a significant challenge. Furthermore, incorporating spatial\nconstraints into text-conditioned motion generation presents additional\nchallenges, as it requires aligning the motion semantics specified by text\ndescriptions with geometric information, such as goal locations and 3D scene\ngeometry. To address these limitations, we propose DART, a Diffusion-based\nAutoregressive motion primitive model for Real-time Text-driven motion control.\nOur model, DART, effectively learns a compact motion primitive space jointly\nconditioned on motion history and text inputs using latent diffusion models. By\nautoregressively generating motion primitives based on the preceding history\nand current text input, DART enables real-time, sequential motion generation\ndriven by natural language descriptions. Additionally, the learned motion\nprimitive space allows for precise spatial motion control, which we formulate\neither as a latent noise optimization problem or as a Markov decision process\naddressed through reinforcement learning. We present effective algorithms for\nboth approaches, demonstrating our model's versatility and superior performance\nin various motion synthesis tasks. Experiments show our method outperforms\nexisting baselines in motion realism, efficiency, and controllability. Video\nresults are available on the project page: https://zkf1997.github.io/DART/.\n","authors":["Kaifeng Zhao","Gen Li","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2410.05260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05259v1","updated":"2024-10-07T17:58:20Z","published":"2024-10-07T17:58:20Z","title":"GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting","summary":"  Diffusion-based 2D virtual try-on (VTON) techniques have recently\ndemonstrated strong performance, while the development of 3D VTON has largely\nlagged behind. Despite recent advances in text-guided 3D scene editing,\nintegrating 2D VTON into these pipelines to achieve vivid 3D VTON remains\nchallenging. The reasons are twofold. First, text prompts cannot provide\nsufficient details in describing clothing. Second, 2D VTON results generated\nfrom different viewpoints of the same 3D scene lack coherence and spatial\nrelationships, hence frequently leading to appearance inconsistencies and\ngeometric distortions. To resolve these problems, we introduce an\nimage-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian\nSplatting (3DGS) as the 3D representation, enables the transfer of pre-trained\nknowledge from 2D VTON models to 3D while improving cross-view consistency. (1)\nSpecifically, we propose a personalized diffusion model that utilizes low-rank\nadaptation (LoRA) fine-tuning to incorporate personalized information into\npre-trained 2D VTON models. To achieve effective LoRA training, we introduce a\nreference-driven image editing approach that enables the simultaneous editing\nof multi-view images while ensuring consistency. (2) Furthermore, we propose a\npersona-aware 3DGS editing framework to facilitate effective editing while\nmaintaining consistent cross-view appearance and high-quality 3D geometry. (3)\nAdditionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which\nfacilitates comprehensive qualitative and quantitative 3D VTON evaluations.\nThrough extensive experiments and comparative analyses with existing methods,\nthe proposed \\OM has demonstrated superior fidelity and advanced editing\ncapabilities, affirming its effectiveness for 3D VTON.\n","authors":["Yukang Cao","Masoud Hadi","Liang Pan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.05259v1.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.05255v1","updated":"2024-10-07T17:56:53Z","published":"2024-10-07T17:56:53Z","title":"SePPO: Semi-Policy Preference Optimization for Diffusion Alignment","summary":"  Reinforcement learning from human feedback (RLHF) methods are emerging as a\nway to fine-tune diffusion models (DMs) for visual generation. However,\ncommonly used on-policy strategies are limited by the generalization capability\nof the reward model, while off-policy approaches require large amounts of\ndifficult-to-obtain paired human-annotated data, particularly in visual\ngeneration tasks. To address the limitations of both on- and off-policy RLHF,\nwe propose a preference optimization method that aligns DMs with preferences\nwithout relying on reward models or paired human-annotated data. Specifically,\nwe introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO\nleverages previous checkpoints as reference models while using them to generate\non-policy reference samples, which replace \"losing images\" in preference pairs.\nThis approach allows us to optimize using only off-policy \"winning images.\"\nFurthermore, we design a strategy for reference model selection that expands\nthe exploration in the policy space. Notably, we do not simply treat reference\nsamples as negative examples for learning. Instead, we design an anchor-based\ncriterion to assess whether the reference samples are likely to be winning or\nlosing images, allowing the model to selectively learn from the generated\nreference samples. This approach mitigates performance degradation caused by\nthe uncertainty in reference sample quality. We validate SePPO across both\ntext-to-image and text-to-video benchmarks. SePPO surpasses all previous\napproaches on the text-to-image benchmarks and also demonstrates outstanding\nperformance on the text-to-video benchmarks. Code will be released in\nhttps://github.com/DwanZhang-AI/SePPO.\n","authors":["Daoan Zhang","Guangchen Lan","Dong-Jun Han","Wenlin Yao","Xiaoman Pan","Hongming Zhang","Mingxiao Li","Pengcheng Chen","Yu Dong","Christopher Brinton","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2410.05255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05249v1","updated":"2024-10-07T17:52:56Z","published":"2024-10-07T17:52:56Z","title":"LoTLIP: Improving Language-Image Pre-training for Long Text\n  Understanding","summary":"  Understanding long text is of great demands in practice but beyond the reach\nof most language-image pre-training (LIP) models. In this work, we empirically\nconfirm that the key reason causing such an issue is that the training images\nare usually paired with short captions, leaving certain tokens easily\novershadowed by salient tokens. Towards this problem, our initial attempt is to\nrelabel the data with long captions, however, directly learning with which may\nlead to performance degradation in understanding short text (e.g., in the image\nclassification task). Then, after incorporating corner tokens to aggregate\ndiverse textual information, we manage to help the model catch up to its\noriginal level of short text understanding yet greatly enhance its capability\nof long text understanding. We further look into whether the model can\ncontinuously benefit from longer captions and notice a clear trade-off between\nthe performance and the efficiency. Finally, we validate the effectiveness of\nour approach using a self-constructed large-scale dataset, which consists of\n100M long caption oriented text-image pairs. It is noteworthy that, on the task\nof long-text image retrieval, we beat the competitor using long captions with\n11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, the\nmodel, and the new dataset to facilitate the reproducibility and further\nresearch. The project page is available at https://wuw2019.github.io/lotlip.\n","authors":["Wei Wu","Kecheng Zheng","Shuailei Ma","Fan Lu","Yuxin Guo","Yifei Zhang","Wei Chen","Qingpei Guo","Yujun Shen","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2410.05249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05243v1","updated":"2024-10-07T17:47:50Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly take\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05239v1","updated":"2024-10-07T17:42:53Z","published":"2024-10-07T17:42:53Z","title":"TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation\n  Models","summary":"  Vision-Language Models (VLMs) have shown impressive performance in vision\ntasks, but adapting them to new domains often requires expensive fine-tuning.\nPrompt tuning techniques, including textual, visual, and multimodal prompting,\noffer efficient alternatives by leveraging learnable prompts. However, their\napplication to Vision-Language Segmentation Models (VLSMs) and evaluation under\nsignificant domain shifts remain unexplored. This work presents an open-source\nbenchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal\nprompt tuning techniques into VLSMs, making prompt tuning usable for downstream\nsegmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt\ntuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$\ndifferent combinations. We test various prompt tuning on $8$ diverse medical\ndatasets, including $3$ radiology datasets (breast tumor, echocardiograph,\nchest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin\ncancer), and two natural domain segmentation datasets. Our study found that\ntextual prompt tuning struggles under significant domain shifts, from\nnatural-domain images to medical data. Furthermore, visual prompt tuning, with\nfewer hyperparameters than multimodal prompt tuning, often achieves performance\ncompetitive to multimodal approaches, making it a valuable first attempt. Our\nwork advances the understanding and applicability of different prompt-tuning\ntechniques for robust domain-specific segmentation. The source code is\navailable at https://github.com/naamiinepal/tunevlseg.\n","authors":["Rabin Adhikari","Safal Thapaliya","Manish Dhakal","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2410.05239v1.pdf","comment":"Accepted at ACCV 2024 (oral presentation)"},{"id":"http://arxiv.org/abs/2410.05234v1","updated":"2024-10-07T17:41:35Z","published":"2024-10-07T17:41:35Z","title":"DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields\n  in Unsupervised Deformable Image Registration","summary":"  Deformable image registration aims to precisely align medical images from\ndifferent modalities or times. Traditional deep learning methods, while\neffective, often lack interpretability, real-time observability and adjustment\ncapacity during registration inference. Denoising diffusion models present an\nalternative by reformulating registration as iterative image denoising.\nHowever, existing diffusion registration approaches do not fully harness\ncapabilities, neglecting the critical sampling phase that enables continuous\nobservability during the inference. Hence, we introduce DiffuseReg, an\ninnovative diffusion-based method that denoises deformation fields instead of\nimages for improved transparency. We also propose a novel denoising network\nupon Swin Transformer, which better integrates moving and fixed images with\ndiffusion time step throughout the denoising process. Furthermore, we enhance\ncontrol over the denoising registration process with a novel similarity\nconsistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg\noutperforms existing diffusion registration methods by 1.32 in Dice score. The\nsampling process in DiffuseReg enables real-time output observability and\nadjustment unmatched by previous deep models.\n","authors":["Yongtai Zhuo","Yiqing Shen"],"pdf_url":"https://arxiv.org/pdf/2410.05234v1.pdf","comment":"MICCAI 2024, W-AM-067, https://github.com/YutaZhuo/DiffuseReg"},{"id":"http://arxiv.org/abs/2410.05233v1","updated":"2024-10-07T17:41:10Z","published":"2024-10-07T17:41:10Z","title":"SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised\n  Contrastive Learning","summary":"  We introduce a novel anchor-free contrastive learning (AFCL) method\nleveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach\nminimizes a semi-metric discriminative loss function that simultaneously\noptimizes two key objectives: reducing the distance and orthogonality between\nembeddings of similar inputs while maximizing these metrics for dissimilar\ninputs, facilitating more fine-grained contrastive learning. The AFCL method,\npowered by SimO loss, creates a fiber bundle topological structure in the\nembedding space, forming class-specific, internally cohesive yet orthogonal\nneighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,\nproviding visualizations that demonstrate the impact of SimO loss on the\nembedding space. Our results illustrate the formation of distinct, orthogonal\nclass neighborhoods, showcasing the method's ability to create well-structured\nembeddings that balance class separation with intra-class variability. This\nwork opens new avenues for understanding and leveraging the geometric\nproperties of learned representations in various machine learning tasks.\n","authors":["Taha Bouhsine","Imad El Aaroussi","Atik Faysal","Wang Huaxia"],"pdf_url":"https://arxiv.org/pdf/2410.05233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00700v4","updated":"2024-10-07T17:40:32Z","published":"2023-12-01T16:33:57Z","title":"Generative Parameter-Efficient Fine-Tuning","summary":"  We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting\npretrained Transformer backbones on downstream tasks. GIFT learns to generate\nthe fine-tuned weights for a layer directly from its pretrained weights. The\nGIFT network is parameterized in a minimally-simple way by two linear layers\n(without bias terms), and is shared by different pretrained layers selected for\nfine-tuning (e.g., the Query layers), which result in significantly fewer\ntrainable parameters compared to the layer-specific methods like Low-Rank\nAdapter (LoRA). We also show this formulation bridges parameter-efficient\nfine-tuning and representation fine-tuning. We perform comprehensive\nexperiments on natural language tasks (commonsense and arithmetic reasoning,\ninstruction tuning, and sequence classification) and computer vision tasks\n(fine-grained classification). We obtain the best performance and parameter\nefficiency among baselines on commonsense and arithmetic reasoning, and\ninstruction following using the Llama family of models and on visual\nrecognition benchmarks using Vision Transformers. Notably, compared to LoRA, we\nobtain 5.7% absolute increase in average accuracy with 14 times reduction of\nparameters on Commonsense170k using Llama-3 (8B), and 5.4% absolute increase in\nthe win rate with 4 times reduction of parameters using Llama-2 (7B) during\ninstruction tuning. Our GIFT also obtains a slightly higher win rate on\ninstruction tuning than GPT 3.5 (Turbo 1106).\n","authors":["Chinmay Savadikar","Xi Song","Tianfu Wu"],"pdf_url":"https://arxiv.org/pdf/2312.00700v4.pdf","comment":"Project page and code: https://savadikarc.github.io/gift"},{"id":"http://arxiv.org/abs/2408.06157v2","updated":"2024-10-07T17:39:52Z","published":"2024-08-12T13:53:40Z","title":"3D-free meets 3D priors: Novel View Synthesis from a Single Image with\n  Pretrained Diffusion Guidance","summary":"  Recent 3D novel view synthesis (NVS) methods are limited to\nsingle-object-centric scenes and struggle with complex environments. They often\nrequire extensive 3D data for training, lacking generalization beyond the\ntraining distribution. Conversely, 3D-free methods can generate text-controlled\nviews of complex, in-the-wild scenes using a pretrained stable diffusion model\nwithout the need for a large amount of 3D-based training data, but lack camera\ncontrol. In this paper, we introduce a method capable of generating\ncamera-controlled viewpoints from a single input image, by combining the\nbenefits of 3D-free and 3D-based approaches. Our method excels in handling\ncomplex and diverse scenes without extensive training or additional 3D and\nmultiview data. It leverages widely available pretrained NVS models for weak\nguidance, integrating this knowledge into a 3D-free view synthesis approach to\nachieve the desired results. Experimental results demonstrate that our method\noutperforms existing models in both qualitative and quantitative evaluations,\nproviding high-fidelity and consistent novel view synthesis at desired camera\nangles across a wide variety of scenes.\n","authors":["Taewon Kang","Divya Kothandaraman","Dinesh Manocha","Ming C. Lin"],"pdf_url":"https://arxiv.org/pdf/2408.06157v2.pdf","comment":"13 pages, 12 figures, v2: analysis studies and more results added"},{"id":"http://arxiv.org/abs/2410.05227v1","updated":"2024-10-07T17:35:10Z","published":"2024-10-07T17:35:10Z","title":"The Dawn of Video Generation: Preliminary Explorations with SORA-like\n  Models","summary":"  High-quality video generation, encompassing text-to-video (T2V),\nimage-to-video (I2V), and video-to-video (V2V) generation, holds considerable\nsignificance in content creation to benefit anyone express their inherent\ncreativity in new ways and world simulation to modeling and understanding the\nworld. Models like SORA have advanced generating videos with higher resolution,\nmore natural motion, better vision-language alignment, and increased\ncontrollability, particularly for long video sequences. These improvements have\nbeen driven by the evolution of model architectures, shifting from UNet to more\nscalable and parameter-rich DiT models, along with large-scale data expansion\nand refined training strategies. However, despite the emergence of DiT-based\nclosed-source and open-source models, a comprehensive investigation into their\ncapabilities and limitations remains lacking. Furthermore, the rapid\ndevelopment has made it challenging for recent benchmarks to fully cover\nSORA-like models and recognize their significant advancements. Additionally,\nevaluation metrics often fail to align with human preferences.\n","authors":["Ailing Zeng","Yuhang Yang","Weidong Chen","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.05227v1.pdf","comment":"project: https://ailab-cvc.github.io/VideoGen-Eval/"},{"id":"http://arxiv.org/abs/2410.05222v1","updated":"2024-10-07T17:26:31Z","published":"2024-10-07T17:26:31Z","title":"Precise Model Benchmarking with Only a Few Observations","summary":"  How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.\n","authors":["Riccardo Fogliato","Pratik Patil","Nil-Jana Akpinar","Mathew Monfort"],"pdf_url":"https://arxiv.org/pdf/2410.05222v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05217v1","updated":"2024-10-07T17:21:46Z","published":"2024-10-07T17:21:46Z","title":"Organizing Unstructured Image Collections using Natural Language","summary":"  Organizing unstructured visual data into semantic clusters is a key challenge\nin computer vision. Traditional deep clustering (DC) approaches focus on a\nsingle partition of data, while multiple clustering (MC) methods address this\nlimitation by uncovering distinct clustering solutions. The rise of large\nlanguage models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing\nusers to define clustering criteria in natural language. However, manually\nspecifying criteria for large datasets is impractical. In this work, we\nintroduce the task Semantic Multiple Clustering (SMC) that aims to\nautomatically discover clustering criteria from large image collections,\nuncovering interpretable substructures without requiring human input. Our\nframework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a\nproxy to concurrently reason over large image collections, discover\npartitioning criteria, expressed in natural language, and reveal semantic\nsubstructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c\nbenchmarks, each containing four grouping criteria and ground-truth\nannotations. We apply TeDeSC to various applications, such as discovering\nbiases and analyzing social media image popularity, demonstrating its utility\nas a tool for automatically organizing image collections and revealing novel\ninsights.\n","authors":["Mingxuan Liu","Zhun Zhong","Jun Li","Gianni Franchi","Subhankar Roy","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2410.05217v1.pdf","comment":"Preprint. Project webpage: https://oatmealliu.github.io/smc.html"},{"id":"http://arxiv.org/abs/2410.05210v1","updated":"2024-10-07T17:16:20Z","published":"2024-10-07T17:16:20Z","title":"Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving\n  Vision-Linguistic Compositionality","summary":"  In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.\n","authors":["Youngtaek Oh","Jae Won Cho","Dong-Jin Kim","In So Kweon","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.05210v1.pdf","comment":"EMNLP 2024 (Long, Main). Project page:\n  https://ytaek-oh.github.io/fsc-clip"},{"id":"http://arxiv.org/abs/2404.05729v2","updated":"2024-10-07T17:10:52Z","published":"2024-04-08T17:59:46Z","title":"Finding Visual Task Vectors","summary":"  Visual Prompting is a technique for teaching models to perform a visual task\nvia in-context examples, without any additional training. In this work, we\nanalyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find\ntask vectors, activations that encode task-specific information. Equipped with\nthis insight, we demonstrate that it is possible to identify the task vectors\nand use them to guide the network towards performing different tasks without\nproviding any input-output examples. To find task vectors, we compute the\naverage intermediate activations per task and use the REINFORCE algorithm to\nsearch for the subset of task vectors. The resulting task vectors guide the\nmodel towards performing a task better than the original model without the need\nfor input-output examples.\n","authors":["Alberto Hojel","Yutong Bai","Trevor Darrell","Amir Globerson","Amir Bar"],"pdf_url":"https://arxiv.org/pdf/2404.05729v2.pdf","comment":"https://github.com/alhojel/visual_task_vectors"},{"id":"http://arxiv.org/abs/2410.05206v1","updated":"2024-10-07T17:09:03Z","published":"2024-10-07T17:09:03Z","title":"Studying and Mitigating Biases in Sign Language Understanding Models","summary":"  Ensuring that the benefits of sign language technologies are distributed\nequitably among all community members is crucial. Thus, it is important to\naddress potential biases and inequities that may arise from the design or use\nof these resources. Crowd-sourced sign language datasets, such as the ASL\nCitizen dataset, are great resources for improving accessibility and preserving\nlinguistic diversity, but they must be used thoughtfully to avoid reinforcing\nexisting biases.\n  In this work, we utilize the rich information about participant demographics\nand lexical features present in the ASL Citizen dataset to study and document\nthe biases that may result from models trained on crowd-sourced sign datasets.\nFurther, we apply several bias mitigation techniques during model training, and\nfind that these techniques reduce performance disparities without decreasing\naccuracy. With the publication of this work, we release the demographic\ninformation about the participants in the ASL Citizen dataset to encourage\nfuture bias mitigation work in this space.\n","authors":["Katherine Atwell","Danielle Bragg","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.05206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05203v1","updated":"2024-10-07T17:07:21Z","published":"2024-10-07T17:07:21Z","title":"Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality","summary":"  The Fr\\'echet Video Distance (FVD) is a widely adopted metric for evaluating\nvideo generation distribution quality. However, its effectiveness relies on\ncritical assumptions. Our analysis reveals three significant limitations: (1)\nthe non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the\ninsensitivity of I3D features to temporal distortions; (3) the impractical\nsample sizes required for reliable estimation. These findings undermine FVD's\nreliability and show that FVD falls short as a standalone metric for video\ngeneration evaluation. After extensive analysis of a wide range of metrics and\nbackbone architectures, we propose JEDi, the JEPA Embedding Distance, based on\nfeatures derived from a Joint Embedding Predictive Architecture, measured using\nMaximum Mean Discrepancy with polynomial kernel. Our experiments on multiple\nopen-source datasets show clear evidence that it is a superior alternative to\nthe widely used FVD metric, requiring only 16% of the samples to reach its\nsteady value, while increasing alignment with human evaluation by 34%, on\naverage.\n","authors":["Ge Ya"," Luo","Gian Favero","Zhi Hao Luo","Alexia Jolicoeur-Martineau","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2410.05203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00678v2","updated":"2024-10-07T16:45:55Z","published":"2024-06-30T12:33:56Z","title":"A Narrative Review of Image Processing Techniques Related to Prostate\n  Ultrasound","summary":"  Prostate cancer (PCa) poses a significant threat to men's health, with early\ndiagnosis being crucial for improving prognosis and reducing mortality rates.\nTransrectal ultrasound (TRUS) plays a vital role in the diagnosis and\nimage-guided intervention of PCa.To facilitate physicians with more accurate\nand efficient computer-assisted diagnosis and interventions, many image\nprocessing algorithms in TRUS have been proposed and achieved state-of-the-art\nperformance in several tasks, including prostate gland segmentation, prostate\nimage registration, PCa classification and detection, and interventional needle\ndetection. The rapid development of these algorithms over the past two decades\nnecessitates a comprehensive summary. In consequence, this survey provides a\n\\textcolor{blue}{narrative } analysis of this field, outlining the evolution of\nimage processing methods in the context of TRUS image analysis and meanwhile\nhighlighting their relevant contributions. Furthermore, this survey discusses\ncurrent challenges and suggests future research directions to possibly advance\nthis field further.\n","authors":["Haiqiao Wang","Hong Wu","Zhuoyuan Wang","Peiyan Yue","Dong Ni","Pheng-Ann Heng","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00678v2.pdf","comment":"Accepted by Ultrasound in Medicine & Biology"},{"id":"http://arxiv.org/abs/2410.05182v1","updated":"2024-10-07T16:41:45Z","published":"2024-10-07T16:41:45Z","title":"MARs: Multi-view Attention Regularizations for Patch-based Feature\n  Recognition of Space Terrain","summary":"  The visual detection and tracking of surface terrain is required for\nspacecraft to safely land on or navigate within close proximity to celestial\nobjects. Current approaches rely on template matching with pre-gathered\npatch-based features, which are expensive to obtain and a limiting factor in\nperceptual capability. While recent literature has focused on in-situ detection\nmethods to enhance navigation and operational autonomy, robust description is\nstill needed. In this work, we explore metric learning as the lightweight\nfeature description mechanism and find that current solutions fail to address\ninter-class similarity and multi-view observational geometry. We attribute this\nto the view-unaware attention mechanism and introduce Multi-view Attention\nRegularizations (MARs) to constrain the channel and spatial attention across\nmultiple feature views, regularizing the what and where of attention focus. We\nthoroughly analyze many modern metric learning losses with and without MARs and\ndemonstrate improved terrain-feature recognition performance by upwards of 85%.\nWe additionally introduce the Luna-1 dataset, consisting of Moon crater\nlandmarks and reference navigation frames from NASA mission data to support\nfuture research in this difficult task. Luna-1 and source code are publicly\navailable at https://droneslab.github.io/mars/.\n","authors":["Timothy Chase Jr","Karthik Dantu"],"pdf_url":"https://arxiv.org/pdf/2410.05182v1.pdf","comment":"ECCV 2024. Project page available at\n  https://droneslab.github.io/mars/"},{"id":"http://arxiv.org/abs/2410.02381v2","updated":"2024-10-07T16:39:24Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.01029v2","updated":"2024-10-07T16:20:39Z","published":"2024-06-03T06:24:55Z","title":"CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship\n  Modeling in Aerial Videos","summary":"  Video scene graph generation (VidSGG) has emerged as a transformative\napproach to capturing and interpreting the intricate relationships among\nobjects and their temporal dynamics in video sequences. In this paper, we\nintroduce the new AeroEye dataset that focuses on multi-object relationship\nmodeling in aerial videos. Our AeroEye dataset features various drone scenes\nand includes a visually comprehensive and precise collection of predicates that\ncapture the intricate relationships and spatial arrangements among objects. To\nthis end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that\nallows the model to capture both direct and long-range temporal dependencies by\ncontinuously updating the history of interactions in a circular manner. The\nproposed approach also allows one to handle sequences with inherent cyclical\npatterns and process object relationships in the correct sequential order.\nTherefore, it can effectively capture periodic and overlapping relationships\nwhile minimizing information loss. The extensive experiments on the AeroEye\ndataset demonstrate the effectiveness of the proposed CYCLO model,\ndemonstrating its potential to perform scene understanding on drone videos.\nFinally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results\non two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.\n","authors":["Trong-Thuan Nguyen","Pha Nguyen","Xin Li","Jackson Cothren","Alper Yilmaz","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2406.01029v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.03361v6","updated":"2024-10-07T16:18:21Z","published":"2024-08-06T17:59:21Z","title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI","summary":"  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 284 datasets\nacross 38 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 53.96%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n","authors":["Pengcheng Chen","Jin Ye","Guoan Wang","Yanjun Li","Zhongying Deng","Wei Li","Tianbin Li","Haodong Duan","Ziyan Huang","Yanzhou Su","Benyou Wang","Shaoting Zhang","Bin Fu","Jianfei Cai","Bohan Zhuang","Eric J Seibel","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.03361v6.pdf","comment":"GitHub: https://github.com/uni-medical/GMAI-MMBench; Hugging face:\n  https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench"},{"id":"http://arxiv.org/abs/2310.03986v6","updated":"2024-10-07T16:15:36Z","published":"2023-10-06T03:04:21Z","title":"Robust Multimodal Learning with Missing Modalities via\n  Parameter-Efficient Adaptation","summary":"  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose a simple and parameter-efficient adaptation\nprocedure for pretrained multimodal networks. In particular, we exploit\nmodulation of intermediate features to compensate for the missing modalities.\nWe demonstrate that such adaptation can partially bridge performance drop due\nto missing modalities and outperform independent, dedicated networks trained\nfor the available modality combinations in some cases. The proposed adaptation\nrequires extremely small number of parameters (e.g., fewer than 1% of the total\nparameters) and applicable to a wide range of modality combinations and tasks.\nWe conduct a series of experiments to highlight the missing modality robustness\nof our proposed method on five different multimodal tasks across seven\ndatasets. Our proposed method demonstrates versatility across various tasks and\ndatasets, and outperforms existing methods for robust multimodal learning with\nmissing modalities.\n","authors":["Md Kaykobad Reza","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2310.03986v6.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). 28 pages, 6 figures, 17 tables"},{"id":"http://arxiv.org/abs/2410.05160v1","updated":"2024-10-07T16:14:05Z","published":"2024-10-07T16:14:05Z","title":"VLM2Vec: Training Vision-Language Models for Massive Multimodal\n  Embedding Tasks","summary":"  Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite their\nimportance. In this work, we aim to explore the potential for building\nuniversal embeddings capable of handling a wide range of downstream tasks. Our\ncontributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark),\nwhich covers 4 meta-tasks (i.e. classification, visual question answering,\nmultimodal retrieval, and visual grounding) and 36 datasets, including 20\ntraining and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, VLM2Vec can process any combination of\nimages and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate\nthem on MMEB's evaluation split. Our results show that \\model achieves an\nabsolute average improvement of 10% to 20% over existing multimodal embedding\nmodels on both in-distribution and out-of-distribution datasets in MMEB.\n","authors":["Ziyan Jiang","Rui Meng","Xinyi Yang","Semih Yavuz","Yingbo Zhou","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05160v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.05159v1","updated":"2024-10-07T16:13:49Z","published":"2024-10-07T16:13:49Z","title":"MIBench: A Comprehensive Benchmark for Model Inversion Attack and\n  Defense","summary":"  Model Inversion (MI) attacks aim at leveraging the output information of\ntarget models to reconstruct privacy-sensitive training data, raising\nwidespread concerns on privacy threats of Deep Neural Networks (DNNs).\nUnfortunately, in tandem with the rapid evolution of MI attacks, the lack of a\ncomprehensive, aligned, and reliable benchmark has emerged as a formidable\nchallenge. This deficiency leads to inadequate comparisons between different\nattack methods and inconsistent experimental setups. In this paper, we\nintroduce the first practical benchmark for model inversion attacks and\ndefenses to address this critical gap, which is named \\textit{MIBench}. This\nbenchmark serves as an extensible and reproducible modular-based toolbox and\ncurrently integrates a total of 16 state-of-the-art attack and defense methods.\nMoreover, we furnish a suite of assessment tools encompassing 9 commonly used\nevaluation protocols to facilitate standardized and fair evaluation and\nanalysis. Capitalizing on this foundation, we conduct extensive experiments\nfrom multiple perspectives to holistically compare and analyze the performance\nof various methods across different scenarios, which overcomes the misalignment\nissues and discrepancy prevalent in previous works. Based on the collected\nattack methods and defense strategies, we analyze the impact of target\nresolution, defense robustness, model predictive power, model architectures,\ntransferability and loss function. Our hope is that this \\textit{MIBench} could\nprovide a unified, practical and extensible toolbox and is widely utilized by\nresearchers in the field to rigorously test and compare their novel methods,\nensuring equitable evaluations and thereby propelling further advancements in\nthe future development.\n","authors":["Yixiang Qiu","Hongyao Yu","Hao Fang","Wenbo Yu","Bin Chen","Xuan Wang","Shu-Tao Xia","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2410.05159v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2309.15608v2","updated":"2024-10-07T16:05:53Z","published":"2023-09-27T12:15:05Z","title":"NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit\n  sensitivity maps","summary":"  We present a novel learned image reconstruction method for accelerated\ncardiac MRI with multiple receiver coils based on deep convolutional neural\nnetworks (CNNs) and algorithm unrolling. In contrast to many existing learned\nMR image reconstruction techniques that necessitate coil-sensitivity map (CSM)\nestimation as a distinct network component, our proposed approach avoids\nexplicit CSM estimation. Instead, it implicitly captures and learns to exploit\nthe inter-coil relationships of the images. Our method consists of a series of\nnovel learned image and k-space blocks with shared latent information and\nadaptation to the acquisition parameters by feature-wise modulation (FiLM), as\nwell as coil-wise data-consistency (DC) blocks.\n  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920\nand 0.942 in the cine track and mapping track validation leaderboard of the\nMICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different\nteams at the time of writing.\n  Code will be made available at https://github.com/fzimmermann89/CMRxRecon\n","authors":["Felix Frederik Zimmermann","Andreas Kofler"],"pdf_url":"https://arxiv.org/pdf/2309.15608v2.pdf","comment":"Accepted at MICCAI STACOM 2023"},{"id":"http://arxiv.org/abs/2410.05143v1","updated":"2024-10-07T15:55:02Z","published":"2024-10-07T15:55:02Z","title":"Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side\n  Information","summary":"  Diffusion models have found phenomenal success as expressive priors for\nsolving inverse problems, but their extension beyond natural images to more\nstructured scientific domains remains limited. Motivated by applications in\nmaterials science, we aim to reduce the number of measurements required from an\nexpensive imaging modality of interest, by leveraging side information from an\nauxiliary modality that is much cheaper to obtain. To deal with the\nnon-differentiable and black-box nature of the forward model, we propose a\nframework to train a multimodal diffusion model over the joint modalities,\nturning inverse problems with black-box forward models into simple linear\ninpainting problems. Numerically, we demonstrate the feasibility of training\ndiffusion models over materials imagery data, and show that our approach\nachieves superior image reconstruction by leveraging the available side\ninformation, requiring significantly less amount of data from the expensive\nmicroscopy modality.\n","authors":["Timofey Efimov","Harry Dong","Megna Shah","Jeff Simmons","Sean Donegan","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2410.05143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16756v2","updated":"2024-10-07T15:53:26Z","published":"2024-09-25T09:07:46Z","title":"Navigating the Maze of Explainable AI: A Systematic Approach to\n  Evaluating Methods and Metrics","summary":"  Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed\nmethods as well as metrics aiming to evaluate their efficacy. However, current\nstudies are often of limited scope, examining only a handful of XAI methods and\nignoring underlying design parameters for performance, such as the model\narchitecture or the nature of input data. Moreover, they often rely on one or a\nfew metrics and neglect thorough validation, increasing the risk of selection\nbias and ignoring discrepancies among metrics. These shortcomings leave\npractitioners confused about which method to choose for their problem. In\nresponse, we introduce LATEC, a large-scale benchmark that critically evaluates\n17 prominent XAI methods using 20 distinct metrics. We systematically\nincorporate vital design parameters like varied architectures and diverse input\nmodalities, resulting in 7,560 examined combinations. Through LATEC, we\nshowcase the high risk of conflicting metrics leading to unreliable rankings\nand consequently propose a more robust evaluation scheme. Further, we\ncomprehensively evaluate various XAI methods to assist practitioners in\nselecting appropriate methods aligning with their needs. Curiously, the\nemerging top-performing method, Expected Gradients, is not examined in any\nrelevant related study. LATEC reinforces its role in future XAI research by\npublicly releasing all 326k saliency maps and 378k metric scores as a\n(meta-)evaluation dataset. The benchmark is hosted at:\nhttps://github.com/IML-DKFZ/latec.\n","authors":["Lukas Klein","Carsten T. Lth","Udo Schlegel","Till J. Bungert","Mennatallah El-Assady","Paul F. Jger"],"pdf_url":"https://arxiv.org/pdf/2409.16756v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2311.18193v3","updated":"2024-10-07T15:36:17Z","published":"2023-11-30T02:24:44Z","title":"Persistent Test-time Adaptation in Recurring Testing Scenarios","summary":"  Current test-time adaptation (TTA) approaches aim to adapt to environments\nthat change continuously. Yet, it is unclear whether TTA methods can maintain\ntheir adaptability over prolonged periods. To answer this question, we\nintroduce a diagnostic setting - **recurring TTA** where environments not only\nchange but also recur over time, creating an extensive data stream. This\nsetting allows us to examine the error accumulation of TTA models, in the most\nbasic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-**perturbed Gaussian Mixture Model Classifier**,\nderiving theoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose **persistent TTA (PeTTA)**, which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks.\n","authors":["Trung-Hieu Hoang","Duc Minh Vo","Minh N. Do"],"pdf_url":"https://arxiv.org/pdf/2311.18193v3.pdf","comment":"Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.05116v1","updated":"2024-10-07T15:12:01Z","published":"2024-10-07T15:12:01Z","title":"Human-Feedback Efficient Reinforcement Learning for Online Diffusion\n  Model Finetuning","summary":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback.\n","authors":["Ayano Hiranaka","Shang-Fu Chen","Chieh-Hsin Lai","Dongjun Kim","Naoki Murata","Takashi Shibuya","Wei-Hsiang Liao","Shao-Hua Sun","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.05116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14327v4","updated":"2024-10-07T15:10:03Z","published":"2024-05-23T08:57:10Z","title":"Autoregressive Image Diffusion: Generation of Image Sequence and\n  Application in MRI","summary":"  Magnetic resonance imaging (MRI) is a widely used non-invasive imaging\nmodality. However, a persistent challenge lies in balancing image quality with\nimaging speed. This trade-off is primarily constrained by k-space measurements,\nwhich traverse specific trajectories in the spatial Fourier domain (k-space).\nThese measurements are often undersampled to shorten acquisition times,\nresulting in image artifacts and compromised quality. Generative models learn\nimage distributions and can be used to reconstruct high-quality images from\nundersampled k-space data. In this work, we present the autoregressive image\ndiffusion (AID) model for image sequences and use it to sample the posterior\nfor accelerated MRI reconstruction. The algorithm incorporates both\nundersampled k-space and pre-existing information. Models trained with fastMRI\ndataset are evaluated comprehensively. The results show that the AID model can\nrobustly generate sequentially coherent image sequences. In MRI applications,\nthe AID can outperform the standard diffusion model and reduce hallucinations,\ndue to the learned inter-image dependencies. The project code is available at\nhttps://github.com/mrirecon/aid.\n","authors":["Guanxiong Luo","Shoujin Huang","Martin Uecker"],"pdf_url":"https://arxiv.org/pdf/2405.14327v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05114v1","updated":"2024-10-07T15:09:50Z","published":"2024-10-07T15:09:50Z","title":"Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form\n  Factorization","summary":"  In the realm of dermatological diagnoses, where the analysis of dermatoscopic\nand microscopic skin lesion images is pivotal for the accurate and early\ndetection of various medical conditions, the costs associated with creating\ndiverse and high-quality annotated datasets have hampered the accuracy and\ngeneralizability of machine learning models. We propose an innovative\nunsupervised augmentation solution that harnesses Generative Adversarial\nNetwork (GAN) based models and associated techniques over their latent space to\ngenerate controlled semiautomatically-discovered semantic variations in\ndermatoscopic images. We created synthetic images to incorporate the semantic\nvariations and augmented the training data with these images. With this\napproach, we were able to increase the performance of machine learning models\nand set a new benchmark amongst non-ensemble based models in skin lesion\nclassification on the HAM10000 dataset; and used the observed analytics and\ngenerated models for detailed studies on model explainability, affirming the\neffectiveness of our solution.\n","authors":["Rohan Reddy Mekala","Frederik Pahde","Simon Baur","Sneha Chandrashekar","Madeline Diep","Markus Wenzel","Eric L. Wisotzky","Galip mit Yolcu","Sebastian Lapuschkin","Jackie Ma","Peter Eisert","Mikael Lindvall","Adam Porter","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2410.05114v1.pdf","comment":"This preprint has been submitted to the Workshop on Synthetic Data\n  for Computer Vision (SyntheticData4CV 2024 is a side event on 18th European\n  Conference on Computer Vision 2024). This preprint has not undergone peer\n  review or any post-submission improvements or corrections"},{"id":"http://arxiv.org/abs/2410.05111v1","updated":"2024-10-07T15:07:56Z","published":"2024-10-07T15:07:56Z","title":"LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting","summary":"  LiDAR simulation plays a crucial role in closed-loop simulation for\nautonomous driving. Although recent advancements, such as the use of\nreconstructed mesh and Neural Radiance Fields (NeRF), have made progress in\nsimulating the physical properties of LiDAR, these methods have struggled to\nachieve satisfactory frame rates and rendering quality. To address these\nlimitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,\nfor real-time high-fidelity re-simulation of LiDAR sensor scans in public urban\nroad scenes. The vanilla Gaussian Splatting, designed for camera models, cannot\nbe directly applied to LiDAR re-simulation. To bridge the gap between passive\ncamera and active LiDAR, our LiDAR-GS designs a differentiable laser beam\nsplatting, grounded in the LiDAR range view model. This innovation allows for\nprecise surface splatting by projecting lasers onto micro cross-sections,\neffectively eliminating artifacts associated with local affine approximations.\nAdditionally, LiDAR-GS leverages Neural Gaussian Fields, which further\nintegrate view-dependent clues, to represent key LiDAR properties that are\ninfluenced by the incident angle and external factors. Combining these\npractices with some essential adaptations, e.g., dynamic instances\ndecomposition, our approach succeeds in simultaneously re-simulating depth,\nintensity, and ray-drop channels, achieving state-of-the-art results in both\nrendering frame rate and quality on publically available large scene datasets.\nOur source code will be made publicly available.\n","authors":["Qifeng Chen","Sheng Yang","Sicong Du","Tao Tang","Peng Chen","Yuchi Huo"],"pdf_url":"https://arxiv.org/pdf/2410.05111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05103v1","updated":"2024-10-07T15:01:57Z","published":"2024-10-07T15:01:57Z","title":"MetaDD: Boosting Dataset Distillation with Neural Network\n  Architecture-Invariant Generalization","summary":"  Dataset distillation (DD) entails creating a refined, compact distilled\ndataset from a large-scale dataset to facilitate efficient training. A\nsignificant challenge in DD is the dependency between the distilled dataset and\nthe neural network (NN) architecture used. Training a different NN architecture\nwith a distilled dataset distilled using a specific architecture often results\nin diminished trainning performance for other architectures. This paper\nintroduces MetaDD, designed to enhance the generalizability of DD across\nvarious NN architectures. Specifically, MetaDD partitions distilled data into\nmeta features (i.e., the data's common characteristics that remain consistent\nacross different NN architectures) and heterogeneous features (i.e., the data's\nunique feature to each NN architecture). Then, MetaDD employs an\narchitecture-invariant loss function for multi-architecture feature alignment,\nwhich increases meta features and reduces heterogeneous features in distilled\ndata. As a low-memory consumption component, MetaDD can be seamlessly\nintegrated into any DD methodology. Experimental results demonstrate that\nMetaDD significantly improves performance across various DD methods. On the\nDistilled Tiny-Imagenet with Sre2L (50 IPC), MetaDD achieves cross-architecture\nNN accuracy of up to 30.1\\%, surpassing the second-best method (GLaD) by 1.7\\%.\n","authors":["Yunlong Zhao","Xiaoheng Deng","Xiu Su","Hongyan Xu","Xiuxing Li","Yijing Liu","Shan You"],"pdf_url":"https://arxiv.org/pdf/2410.05103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05100v1","updated":"2024-10-07T14:55:50Z","published":"2024-10-07T14:55:50Z","title":"IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral\n  Image Classification","summary":"  Hyperspectral image (HSI) classification has garnered substantial attention\nin remote sensing fields. Recent Mamba architectures built upon the Selective\nState Space Models (S6) have demonstrated enormous potential in long-range\nsequence modeling. However, the high dimensionality of hyperspectral data and\ninformation redundancy pose challenges to the application of Mamba in HSI\nclassification, suffering from suboptimal performance and computational\nefficiency. In light of this, this paper investigates a lightweight Interval\nGroup Spatial-Spectral Mamba framework (IGroupSS-Mamba) for HSI classification,\nwhich allows for multi-directional and multi-scale global spatial-spectral\ninformation extraction in a grouping and hierarchical manner. Technically, an\nInterval Group S6 Mechanism (IGSM) is developed as the core component, which\npartitions high-dimensional features into multiple non-overlapping groups at\nintervals, and then integrates a unidirectional S6 for each group with a\nspecific scanning direction to achieve non-redundant sequence modeling.\nCompared to conventional applying multi-directional scanning to all bands, this\ngrouping strategy leverages the complementary strengths of different scanning\ndirections while decreasing computational costs. To adequately capture the\nspatial-spectral contextual information, an Interval Group Spatial-Spectral\nBlock (IGSSB) is introduced, in which two IGSM-based spatial and spectral\noperators are cascaded to characterize the global spatial-spectral relationship\nalong the spatial and spectral dimensions, respectively. IGroupSS-Mamba is\nconstructed as a hierarchical structure stacked by multiple IGSSB blocks,\nintegrating a pixel aggregation-based downsampling strategy for multiscale\nspatial-spectral semantic learning from shallow to deep stages. Extensive\nexperiments demonstrate that IGroupSS-Mamba outperforms the state-of-the-art\nmethods.\n","authors":["Yan He","Bing Tu","Puzhao Jiang","Bo Liu","Jun Li","Antonio Plaza"],"pdf_url":"https://arxiv.org/pdf/2410.05100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05097v1","updated":"2024-10-07T14:51:54Z","published":"2024-10-07T14:51:54Z","title":"DreamSat: Towards a General 3D Model for Novel View Synthesis of Space\n  Objects","summary":"  Novel view synthesis (NVS) enables to generate new images of a scene or\nconvert a set of 2D images into a comprehensive 3D model. In the context of\nSpace Domain Awareness, since space is becoming increasingly congested, NVS can\naccurately map space objects and debris, improving the safety and efficiency of\nspace operations. Similarly, in Rendezvous and Proximity Operations missions,\n3D models can provide details about a target object's shape, size, and\norientation, allowing for better planning and prediction of the target's\nbehavior. In this work, we explore the generalization abilities of these\nreconstruction techniques, aiming to avoid the necessity of retraining for each\nnew scene, by presenting a novel approach to 3D spacecraft reconstruction from\nsingle-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art\nsingle-view reconstruction model, on a high-quality dataset of 190 high-quality\nspacecraft models and integrating it into the DreamGaussian framework. We\ndemonstrate consistent improvements in reconstruction quality across multiple\nmetrics, including Contrastive Language-Image Pretraining (CLIP) score\n(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity\nIndex (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)\n(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method\naddresses the lack of domain-specific 3D reconstruction tools in the space\nindustry by leveraging state-of-the-art diffusion models and 3D Gaussian\nsplatting techniques. This approach maintains the efficiency of the\nDreamGaussian framework while enhancing the accuracy and detail of spacecraft\nreconstructions. The code for this work can be accessed on GitHub\n(https://github.com/ARCLab-MIT/space-nvs).\n","authors":["Nidhi Mathihalli","Audrey Wei","Giovanni Lavezzi","Peng Mun Siew","Victor Rodriguez-Fernandez","Hodei Urrutxua","Richard Linares"],"pdf_url":"https://arxiv.org/pdf/2410.05097v1.pdf","comment":"Presented at the 75th International Astronautical Congress, October\n  2024, Milan, Italy"},{"id":"http://arxiv.org/abs/2410.05096v1","updated":"2024-10-07T14:50:56Z","published":"2024-10-07T14:50:56Z","title":"Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative\n  Approach Yolo With Video-llava","summary":"  Traffic Sign Recognition (TSR) detection is a crucial component of autonomous\nvehicles. While You Only Look Once (YOLO) is a popular real-time object\ndetection algorithm, factors like training data quality and adverse weather\nconditions (e.g., heavy rain) can lead to detection failures. These failures\ncan be particularly dangerous when visual similarities between objects exist,\nsuch as mistaking a 30 km/h sign for a higher speed limit sign. This paper\nproposes a method that combines video analysis and reasoning, prompting with a\nhuman-in-the-loop guide large vision model to improve YOLOs accuracy in\ndetecting road speed limit signs, especially in semi-real-world conditions. It\nis hypothesized that the guided prompting and reasoning abilities of\nVideo-LLava can enhance YOLOs traffic sign detection capabilities. This\nhypothesis is supported by an evaluation based on human-annotated accuracy\nmetrics within a dataset of recorded videos from the CARLA car simulator. The\nresults demonstrate that a collaborative approach combining YOLO with\nVideo-LLava and reasoning can effectively address challenging situations such\nas heavy rain and overcast conditions that hinder YOLOs detection capabilities.\n","authors":["Mehdi Azarafza","Fatima Idrees","Ali Ehteshami Bejnordi","Charles Steinmetz","Stefan Henkler","Achim Rettberg"],"pdf_url":"https://arxiv.org/pdf/2410.05096v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.14768v2","updated":"2024-10-07T14:35:14Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.05074v1","updated":"2024-10-07T14:29:24Z","published":"2024-10-07T14:29:24Z","title":"xLSTM-FER: Enhancing Student Expression Recognition with Extended Vision\n  Long Short-Term Memory Network","summary":"  Student expression recognition has become an essential tool for assessing\nlearning experiences and emotional states. This paper introduces xLSTM-FER, a\nnovel architecture derived from the Extended Long Short-Term Memory (xLSTM),\ndesigned to enhance the accuracy and efficiency of expression recognition\nthrough advanced sequence processing capabilities for student facial expression\nrecognition. xLSTM-FER processes input images by segmenting them into a series\nof patches and leveraging a stack of xLSTM blocks to handle these patches.\nxLSTM-FER can capture subtle changes in real-world students' facial expressions\nand improve recognition accuracy by learning spatial-temporal relationships\nwithin the sequence. Experiments on CK+, RAF-DF, and FERplus demonstrate the\npotential of xLSTM-FER in expression recognition tasks, showing better\nperformance compared to state-of-the-art methods on standard datasets. The\nlinear computational and memory complexity of xLSTM-FER make it particularly\nsuitable for handling high-resolution images. Moreover, the design of xLSTM-FER\nallows for efficient processing of non-sequential inputs such as images without\nadditional computation.\n","authors":["Qionghao Huang","Jili Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05074v1.pdf","comment":"The paper, consisting of 10 pages and 3 figures, has been accepted by\n  the AIEDM Workshop at the 8th APWeb-WAIM Joint International Conference on\n  Web and Big Data"},{"id":"http://arxiv.org/abs/2410.03171v2","updated":"2024-10-07T14:28:41Z","published":"2024-10-04T06:05:26Z","title":"Selective Transformer for Hyperspectral Image Classification","summary":"  Transformer has achieved satisfactory results in the field of hyperspectral\nimage (HSI) classification. However, existing Transformer models face two key\nchallenges when dealing with HSI scenes characterized by diverse land cover\ntypes and rich spectral information: (1) fixed receptive field representation\noverlooks effective contextual information; (2) redundant self-attention\nfeature representation. To address these limitations, we propose a novel\nSelective Transformer (SFormer) for HSI classification. The SFormer is designed\nto dynamically select receptive fields for capturing both spatial and spectral\ncontextual information, while mitigating the impact of redundant data by\nprioritizing the most relevant features. This enables a highly accurate\nclassification of the land covers of the HSI. Specifically, a Kernel Selective\nTransformer Block (KSTB) is first utilized to dynamically select an appropriate\nreceptive field range to effectively extract spatial-spectral features.\nFurthermore, to capture the most crucial tokens, a Token Selective Transformer\nBlock (TSTB) is introduced, which selects the most relevant tokens based on the\nranking of attention scores for each query. Extensive experiments on four\nbenchmark HSI datasets demonstrate that the proposed SFormer outperforms the\nstate-of-the-art HSI classification models. The codes will be released.\n","authors":["Yichu Xu","Di Wang","Lefei Zhang","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05063v1","updated":"2024-10-07T14:21:51Z","published":"2024-10-07T14:21:51Z","title":"Control-oriented Clustering of Visual Latent Representation","summary":"  We initiate a study of the geometry of the visual representation space -- the\ninformation channel from the vision encoder to the action decoder -- in an\nimage-based control pipeline learned from behavior cloning. Inspired by the\nphenomenon of neural collapse (NC) in image classification, we investigate\nwhether a similar law of clustering emerges in the visual representation space.\nSince image-based control is a regression task without explicitly defined\nclasses, the central piece of the puzzle lies in determining according to what\nimplicit classes the visual features cluster, if such a law exists. Focusing on\nimage-based planar pushing, we posit the most important role of the visual\nrepresentation in a control task is to convey a goal to the action decoder. We\nthen classify training samples of expert demonstrations into eight\n\"control-oriented\" classes based on (a) the relative pose between the object\nand the target in the input or (b) the relative pose of the object induced by\nexpert actions in the output, where one class corresponds to one relative pose\northant (REPO). Across four different instantiations of architecture, we report\nthe prevalent emergence of control-oriented clustering in the visual\nrepresentation space according to the eight REPOs. Beyond empirical\nobservation, we show such a law of clustering can be leveraged as an\nalgorithmic tool to improve test-time performance when training a policy with\nlimited expert demonstrations. Particularly, we pretrain the vision encoder\nusing NC as a regularization to encourage control-oriented clustering of the\nvisual features. Surprisingly, such an NC-pretrained vision encoder, when\nfinetuned end-to-end with the action decoder, boosts the test-time performance\nby 10% to 35% in the low-data regime. Real-world vision-based planar pushing\nexperiments confirmed the surprising advantage of control-oriented visual\nrepresentation pretraining.\n","authors":["Han Qi","Haocheng Yin","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.05063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05058v1","updated":"2024-10-07T14:18:32Z","published":"2024-10-07T14:18:32Z","title":"Improving Object Detection via Local-global Contrastive Learning","summary":"  Visual domain gaps often impact object detection performance. Image-to-image\ntranslation can mitigate this effect, where contrastive approaches enable\nlearning of the image-to-image mapping under unsupervised regimes. However,\nexisting methods often fail to handle content-rich scenes with multiple object\ninstances, which manifests in unsatisfactory detection performance. Sensitivity\nto such instance-level content is typically only gained through object\nannotations, which can be expensive to obtain. Towards addressing this issue,\nwe present a novel image-to-image translation method that specifically targets\ncross-domain object detection. We formulate our approach as a contrastive\nlearning framework with an inductive prior that optimises the appearance of\nobject instances through spatial attention masks, implicitly delineating the\nscene into foreground regions associated with the target object instances and\nbackground non-object regions. Instead of relying on object annotations to\nexplicitly account for object instances during translation, our approach learns\nto represent objects by contrasting local-global information. This affords\ninvestigation of an under-explored challenge: obtaining performant detection,\nunder domain shifts, without relying on object annotations nor detector model\nfine-tuning. We experiment with multiple cross-domain object detection settings\nacross three challenging benchmarks and report state-of-the-art performance.\nProject page: https://local-global-detection.github.io\n","authors":["Danai Triantafyllidou","Sarah Parisot","Ales Leonardis","Steven McDonagh"],"pdf_url":"https://arxiv.org/pdf/2410.05058v1.pdf","comment":"BMVC 2024 - Project page: https://local-global-detection.github.io"},{"id":"http://arxiv.org/abs/2410.05057v1","updated":"2024-10-07T14:14:38Z","published":"2024-10-07T14:14:38Z","title":"SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image\n  Classification","summary":"  Data curation is the problem of how to collect and organize samples into a\ndataset that supports efficient learning. Despite the centrality of the task,\nlittle work has been devoted towards a large-scale, systematic comparison of\nvarious curation methods. In this work, we take steps towards a formal\nevaluation of data curation strategies and introduce SELECT, the first\nlarge-scale benchmark of curation strategies for image classification.\n  In order to generate baseline methods for the SELECT benchmark, we create a\nnew dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K\nto date. Our dataset extends ImageNet with 5 new training-data shifts, each\napproximately the size of ImageNet-1K itself, and each assembled using a\ndistinct curation strategy. We evaluate our data curation baselines in two\nways: (i) using each training-data shift to train identical image\nclassification models from scratch (ii) using the data itself to fit a\npretrained self-supervised representation.\n  Our findings show interesting trends, particularly pertaining to recent\nmethods for data curation such as synthetic data generation and lookup based on\nCLIP embeddings. We show that although these strategies are highly competitive\nfor certain tasks, the curation strategy used to assemble the original\nImageNet-1K dataset remains the gold standard. We anticipate that our benchmark\ncan illuminate the path for new methods to further reduce the gap. We release\nour checkpoints, code, documentation, and a link to our dataset at\nhttps://github.com/jimmyxu123/SELECT.\n","authors":["Benjamin Feuer","Jiawei Xu","Niv Cohen","Patrick Yubeaton","Govind Mittal","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2410.05057v1.pdf","comment":"NeurIPS 2024, Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.05051v1","updated":"2024-10-07T14:06:16Z","published":"2024-10-07T14:06:16Z","title":"HE-Drive: Human-Like End-to-End Driving with Vision Language Models","summary":"  In this paper, we propose HE-Drive: the first human-like-centric end-to-end\nautonomous driving system to generate trajectories that are both temporally\nconsistent and comfortable. Recent studies have shown that imitation\nlearning-based planners and learning-based trajectory scorers can effectively\ngenerate and select accuracy trajectories that closely mimic expert\ndemonstrations. However, such trajectory planners and scorers face the dilemma\nof generating temporally inconsistent and uncomfortable trajectories. To solve\nthe above problems, Our HE-Drive first extracts key 3D spatial representations\nthrough sparse perception, which then serves as conditional inputs for a\nConditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion\nplanner to generate temporal consistency multi-modal trajectories. A\nVision-Language Models (VLMs)-guided trajectory scorer subsequently selects the\nmost comfortable trajectory from these candidates to control the vehicle,\nensuring human-like end-to-end driving. Experiments show that HE-Drive not only\nachieves state-of-the-art performance (i.e., reduces the average collision rate\nby 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the\nchallenging nuScenes and OpenScene datasets but also provides the most\ncomfortable driving experience on real-world data.For more information, visit\nthe project website: https://jmwang0117.github.io/HE-Drive/.\n","authors":["Junming Wang","Xingyu Zhang","Zebin Xing","Songen Gu","Xiaoyang Guo","Yang Hu","Ziying Song","Qian Zhang","Xiaoxiao Long","Wei Yin"],"pdf_url":"https://arxiv.org/pdf/2410.05051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05044v1","updated":"2024-10-07T13:58:40Z","published":"2024-10-07T13:58:40Z","title":"PhotoReg: Photometrically Registering 3D Gaussian Splatting Models","summary":"  Building accurate representations of the environment is critical for\nintelligent robots to make decisions during deployment. Advances in\nphotorealistic environment models have enabled robots to develop\nhyper-realistic reconstructions, which can be used to generate images that are\nintuitive for human inspection. In particular, the recently introduced\n\\ac{3DGS}, which describes the scene with up to millions of primitive\nellipsoids, can be rendered in real time. \\ac{3DGS} has rapidly gained\nprominence. However, a critical unsolved problem persists: how can we fuse\nmultiple \\ac{3DGS} into a single coherent model? Solving this problem will\nenable robot teams to jointly build \\ac{3DGS} models of their surroundings. A\nkey insight of this work is to leverage the {duality} between photorealistic\nreconstructions, which render realistic 2D images from 3D structure, and\n\\emph{3D foundation models}, which predict 3D structure from image pairs. To\nthis end, we develop PhotoReg, a framework to register multiple photorealistic\n\\ac{3DGS} models with 3D foundation models. As \\ac{3DGS} models are generally\nbuilt from monocular camera images, they have \\emph{arbitrary scale}. To\nresolve this, PhotoReg actively enforces scale consistency among the different\n\\ac{3DGS} models by considering depth estimates within these models. Then, the\nalignment is iteratively refined with fine-grained photometric losses to\nproduce high-quality fused \\ac{3DGS} models. We rigorously evaluate PhotoReg on\nboth standard benchmark datasets and our custom-collected datasets, including\nwith two quadruped robots. The code is released at\n\\url{ziweny11.github.io/photoreg}.\n","authors":["Ziwen Yuan","Tianyi Zhang","Matthew Johnson-Roberson","Weiming Zhi"],"pdf_url":"https://arxiv.org/pdf/2410.05044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05041v1","updated":"2024-10-07T13:53:17Z","published":"2024-10-07T13:53:17Z","title":"Systematic Literature Review of Vision-Based Approaches to Outdoor\n  Livestock Monitoring with Lessons from Wildlife Studies","summary":"  Precision livestock farming (PLF) aims to improve the health and welfare of\nlivestock animals and farming outcomes through the use of advanced\ntechnologies. Computer vision, combined with recent advances in machine\nlearning and deep learning artificial intelligence approaches, offers a\npossible solution to the PLF ideal of 24/7 livestock monitoring that helps\nfacilitate early detection of animal health and welfare issues. However, a\nsignificant number of livestock species are raised in large outdoor habitats\nthat pose technological challenges for computer vision approaches. This review\nprovides a comprehensive overview of computer vision methods and open\nchallenges in outdoor animal monitoring. We include research from both the\nlivestock and wildlife fields in the review because of the similarities in\nappearance, behaviour, and habitat for many livestock and wildlife. We focus on\nlarge terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,\ngiraffes, and elephants. We use an image processing pipeline to frame our\ndiscussion and highlight the current capabilities and open technical challenges\nat each stage of the pipeline. The review found a clear trend towards the use\nof deep learning approaches for animal detection, counting, and multi-species\nclassification. We discuss in detail the applicability of current vision-based\nmethods to PLF contexts and promising directions for future research.\n","authors":["Stacey D. Scott","Zayn J. Abbas","Feerass Ellid","Eli-Henry Dykhne","Muhammad Muhaiminul Islam","Weam Ayad","Kristina Kacmorova","Dan Tulpan","Minglun Gong"],"pdf_url":"https://arxiv.org/pdf/2410.05041v1.pdf","comment":"28 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2405.18213v2","updated":"2024-10-07T13:52:00Z","published":"2024-05-28T14:17:41Z","title":"NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields","summary":"  Sound plays a major role in human perception. Along with vision, it provides\nessential information for understanding our surroundings. Despite advances in\nneural implicit representations, learning acoustics that align with visual\nscenes remains a challenge. We propose NeRAF, a method that jointly learns\nacoustic and radiance fields. NeRAF synthesizes both novel views and\nspatialized room impulse responses (RIR) at new positions by conditioning the\nacoustic field on 3D scene geometric and appearance priors from the radiance\nfield. The generated RIR can be applied to auralize any audio signal. Each\nmodality can be rendered independently and at spatially distinct positions,\noffering greater versatility. We demonstrate that NeRAF generates high-quality\naudio on SoundSpaces and RAF datasets, achieving significant performance\nimprovements over prior methods while being more data-efficient. Additionally,\nNeRAF enhances novel view synthesis of complex scenes trained with sparse data\nthrough cross-modal learning. NeRAF is designed as a Nerfstudio module,\nproviding convenient access to realistic audio-visual generation.\n","authors":["Amandine Brunetto","Sascha Hornauer","Fabien Moutarde"],"pdf_url":"https://arxiv.org/pdf/2405.18213v2.pdf","comment":"Project Page: https://amandinebtto.github.io/NeRAF"},{"id":"http://arxiv.org/abs/2409.16845v2","updated":"2024-10-07T13:39:35Z","published":"2024-09-25T11:53:58Z","title":"IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized\n  SAR-ATR","summary":"  Recently, computer-aided design models and electromagnetic simulations have\nbeen used to augment synthetic aperture radar (SAR) data for deep learning.\nHowever, an automatic target recognition (ATR) model struggles with domain\nshift when using synthetic data because the model learns specific clutter\npatterns present in such data, which disturbs performance when applied to\nmeasured data with different clutter distributions. This study proposes a\nframework particularly designed for domain-generalized SAR-ATR called IRASNet,\nenabling effective feature-level clutter reduction and domain-invariant feature\nlearning. First, we propose a clutter reduction module (CRM) that maximizes the\nsignal-to-clutter ratio on feature maps. The module reduces the impact of\nclutter at the feature level while preserving target and shadow information,\nthereby improving ATR performance. Second, we integrate adversarial learning\nwith CRM to extract clutter-reduced domain-invariant features. The integration\nbridges the gap between synthetic and measured datasets without requiring\nmeasured data during training. Third, we improve feature extraction from target\nand shadow regions by implementing a positional supervision task using mask\nground truth encoding. The improvement enhances the ability of the model to\ndiscriminate between classes. Our proposed IRASNet presents new\nstate-of-the-art public SAR datasets utilizing target and shadow information to\nachieve superior performance across various test conditions. IRASNet not only\nenhances generalization performance but also significantly improves\nfeature-level clutter reduction, making it a valuable advancement in the field\nof radar image pattern recognition.\n","authors":["Oh-Tae Jang","Hae-Kang Song","Min-Jun Kim","Kyung-Hwan Lee","Geon Lee","Sung-Ho Kim","Hee-Sub Shin","Jae-Woo Ok","Min-Young Back","Jae-Hyuk Yoon","Kyung-Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16845v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.10389v3","updated":"2024-10-07T13:27:28Z","published":"2024-07-15T01:58:54Z","title":"Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High\n  Quality and Efficient Rendering","summary":"  Since the introduction of NeRFs, considerable attention has been focused on\nimproving their training and inference times, leading to the development of\nFast-NeRFs models. Despite demonstrating impressive rendering speed and\nquality, the rapid convergence of such models poses challenges for further\nimproving reconstruction quality. Common strategies to improve rendering\nquality involves augmenting model parameters or increasing the number of\nsampled points. However, these computationally intensive approaches encounter\nlimitations in achieving significant quality enhancements. This study\nintroduces a model-agnostic framework inspired by Sparsely-Gated Mixture of\nExperts to enhance rendering quality without escalating computational\ncomplexity. Our approach enables specialization in rendering different scene\ncomponents by employing a mixture of experts with varying resolutions. We\npresent a novel gate formulation designed to maximize expert capabilities and\npropose a resolution-based routing technique to effectively induce sparsity and\ndecompose scenes. Our work significantly improves reconstruction quality while\nmaintaining competitive performance.\n","authors":["Francesco Di Sario","Riccardo Renzulli","Enzo Tartaglione","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2407.10389v3.pdf","comment":"The paper has been accepted to the ECCV 2024 conference"},{"id":"http://arxiv.org/abs/2310.19351v3","updated":"2024-10-07T13:23:08Z","published":"2023-10-30T08:46:26Z","title":"Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised\n  Domain Generalization for Object Detection","summary":"  Object detectors do not work well when domains largely differ between\ntraining and testing data. To overcome this domain gap in object detection\nwithout requiring expensive annotations, we consider two problem settings:\nsemi-supervised domain generalizable object detection (SS-DGOD) and\nweakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain\ngeneralization for object detection that requires labeled data from multiple\ndomains, SS-DGOD and WS-DGOD require labeled data only from one domain and\nunlabeled or weakly-labeled data from multiple domains for training. In this\npaper, we show that object detectors can be effectively trained on the two\nsettings with the same Mean Teacher learning framework, where a student network\nis trained with pseudo-labels output from a teacher on the unlabeled or\nweakly-labeled data. We provide novel interpretations of why the Mean Teacher\nlearning framework works well on the two settings in terms of the relationships\nbetween the generalization gap and flat minima in parameter space. On the basis\nof the interpretations, we also show that incorporating a simple regularization\nmethod into the Mean Teacher learning framework leads to flatter minima. The\nexperimental results demonstrate that the regularization leads to flatter\nminima and boosts the performance of the detectors trained with the Mean\nTeacher learning framework on the two settings.\n","authors":["Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2310.19351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16204v2","updated":"2024-10-07T13:11:05Z","published":"2024-06-23T20:00:20Z","title":"Breaking the Frame: Visual Place Recognition by Overlap Prediction","summary":"  Visual place recognition methods struggle with occlusions and partial visual\noverlaps. We propose a novel visual place recognition approach based on overlap\nprediction, called VOP, shifting from traditional reliance on global image\nsimilarities and local features to image overlap prediction. VOP proceeds\nco-visible image sections by obtaining patch-level embeddings using a Vision\nTransformer backbone and establishing patch-to-patch correspondences without\nrequiring expensive feature detection and matching. Our approach uses a voting\nmechanism to assess overlap scores for potential database images. It provides a\nnuanced image retrieval metric in challenging scenarios. Experimental results\nshow that VOP leads to more accurate relative pose estimation and localization\nresults on the retrieved image pairs than state-of-the-art baselines on a\nnumber of large-scale, real-world indoor and outdoor benchmarks. The code is\navailable at https://github.com/weitong8591/vop.git.\n","authors":["Tong Wei","Philipp Lindenberger","Jiri Matas","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2406.16204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16017v2","updated":"2024-10-07T12:52:19Z","published":"2024-02-25T07:28:28Z","title":"Spectrum Extraction and Clipping for Implicitly Linear Layers","summary":"  We show the effectiveness of automatic differentiation in efficiently and\ncorrectly computing and controlling the spectrum of implicitly linear\noperators, a rich family of layer types including all standard convolutional\nand dense layers. We provide the first clipping method which is correct for\ngeneral convolution layers, and illuminate the representational limitation that\ncaused correctness issues in prior work. We study the effect of the batch\nnormalization layers when concatenated with convolutional layers and show how\nour clipping method can be applied to their composition. By comparing the\naccuracy and performance of our algorithms to the state-of-the-art methods,\nusing various experiments, we show they are more precise and efficient and lead\nto better generalization and adversarial robustness. We provide the code for\nusing our methods at https://github.com/Ali-E/FastClip.\n","authors":["Ali Ebrahimpour Boroojeny","Matus Telgarsky","Hari Sundaram"],"pdf_url":"https://arxiv.org/pdf/2402.16017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04989v1","updated":"2024-10-07T12:43:50Z","published":"2024-10-07T12:43:50Z","title":"Conditional Variational Autoencoders for Probabilistic Pose Regression","summary":"  Robots rely on visual relocalization to estimate their pose from camera\nimages when they lose track. One of the challenges in visual relocalization is\nrepetitive structures in the operation environment of the robot. This calls for\nprobabilistic methods that support multiple hypotheses for robot's pose. We\npropose such a probabilistic method to predict the posterior distribution of\ncamera poses given an observed image. Our proposed training strategy results in\na generative model of camera poses given an image, which can be used to draw\nsamples from the pose posterior distribution. Our method is streamlined and\nwell-founded in theory and outperforms existing methods on localization in\npresence of ambiguities.\n","authors":["Fereidoon Zangeneh","Leonard Bruns","Amit Dekel","Alessandro Pieropan","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2410.04989v1.pdf","comment":"Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2405.15118v2","updated":"2024-10-07T12:28:43Z","published":"2024-05-24T00:18:15Z","title":"GS-Hider: Hiding Messages into 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has already become the emerging research focus\nin the fields of 3D scene reconstruction and novel view synthesis. Given that\ntraining a 3DGS requires a significant amount of time and computational cost,\nit is crucial to protect the copyright, integrity, and privacy of such 3D\nassets. Steganography, as a crucial technique for encrypted transmission and\ncopyright protection, has been extensively studied. However, it still lacks\nprofound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS\npossesses two distinct features: 1) explicit 3D representation; and 2)\nreal-time rendering speeds. These characteristics result in the 3DGS point\ncloud files being public and transparent, with each Gaussian point having a\nclear physical significance. Therefore, ensuring the security and fidelity of\nthe original 3D scene while embedding information into the 3DGS point cloud\nfiles is an extremely challenging task. To solve the above-mentioned issue, we\nfirst propose a steganography framework for 3DGS, dubbed GS-Hider, which can\nembed 3D scenes and images into original GS point clouds in an invisible manner\nand accurately extract the hidden messages. Specifically, we design a coupled\nsecured feature attribute to replace the original 3DGS's spherical harmonics\ncoefficients and then use a scene decoder and a message decoder to disentangle\nthe original RGB scene and the hidden message. Extensive experiments\ndemonstrated that the proposed GS-Hider can effectively conceal multimodal\nmessages without compromising rendering quality and possesses exceptional\nsecurity, robustness, capacity, and flexibility. Our project is available at:\nhttps://xuanyuzhang21.github.io/project/gshider.\n","authors":["Xuanyu Zhang","Jiarui Meng","Runyi Li","Zhipei Xu","Yongbing Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.15118v2.pdf","comment":"Accepted by NeurIPS 2024, 3DGS steganography"},{"id":"http://arxiv.org/abs/2410.04983v1","updated":"2024-10-07T12:26:22Z","published":"2024-10-07T12:26:22Z","title":"RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection","summary":"  Precision agriculture relies heavily on effective weed management to ensure\nrobust crop yields. This study presents RoWeeder, an innovative framework for\nunsupervised weed mapping that combines crop-row detection with a\nnoise-resilient deep learning model. By leveraging crop-row information to\ncreate a pseudo-ground truth, our method trains a lightweight deep learning\nmodel capable of distinguishing between crops and weeds, even in the presence\nof noisy data. Evaluated on the WeedMap dataset, RoWeeder achieves an F1 score\nof 75.3, outperforming several baselines. Comprehensive ablation studies\nfurther validated the model's performance. By integrating RoWeeder with drone\ntechnology, farmers can conduct real-time aerial surveys, enabling precise weed\nmanagement across large fields. The code is available at:\n\\url{https://github.com/pasqualedem/RoWeeder}.\n","authors":["Pasquale De Marinis","Rino Vessio","Giovanna Castellano"],"pdf_url":"https://arxiv.org/pdf/2410.04983v1.pdf","comment":"Computer Vision for Plant Phenotyping and Agriculture (CVPPA)\n  workshop at ECCV 2024"},{"id":"http://arxiv.org/abs/2410.04980v1","updated":"2024-10-07T12:21:49Z","published":"2024-10-07T12:21:49Z","title":"Comparison of marker-less 2D image-based methods for infant pose\n  estimation","summary":"  There are increasing efforts to automate clinical methods for early diagnosis\nof developmental disorders, among them the General Movement Assessment (GMA), a\nvideo-based tool to classify infant motor functioning. Optimal pose estimation\nis a crucial part of the automated GMA. In this study we compare the\nperformance of available generic- and infant-pose estimators, and the choice of\nviewing angle for optimal recordings, i.e., conventional diagonal view used in\nGMA vs. top-down view. For this study, we used 4500 annotated video-frames from\n75 recordings of infant spontaneous motor functions from 4 to 26 weeks. To\ndetermine which available pose estimation method and camera angle yield the\nbest pose estimation accuracy on infants in a GMA related setting, the distance\nto human annotations as well as the percentage of correct key-points (PCK) were\ncomputed and compared. The results show that the best performing generic model\ntrained on adults, ViTPose, also performs best on infants. We see no\nimprovement from using specialized infant-pose estimators over the generic pose\nestimators on our own infant dataset. However, when retraining a generic model\non our data, there is a significant improvement in pose estimation accuracy.\nThe pose estimation accuracy obtained from the top-down view is significantly\nbetter than that obtained from the diagonal view, especially for the detection\nof the hip key-points. The results also indicate only limited generalization\ncapabilities of infant-pose estimators to other infant datasets, which hints\nthat one should be careful when choosing infant pose estimators and using them\non infant datasets which they were not trained on. While the standard GMA\nmethod uses a diagonal view for assessment, pose estimation accuracy\nsignificantly improves using a top-down view. This suggests that a top-down\nview should be included in recording setups for automated GMA research.\n","authors":["Lennart Jahn","Sarah Flgge","Dajie Zhang","Luise Poustka","Sven Blte","Florentin Wrgtter","Peter B Marschik","Tomas Kulvicius"],"pdf_url":"https://arxiv.org/pdf/2410.04980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04974v1","updated":"2024-10-07T12:16:36Z","published":"2024-10-07T12:16:36Z","title":"6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric\n  Rendering","summary":"  Novel view synthesis has advanced significantly with the development of\nneural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,\nachieving high quality without compromising real-time rendering remains\nchallenging, particularly for physically-based ray tracing with view-dependent\neffects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D\nspatial-angular representation to better incorporate view-dependent effects,\nbut the Gaussian representation and control scheme are sub-optimal. In this\npaper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),\nwhich enhances color and opacity representations and leverages the additional\ndirectional information in the 6D space for optimized Gaussian control. Our\napproach is fully compatible with the 3DGS framework and significantly improves\nreal-time radiance field rendering by better modeling view-dependent effects\nand fine details. Experiments demonstrate that 6DGS significantly outperforms\n3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction\nof 66.5% Gaussian points compared to 3DGS.\n","authors":["Zhongpai Gao","Benjamin Planche","Meng Zheng","Anwesa Choudhuri","Terrence Chen","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.04974v1.pdf","comment":"Demo Video: https://www.youtube.com/watch?v=77wN-K6Q9aM"},{"id":"http://arxiv.org/abs/2410.04972v1","updated":"2024-10-07T12:16:21Z","published":"2024-10-07T12:16:21Z","title":"L-C4: Language-Based Video Colorization for Creative and Consistent\n  Color","summary":"  Automatic video colorization is inherently an ill-posed problem because each\nmonochrome frame has multiple optional color candidates. Previous\nexemplar-based video colorization methods restrict the user's imagination due\nto the elaborate retrieval process. Alternatively, conditional image\ncolorization methods combined with post-processing algorithms still struggle to\nmaintain temporal consistency. To address these issues, we present\nLanguage-based video Colorization for Creative and Consistent Colors (L-C4) to\nguide the colorization process using user-provided language descriptions. Our\nmodel is built upon a pre-trained cross-modality generative model, leveraging\nits comprehensive language understanding and robust color representation\nabilities. We introduce the cross-modality pre-fusion module to generate\ninstance-aware text embeddings, enabling the application of creative colors.\nAdditionally, we propose temporally deformable attention to prevent flickering\nor color shifts, and cross-clip fusion to maintain long-term color consistency.\nExtensive experimental results demonstrate that L-C4 outperforms relevant\nmethods, achieving semantically accurate colors, unrestricted creative\ncorrespondence, and temporally robust consistency.\n","authors":["Zheng Chang","Shuchen Weng","Huan Ouyang","Yu Li","Si Li","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2410.04972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18082v2","updated":"2024-10-07T12:06:17Z","published":"2024-09-26T17:26:16Z","title":"SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language\n  Models for Robotic Garment Manipulation","summary":"  Automating garment manipulation poses a significant challenge for assistive\nrobotics due to the diverse and deformable nature of garments. Traditional\napproaches typically require separate models for each garment type, which\nlimits scalability and adaptability. In contrast, this paper presents a unified\napproach using vision-language models (VLMs) to improve keypoint prediction\nacross various garment categories. By interpreting both visual and semantic\ninformation, our model enables robots to manage different garment states with a\nsingle model. We created a large-scale synthetic dataset using advanced\nsimulation techniques, allowing scalable training without extensive real-world\ndata. Experimental results indicate that the VLM-based method significantly\nenhances keypoint detection accuracy and task success rates, providing a more\nflexible and general solution for robotic garment manipulation. In addition,\nthis research also underscores the potential of VLMs to unify various garment\nmanipulation tasks within a single framework, paving the way for broader\napplications in home automation and assistive robotics for future.\n","authors":["Xin Li","Siyuan Huang","Qiaojun Yu","Zhengkai Jiang","Ce Hao","Yimeng Zhu","Hongsheng Li","Peng Gao","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2409.18082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19339v2","updated":"2024-10-07T12:05:55Z","published":"2024-09-28T12:49:16Z","title":"Visual Question Decomposition on Multimodal Large Language Models","summary":"  Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.\n","authors":["Haowei Zhang","Jianzhe Liu","Zhen Han","Shuo Chen","Bailan He","Volker Tresp","Zhiqiang Xu","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2409.19339v2.pdf","comment":"Accepted to EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.04965v1","updated":"2024-10-07T12:04:39Z","published":"2024-10-07T12:04:39Z","title":"Revealing Directions for Text-guided 3D Face Editing","summary":"  3D face editing is a significant task in multimedia, aimed at the\nmanipulation of 3D face models across various control signals. The success of\n3D-aware GAN provides expressive 3D models learned from 2D single-view images\nonly, encouraging researchers to discover semantic editing directions in its\nlatent space. However, previous methods face challenges in balancing quality,\nefficiency, and generalization. To solve the problem, we explore the\npossibility of introducing the strength of diffusion model into 3D-aware GANs.\nIn this paper, we present Face Clan, a fast and text-general approach for\ngenerating and manipulating 3D faces based on arbitrary attribute descriptions.\nTo achieve disentangled editing, we propose to diffuse on the latent space\nunder a pair of opposite prompts to estimate the mask indicating the region of\ninterest on latent codes. Based on the mask, we then apply denoising to the\nmasked latent codes to reveal the editing direction. Our method offers a\nprecisely controllable manipulation method, allowing users to intuitively\ncustomize regions of interest with the text description. Experiments\ndemonstrate the effectiveness and generalization of our Face Clan for various\npre-trained GANs. It offers an intuitive and wide application for text-guided\nface editing that contributes to the landscape of multimedia content creation.\n","authors":["Zhuo Chen","Yichao Yan","Sehngqi Liu","Yuhao Cheng","Weiming Zhao","Lincheng Li","Mengxiao Bi","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15341v2","updated":"2024-10-07T12:04:11Z","published":"2024-09-09T21:09:47Z","title":"StructuReiser: A Structure-preserving Video Stylization Method","summary":"  We introduce StructuReiser, a novel video-to-video translation method that\ntransforms input videos into stylized sequences using a set of user-provided\nkeyframes. Unlike existing approaches, StructuReiser maintains strict adherence\nto the structural elements of the target video, preserving the original\nidentity while seamlessly applying the desired stylistic transformations. This\nenables a level of control and consistency that was previously unattainable\nwith traditional text-driven or keyframe-based methods. Furthermore,\nStructuReiser supports real-time inference and custom keyframe editing, making\nit ideal for interactive applications and expanding the possibilities for\ncreative expression and video manipulation.\n","authors":["Radim Spetlik","David Futschik","Daniel Sykora"],"pdf_url":"https://arxiv.org/pdf/2409.15341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04960v1","updated":"2024-10-07T11:59:54Z","published":"2024-10-07T11:59:54Z","title":"On Efficient Variants of Segment Anything Model: A Survey","summary":"  The Segment Anything Model (SAM) is a foundational model for image\nsegmentation tasks, known for its strong generalization across diverse\napplications. However, its impressive performance comes with significant\ncomputational and resource demands, making it challenging to deploy in\nresource-limited environments such as mobile devices. To address this, a\nvariety of SAM variants have been proposed to enhance efficiency without\nsacrificing accuracy. This survey provides the first comprehensive review of\nthese efficient SAM variants. We begin by exploring the motivations driving\nthis research. We then present core techniques used in SAM and model\nacceleration. This is followed by an in-depth analysis of various acceleration\nstrategies, categorized by approach. Finally, we offer a unified and extensive\nevaluation of these methods, assessing their efficiency and accuracy on\nrepresentative benchmarks, and providing a clear comparison of their overall\nperformance.\n","authors":["Xiaorui Sun","Jun Liu","Heng Tao Shen","Xiaofeng Zhu","Ping Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04960v1.pdf","comment":"Report in progress"},{"id":"http://arxiv.org/abs/2410.04946v1","updated":"2024-10-07T11:43:42Z","published":"2024-10-07T11:43:42Z","title":"Real-time Ship Recognition and Georeferencing for the Improvement of\n  Maritime Situational Awareness","summary":"  In an era where maritime infrastructures are crucial, advanced situational\nawareness solutions are increasingly important. The use of optical camera\nsystems can allow real-time usage of maritime footage. This thesis presents an\ninvestigation into leveraging deep learning and computer vision to advance\nreal-time ship recognition and georeferencing for the improvement of maritime\nsituational awareness. A novel dataset, ShipSG, is introduced, containing 3,505\nimages and 11,625 ship masks with corresponding class and geographic position.\nAfter an exploration of state-of-the-art, a custom real-time segmentation\narchitecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier\nembedded system. This architecture adds the 2D scattering transform and\nattention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per\nframe, outperforming state-of-the-art methods by over 5%. To improve small and\ndistant ship recognition in high-resolution images on embedded systems, an\nenhanced slicing mechanism is introduced, improving mAP by 8% to 11%.\nAdditionally, a georeferencing method is proposed, achieving positioning errors\nof 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m.\nThe findings are also applied in real-world scenarios, such as the detection of\nabnormal ship behaviour, camera integrity assessment and 3D reconstruction. The\napproach of this thesis outperforms existing methods and provides a framework\nfor integrating recognized and georeferenced ships into real-time systems,\nenhancing operational effectiveness and decision-making for maritime\nstakeholders. This thesis contributes to the maritime computer vision field by\nestablishing a benchmark for ship segmentation and georeferencing research,\ndemonstrating the viability of deep-learning-based recognition and\ngeoreferencing methods for real-time maritime monitoring.\n","authors":["Borja Carrillo Perez"],"pdf_url":"https://arxiv.org/pdf/2410.04946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04940v1","updated":"2024-10-07T11:32:17Z","published":"2024-10-07T11:32:17Z","title":"Next state prediction gives rise to entangled, yet compositional\n  representations of objects","summary":"  Compositional representations are thought to enable humans to generalize\nacross combinatorially vast state spaces. Models with learnable object slots,\nwhich encode information about objects in separate latent codes, have shown\npromise for this type of generalization but rely on strong architectural\npriors. Models with distributed representations, on the other hand, use\noverlapping, potentially entangled neural codes, and their ability to support\ncompositional generalization remains underexplored. In this paper we examine\nwhether distributed models can develop linearly separable representations of\nobjects, like slotted models, through unsupervised training on videos of object\ninteractions. We show that, surprisingly, models with distributed\nrepresentations often match or outperform models with object slots in\ndownstream prediction tasks. Furthermore, we find that linearly separable\nobject representations can emerge without object-centric priors, with auxiliary\nobjectives like next-state prediction playing a key role. Finally, we observe\nthat distributed models' object representations are never fully disentangled,\neven if they are linearly separable: Multiple objects can be encoded through\npartially overlapping neural populations while still being highly separable\nwith a linear classifier. We hypothesize that maintaining partially shared\ncodes enables distributed models to better compress object dynamics,\npotentially enhancing generalization.\n","authors":["Tankred Saanum","Luca M. Schulze Buschoff","Peter Dayan","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2410.04940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04939v1","updated":"2024-10-07T11:31:12Z","published":"2024-10-07T11:31:12Z","title":"PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with\n  Image and Point Cloud Fusion","summary":"  Place recognition plays a crucial role in the fields of robotics and computer\nvision, finding applications in areas such as autonomous driving, mapping, and\nlocalization. Place recognition identifies a place using query sensor data and\na known database. One of the main challenges is to develop a model that can\ndeliver accurate results while being robust to environmental variations. We\npropose two multi-modal place recognition models, namely PRFusion and\nPRFusion++. PRFusion utilizes global fusion with manifold metric attention,\nenabling effective interaction between features without requiring camera-LiDAR\nextrinsic calibrations. In contrast, PRFusion++ assumes the availability of\nextrinsic calibrations and leverages pixel-point correspondences to enhance\nfeature learning on local windows. Additionally, both models incorporate neural\ndiffusion layers, which enable reliable operation even in challenging\nenvironments. We verify the state-of-the-art performance of both models on\nthree large-scale benchmarks. Notably, they outperform existing models by a\nsubstantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore,\nwe conduct ablation studies to validate the effectiveness of our proposed\nmethods. The codes are available at: https://github.com/sijieaaa/PRFusion\n","authors":["Sijie Wang","Qiyu Kang","Rui She","Kai Zhao","Yang Song","Wee Peng Tay"],"pdf_url":"https://arxiv.org/pdf/2410.04939v1.pdf","comment":"accepted by IEEE TITS 2024"},{"id":"http://arxiv.org/abs/2107.07243v3","updated":"2024-10-07T11:27:30Z","published":"2021-07-15T11:05:00Z","title":"VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged\n  Robots","summary":"  We present visual inertial lidar legged navigation system (VILENS), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term, which is estimated online. This bias is observable\nbecause of the tight fusion of this preintegrated velocity factor with vision,\nlidar, and inertial measurement unit (IMU) factors. Extensive experimental\nvalidation on different ANYmal quadruped robots is presented, for a total\nduration of 2 h and 1.8 km traveled. The experiments involved dynamic\nlocomotion over loose rocks, slopes, and mud, which caused challenges such as\nslippage and terrain deformation. Perceptual challenges included dark and dusty\nunderground caverns, and open and feature-deprived areas. We show an average\nimprovement of 62% translational and 51% rotational errors compared to a\nstate-of-the-art loosely coupled approach. To demonstrate its robustness,\nVILENS was also integrated with a perceptive controller and a local path\nplanner.\n","authors":["David Wisth","Marco Camurri","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2107.07243v3.pdf","comment":"Video: https://youtu.be/NG4pkjJKhus"},{"id":"http://arxiv.org/abs/2410.04932v1","updated":"2024-10-07T11:26:13Z","published":"2024-10-07T11:26:13Z","title":"OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal\n  Instruction","summary":"  We present OmniBooth, an image generation framework that enables spatial\ncontrol with instance-level multi-modal customization. For all instances, the\nmultimodal instruction can be described through text prompts or image\nreferences. Given a set of user-defined masks and associated text or image\nguidance, our objective is to generate an image, where multiple objects are\npositioned at specified coordinates and their attributes are precisely aligned\nwith the corresponding guidance. This approach significantly expands the scope\nof text-to-image generation, and elevates it to a more versatile and practical\ndimension in controllability. In this paper, our core contribution lies in the\nproposed latent control signals, a high-dimensional spatial feature that\nprovides a unified representation to integrate the spatial, textual, and image\nconditions seamlessly. The text condition extends ControlNet to provide\ninstance-level open-vocabulary generation. The image condition further enables\nfine-grained control with personalized identity. In practice, our method\nempowers users with more flexibility in controllable generation, as users can\nchoose multi-modal conditions from text or images as needed. Furthermore,\nthorough experiments demonstrate our enhanced performance in image synthesis\nfidelity and alignment across different tasks and datasets. Project page:\nhttps://len-li.github.io/omnibooth-web/\n","authors":["Leheng Li","Weichao Qiu","Xu Yan","Jing He","Kaiqiang Zhou","Yingjie Cai","Qing Lian","Bingbing Liu","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04906v1","updated":"2024-10-07T10:48:08Z","published":"2024-10-07T10:48:08Z","title":"Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation","summary":"  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n","authors":["Ivan Rinaldi","Nicola Fanelli","Giovanna Castellano","Gennaro Vessio"],"pdf_url":"https://arxiv.org/pdf/2410.04906v1.pdf","comment":"Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024"},{"id":"http://arxiv.org/abs/2403.12510v2","updated":"2024-10-07T10:31:58Z","published":"2024-03-19T07:24:54Z","title":"Generalized Consistency Trajectory Models for Image Manipulation","summary":"  Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing.\n","authors":["Beomsu Kim","Jaemin Kim","Jeongsol Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04889v1","updated":"2024-10-07T10:17:46Z","published":"2024-10-07T10:17:46Z","title":"D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and\n  Shape Estimation","summary":"  We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose\nand Shape Estimation), a one-stage method that estimates human pose and SMPL-X\nshape parameters from a single RGB image. Recent works use larger models with\ntransformer backbones and decoders to improve the accuracy in human pose and\nshape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the\nestimated human depth-maps as an intermediate representation for HPS and\nleverages training with synthetic data and the ground-truth depth-maps provided\nwith them for depth supervision during training. Although trained on synthetic\ndatasets, D-PoSE achieves state-of-the-art performance on the real-world\nbenchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and\nthe CNN backbone, it outperforms ViT-based models that have a number of\nparameters that is larger by almost an order of magnitude. D-PoSE code is\navailable at: https://github.com/nvasilik/D-PoSE\n","authors":["Nikolaos Vasilikopoulos","Drosakis Drosakis","Antonis Argyros"],"pdf_url":"https://arxiv.org/pdf/2410.04889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06165v4","updated":"2024-10-07T10:14:00Z","published":"2024-02-09T03:48:20Z","title":"Learning Contrastive Feature Representations for Facial Action Unit\n  Detection","summary":"  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n","authors":["Ziqiao Shang","Bin Liu","Fengmao Lv","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.06165v4.pdf","comment":"13 pages, 17 figures, submitted to IEEE Transactions on Circuits and\n  Systems for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2410.04884v1","updated":"2024-10-07T10:06:01Z","published":"2024-10-07T10:06:01Z","title":"Patch is Enough: Naturalistic Adversarial Patch against Vision-Language\n  Pre-training Models","summary":"  Visual language pre-training (VLP) models have demonstrated significant\nsuccess across various domains, yet they remain vulnerable to adversarial\nattacks. Addressing these adversarial vulnerabilities is crucial for enhancing\nsecurity in multimodal learning. Traditionally, adversarial methods targeting\nVLP models involve simultaneously perturbing images and text. However, this\napproach faces notable challenges: first, adversarial perturbations often fail\nto translate effectively into real-world scenarios; second, direct\nmodifications to the text are conspicuously visible. To overcome these\nlimitations, we propose a novel strategy that exclusively employs image patches\nfor attacks, thus preserving the integrity of the original text. Our method\nleverages prior knowledge from diffusion models to enhance the authenticity and\nnaturalness of the perturbations. Moreover, to optimize patch placement and\nimprove the efficacy of our attacks, we utilize the cross-attention mechanism,\nwhich encapsulates intermodal interactions by generating attention maps to\nguide strategic patch placements. Comprehensive experiments conducted in a\nwhite-box setting for image-to-text scenarios reveal that our proposed method\nsignificantly outperforms existing techniques, achieving a 100% attack success\nrate. Additionally, it demonstrates commendable performance in transfer tasks\ninvolving text-to-image configurations.\n","authors":["Dehong Kong","Siyuan Liang","Xiaopeng Zhu","Yuansheng Zhong","Wenqi Ren"],"pdf_url":"https://arxiv.org/pdf/2410.04884v1.pdf","comment":"accepted by Visual Intelligence"},{"id":"http://arxiv.org/abs/2410.04880v1","updated":"2024-10-07T10:01:30Z","published":"2024-10-07T10:01:30Z","title":"Improved detection of discarded fish species through BoxAL active\n  learning","summary":"  In recent years, powerful data-driven deep-learning techniques have been\ndeveloped and applied for automated catch registration. However, these methods\nare dependent on the labelled data, which is time-consuming, labour-intensive,\nexpensive to collect and need expert knowledge. In this study, we present an\nactive learning technique, named BoxAL, which includes estimation of epistemic\ncertainty of the Faster R-CNN object-detection model. The method allows\nselecting the most uncertain training images from an unlabeled pool, which are\nthen used to train the object-detection model. To evaluate the method, we used\nan open-source image dataset obtained with a dedicated image-acquisition system\ndeveloped for commercial trawlers targeting demersal species. We demonstrated,\nthat our approach allows reaching the same object-detection performance as with\nthe random sampling using 400 fewer labelled images. Besides, mean AP score was\nsignificantly higher at the last training iteration with 1100 training images,\nspecifically, 39.0&plusmn;1.6 and 34.8&plusmn;1.8 for certainty-based sampling\nand random sampling, respectively. Additionally, we showed that epistemic\ncertainty is a suitable method to sample images that the current iteration of\nthe model cannot deal with yet. Our study additionally showed that the sampled\nnew data is more valuable for training than the remaining unlabeled data. Our\nsoftware is available on https://github.com/pieterblok/boxal.\n","authors":["Maria Sokolova","Pieter M. Blok","Angelo Mencarelli","Arjan Vroegop","Aloysius van Helmond","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2410.04880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06300v3","updated":"2024-10-07T09:48:14Z","published":"2023-08-11T07:57:12Z","title":"Classification of All Blood Cell Images using ML and DL Models","summary":"  Human blood primarily comprises plasma, red blood cells, white blood cells,\nand platelets. It plays a vital role in transporting nutrients to different\norgans, where it stores essential health-related data about the human body.\nBlood cells are utilized to defend the body against diverse infections,\nincluding fungi, viruses, and bacteria. Hence, blood analysis can help\nphysicians assess an individual's physiological condition. Blood cells have\nbeen sub-classified into eight groups: Neutrophils, eosinophils, basophils,\nlymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and\nmetamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of\ntheir nucleus, shape, and cytoplasm. Traditionally, pathologists and\nhematologists in laboratories have examined these blood cells using a\nmicroscope before manually classifying them. The manual approach is slower and\nmore prone to human error. Therefore, it is essential to automate this process.\nIn our paper, transfer learning with CNN pre-trained models. VGG16, VGG19,\nResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20\napplied to the PBC dataset's normal DIB. The overall accuracy achieved with\nthese models lies between 91.375 and 94.72%. Hence, inspired by these\npre-trained architectures, a model has been proposed to automatically classify\nthe ten types of blood cells with increased accuracy. A novel CNN-based\nframework has been presented to improve accuracy. The proposed CNN model has\nbeen tested on the PBC dataset normal DIB. The outcomes of the experiments\ndemonstrate that our CNN-based framework designed for blood cell classification\nattains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional\nneural network model performs competitively when compared to earlier results\nreported in the literature.\n","authors":["Rabia Asghar","Sanjay Kumar","Paul Hynds","Abeera Mahfooz"],"pdf_url":"https://arxiv.org/pdf/2308.06300v3.pdf","comment":"15"},{"id":"http://arxiv.org/abs/2407.16665v2","updated":"2024-10-07T09:46:07Z","published":"2024-07-23T17:32:02Z","title":"A Framework for Pupil Tracking with Event Cameras","summary":"  Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction.\n","authors":["Khadija Iddrisu","Waseem Shariff","Suzanne Little"],"pdf_url":"https://arxiv.org/pdf/2407.16665v2.pdf","comment":"This paper is a preprint of a paper submitted to the 26th Irish\n  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the\n  copy of record will be available at IET Digital Library"},{"id":"http://arxiv.org/abs/2410.04873v1","updated":"2024-10-07T09:43:28Z","published":"2024-10-07T09:43:28Z","title":"TeX-NeRF: Neural Radiance Fields from Pseudo-TeX Vision","summary":"  Neural radiance fields (NeRF) has gained significant attention for its\nexceptional visual effects. However, most existing NeRF methods reconstruct 3D\nscenes from RGB images captured by visible light cameras. In practical\nscenarios like darkness, low light, or bad weather, visible light cameras\nbecome ineffective. Therefore, we propose TeX-NeRF, a 3D reconstruction method\nusing only infrared images, which introduces the object material emissivity as\na priori, preprocesses the infrared images using Pseudo-TeX vision, and maps\nthe temperatures (T), emissivities (e), and textures (X) of the scene into the\nsaturation (S), hue (H), and value (V) channels of the HSV color space,\nrespectively. Novel view synthesis using the processed images has yielded\nexcellent results. Additionally, we introduce 3D-TeX Datasets, the first\ndataset comprising infrared images and their corresponding Pseudo-TeX vision\nimages. Experiments demonstrate that our method not only matches the quality of\nscene reconstruction achieved with high-quality RGB images but also provides\naccurate temperature estimations for objects in the scene.\n","authors":["Chonghao Zhong","Chao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.04873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19890v2","updated":"2024-10-07T09:35:44Z","published":"2024-09-30T02:39:42Z","title":"Universal Medical Image Representation Learning with Compositional\n  Decoders","summary":"  Visual-language models have advanced the development of universal models, yet\ntheir application in medical imaging remains constrained by specific functional\nrequirements and the limited data. Current general-purpose models are typically\ndesigned with task-specific branches and heads, which restricts the shared\nfeature space and the flexibility of model. To address these challenges, we\nhave developed a decomposed-composed universal medical imaging paradigm\n(UniMed) that supports tasks at all levels. To this end, we first propose a\ndecomposed decoder that can predict two types of outputs -- pixel and semantic,\nbased on a defined input queue. Additionally, we introduce a composed decoder\nthat unifies the input and output spaces and standardizes task annotations\nacross different levels into a discrete token format. The coupled design of\nthese two components enables the model to flexibly combine tasks and mutual\nbenefits. Moreover, our joint representation learning strategy skilfully\nleverages large amounts of unlabeled data and unsupervised loss, achieving\nefficient one-stage pretraining for more robust performance. Experimental\nresults show that UniMed achieves state-of-the-art performance on eight\ndatasets across all three tasks and exhibits strong zero-shot and 100-shot\ntransferability. We will release the code and trained models upon the paper's\nacceptance.\n","authors":["Kaini Wang","Ling Yang","Siping Zhou","Guangquan Zhou","Wentao Zhang","Bin Cui","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2409.19890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04866v1","updated":"2024-10-07T09:32:11Z","published":"2024-10-07T09:32:11Z","title":"Art Forgery Detection using Kolmogorov Arnold and Convolutional Neural\n  Networks","summary":"  Art authentication has historically established itself as a task requiring\nprofound connoisseurship of one particular artist. Nevertheless, famous art\nforgers such as Wolfgang Beltracchi were able to deceive dozens of art experts.\nIn recent years Artificial Intelligence algorithms have been successfully\napplied to various image processing tasks. In this work, we leverage the\ngrowing improvements in AI to present an art authentication framework for the\nidentification of the forger Wolfgang Beltracchi. Differently from existing\nliterature on AI-aided art authentication, we focus on a specialized model of a\nforger, rather than an artist, flipping the approach of traditional AI methods.\nWe use a carefully compiled dataset of known artists forged by Beltracchi and a\nset of known works by the forger to train a multiclass image classification\nmodel based on EfficientNet. We compare the results with Kolmogorov Arnold\nNetworks (KAN) which, to the best of our knowledge, have never been tested in\nthe art domain. The results show a general agreement between the different\nmodels' predictions on artworks flagged as forgeries, which are then closely\nstudied using visual analysis.\n","authors":["Sandro Boccuzzo","Deborah Desire Meyer","Ludovica Schaerf"],"pdf_url":"https://arxiv.org/pdf/2410.04866v1.pdf","comment":"Accepted to ECCV 2024 workshop AI4VA, oral presentation"},{"id":"http://arxiv.org/abs/2407.11514v2","updated":"2024-10-07T09:22:36Z","published":"2024-07-16T08:51:01Z","title":"ColorwAI: Generative Colorways of Textiles through GAN and Diffusion\n  Disentanglement","summary":"  Colorway creation is the task of generating textile samples in alternate\ncolor variations maintaining an underlying pattern. The individuation of a\nsuitable color palette for a colorway is a complex creative task, responding to\nclient and market needs, stylistic and cultural specifications, and mood. We\nintroduce a modification of this task, the \"generative colorway\" creation, that\nincludes minimal shape modifications, and propose a framework, \"ColorwAI\", to\ntackle this task using color disentanglement on StyleGAN and Diffusion. We\nintroduce a variation of the InterfaceGAN method for supervised\ndisentanglement, ShapleyVec. We use Shapley values to subselect a few\ndimensions of the detected latent direction. Moreover, we introduce a general\nframework to adopt common disentanglement methods on any architecture with a\nsemantic latent space and test it on Diffusion and GANs. We interpret the color\nrepresentations within the models' latent space. We find StyleGAN's W space to\nbe the most aligned with human notions of color. Finally, we suggest that\ndisentanglement can solicit a creative system for colorway creation, and\nevaluate it through expert questionnaires and creativity theory.\n","authors":["Ludovica Schaerf","Andrea Alfarano","Eric Postma"],"pdf_url":"https://arxiv.org/pdf/2407.11514v2.pdf","comment":"Accepted to ECCV 2024 VISART workshop, oral presentation"},{"id":"http://arxiv.org/abs/2410.04847v1","updated":"2024-10-07T09:08:32Z","published":"2024-10-07T09:08:32Z","title":"Causal Context Adjustment Loss for Learned Image Compression","summary":"  In recent years, learned image compression (LIC) technologies have surpassed\nconventional methods notably in terms of rate-distortion (RD) performance. Most\npresent learned techniques are VAE-based with an autoregressive entropy model,\nwhich obviously promotes the RD performance by utilizing the decoded causal\ncontext. However, extant methods are highly dependent on the fixed hand-crafted\ncausal context. The question of how to guide the auto-encoder to generate a\nmore effective causal context benefit for the autoregressive entropy models is\nworth exploring. In this paper, we make the first attempt in investigating the\nway to explicitly adjust the causal context with our proposed Causal Context\nAdjustment loss (CCA-loss). By imposing the CCA-loss, we enable the neural\nnetwork to spontaneously adjust important information into the early stage of\nthe autoregressive entropy model. Furthermore, as transformer technology\ndevelops remarkably, variants of which have been adopted by many\nstate-of-the-art (SOTA) LIC techniques. The existing computing devices have not\nadapted the calculation of the attention mechanism well, which leads to a\nburden on computation quantity and inference latency. To overcome it, we\nestablish a convolutional neural network (CNN) image compression model and\nadopt the unevenly channel-wise grouped strategy for high efficiency.\nUltimately, the proposed CNN-based LIC network trained with our Causal Context\nAdjustment loss attains a great trade-off between inference latency and\nrate-distortion performance.\n","authors":["Minghao Han","Shiyin Jiang","Shengxi Li","Xin Deng","Mai Xu","Ce Zhu","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2410.04847v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.04844v1","updated":"2024-10-07T09:04:50Z","published":"2024-10-07T09:04:50Z","title":"PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing","summary":"  In the field of image editing, three core challenges persist:\ncontrollability, background preservation, and efficiency. Inversion-based\nmethods rely on time-consuming optimization to preserve the features of the\ninitial images, which results in low efficiency due to the requirement for\nextensive network inference. Conversely, inversion-free methods lack\ntheoretical support for background similarity, as they circumvent the issue of\nmaintaining initial features to achieve efficiency. As a consequence, none of\nthese methods can achieve both high efficiency and background consistency. To\ntackle the challenges and the aforementioned disadvantages, we introduce\nPostEdit, a method that incorporates a posterior scheme to govern the diffusion\nsampling process. Specifically, a corresponding measurement term related to\nboth the initial features and Langevin dynamics is introduced to optimize the\nestimated image generated by the given target prompt. Extensive experimental\nresults indicate that the proposed PostEdit achieves state-of-the-art editing\nperformance while accurately preserving unedited regions. Furthermore, the\nmethod is both inversion- and training-free, necessitating approximately 1.5\nseconds and 18 GB of GPU memory to generate high-quality results.\n","authors":["Feng Tian","Yixuan Li","Yichao Yan","Shanyan Guan","Yanhao Ge","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04842v1","updated":"2024-10-07T08:59:05Z","published":"2024-10-07T08:59:05Z","title":"A Simple Image Segmentation Framework via In-Context Examples","summary":"  Recently, there have been explorations of generalist segmentation models that\ncan effectively tackle a variety of image segmentation tasks within a unified\nin-context learning framework. However, these methods still struggle with task\nambiguity in in-context segmentation, as not all in-context examples can\naccurately convey the task information. In order to address this issue, we\npresent SINE, a simple image Segmentation framework utilizing in-context\nexamples. Our approach leverages a Transformer encoder-decoder structure, where\nthe encoder provides high-quality image representations, and the decoder is\ndesigned to yield multiple task-specific output masks to effectively eliminate\ntask ambiguity. Specifically, we introduce an In-context Interaction module to\ncomplement in-context information and produce correlations between the target\nimage and the in-context example and a Matching Transformer that uses fixed\nmatching and a Hungarian algorithm to eliminate differences between different\ntasks. In addition, we have further perfected the current evaluation system for\nin-context image segmentation, aiming to facilitate a holistic appraisal of\nthese models. Experiments on various segmentation tasks show the effectiveness\nof the proposed method.\n","authors":["Yang Liu","Chenchen Jing","Hengtao Li","Muzhi Zhu","Hao Chen","Xinlong Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2410.04842v1.pdf","comment":"Accepted to Proc. Conference on Neural Information Processing Systems\n  (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/SINE"},{"id":"http://arxiv.org/abs/2402.14407v3","updated":"2024-10-07T08:45:35Z","published":"2024-02-22T09:48:47Z","title":"Learning an Actionable Discrete Diffusion Policy via Large-Scale\n  Actionless Video Pre-Training","summary":"  Learning a generalist embodied agent capable of completing multiple tasks\nposes challenges, primarily stemming from the scarcity of action-labeled\nrobotic datasets. In contrast, a vast amount of human videos exist, capturing\nintricate tasks and interactions with the physical world. Promising prospects\narise for utilizing actionless human videos for pre-training and transferring\nthe knowledge to facilitate robot policy learning through limited robot\ndemonstrations. However, it remains a challenge due to the domain gap between\nhumans and robots. Moreover, it is difficult to extract useful information\nrepresenting the dynamic world from human videos, because of its noisy and\nmultimodal data structure. In this paper, we introduce a novel framework to\ntackle these challenges, which leverages a unified discrete diffusion to\ncombine generative pre-training on human videos and policy fine-tuning on a\nsmall number of action-labeled robot videos. We start by compressing both human\nand robot videos into unified video tokens. In the pre-training stage, we\nemploy a discrete diffusion model with a mask-and-replace diffusion strategy to\npredict future video tokens in the latent space. In the fine-tuning stage, we\nharness the imagined future videos to guide low-level action learning with a\nlimited set of robot data. Experiments demonstrate that our method generates\nhigh-fidelity future videos for planning and enhances the fine-tuned policies\ncompared to previous state-of-the-art approaches with superior performance. Our\nproject website is available at https://video-diff.github.io/.\n","authors":["Haoran He","Chenjia Bai","Ling Pan","Weinan Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2402.14407v3.pdf","comment":"Accepted by NeurIPS 2024. 24 pages"},{"id":"http://arxiv.org/abs/2408.08313v2","updated":"2024-10-07T08:44:35Z","published":"2024-08-15T17:59:57Z","title":"Can Large Language Models Understand Symbolic Graphics Programs?","summary":"  Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n","authors":["Zeju Qiu","Weiyang Liu","Haiwen Feng","Zhen Liu","Tim Z. Xiao","Katherine M. Collins","Joshua B. Tenenbaum","Adrian Weller","Michael J. Black","Bernhard Schlkopf"],"pdf_url":"https://arxiv.org/pdf/2408.08313v2.pdf","comment":"Technical Report v2 (46 pages, 24 figures, project page:\n  https://sgp-bench.github.io/, substantial update from v1)"},{"id":"http://arxiv.org/abs/2410.04833v1","updated":"2024-10-07T08:40:29Z","published":"2024-10-07T08:40:29Z","title":"Multimodal Fusion Strategies for Mapping Biophysical Landscape Features","summary":"  Multimodal aerial data are used to monitor natural systems, and machine\nlearning can significantly accelerate the classification of landscape features\nwithin such imagery to benefit ecology and conservation. It remains\nunder-explored, however, how these multiple modalities ought to be fused in a\ndeep learning model. As a step towards filling this gap, we study three\nstrategies (Early fusion, Late fusion, and Mixture of Experts) for fusing\nthermal, RGB, and LiDAR imagery using a dataset of spatially-aligned\northomosaics in these three modalities. In particular, we aim to map three\necologically-relevant biophysical landscape features in African savanna\necosystems: rhino middens, termite mounds, and water. The three fusion\nstrategies differ in whether the modalities are fused early or late, and if\nlate, whether the model learns fixed weights per modality for each class or\ngenerates weights for each class adaptively, based on the input. Overall, the\nthree methods have similar macro-averaged performance with Late fusion\nachieving an AUC of 0.698, but their per-class performance varies strongly,\nwith Early fusion achieving the best recall for middens and water and Mixture\nof Experts achieving the best recall for mounds.\n","authors":["Lucia Gordon","Nico Lang","Catherine Ressijac","Andrew Davies"],"pdf_url":"https://arxiv.org/pdf/2410.04833v1.pdf","comment":"9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology"},{"id":"http://arxiv.org/abs/2405.07027v2","updated":"2024-10-07T08:28:43Z","published":"2024-05-11T14:57:42Z","title":"TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural\n  Radiance Field Optimization","summary":"  The reliance on accurate camera poses is a significant barrier to the\nwidespread deployment of Neural Radiance Fields (NeRF) models for 3D\nreconstruction and SLAM tasks. The existing method introduces monocular depth\npriors to jointly optimize the camera poses and NeRF, which fails to fully\nexploit the depth priors and neglects the impact of their inherent noise. In\nthis paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that\nenables training NeRF from unknown camera poses - by jointly optimizing\nlearnable parameters of the radiance field and camera poses. Our approach\nexplicitly utilizes monocular depth priors through three key advancements: 1)\nwe propose a novel depth-based ray sampling strategy based on the truncated\nnormal distribution, which improves the convergence speed and accuracy of pose\nestimation; 2) to circumvent local minima and refine depth geometry, we\nintroduce a coarse-to-fine training strategy that progressively improves the\ndepth precision; 3) we propose a more robust inter-frame point constraint that\nenhances robustness against depth noise during training. The experimental\nresults on three datasets demonstrate that TD-NeRF achieves superior\nperformance in the joint optimization of camera pose and NeRF, surpassing prior\nworks, and generates more accurate depth geometry. The implementation of our\nmethod has been released at https://github.com/nubot-nudt/TD-NeRF.\n","authors":["Zhen Tan","Zongtan Zhou","Yangbing Ge","Zi Wang","Xieyuanli Chen","Dewen Hu"],"pdf_url":"https://arxiv.org/pdf/2405.07027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03961v2","updated":"2024-10-07T08:23:50Z","published":"2024-06-06T11:13:44Z","title":"Exploring Distortion Prior with Latent Diffusion Models for Remote\n  Sensing Image Compression","summary":"  Deep learning-based image compression algorithms typically focus on designing\nencoding and decoding networks and improving the accuracy of entropy model\nestimation to enhance the rate-distortion (RD) performance. However, few\nalgorithms leverage the compression distortion prior from existing compression\nalgorithms to improve RD performance. In this paper, we propose a latent\ndiffusion model-based remote sensing image compression (LDM-RSIC) method, which\naims to enhance the final decoding quality of RS images by utilizing the\ngenerated distortion prior from a LDM. Our approach consists of two stages. In\nthe first stage, a self-encoder learns prior from the high-quality input image.\nIn the second stage, the prior is generated through an LDM, conditioned on the\ndecoded image of an existing learning-based image compression algorithm, to be\nused as auxiliary information for generating the texture-rich enhanced image.\nTo better utilize the prior, a channel attention and gate-based dynamic feature\nattention module (DFAM) is embedded into a Transformer-based multi-scale\nenhancement network (MEN) for image enhancement. Extensive experiments\ndemonstrate the proposed LDM-RSIC significantly outperforms existing\nstate-of-the-art traditional and learning-based image compression algorithms in\nterms of both subjective perception and objective metrics. Additionally, we use\nthe LDM-based scheme to improve the traditional image compression algorithm\nJPEG2000 and obtain 32.00% bit savings on the DOTA testing set. The code will\nbe available at https://github.com/mlkk518/LDM-RSIC.\n","authors":["Junhui Li","Jutao Li","Xingsong Hou","Huake Wang"],"pdf_url":"https://arxiv.org/pdf/2406.03961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07389v2","updated":"2024-10-07T08:20:03Z","published":"2024-07-10T06:28:25Z","title":"Greit-HRNet: Grouped Lightweight High-Resolution Network for Human Pose\n  Estimation","summary":"  As multi-scale features are necessary for human pose estimation tasks,\nhigh-resolution networks are widely applied.\n  To improve efficiency, lightweight modules are proposed to replace costly\npoint-wise convolutions in high-resolution networks, including channel\nweighting and spatial weighting methods.\n  However, they fail to maintain the consistency of weights and capture global\nspatial information.\n  To address these problems, we present a Grouped lightweight High-Resolution\nNetwork (Greit-HRNet), in which we propose a Greit block including a group\nmethod Grouped Channel Weighting (GCW) and a spatial weighting method Global\nSpatial Weighting (GSW).\n  GCW modules group conditional channel weighting to make weights stable and\nmaintain the high-resolution features with the deepening of the network, while\nGSW modules effectively extract global spatial information and exchange\ninformation across channels.\n  In addition, we apply the Large Kernel Attention (LKA) method to improve the\nwhole efficiency of our Greit-HRNet.\n  Our experiments on both MS-COCO and MPII human pose estimation datasets\ndemonstrate the superior performance of our Greit-HRNet, outperforming other\nstate-of-the-art lightweight networks.\n","authors":["Junjia Han"],"pdf_url":"https://arxiv.org/pdf/2407.07389v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.04823v1","updated":"2024-10-07T08:14:17Z","published":"2024-10-07T08:14:17Z","title":"CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models","summary":"  Despite the transformative impact of deep learning across multiple domains,\nthe inherent opacity of these models has driven the development of Explainable\nArtificial Intelligence (XAI). Among these efforts, Concept Bottleneck Models\n(CBMs) have emerged as a key approach to improve interpretability by leveraging\nhigh-level semantic information. However, CBMs, like other machine learning\nmodels, are susceptible to security threats, particularly backdoor attacks,\nwhich can covertly manipulate model behaviors. Understanding that the community\nhas not yet studied the concept level backdoor attack of CBM, because of\n\"Better the devil you know than the devil you don't know.\", we introduce CAT\n(Concept-level Backdoor ATtacks), a methodology that leverages the conceptual\nrepresentations within CBMs to embed triggers during training, enabling\ncontrolled manipulation of model predictions at inference time. An enhanced\nattack pattern, CAT+, incorporates a correlation function to systematically\nselect the most effective and stealthy concept triggers, thereby optimizing the\nattack's impact. Our comprehensive evaluation framework assesses both the\nattack success rate and stealthiness, demonstrating that CAT and CAT+ maintain\nhigh performance on clean data while achieving significant targeted effects on\nbackdoored datasets. This work underscores the potential security risks\nassociated with CBMs and provides a robust testing methodology for future\nsecurity assessments.\n","authors":["Songning Lai","Jiayu Yang","Yu Huang","Lijie Hu","Tianlang Xue","Zhangyi Hu","Jiaxu Li","Haicheng Liao","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2410.04823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04817v1","updated":"2024-10-07T08:06:41Z","published":"2024-10-07T08:06:41Z","title":"Resource-Efficient Multiview Perception: Integrating Semantic Masking\n  with Masked Autoencoders","summary":"  Multiview systems have become a key technology in modern computer vision,\noffering advanced capabilities in scene understanding and analysis. However,\nthese systems face critical challenges in bandwidth limitations and\ncomputational constraints, particularly for resource-limited camera nodes like\ndrones. This paper presents a novel approach for communication-efficient\ndistributed multiview detection and tracking using masked autoencoders (MAEs).\nWe introduce a semantic-guided masking strategy that leverages pre-trained\nsegmentation models and a tunable power function to prioritize informative\nimage regions. This approach, combined with an MAE, reduces communication\noverhead while preserving essential visual information. We evaluate our method\non both virtual and real-world multiview datasets, demonstrating comparable\nperformance in terms of detection and tracking performance metrics compared to\nstate-of-the-art techniques, even at high masking ratios. Our selective masking\nalgorithm outperforms random masking, maintaining higher accuracy and precision\nas the masking ratio increases. Furthermore, our approach achieves a\nsignificant reduction in transmission data volume compared to baseline methods,\nthereby balancing multiview tracking performance with communication efficiency.\n","authors":["Kosta Dakic","Kanchana Thilakarathna","Rodrigo N. Calheiros","Teng Joon Lim"],"pdf_url":"https://arxiv.org/pdf/2410.04817v1.pdf","comment":"10 pages, conference"},{"id":"http://arxiv.org/abs/2404.12372v2","updated":"2024-10-07T08:06:13Z","published":"2024-04-18T17:53:19Z","title":"MedThink: Explaining Medical Visual Question Answering via Multimodal\n  Decision-Making Rationale","summary":"  Medical Visual Question Answering (MedVQA), which offers language responses\nto image-based medical inquiries, represents a challenging task and significant\nadvancement in healthcare. It assists medical experts to swiftly interpret\nmedical images, thereby enabling faster and more accurate diagnoses. However,\nthe model interpretability and transparency of existing MedVQA solutions are\noften limited, posing challenges in understanding their decision-making\nprocesses. To address this issue, we devise a semi-automated annotation process\nto streamline data preparation and build new benchmark MedVQA datasets R-RAD,\nR-SLAKE and R-Path. These datasets provide intermediate medical decision-making\nrationales generated by multimodal large language models and human annotations\nfor question-answering pairs in existing MedVQA datasets, i.e., VQA-RAD, SLAKE\nand PathVQA. Moreover, we design a novel framework, MedThink, which finetunes\nlightweight pretrained generative models by incorporating medical\ndecision-making rationales. MedThink includes three distinct strategies to\ngenerate decision outcomes and corresponding rationales, thereby clearly\nshowcasing the medical decision-making process during reasoning. Our\ncomprehensive experiments show that our method achieves an accuracy of 83.5% on\nR-RAD, 86.3% on R-SLAKE and 87.2% on R-Path. These results significantly exceed\nthose of existing state-of-the-art models with comparable parameters. Datasets\nand code will be released.\n","authors":["Xiaotang Gai","Chenyi Zhou","Jiaxiang Liu","Yang Feng","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.12372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19215v2","updated":"2024-10-07T07:47:45Z","published":"2024-09-28T02:51:59Z","title":"1st Place Solution to the 8th HANDS Workshop Challenge -- ARCTIC Track:\n  3DGS-based Bimanual Category-agnostic Interaction Reconstruction","summary":"  This report describes our 1st place solution to the 8th HANDS workshop\nchallenge (ARCTIC track) in conjunction with ECCV 2024. In this challenge, we\naddress the task of bimanual category-agnostic hand-object interaction\nreconstruction, which aims to generate 3D reconstructions of both hands and the\nobject from a monocular video, without relying on predefined templates. This\ntask is particularly challenging due to the significant occlusion and dynamic\ncontact between the hands and the object during bimanual manipulation. We\nworked to resolve these issues by introducing a mask loss and a 3D contact\nloss, respectively. Moreover, we applied 3D Gaussian Splatting (3DGS) to this\ntask. As a result, our method achieved a value of 38.69 in the main metric,\nCD$_h$, on the ARCTIC test set.\n","authors":["Jeongwan On","Kyeonghwan Gwak","Gunyoung Kang","Hyein Hwang","Soohyun Hwang","Junuk Cha","Jaewook Han","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2409.19215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04811v1","updated":"2024-10-07T07:46:08Z","published":"2024-10-07T07:46:08Z","title":"Learning Efficient and Effective Trajectories for Differential\n  Equation-based Image Restoration","summary":"  The differential equation-based image restoration approach aims to establish\nlearnable trajectories connecting high-quality images to a tractable\ndistribution, e.g., low-quality images or a Gaussian distribution. In this\npaper, we reformulate the trajectory optimization of this kind of method,\nfocusing on enhancing both reconstruction quality and efficiency. Initially, we\nnavigate effective restoration paths through a reinforcement learning process,\ngradually steering potential trajectories toward the most precise options.\nAdditionally, to mitigate the considerable computational burden associated with\niterative sampling, we propose cost-aware trajectory distillation to streamline\ncomplex paths into several manageable steps with adaptable sizes. Moreover, we\nfine-tune a foundational diffusion model (FLUX) with 12B parameters by using\nour algorithms, producing a unified framework for handling 7 kinds of image\nrestoration tasks. Extensive experiments showcase the significant superiority\nof the proposed method, achieving a maximum PSNR improvement of 2.1 dB over\nstate-of-the-art methods, while also greatly enhancing visual perceptual\nquality. Project page: \\url{https://zhu-zhiyu.github.io/FLUX-IR/}.\n","authors":["Zhiyu Zhu","Jinhui Hou","Hui Liu","Huanqiang Zeng","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2410.04811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01449v3","updated":"2024-10-07T07:46:00Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Cline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.04810v1","updated":"2024-10-07T07:45:18Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Gengyuan Zhang","Jinhe Bi","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09365v2","updated":"2024-10-07T07:39:40Z","published":"2024-05-15T14:17:44Z","title":"SARatrX: Towards Building A Foundation Model for SAR Target Recognition","summary":"  Despite the remarkable progress in synthetic aperture radar automatic target\nrecognition (SAR ATR), recent efforts have concentrated on the detection or\nclassification of a specific and coarse category, e.g., vehicles, ships,\nairplanes, or buildings. One of the fundamental limitations of the\ntop-performing SAR ATR methods is that the learning paradigm is supervised,\ntask-specific, limited-category, closed-world learning, which depends on\nmassive amounts of accurately annotated samples that are expensively labeled by\nexpert SAR analysts and has limited generalization capability and scalability.\nIn this work, we make the first attempt towards building a foundation model for\nSAR ATR, termed SARatrX. SARatrX learns generalizable representations via\nself-supervised learning (SSL) and provides a basis for label-efficient model\nadaptation to generic SAR target detection and classification tasks.\nSpecifically, SARatrX is trained on 0.18 M unlabelled SAR target samples, which\nare curated by combining contemporary benchmarks and constitute the largest\npublicly available dataset till now. Considering the characteristics of SAR\nimages, a backbone tailored for SAR ATR is carefully designed, and a two-step\nSSL method endowed with multi-scale gradient features was applied to ensure the\nfeature diversity and model scalability of SARatrX. The capabilities of SARatrX\nare evaluated on classification under few-shot and robustness settings and\ndetection across various categories and scenes, and impressive performance is\nachieved, often competitive with or even superior to prior fully supervised,\nsemi-supervised, or self-supervised algorithms. Our SARatrX and the curated\ndataset are released at https://github.com/waterdisappear/SARatrX to foster\nresearch into foundation models for SAR ATR and SAR image interpretation.\n","authors":["Weijie Li","Wei Yang","Yuenan Hou","Li Liu","Yongxiang Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2405.09365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04802v1","updated":"2024-10-07T07:26:38Z","published":"2024-10-07T07:26:38Z","title":"Building Damage Assessment in Conflict Zones: A Deep Learning Approach\n  Using Geospatial Sub-Meter Resolution Data","summary":"  Very High Resolution (VHR) geospatial image analysis is crucial for\nhumanitarian assistance in both natural and anthropogenic crises, as it allows\nto rapidly identify the most critical areas that need support. Nonetheless,\nmanually inspecting large areas is time-consuming and requires domain\nexpertise. Thanks to their accuracy, generalization capabilities, and highly\nparallelizable workload, Deep Neural Networks (DNNs) provide an excellent way\nto automate this task. Nevertheless, there is a scarcity of VHR data pertaining\nto conflict situations, and consequently, of studies on the effectiveness of\nDNNs in those scenarios. Motivated by this, our work extensively studies the\napplicability of a collection of state-of-the-art Convolutional Neural Networks\n(CNNs) originally developed for natural disasters damage assessment in a war\nscenario. To this end, we build an annotated dataset with pre- and\npost-conflict images of the Ukrainian city of Mariupol. We then explore the\ntransferability of the CNN models in both zero-shot and learning scenarios,\ndemonstrating their potential and limitations. To the best of our knowledge,\nthis is the first study to use sub-meter resolution imagery to assess building\ndamage in combat zones.\n","authors":["Matteo Risso","Alessia Goffi","Beatrice Alessandra Motetti","Alessio Burrello","Jean Baptiste Bove","Enrico Macii","Massimo Poncino","Daniele Jahier Pagliari","Giuseppe Maffeis"],"pdf_url":"https://arxiv.org/pdf/2410.04802v1.pdf","comment":"This paper has been accepted for publication in the Sixth IEEE\n  International Conference on Image Processing Applications and Systems 2024\n  copyright IEEE"},{"id":"http://arxiv.org/abs/2410.04801v1","updated":"2024-10-07T07:26:10Z","published":"2024-10-07T07:26:10Z","title":"Improving Image Clustering with Artifacts Attenuation via Inference-Time\n  Attention Engineering","summary":"  The goal of this paper is to improve the performance of pretrained Vision\nTransformer (ViT) models, particularly DINOv2, in image clustering task without\nrequiring re-training or fine-tuning. As model size increases, high-norm\nartifacts anomaly appears in the patches of multi-head attention. We observe\nthat this anomaly leads to reduced accuracy in zero-shot image clustering.\nThese artifacts are characterized by disproportionately large values in the\nattention map compared to other patch tokens. To address these artifacts, we\npropose an approach called Inference-Time Attention Engineering (ITAE), which\nmanipulates attention function during inference. Specifically, we identify the\nartifacts by investigating one of the Query-Key-Value (QKV) patches in the\nmulti-head attention and attenuate their corresponding attention values inside\nthe pretrained models. ITAE shows improved clustering accuracy on multiple\ndatasets by exhibiting more expressive features in latent space. Our findings\nhighlight the potential of ITAE as a practical solution for reducing artifacts\nin pretrained ViT models and improving model performance in clustering tasks\nwithout the need for re-training or fine-tuning.\n","authors":["Kazumoto Nakamura","Yuji Nozawa","Yu-Chieh Lin","Kengo Nakata","Youyang Ng"],"pdf_url":"https://arxiv.org/pdf/2410.04801v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.04799v1","updated":"2024-10-07T07:23:42Z","published":"2024-10-07T07:23:42Z","title":"Transforming Color: A Novel Image Colorization Method","summary":"  This paper introduces a novel method for image colorization that utilizes a\ncolor transformer and generative adversarial networks (GANs) to address the\nchallenge of generating visually appealing colorized images. Conventional\napproaches often struggle with capturing long-range dependencies and producing\nrealistic colorizations. The proposed method integrates a transformer\narchitecture to capture global information and a GAN framework to improve\nvisual quality. In this study, a color encoder that utilizes a random normal\ndistribution to generate color features is applied. These features are then\nintegrated with grayscale image features to enhance the overall representation\nof the images. Our method demonstrates superior performance compared with\nexisting approaches by utilizing the capacity of the transformer, which can\ncapture long-range dependencies and generate a realistic colorization of the\nGAN. Experimental results show that the proposed network significantly\noutperforms other state-of-the-art colorization techniques, highlighting its\npotential for image colorization. This research opens new possibilities for\nprecise and visually compelling image colorization in domains such as digital\nrestoration and historical image analysis.\n","authors":["Hamza Shafiq","Bumshik Lee"],"pdf_url":"https://arxiv.org/pdf/2410.04799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10988v2","updated":"2024-10-07T07:15:19Z","published":"2023-11-18T06:49:17Z","title":"Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph\n  Generation via Visual-Concept Alignment and Retention","summary":"  Scene Graph Generation (SGG) offers a structured representation critical in\nmany computer vision applications. Traditional SGG approaches, however, are\nlimited by a closed-set assumption, restricting their ability to recognize only\npredefined object and relation categories. To overcome this, we categorize SGG\nscenarios into four distinct settings based on the node and edge: Closed-set\nSGG, Open Vocabulary (object) Detection-based SGG (OvD-SGG), Open Vocabulary\nRelation-based SGG (OvR-SGG), and Open Vocabulary Detection + Relationbased SGG\n(OvD+R-SGG). While object-centric open vocabulary SGG has been studied\nrecently, the more challenging problem of relation-involved open-vocabulary SGG\nremains relatively unexplored. To fill this gap, we propose a unified framework\nnamed OvSGTR towards fully open vocabulary SGG from a holistic view. The\nproposed framework is an end-to-end transformer architecture, which learns a\nvisual-concept alignment for both nodes and edges, enabling the model to\nrecognize unseen categories. For the more challenging settings of\nrelation-involved open vocabulary SGG, the proposed approach integrates\nrelation-aware pretraining utilizing image-caption data and retains\nvisual-concept alignment through knowledge distillation. Comprehensive\nexperimental results on the Visual Genome benchmark demonstrate the\neffectiveness and superiority of the proposed framework. Our code is available\nat https://github.com/gpt4vision/OvSGTR/.\n","authors":["Zuyao Chen","Jinlin Wu","Zhen Lei","Zhaoxiang Zhang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2311.10988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00036v2","updated":"2024-10-07T07:14:23Z","published":"2023-12-29T18:35:04Z","title":"Discrete Distribution Networks","summary":"  We introduce a novel generative model, the Discrete Distribution Networks\n(DDN), that approximates data distribution using hierarchical discrete\ndistributions. We posit that since the features within a network inherently\ncapture distributional information, enabling the network to generate multiple\nsamples simultaneously, rather than a single output, may offer an effective way\nto represent distributions. Therefore, DDN fits the target distribution,\nincluding continuous ones, by generating multiple discrete sample points. To\ncapture finer details of the target data, DDN selects the output that is\nclosest to the Ground Truth (GT) from the coarse results generated in the first\nlayer. This selected output is then fed back into the network as a condition\nfor the second layer, thereby generating new outputs more similar to the GT. As\nthe number of DDN layers increases, the representational space of the outputs\nexpands exponentially, and the generated samples become increasingly similar to\nthe GT. This hierarchical output pattern of discrete distributions endows DDN\nwith unique property: more general zero-shot conditional generation. We\ndemonstrate the efficacy of DDN and its intriguing properties through\nexperiments on CIFAR-10 and FFHQ. The code is available at\nhttps://discrete-distribution-networks.github.io/\n","authors":["Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00036v2.pdf","comment":"TL;DR: A Novel Generative Model with Simple Principles and Unique\n  Properties"},{"id":"http://arxiv.org/abs/2408.02901v3","updated":"2024-10-07T07:13:24Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v3.pdf","comment":"accepted at EMNLP2024 - system demonstration track"},{"id":"http://arxiv.org/abs/2410.04789v1","updated":"2024-10-07T06:57:23Z","published":"2024-10-07T06:57:23Z","title":"Analysis of Hybrid Compositions in Animation Film with Weakly Supervised\n  Learning","summary":"  We present an approach for the analysis of hybrid visual compositions in\nanimation in the domain of ephemeral film. We combine ideas from\nsemi-supervised and weakly supervised learning to train a model that can\nsegment hybrid compositions without requiring pre-labeled segmentation masks.\nWe evaluate our approach on a set of ephemeral films from 13 film archives.\nResults demonstrate that the proposed learning strategy yields a performance\nclose to a fully supervised baseline. On a qualitative level the performed\nanalysis provides interesting insights on hybrid compositions in animation\nfilm.\n","authors":["Mnica Apellaniz Portos","Roberto Labadie-Tamayo","Claudius Stemmler","Erwin Feyersinger","Andreas Babic","Franziska Bruckner","Vrth hner","Matthias Zeppelzauer"],"pdf_url":"https://arxiv.org/pdf/2410.04789v1.pdf","comment":"Vision for Art (VISART VII) Workshop at the European Conference of\n  Computer Vision (ECCV)"},{"id":"http://arxiv.org/abs/2410.04780v1","updated":"2024-10-07T06:45:22Z","published":"2024-10-07T06:45:22Z","title":"Mitigating Modality Prior-Induced Hallucinations in Multimodal Large\n  Language Models via Deciphering Attention Causality","summary":"  Multimodal Large Language Models (MLLMs) have emerged as a central focus in\nboth industry and academia, but often suffer from biases introduced by visual\nand language priors, which can lead to multimodal hallucination. These biases\narise from the visual encoder and the Large Language Model (LLM) backbone,\naffecting the attention mechanism responsible for aligning multimodal inputs.\nExisting decoding-based mitigation methods focus on statistical correlations\nand overlook the causal relationships between attention mechanisms and model\noutput, limiting their effectiveness in addressing these biases. To tackle this\nissue, we propose a causal inference framework termed CausalMM that applies\nstructural causal modeling to MLLMs, treating modality priors as a confounder\nbetween attention mechanisms and output. Specifically, by employing backdoor\nadjustment and counterfactual reasoning at both the visual and language\nattention levels, our method mitigates the negative effects of modality priors\nand enhances the alignment of MLLM's inputs and outputs, with a maximum score\nimprovement of 65.3% on 6 VLind-Bench indicators and 164 points on MME\nBenchmark compared to conventional methods. Extensive experiments validate the\neffectiveness of our approach while being a plug-and-play solution. Our code is\navailable at: https://github.com/The-Martyr/CausalMM\n","authors":["Guanyu Zhou","Yibo Yan","Xin Zou","Kun Wang","Aiwei Liu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04778v1","updated":"2024-10-07T06:36:55Z","published":"2024-10-07T06:36:55Z","title":"MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models\n  (MLLMs)","summary":"  With the advent of Large Language Models (LLMs) and Multimodal\n(Visio-lingual) LLMs, a flurry of research has emerged, analyzing the\nperformance of such models across a diverse array of tasks. While most studies\nfocus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models\nthrough task accuracy (e.g., Visual Question Answering, grounding) across\nvarious datasets, our work explores the related but complementary aspect of\nconsistency - the ability of an MLLM model to produce semantically similar or\nidentical responses to semantically similar queries. We note that consistency\nis a fundamental prerequisite (necessary but not sufficient condition) for\nrobustness and trust in MLLMs. Humans, in particular, are known to be highly\nconsistent (even if not always accurate) in their responses, and consistency is\ninherently expected from AI systems. Armed with this perspective, we propose\nthe MM-R$^3$ benchmark, which analyses the performance in terms of consistency\nand accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image\nRestyling, and Context Reasoning. Our analysis reveals that consistency does\nnot always align with accuracy, indicating that models with higher accuracy are\nnot necessarily more consistent, and vice versa. Furthermore, we propose a\nsimple yet effective mitigation strategy in the form of an adapter module\ntrained to minimize inconsistency across prompts. With our proposed strategy,\nwe are able to achieve absolute improvements of 5.7% and 12.5%, on average on\nwidely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over\ntheir existing counterparts.\n","authors":["Shih-Han Chou","Shivam Chandhok","James J. Little","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2410.04778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17996v2","updated":"2024-10-07T06:23:51Z","published":"2024-09-26T16:07:24Z","title":"PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless\n  Imaging","summary":"  Lensless cameras offer significant advantages in size, weight, and cost\ncompared to traditional lens-based systems. Without a focusing lens, lensless\ncameras rely on computational algorithms to recover the scenes from multiplexed\nmeasurements. However, current algorithms struggle with inaccurate forward\nimaging models and insufficient priors to reconstruct high-quality images. To\novercome these limitations, we introduce a novel two-stage approach for\nconsistent and photorealistic lensless image reconstruction. The first stage of\nour approach ensures data consistency by focusing on accurately reconstructing\nthe low-frequency content with a spatially varying deconvolution method that\nadjusts to changes in the Point Spread Function (PSF) across the camera's field\nof view. The second stage enhances photorealism by incorporating a generative\nprior from pre-trained diffusion models. By conditioning on the low-frequency\ncontent retrieved in the first stage, the diffusion model effectively\nreconstructs the high-frequency details that are typically lost in the lensless\nimaging process, while also maintaining image fidelity. Our method achieves a\nsuperior balance between data fidelity and visual quality compared to existing\nmethods, as demonstrated with two popular lensless systems, PhlatCam and\nDiffuserCam. Project website: https://phocolens.github.io/.\n","authors":["Xin Cai","Zhiyuan You","Hailong Zhang","Wentao Liu","Jinwei Gu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2409.17996v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2404.13026v2","updated":"2024-10-07T06:08:09Z","published":"2024-04-19T17:41:05Z","title":"PhysDreamer: Physics-Based Interaction with 3D Objects via Video\n  Generation","summary":"  Realistic object interactions are crucial for creating immersive virtual\nexperiences, yet synthesizing realistic 3D object dynamics in response to novel\ninteractions remains a significant challenge. Unlike unconditional or\ntext-conditioned dynamics generation, action-conditioned dynamics requires\nperceiving the physical material properties of objects and grounding the 3D\nmotion prediction on these properties, such as object stiffness. However,\nestimating physical material properties is an open problem due to the lack of\nmaterial ground-truth data, as measuring these properties for real objects is\nhighly difficult. We present PhysDreamer, a physics-based approach that endows\nstatic 3D objects with interactive dynamics by leveraging the object dynamics\npriors learned by video generation models. By distilling these priors,\nPhysDreamer enables the synthesis of realistic object responses to novel\ninteractions, such as external forces or agent manipulations. We demonstrate\nour approach on diverse examples of elastic objects and evaluate the realism of\nthe synthesized interactions through a user study. PhysDreamer takes a step\ntowards more engaging and realistic virtual experiences by enabling static 3D\nobjects to dynamically respond to interactive stimuli in a physically plausible\nmanner. See our project page at https://physdreamer.github.io/.\n","authors":["Tianyuan Zhang","Hong-Xing Yu","Rundi Wu","Brandon Y. Feng","Changxi Zheng","Noah Snavely","Jiajun Wu","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2404.13026v2.pdf","comment":"Project website at: https://physdreamer.github.io/ Appear on ECCV\n  2024"},{"id":"http://arxiv.org/abs/2410.04762v1","updated":"2024-10-07T05:36:11Z","published":"2024-10-07T05:36:11Z","title":"WTCL-Dehaze: Rethinking Real-world Image Dehazing via Wavelet Transform\n  and Contrastive Learning","summary":"  Images captured in hazy outdoor conditions often suffer from colour\ndistortion, low contrast, and loss of detail, which impair high-level vision\ntasks. Single image dehazing is essential for applications such as autonomous\ndriving and surveillance, with the aim of restoring image clarity. In this\nwork, we propose WTCL-Dehaze an enhanced semi-supervised dehazing network that\nintegrates Contrastive Loss and Discrete Wavelet Transform (DWT). We\nincorporate contrastive regularization to enhance feature representation by\ncontrasting hazy and clear image pairs. Additionally, we utilize DWT for\nmulti-scale feature extraction, effectively capturing high-frequency details\nand global structures. Our approach leverages both labelled and unlabelled data\nto mitigate the domain gap and improve generalization. The model is trained on\na combination of synthetic and real-world datasets, ensuring robust performance\nacross different scenarios. Extensive experiments demonstrate that our proposed\nalgorithm achieves superior performance and improved robustness compared to\nstate-of-the-art single image dehazing methods on both benchmark datasets and\nreal-world images.\n","authors":["Divine Joseph Appiah","Donghai Guan","Abdul Nasser Kasule","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2410.04762v1.pdf","comment":"15 pages,4 figures"},{"id":"http://arxiv.org/abs/2410.04751v1","updated":"2024-10-07T05:07:01Z","published":"2024-10-07T05:07:01Z","title":"Intriguing Properties of Large Language and Vision Models","summary":"  Recently, large language and vision models (LLVMs) have received significant\nattention and development efforts due to their remarkable generalization\nperformance across a wide range of tasks requiring perception and cognitive\nabilities. A key factor behind their success is their simple architecture,\nwhich consists of a vision encoder, a projector, and a large language model\n(LLM). Despite their achievements in advanced reasoning tasks, their\nperformance on fundamental perception-related tasks (e.g., MMVP) remains\nsurprisingly low. This discrepancy raises the question of how LLVMs truly\nperceive images and exploit the advantages of the vision encoder. To address\nthis, we systematically investigate this question regarding several aspects:\npermutation invariance, robustness, math reasoning, alignment preserving and\nimportance, by evaluating the most common LLVM's families (i.e., LLaVA) across\n10 evaluation benchmarks. Our extensive experiments reveal several intriguing\nproperties of current LLVMs: (1) they internally process the image in a global\nmanner, even when the order of visual patch sequences is randomly permuted; (2)\nthey are sometimes able to solve math problems without fully perceiving\ndetailed numerical information; (3) the cross-modal alignment is overfitted to\ncomplex reasoning tasks, thereby, causing them to lose some of the original\nperceptual capabilities of their vision encoder; (4) the representation space\nin the lower layers (<25%) plays a crucial role in determining performance and\nenhancing visual understanding. Lastly, based on the above observations, we\nsuggest potential future directions for building better LLVMs and constructing\nmore challenging evaluation benchmarks.\n","authors":["Young-Jun Lee","Byungsoo Ko","Han-Gyu Kim","Yechan Hwang","Ho-Jin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.04751v1.pdf","comment":"Code is available in https://github.com/passing2961/IP-LLVM"},{"id":"http://arxiv.org/abs/2410.04749v1","updated":"2024-10-07T04:59:08Z","published":"2024-10-07T04:59:08Z","title":"LLaVA Needs More Knowledge: Retrieval Augmented Natural Language\n  Generation with Knowledge Graph for Explaining Thoracic Pathologies","summary":"  Generating Natural Language Explanations (NLEs) for model predictions on\nmedical images, particularly those depicting thoracic pathologies, remains a\ncritical and challenging task. Existing methodologies often struggle due to\ngeneral models' insufficient domain-specific medical knowledge and privacy\nconcerns associated with retrieval-based augmentation techniques. To address\nthese issues, we propose a novel Vision-Language framework augmented with a\nKnowledge Graph (KG)-based datastore, which enhances the model's understanding\nby incorporating additional domain-specific medical knowledge essential for\ngenerating accurate and informative NLEs. Our framework employs a KG-based\nretrieval mechanism that not only improves the precision of the generated\nexplanations but also preserves data privacy by avoiding direct data retrieval.\nThe KG datastore is designed as a plug-and-play module, allowing for seamless\nintegration with various model architectures. We introduce and evaluate three\ndistinct frameworks within this paradigm: KG-LLaVA, which integrates the\npre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining\nMedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts\nLLaVA by incorporating the Bio-ViT-L vision model. These frameworks are\nvalidated on the MIMIC-NLE dataset, where they achieve state-of-the-art\nresults, underscoring the effectiveness of KG augmentation in generating\nhigh-quality NLEs for thoracic pathologies.\n","authors":["Ameer Hamza"," Abdullah","Yong Hyun Ahn","Sungyoung Lee","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07972v2","updated":"2024-10-07T04:17:01Z","published":"2024-09-12T12:12:19Z","title":"Deep Height Decoupling for Precise Vision-based 3D Occupancy Prediction","summary":"  The task of vision-based 3D occupancy prediction aims to reconstruct 3D\ngeometry and estimate its semantic classes from 2D color images, where the\n2D-to-3D view transformation is an indispensable step. Most previous methods\nconduct forward projection, such as BEVPooling and VoxelPooling, both of which\nmap the 2D image features into 3D grids. However, the current grid representing\nfeatures within a certain height range usually introduces many confusing\nfeatures that belong to other height ranges. To address this challenge, we\npresent Deep Height Decoupling (DHD), a novel framework that incorporates\nexplicit height prior to filter out the confusing features. Specifically, DHD\nfirst predicts height maps via explicit supervision. Based on the height\ndistribution statistics, DHD designs Mask Guided Height Sampling (MGHS) to\nadaptively decouple the height map into multiple binary masks. MGHS projects\nthe 2D image features into multiple subspaces, where each grid contains\nfeatures within reasonable height ranges. Finally, a Synergistic Feature\nAggregation (SFA) module is deployed to enhance the feature representation\nthrough channel and spatial affinities, enabling further occupancy refinement.\nOn the popular Occ3D-nuScenes benchmark, our method achieves state-of-the-art\nperformance even with minimal input frames. Code is available at\nhttps://github.com/yanzq95/DHD.\n","authors":["Yuan Wu","Zhiqiang Yan","Zhengxue Wang","Xiang Li","Le Hui","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2409.07972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04738v1","updated":"2024-10-07T04:12:23Z","published":"2024-10-07T04:12:23Z","title":"Diffusion Models in 3D Vision: A Survey","summary":"  In recent years, 3D vision has become a crucial field within computer vision,\npowering a wide range of applications such as autonomous driving, robotics,\naugmented reality (AR), and medical imaging. This field relies on the accurate\nperception, understanding, and reconstruction of 3D scenes from 2D data sources\nlike images and videos. Diffusion models, originally designed for 2D generative\ntasks, offer the potential for more flexible, probabilistic approaches that can\nbetter capture the variability and uncertainty present in real-world 3D data.\nHowever, traditional methods often struggle with efficiency and scalability. In\nthis paper, we review the state-of-the-art approaches that leverage diffusion\nmodels for 3D visual tasks, including but not limited to 3D object generation,\nshape completion, point cloud reconstruction, and scene understanding. We\nprovide an in-depth discussion of the underlying mathematical principles of\ndiffusion models, outlining their forward and reverse processes, as well as the\nvarious architectural advancements that enable these models to work with 3D\ndatasets. We also discuss the key challenges in applying diffusion models to 3D\nvision, such as handling occlusions and varying point densities, and the\ncomputational demands of high-dimensional data. Finally, we discuss potential\nsolutions, including improving computational efficiency, enhancing multimodal\nfusion, and exploring the use of large-scale pretraining for better\ngeneralization across 3D tasks. This paper serves as a foundation for future\nexploration and development in this rapidly evolving field.\n","authors":["Zhen Wang","Dongyuan Li","Renhe Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.04738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04734v1","updated":"2024-10-07T04:00:22Z","published":"2024-10-07T04:00:22Z","title":"TLDR: Token-Level Detective Reward Model for Large Vision Language\n  Models","summary":"  Although reward models have been successful in improving multimodal large\nlanguage models, the reward models themselves remain brutal and contain minimal\ninformation. Notably, existing reward models only mimic human annotations by\nassigning only one binary feedback to any text, no matter how long the text is.\nIn the realm of multimodal language models, where models are required to\nprocess both images and texts, a naive reward model may learn implicit biases\ntoward texts and become less grounded in images. In this paper, we propose a\n$\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\n($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We\nfirst introduce a perturbation-based method to generate synthetic hard\nnegatives and their token-level labels to train TLDR models. Then we show the\nrich usefulness of TLDR models both in assisting off-the-shelf models to\nself-correct their generations, and in serving as a hallucination evaluation\ntool. Finally, we show that TLDR models can significantly speed up human\nannotation by 3 times to acquire a broader range of high-quality vision\nlanguage data.\n","authors":["Deqing Fu","Tong Xiao","Rui Wang","Wang Zhu","Pengchuan Zhang","Guan Pang","Robin Jia","Lawrence Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04734v1.pdf","comment":"Work done at Meta"},{"id":"http://arxiv.org/abs/2312.06038v2","updated":"2024-10-07T03:59:27Z","published":"2023-12-10T23:35:13Z","title":"Correcting Diffusion Generation through Resampling","summary":"  Despite diffusion models' superior capabilities in modeling complex\ndistributions, there are still non-trivial distributional discrepancies between\ngenerated and ground-truth images, which has resulted in several notable\nproblems in image generation, including missing object errors in text-to-image\ngeneration and low image quality. Existing methods that attempt to address\nthese problems mostly do not tend to address the fundamental cause behind these\nproblems, which is the distributional discrepancies, and hence achieve\nsub-optimal results. In this paper, we propose a particle filtering framework\nthat can effectively address both problems by explicitly reducing the\ndistributional discrepancies. Specifically, our method relies on a set of\nexternal guidance, including a small set of real images and a pre-trained\nobject detector, to gauge the distribution gap, and then design the resampling\nweight accordingly to correct the gap. Experiments show that our methods can\neffectively correct missing object errors and improve image quality in various\nimage generation tasks. Notably, our method outperforms the existing strongest\nbaseline by 5% in object occurrence and 1.0 in FID on MS-COCO. Our code is\npublicly available at\nhttps://github.com/UCSB-NLP-Chang/diffusion_resampling.git.\n","authors":["Yujian Liu","Yang Zhang","Tommi Jaakkola","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2312.06038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03141v2","updated":"2024-10-07T03:53:15Z","published":"2024-10-04T04:37:29Z","title":"Machine Learning for Asymptomatic Ratoon Stunting Disease Detection With\n  Freely Available Satellite Based Multispectral Imaging","summary":"  Disease detection in sugarcane, particularly the identification of\nasymptomatic infectious diseases such as Ratoon Stunting Disease (RSD), is\ncritical for effective crop management. This study employed various machine\nlearning techniques to detect the presence of RSD in different sugarcane\nvarieties, using vegetation indices derived from freely available\nsatellite-based spectral data. Our results show that the Support Vector Machine\nwith a Radial Basis Function Kernel (SVM-RBF) was the most effective algorithm,\nachieving classification accuracy between 85.64% and 96.55%, depending on the\nvariety. Gradient Boosting and Random Forest also demonstrated high performance\nachieving accuracy between 83.33% to 96.55%, while Logistic Regression and\nQuadratic Discriminant Analysis showed variable results across different\nvarieties. The inclusion of sugarcane variety and vegetation indices was\nimportant in the detection of RSD. This agreed with what was identified in the\ncurrent literature. Our study highlights the potential of satellite-based\nremote sensing as a cost-effective and efficient method for large-scale\nsugarcane disease detection alternative to traditional manual laboratory\ntesting methods.\n","authors":["Ethan Kane Waters","Carla Chia-ming Chen","Mostafa Rahimi Azghadi"],"pdf_url":"https://arxiv.org/pdf/2410.03141v2.pdf","comment":"13 pages, 1 figure and 3 tables (main text), 1 figure and 2 tables\n  (appendices). Submitted to \"Computers and Electronics in Agriculture\""},{"id":"http://arxiv.org/abs/2410.04733v1","updated":"2024-10-07T03:52:06Z","published":"2024-10-07T03:52:06Z","title":"PredFormer: Transformers Are Effective Spatial-Temporal Predictive\n  Learners","summary":"  Spatiotemporal predictive learning methods generally fall into two\ncategories: recurrent-based approaches, which face challenges in\nparallelization and performance, and recurrent-free methods, which employ\nconvolutional neural networks (CNNs) as encoder-decoder architectures. These\nmethods benefit from strong inductive biases but often at the expense of\nscalability and generalization. This paper proposes PredFormer, a pure\ntransformer-based framework for spatiotemporal predictive learning. Motivated\nby the Vision Transformers (ViT) design, PredFormer leverages carefully\ndesigned Gated Transformer blocks, following a comprehensive analysis of 3D\nattention mechanisms, including full-, factorized-, and interleaved-\nspatial-temporal attention. With its recurrent-free, transformer-based design,\nPredFormer is both simple and efficient, significantly outperforming previous\nmethods by large margins. Extensive experiments on synthetic and real-world\ndatasets demonstrate that PredFormer achieves state-of-the-art performance. On\nMoving MNIST, PredFormer achieves a 51.3% reduction in MSE relative to SimVP.\nFor TaxiBJ, the model decreases MSE by 33.1% and boosts FPS from 533 to 2364.\nAdditionally, on WeatherBench, it reduces MSE by 11.1% while enhancing FPS from\n196 to 404. These performance gains in both accuracy and efficiency demonstrate\nPredFormer's potential for real-world applications. The source code will be\nreleased at https://github.com/yyyujintang/PredFormer.\n","authors":["Yujin Tang","Lu Qi","Fei Xie","Xiangtai Li","Chao Ma","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04733v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.10161v3","updated":"2024-10-07T03:37:36Z","published":"2024-09-16T10:52:16Z","title":"SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using\n  Gaussian Splatting","summary":"  Sim2Real transfer, particularly for manipulation policies relying on RGB\nimages, remains a critical challenge in robotics due to the significant domain\nshift between synthetic and real-world visual data. In this paper, we propose\nSplatSim, a novel framework that leverages Gaussian Splatting as the primary\nrendering primitive to reduce the Sim2Real gap for RGB-based manipulation\npolicies. By replacing traditional mesh representations with Gaussian Splats in\nsimulators, SplatSim produces highly photorealistic synthetic data while\nmaintaining the scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation policies within\nSplatSim and deploying them in the real world in a zero-shot manner, achieving\nan average success rate of 86.25%, compared to 97.5% for policies trained on\nreal-world data. Videos can be found on our project page:\nhttps://splatsim.github.io\n","authors":["Mohammad Nomaan Qureshi","Sparsh Garg","Francisco Yandun","David Held","George Kantor","Abhisesh Silwal"],"pdf_url":"https://arxiv.org/pdf/2409.10161v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03209v3","updated":"2024-10-07T03:37:15Z","published":"2024-09-05T03:07:26Z","title":"iSeg: An Iterative Refinement-based Framework for Training-free\n  Segmentation","summary":"  Stable diffusion has demonstrated strong image synthesis ability to given\ntext descriptions, suggesting it to contain strong semantic clue for grouping\nobjects. The researchers have explored employing stable diffusion for\ntraining-free segmentation. Most existing approaches refine cross-attention map\nby self-attention map once, demonstrating that self-attention map contains\nuseful semantic information to improve segmentation. To fully utilize\nself-attention map, we present a deep experimental analysis on iteratively\nrefining cross-attention map with self-attention map, and propose an effective\niterative refinement framework for training-free segmentation, named iSeg. The\nproposed iSeg introduces an entropy-reduced self-attention module that utilizes\na gradient descent scheme to reduce the entropy of self-attention map, thereby\nsuppressing the weak responses corresponding to irrelevant global information.\nLeveraging the entropy-reduced self-attention module, our iSeg stably improves\nrefined cross-attention map with iterative refinement. Further, we design a\ncategory-enhanced cross-attention module to generate accurate cross-attention\nmap, providing a better initial input for iterative refinement. Extensive\nexperiments across different datasets and diverse segmentation tasks reveal the\nmerits of proposed contributions, leading to promising performance on diverse\nsegmentation tasks. For unsupervised semantic segmentation on Cityscapes, our\niSeg achieves an absolute gain of 3.8% in terms of mIoU compared to the best\nexisting training-free approach in literature. Moreover, our proposed iSeg can\nsupport segmentation with different kinds of images and interactions. The\nproject is available at https://linsun449.github.io/iSeg.\n","authors":["Lin Sun","Jiale Cao","Jin Xie","Fahad Shahbaz Khan","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2409.03209v3.pdf","comment":"Project Page: https://linsun449.github.io/iSeg/ Code:\n  https://github.com/linsun449/iseg.code"},{"id":"http://arxiv.org/abs/2410.04721v1","updated":"2024-10-07T03:22:51Z","published":"2024-10-07T03:22:51Z","title":"ACDC: Autoregressive Coherent Multimodal Generation using Diffusion\n  Correction","summary":"  Autoregressive models (ARMs) and diffusion models (DMs) represent two leading\nparadigms in generative modeling, each excelling in distinct areas: ARMs in\nglobal context modeling and long-sequence generation, and DMs in generating\nhigh-quality local contexts, especially for continuous data such as images and\nshort videos. However, ARMs often suffer from exponential error accumulation\nover long sequences, leading to physically implausible results, while DMs are\nlimited by their local context generation capabilities. In this work, we\nintroduce Autoregressive Coherent multimodal generation with Diffusion\nCorrection (ACDC), a zero-shot approach that combines the strengths of both\nARMs and DMs at the inference stage without the need for additional\nfine-tuning. ACDC leverages ARMs for global context generation and\nmemory-conditioned DMs for local correction, ensuring high-quality outputs by\ncorrecting artifacts in generated multimodal tokens. In particular, we propose\na memory module based on large language models (LLMs) that dynamically adjusts\nthe conditioning texts for the DMs, preserving crucial global context\ninformation. Our experiments on multimodal tasks, including coherent\nmulti-frame story generation and autoregressive video generation, demonstrate\nthat ACDC effectively mitigates the accumulation of errors and significantly\nenhances the quality of generated outputs, achieving superior performance while\nremaining agnostic to specific ARM and DM architectures. Project page:\nhttps://acdc2025.github.io/\n","authors":["Hyungjin Chung","Dohun Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2410.04721v1.pdf","comment":"25 pages, 10 figures. Project page: https://acdc2025.github.io/"},{"id":"http://arxiv.org/abs/2410.04716v1","updated":"2024-10-07T03:13:19Z","published":"2024-10-07T03:13:19Z","title":"H-SIREN: Improving implicit neural representations with hyperbolic\n  periodic functions","summary":"  Implicit neural representations (INR) have been recently adopted in various\napplications ranging from computer vision tasks to physics simulations by\nsolving partial differential equations. Among existing INR-based works,\nmulti-layer perceptrons with sinusoidal activation functions find widespread\napplications and are also frequently treated as a baseline for the development\nof better activation functions for INR applications. Recent investigations\nclaim that the use of sinusoidal activation functions could be sub-optimal due\nto their limited supported frequency set as well as their tendency to generate\nover-smoothed solutions. We provide a simple solution to mitigate such an issue\nby changing the activation function at the first layer from $\\sin(x)$ to\n$\\sin(\\sinh(2x))$. We demonstrate H-SIREN in various computer vision and fluid\nflow problems, where it surpasses the performance of several state-of-the-art\nINRs.\n","authors":["Rui Gao","Rajeev K. Jaiman"],"pdf_url":"https://arxiv.org/pdf/2410.04716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03226v2","updated":"2024-10-07T03:01:01Z","published":"2024-10-04T08:26:06Z","title":"Frame-Voyager: Learning to Query Frames for Video Large Language Models","summary":"  Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.\n","authors":["Sicheng Yu","Chengkai Jin","Huanyu Wang","Zhenghao Chen","Sheng Jin","Zhongrong Zuo","Xiaolei Xu","Zhenbang Sun","Bingni Zhang","Jiawei Wu","Hao Zhang","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2410.03226v2.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.01225v2","updated":"2024-10-07T02:28:08Z","published":"2024-10-02T04:03:07Z","title":"Perceptual Piercing: Human Visual Cue-based Object Detection in Low\n  Visibility Conditions","summary":"  This study proposes a novel deep learning framework inspired by atmospheric\nscattering and human visual cortex mechanisms to enhance object detection under\npoor visibility scenarios such as fog, smoke, and haze. These conditions pose\nsignificant challenges for object recognition, impacting various sectors,\nincluding autonomous driving, aviation management, and security systems. The\nobjective is to enhance the precision and reliability of detection systems\nunder adverse environmental conditions. The research investigates the\nintegration of human-like visual cues, particularly focusing on selective\nattention and environmental adaptability, to ascertain their impact on object\ndetection's computational efficiency and accuracy. This paper proposes a\nmulti-tiered strategy that integrates an initial quick detection process,\nfollowed by targeted region-specific dehazing, and concludes with an in-depth\ndetection phase. The approach is validated using the Foggy Cityscapes,\nRESIDE-beta (OTS and RTTS) datasets and is anticipated to set new performance\nstandards in detection accuracy while significantly optimizing computational\nefficiency. The findings offer a viable solution for enhancing object detection\nin poor visibility and contribute to the broader understanding of integrating\nhuman visual principles into deep learning algorithms for intricate visual\nrecognition challenges.\n","authors":["Ashutosh Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.01225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03038v2","updated":"2024-10-07T02:04:21Z","published":"2024-10-03T22:58:56Z","title":"CPFD: Confidence-aware Privileged Feature Distillation for Short Video\n  Classification","summary":"  Dense features, customized for different business scenarios, are essential in\nshort video classification. However, their complexity, specific adaptation\nrequirements, and high computational costs make them resource-intensive and\nless accessible during online inference. Consequently, these dense features are\ncategorized as `Privileged Dense Features'.Meanwhile, end-to-end multi-modal\nmodels have shown promising results in numerous computer vision tasks. In\nindustrial applications, prioritizing end-to-end multi-modal features, can\nenhance efficiency but often leads to the loss of valuable information from\nhistorical privileged dense features. To integrate both features while\nmaintaining efficiency and manageable resource costs, we present\nConfidence-aware Privileged Feature Distillation (CPFD), which empowers\nfeatures of an end-to-end multi-modal model by adaptively distilling privileged\nfeatures during training. Unlike existing privileged feature distillation (PFD)\nmethods, which apply uniform weights to all instances during distillation,\npotentially causing unstable performance across different business scenarios\nand a notable performance gap between teacher model (Dense Feature enhanced\nmultimodal-model DF-X-VLM) and student model (multimodal-model only X-VLM), our\nCPFD leverages confidence scores derived from the teacher model to adaptively\nmitigate the performance variance with the student model. We conducted\nextensive offline experiments on five diverse tasks demonstrating that CPFD\nimproves the video classification F1 score by 6.76% compared with end-to-end\nmultimodal-model (X-VLM) and by 2.31% with vanilla PFD on-average. And it\nreduces the performance gap by 84.6% and achieves results comparable to teacher\nmodel DF-X-VLM. The effectiveness of CPFD is further substantiated by online\nexperiments, and our framework has been deployed in production systems for over\na dozen models.\n","authors":["Jinghao Shi","Xiang Shen","Kaili Zhao","Xuedong Wang","Vera Wen","Zixuan Wang","Yifan Wu","Zhixin Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.03038v2.pdf","comment":"Camera ready for CIKM 2024"},{"id":"http://arxiv.org/abs/2407.15851v2","updated":"2024-10-07T02:03:30Z","published":"2024-07-03T18:07:57Z","title":"A Survey on Trustworthiness in Foundation Models for Medical Image\n  Analysis","summary":"  The rapid advancement of foundation models in medical imaging represents a\nsignificant leap toward enhancing diagnostic accuracy and personalized\ntreatment. However, the deployment of foundation models in healthcare\nnecessitates a rigorous examination of their trustworthiness, encompassing\nprivacy, robustness, reliability, explainability, and fairness. The current\nbody of survey literature on foundation models in medical imaging reveals\nconsiderable gaps, particularly in the area of trustworthiness. Additionally,\nexisting surveys on the trustworthiness of foundation models do not adequately\naddress their specific variations and applications within the medical imaging\ndomain. This survey aims to fill that gap by presenting a novel taxonomy of\nfoundation models used in medical imaging and analyzing the key motivations for\nensuring their trustworthiness. We review current research on foundation models\nin major medical imaging applications, focusing on segmentation, medical report\ngeneration, medical question and answering (Q\\&A), and disease diagnosis. These\nareas are highlighted because they have seen a relatively mature and\nsubstantial number of foundation models compared to other applications. We\nfocus on literature that discusses trustworthiness in medical image analysis\nmanuscripts. We explore the complex challenges of building trustworthy\nfoundation models for each application, summarizing current concerns and\nstrategies for enhancing trustworthiness. Furthermore, we examine the potential\nof these models to revolutionize patient care. Our analysis underscores the\nimperative for advancing towards trustworthy AI in medical image analysis,\nadvocating for a balanced approach that fosters innovation while ensuring\nethical and equitable healthcare delivery.\n","authors":["Congzhen Shi","Ryan Rezai","Jiaxi Yang","Qi Dou","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2407.15851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04689v1","updated":"2024-10-07T02:00:13Z","published":"2024-10-07T02:00:13Z","title":"Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment\n  Whole-Body Organs in CT with Light-Weighted Adaptation","summary":"  Deep segmentation networks achieve high performance when trained on specific\ndatasets. However, in clinical practice, it is often desirable that pretrained\nsegmentation models can be dynamically extended to enable segmenting new organs\nwithout access to previous training datasets or without training from scratch.\nThis would ensure a much more efficient model development and deployment\nparadigm accounting for the patient privacy and data storage issues. This\nclinically preferred process can be viewed as a continual semantic segmentation\n(CSS) problem. Previous CSS works would either experience catastrophic\nforgetting or lead to unaffordable memory costs as models expand. In this work,\nwe propose a new continual whole-body organ segmentation model with\nlight-weighted low-rank adaptation (LoRA). We first train and freeze a pyramid\nvision transformer (PVT) base segmentation model on the initial task, then\ncontinually add light-weighted trainable LoRA parameters to the frozen model\nfor each new learning task. Through a holistically exploration of the\narchitecture modification, we identify three most important layers (i.e.,\npatch-embedding, multi-head attention and feed forward layers) that are\ncritical in adapting to the new segmentation tasks, while retaining the\nmajority of the pretrained parameters fixed. Our proposed model continually\nsegments new organs without catastrophic forgetting and meanwhile maintaining a\nlow parameter increasing rate. Continually trained and tested on four datasets\ncovering different body parts of a total of 121 organs, results show that our\nmodel achieves high segmentation accuracy, closely reaching the PVT and nnUNet\nupper bounds, and significantly outperforms other regularization-based CSS\nmethods. When comparing to the leading architecture-based CSS method, our model\nhas a substantial lower parameter increasing rate while achieving comparable\nperformance.\n","authors":["Vince Zhu","Zhanghexuan Ji","Dazhou Guo","Puyang Wang","Yingda Xia","Le Lu","Xianghua Ye","Wei Zhu","Dakai Jin"],"pdf_url":"https://arxiv.org/pdf/2410.04689v1.pdf","comment":"Accepted by Medical Image Computing and Computer Assisted\n  Intervention -- MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.15269v2","updated":"2024-10-07T01:47:28Z","published":"2024-05-24T06:52:54Z","title":"BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection","summary":"  Multimodal contrastive learning methods (e.g., CLIP) have shown impressive\nzero-shot classification performance due to their strong ability to joint\nrepresentation learning for visual and textual modalities. However, recent\nresearch revealed that multimodal contrastive learning on poisoned pre-training\ndata with a small proportion of maliciously backdoored data can induce\nbackdoored CLIP that could be attacked by inserted triggers in downstream tasks\nwith a high success rate. To defend against backdoor attacks on CLIP, existing\ndefense methods focus on either the pre-training stage or the fine-tuning\nstage, which would unfortunately cause high computational costs due to numerous\nparameter updates. In this paper, we provide the first attempt at a\ncomputationally efficient backdoor detection method to defend against\nbackdoored CLIP in the inference stage. We empirically find that the visual\nrepresentations of backdoored images are insensitive to both benign and\nmalignant changes in class description texts. Motivated by this observation, we\npropose BDetCLIP, a novel test-time backdoor detection method based on\ncontrastive prompting. Specifically, we first prompt the language model (e.g.,\nGPT-4) to produce class-related description texts (benign) and class-perturbed\nrandom texts (malignant) by specially designed instructions. Then, the\ndistribution difference in cosine similarity between images and the two types\nof class description texts can be used as the criterion to detect backdoor\nsamples. Extensive experiments validate that our proposed BDetCLIP is superior\nto state-of-the-art backdoor detection methods, in terms of both effectiveness\nand efficiency.\n","authors":["Yuwei Niu","Shuo He","Qi Wei","Zongyu Wu","Feng Liu","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2405.15269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04682v1","updated":"2024-10-07T01:29:19Z","published":"2024-10-07T01:29:19Z","title":"On the Adversarial Risk of Test Time Adaptation: An Investigation into\n  Realistic Test-Time Data Poisoning","summary":"  Test-time adaptation (TTA) updates the model weights during the inference\nstage using testing data to enhance generalization. However, this practice\nexposes TTA to adversarial risks. Existing studies have shown that when TTA is\nupdated with crafted adversarial test samples, also known as test-time poisoned\ndata, the performance on benign samples can deteriorate. Nonetheless, the\nperceived adversarial risk may be overstated if the poisoned data is generated\nunder overly strong assumptions. In this work, we first review realistic\nassumptions for test-time data poisoning, including white-box versus grey-box\nattacks, access to benign data, attack budget, and more. We then propose an\neffective and realistic attack method that better produces poisoned samples\nwithout access to benign samples, and derive an effective in-distribution\nattack objective. We also design two TTA-aware attack objectives. Our\nbenchmarks of existing attack methods reveal that the TTA methods are more\nrobust than previously believed. In addition, we analyze effective defense\nstrategies to help develop adversarially robust TTA methods.\n","authors":["Yongyi Su","Yushu Li","Nanqing Liu","Kui Jia","Xulei Yang","Chuan-Sheng Foo","Xun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.04682v1.pdf","comment":"19 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2303.04508v3","updated":"2024-10-07T01:29:12Z","published":"2023-03-08T10:57:14Z","title":"InFusionSurf: Refining Neural RGB-D Surface Reconstruction Using\n  Per-Frame Intrinsic Refinement and TSDF Fusion Prior Learning","summary":"  We introduce InFusionSurf, an innovative enhancement for neural radiance\nfield (NeRF) frameworks in 3D surface reconstruction using RGB-D video frames.\nBuilding upon previous methods that have employed feature encoding to improve\noptimization speed, we further improve the reconstruction quality with minimal\nimpact on optimization time by refining depth information. InFusionSurf\naddresses camera motion-induced blurs in each depth frame through a per-frame\nintrinsic refinement scheme. It incorporates the truncated signed distance\nfield (TSDF) Fusion, a classical real-time 3D surface reconstruction method, as\na pretraining tool for the feature grid, enhancing reconstruction details and\ntraining speed. Comparative quantitative and qualitative analyses show that\nInFusionSurf reconstructs scenes with high accuracy while maintaining\noptimization efficiency. The effectiveness of our intrinsic refinement and TSDF\nFusion-based pretraining is further validated through an ablation study.\n","authors":["Seunghwan Lee","Gwanmo Park","Hyewon Son","Jiwon Ryu","Han Joo Chae"],"pdf_url":"https://arxiv.org/pdf/2303.04508v3.pdf","comment":"ICME'24 (Oral), Project page:\n  https://rokit-healthcare.github.io/InFusionSurf/"},{"id":"http://arxiv.org/abs/2405.16406v3","updated":"2024-10-07T01:27:59Z","published":"2024-05-26T02:15:49Z","title":"SpinQuant: LLM quantization with learned rotations","summary":"  Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\n","authors":["Zechun Liu","Changsheng Zhao","Igor Fedorov","Bilge Soran","Dhruv Choudhary","Raghuraman Krishnamoorthi","Vikas Chandra","Yuandong Tian","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2405.16406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04680v1","updated":"2024-10-07T01:24:39Z","published":"2024-10-07T01:24:39Z","title":"Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian\n  Splatting","summary":"  We propose a framework for active next best view and touch selection for\nrobotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a\nuseful explicit 3D scene representation for robotics, as it has the ability to\nrepresent scenes in a both photorealistic and geometrically accurate manner.\nHowever, in real-world, online robotic scenes where the number of views is\nlimited given efficiency requirements, random view selection for 3DGS becomes\nimpractical as views are often overlapping and redundant. We address this issue\nby proposing an end-to-end online training and active view selection pipeline,\nwhich enhances the performance of 3DGS in few-view robotics settings. We first\nelevate the performance of few-shot 3DGS with a novel semantic depth alignment\nmethod using Segment Anything Model 2 (SAM2) that we supplement with Pearson\ndepth and surface normal loss to improve color and depth reconstruction of\nreal-world scenes. We then extend FisherRF, a next-best-view selection method\nfor 3DGS, to select views and touch poses based on depth uncertainty. We\nperform online view selection on a real robot system during live 3DGS training.\nWe motivate our improvements to few-shot GS scenes, and extend depth-based\nFisherRF to them, where we demonstrate both qualitative and quantitative\nimprovements on challenging robot scenes. For more information, please see our\nproject page at https://armlabstanford.github.io/next-best-sense.\n","authors":["Matthew Strong","Boshu Lei","Aiden Swann","Wen Jiang","Kostas Daniilidis","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2410.04680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17137v2","updated":"2024-10-07T01:00:46Z","published":"2024-09-25T17:56:00Z","title":"PACE: marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization","summary":"  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision\ntransformers to downstream tasks. However, the optimization for tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improved model generalization.\nMotivated by this connection, we propose reducing gradient norms for enhanced\ngeneralization and aligning fine-tuned model with the pre-trained counterpart\nto retain knowledge from large-scale pre-training data. Yet, naive alignment\ndoes not guarantee gradient reduction and can potentially cause gradient\nexplosion, complicating efforts to manage gradients. To address such issues, we\npropose PACE, marrying generalization of PArameter-efficient fine-tuning with\nConsistency rEgularization. We perturb features learned from the adapter with\nthe multiplicative noise and ensure the fine-tuned model remains consistent for\nsame sample under different perturbations. Theoretical analysis shows that PACE\nnot only implicitly regularizes gradients for enhanced generalization, but also\nimplicitly aligns the fine-tuned and pre-trained models to retain knowledge.\nExperimental evidence supports our theories. PACE outperforms existing PEFT\nmethods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and\ndomain adaptation. Code will be available at\nhttps://github.com/MaxwellYaoNi/PACE\n","authors":["Yao Ni","Shan Zhang","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2409.17137v2.pdf","comment":"Accepted by NeurIPS 2024 as a spotlight. This preliminary version\n  will soon be extended with the experiments and analyses from the rebuttal"},{"id":"http://arxiv.org/abs/2410.04671v1","updated":"2024-10-07T00:55:42Z","published":"2024-10-07T00:55:42Z","title":"CAR: Controllable Autoregressive Modeling for Visual Generation","summary":"  Controllable generation, which enables fine-grained control over generated\noutputs, has emerged as a critical focus in visual generative models.\nCurrently, there are two primary technical approaches in visual generation:\ndiffusion models and autoregressive models. Diffusion models, as exemplified by\nControlNet and T2I-Adapter, offer advanced control mechanisms, whereas\nautoregressive models, despite showcasing impressive generative quality and\nscalability, remain underexplored in terms of controllability and flexibility.\nIn this study, we introduce Controllable AutoRegressive Modeling (CAR), a\nnovel, plug-and-play framework that integrates conditional control into\nmulti-scale latent variable modeling, enabling efficient control generation\nwithin a pre-trained visual autoregressive model. CAR progressively refines and\ncaptures control representations, which are injected into each autoregressive\nstep of the pre-trained model to guide the generation process. Our approach\ndemonstrates excellent controllability across various types of conditions and\ndelivers higher image quality compared to previous methods. Additionally, CAR\nachieves robust generalization with significantly fewer training resources\ncompared to those required for pre-training the model. To the best of our\nknowledge, we are the first to propose a control framework for pre-trained\nautoregressive visual generation models.\n","authors":["Ziyu Yao","Jialin Li","Yifeng Zhou","Yong Liu","Xi Jiang","Chengjie Wang","Feng Zheng","Yuexian Zou","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2410.04671v1.pdf","comment":"Code available at: https://github.com/MiracleDance/CAR"},{"id":"http://arxiv.org/abs/2410.04659v1","updated":"2024-10-07T00:16:26Z","published":"2024-10-07T00:16:26Z","title":"ActiView: Evaluating Active Perception Ability for Multimodal Large\n  Language Models","summary":"  Active perception, a crucial human capability, involves setting a goal based\non the current understanding of the environment and performing actions to\nachieve that goal. Despite significant efforts in evaluating Multimodal Large\nLanguage Models (MLLMs), active perception has been largely overlooked. To\naddress this gap, we propose a novel benchmark named ActiView to evaluate\nactive perception in MLLMs. Since comprehensively assessing active perception\nis challenging, we focus on a specialized form of Visual Question Answering\n(VQA) that eases the evaluation yet challenging for existing MLLMs. Given an\nimage, we restrict the perceptual field of a model, requiring it to actively\nzoom or shift its perceptual field based on reasoning to answer the question\nsuccessfully. We conduct extensive evaluation over 27 models, including\nproprietary and open-source models, and observe that the ability to read and\ncomprehend multiple images simultaneously plays a significant role in enabling\nactive perception. Results reveal a significant gap in the active perception\ncapability of MLLMs, indicating that this area deserves more attention. We hope\nthat our benchmark could help develop methods for MLLMs to understand\nmultimodal inputs in more natural and holistic ways.\n","authors":["Ziyue Wang","Chi Chen","Fuwen Luo","Yurui Dong","Yuanchi Zhang","Yuzhuang Xu","Xiaolong Wang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04659v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.05252v1","updated":"2024-10-07T17:55:10Z","published":"2024-10-07T17:55:10Z","title":"Causal Micro-Narratives","summary":"  We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.\n","authors":["Mourad Heddaya","Qingcheng Zeng","Chenhao Tan","Rob Voigt","Alexander Zentefis"],"pdf_url":"https://arxiv.org/pdf/2410.05252v1.pdf","comment":"Accepted to EMNLP 2024 Workshop on Narrative Understanding"},{"id":"http://arxiv.org/abs/2410.05165v1","updated":"2024-10-07T16:23:36Z","published":"2024-10-07T16:23:36Z","title":"Efficient Inference for Large Language Model-based Generative\n  Recommendation","summary":"  Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.\n","authors":["Xinyu Lin","Chaoqun Yang","Wenjie Wang","Yongqi Li","Cunxiao Du","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.05165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14768v2","updated":"2024-10-07T14:35:14Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.05018v1","updated":"2024-10-07T13:19:08Z","published":"2024-10-07T13:19:08Z","title":"On the Biased Assessment of Expert Finding Systems","summary":"  In large organisations, identifying experts on a given topic is crucial in\nleveraging the internal knowledge spread across teams and departments.\nSo-called enterprise expert retrieval systems automatically discover and\nstructure employees' expertise based on the vast amount of heterogeneous data\navailable about them and the work they perform. Evaluating these systems\nrequires comprehensive ground truth expert annotations, which are hard to\nobtain. Therefore, the annotation process typically relies on automated\nrecommendations of knowledge areas to validate. This case study provides an\nanalysis of how these recommendations can impact the evaluation of expert\nfinding systems. We demonstrate on a popular benchmark that system-validated\nannotations lead to overestimated performance of traditional term-based\nretrieval models and even invalidate comparisons with more recent neural\nmethods. We also augment knowledge areas with synonyms to uncover a strong bias\ntowards literal mentions of their constituent words. Finally, we propose\nconstraints to the annotation process to prevent these biased evaluations, and\nshow that this still allows annotation suggestions of high utility. These\nfindings should inform benchmark creation or selection for expert finding, to\nguarantee meaningful comparison of methods.\n","authors":["Jens-Joris Decorte","Jeroen Van Hautte","Chris Develder","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2410.05018v1.pdf","comment":"Accepted to the 4th Workshop on Recommender Systems for Human\n  Resources (RecSys in HR 2024) as part of RecSys 2024"},{"id":"http://arxiv.org/abs/2409.18427v2","updated":"2024-10-07T13:07:11Z","published":"2024-09-27T03:28:11Z","title":"Neural Collaborative Filtering to Detect Anomalies in Human Semantic\n  Trajectories","summary":"  Human trajectory anomaly detection has become increasingly important across a\nwide range of applications, including security surveillance and public health.\nHowever, existing trajectory anomaly detection methods are primarily focused on\nvehicle-level traffic, while human-level trajectory anomaly detection remains\nunder-explored. Since human trajectory data is often very sparse, machine\nlearning methods have become the preferred approach for identifying complex\npatterns. However, concerns regarding potential biases and the robustness of\nthese models have intensified the demand for more transparent and explainable\nalternatives. In response to these challenges, our research focuses on\ndeveloping a lightweight anomaly detection model specifically designed to\ndetect anomalies in human trajectories. We propose a Neural Collaborative\nFiltering approach to model and predict normal mobility. Our method is designed\nto model users' daily patterns of life without requiring prior knowledge,\nthereby enhancing performance in scenarios where data is sparse or incomplete,\nsuch as in cold start situations. Our algorithm consists of two main modules.\nThe first is the collaborative filtering module, which applies collaborative\nfiltering to model normal mobility of individual humans to places of interest.\nThe second is the neural module, responsible for interpreting the complex\nspatio-temporal relationships inherent in human trajectory data. To validate\nour approach, we conducted extensive experiments using simulated and real-world\ndatasets comparing to numerous state-of-the-art trajectory anomaly detection\napproaches.\n","authors":["Yueyang Liu","Lance Kennedy","Hossein Amiri","Andreas Zfle"],"pdf_url":"https://arxiv.org/pdf/2409.18427v2.pdf","comment":"Accepted for publication in the 1st ACM SIGSPATIAL International\n  Workshop on Geospatial Anomaly Detection (GeoAnomalies'24)"},{"id":"http://arxiv.org/abs/2410.04949v1","updated":"2024-10-07T11:45:04Z","published":"2024-10-07T11:45:04Z","title":"Leverage Knowledge Graph and Large Language Model for Law Article\n  Recommendation: A Case Study of Chinese Criminal Law","summary":"  Court efficiency is vital for social stability. However, in most countries\naround the world, the grassroots courts face case backlogs, with decisions\nrelying heavily on judicial personnel's cognitive labor, lacking intelligent\ntools to improve efficiency. To address this issue, we propose an efficient law\narticle recommendation approach utilizing a Knowledge Graph (KG) and a Large\nLanguage Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge\nGraph (CLAKG) as a database to store current law statutes, historical case\ninformation, and correspondence between law articles and historical cases.\nAdditionally, we introduce an automated CLAKG construction method based on LLM.\nOn this basis, we propose a closed-loop law article recommendation method.\nFinally, through a series of experiments using judgment documents from the\nwebsite \"China Judgements Online\", we have improved the accuracy of law article\nrecommendation in cases from 0.549 to 0.694, demonstrating that our proposed\nmethod significantly outperforms baseline approaches.\n","authors":["Yongming Chen","Miner Chen","Ye Zhu","Juan Pei","Siyu Chen","Yu Zhou","Yi Wang","Yifan Zhou","Hao Li","Songan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.04949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04927v1","updated":"2024-10-07T11:19:05Z","published":"2024-10-07T11:19:05Z","title":"FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services","summary":"  Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS.\n","authors":["Wei Yuan","Chaoqun Yang","Guanhua Ye","Tong Chen","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.04927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08227v2","updated":"2024-10-07T09:51:46Z","published":"2024-07-11T07:01:50Z","title":"DALL-M: Context-Aware Clinical Data Augmentation with LLMs","summary":"  X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets.\n","authors":["Chihcheng Hsieh","Catarina Moreira","Isabel Blanco Nobre","Sandra Costa Sousa","Chun Ouyang","Margot Brereton","Joaquim Jorge","Jacinto C. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2407.08227v2.pdf","comment":"we introduce a pioneering approach to clinical data augmentation that\n  employs large language models (LLMs) to generate patient contextual synthetic\n  data. It preserves the integrity of real patient data while enriching the\n  dataset with contextually relevant synthetic features, significantly\n  enhancing model performance"},{"id":"http://arxiv.org/abs/2410.04830v1","updated":"2024-10-07T08:34:18Z","published":"2024-10-07T08:34:18Z","title":"Correcting for Popularity Bias in Recommender Systems via Item Loss\n  Equalization","summary":"  Recommender Systems (RS) often suffer from popularity bias, where a small set\nof popular items dominate the recommendation results due to their high\ninteraction rates, leaving many less popular items overlooked. This phenomenon\ndisproportionately benefits users with mainstream tastes while neglecting those\nwith niche interests, leading to unfairness among users and exacerbating\ndisparities in recommendation quality across different user groups. In this\npaper, we propose an in-processing approach to address this issue by\nintervening in the training process of recommendation models. Drawing\ninspiration from fair empirical risk minimization in machine learning, we\naugment the objective function of the recommendation model with an additional\nterm aimed at minimizing the disparity in loss values across different item\ngroups during the training process. Our approach is evaluated through extensive\nexperiments on two real-world datasets and compared against state-of-the-art\nbaselines. The results demonstrate the superior efficacy of our method in\nmitigating the unfairness of popularity bias while incurring only negligible\nloss in recommendation accuracy.\n","authors":["Juno Prent","Masoud Mansoury"],"pdf_url":"https://arxiv.org/pdf/2410.04830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01449v3","updated":"2024-10-07T07:46:00Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nas well as tables, figures, page layouts, or fonts. While modern document\nretrieval systems exhibit strong performance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hindering their performance on\npractical document retrieval applications such as Retrieval Augmented\nGeneration. To benchmark current systems on visually rich document retrieval,\nwe introduce the Visual Document Retrieval Benchmark ViDoRe, composed of\nvarious page-level retrieving tasks spanning multiple domains, languages, and\nsettings. The inherent shortcomings of modern systems motivate the introduction\nof a new retrieval model architecture, ColPali, which leverages the document\nunderstanding capabilities of recent Vision Language Models to produce\nhigh-quality contextualized embeddings solely from images of document pages.\nCombined with a late interaction matching mechanism, ColPali largely\noutperforms modern document retrieval pipelines while being drastically faster\nand end-to-end trainable.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Cline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2403.17372v5","updated":"2024-10-07T07:33:35Z","published":"2024-03-26T04:16:57Z","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders","summary":"  Sequential Recommendation (SR) aims to predict future user-item interactions\nbased on historical interactions. While many SR approaches concentrate on user\nIDs and item IDs, the human perception of the world through multi-modal\nsignals, like text and images, has inspired researchers to delve into\nconstructing SR from multi-modal information without using IDs. However, the\ncomplexity of multi-modal learning manifests in diverse feature extractors,\nfusion methods, and pre-trained models. Consequently, designing a simple and\nuniversal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential\n\\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable\nchallenge. We systematically summarize the existing multi-modal related SR\nmethods and distill the essence into four core components: visual encoder, text\nencoder, multimodal fusion module, and sequential architecture. Along these\ndimensions, we dissect the model designs, and answer the following\nsub-questions: First, we explore how to construct MMSR from scratch, ensuring\nits performance either on par with or exceeds existing SR methods without\ncomplex techniques. Second, we examine if MMSR can benefit from existing\nmulti-modal pre-training paradigms. Third, we assess MMSR's capability in\ntackling common challenges like cold start and domain transferring. Our\nexperiment results across four real-world recommendation scenarios demonstrate\nthe great potential ID-agnostic multi-modal sequential recommendation. Our\nframework can be found at: https://github.com/MMSR23/MMSR.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Yuanqi He","Junchen Fu","Xiangyan Liu","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.17372v5.pdf","comment":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders"},{"id":"http://arxiv.org/abs/2410.04756v1","updated":"2024-10-07T05:20:21Z","published":"2024-10-07T05:20:21Z","title":"Item Cluster-aware Prompt Learning for Session-based Recommendation","summary":"  Session-based recommendation (SBR) aims to capture dynamic user preferences\nby analyzing item sequences within individual sessions. However, most existing\napproaches focus mainly on intra-session item relationships, neglecting the\nconnections between items across different sessions (inter-session\nrelationships), which limits their ability to fully capture complex item\ninteractions. While some methods incorporate inter-session information, they\noften suffer from high computational costs, leading to longer training times\nand reduced efficiency. To address these challenges, we propose the CLIP-SBR\n(Cluster-aware Item Prompt learning for Session-Based Recommendation)\nframework. CLIP-SBR is composed of two modules: 1) an item relationship mining\nmodule that builds a global graph to effectively model both intra- and\ninter-session relationships, and 2) an item cluster-aware prompt learning\nmodule that uses soft prompts to integrate these relationships into SBR models\nefficiently. We evaluate CLIP-SBR across eight SBR models and three benchmark\ndatasets, consistently demonstrating improved recommendation performance and\nestablishing CLIP-SBR as a robust solution for session-based recommendation\ntasks.\n","authors":["Wooseong Yang","Chen Wang","Zihe Song","Weizhi Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2410.04756v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.04567v3","updated":"2024-10-07T05:07:07Z","published":"2024-06-07T01:07:35Z","title":"Error Bounds of Supervised Classification from Information-Theoretic\n  Perspective","summary":"  In this paper, we explore bounds on the expected risk when using deep neural\nnetworks for supervised classification from an information theoretic\nperspective. Firstly, we introduce model risk and fitting error, which are\nderived from further decomposing the empirical risk. Model risk represents the\nexpected value of the loss under the model's predicted probabilities and is\nexclusively dependent on the model. Fitting error measures the disparity\nbetween the empirical risk and model risk. Then, we derive the upper bound on\nfitting error, which links the back-propagated gradient and the model's\nparameter count with the fitting error. Furthermore, we demonstrate that the\ngeneralization errors are bounded by the classification uncertainty, which is\ncharacterized by both the smoothness of the distribution and the sample size.\nBased on the bounds on fitting error and generalization, by utilizing the\ntriangle inequality, we establish an upper bound on the expected risk. This\nbound is applied to provide theoretical explanations for overparameterization,\nnon-convex optimization and flat minima in deep learning. Finally, empirical\nverification confirms a significant positive correlation between the derived\ntheoretical bounds and the practical expected risk, thereby affirming the\npractical relevance of the theoretical findings.\n","authors":["Binchuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.04567v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04739v1","updated":"2024-10-07T04:15:02Z","published":"2024-10-07T04:15:02Z","title":"TableRAG: Million-Token Table Understanding with Language Models","summary":"  Recent advancements in language models (LMs) have notably enhanced their\nability to reason with tabular data, primarily through program-aided mechanisms\nthat manipulate and analyze tables. However, these methods often require the\nentire table as input, leading to scalability challenges due to the positional\nbias or context length constraints. In response to these challenges, we\nintroduce TableRAG, a Retrieval-Augmented Generation (RAG) framework\nspecifically designed for LM-based table understanding. TableRAG leverages\nquery expansion combined with schema and cell retrieval to pinpoint crucial\ninformation before providing it to the LMs. This enables more efficient data\nencoding and precise retrieval, significantly reducing prompt lengths and\nmitigating information loss. We have developed two new million-token benchmarks\nfrom the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's\neffectiveness at scale. Our results demonstrate that TableRAG's retrieval\ndesign achieves the highest retrieval quality, leading to the new\nstate-of-the-art performance on large-scale table understanding.\n","authors":["Si-An Chen","Lesly Miculicich","Julian Martin Eisenschlos","Zifeng Wang","Zilong Wang","Yanfei Chen","Yasuhisa Fujii","Hsuan-Tien Lin","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04739v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.03265v2","updated":"2024-10-07T04:06:48Z","published":"2024-10-04T09:34:07Z","title":"Multimodal Point-of-Interest Recommendation","summary":"  Large Language Models are applied to recommendation tasks such as items to\nbuy and news articles to read. Point of Interest is quite a new area to\nsequential recommendation based on language representations of multimodal\ndatasets. As a first step to prove our concepts, we focused on restaurant\nrecommendation based on each user's past visit history. When choosing a next\nrestaurant to visit, a user would consider genre and location of the venue and,\nif available, pictures of dishes served there. We created a pseudo restaurant\ncheck-in history dataset from the Foursquare dataset and the FoodX-251 dataset\nby converting pictures into text descriptions with a multimodal model called\nLLaVA, and used a language-based sequential recommendation framework named\nRecformer proposed in 2023. A model trained on this semi-multimodal dataset has\noutperformed another model trained on the same dataset without picture\ndescriptions. This suggests that this semi-multimodal model reflects actual\nhuman behaviours and that our path to a multimodal recommendation model is in\nthe right direction.\n","authors":["Yuta Kanzawa","Toyotaro Suzumura","Hiroki Kanezashi","Jiawei Yong","Shintaro Fukushima"],"pdf_url":"https://arxiv.org/pdf/2410.03265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05536v1","updated":"2024-10-07T22:25:37Z","published":"2024-10-07T22:25:37Z","title":"On Feature Decorrelation in Cloth-Changing Person Re-identification","summary":"  Cloth-changing person re-identification (CC-ReID) poses a significant\nchallenge in computer vision. A prevailing approach is to prompt models to\nconcentrate on causal attributes, like facial features and hairstyles, rather\nthan confounding elements such as clothing appearance. Traditional methods to\nachieve this involve integrating multi-modality data or employing manually\nannotated clothing labels, which tend to complicate the model and require\nextensive human effort. In our study, we demonstrate that simply reducing\nfeature correlations during training can significantly enhance the baseline\nmodel's performance. We theoretically elucidate this effect and introduce a\nnovel regularization technique based on density ratio estimation. This\ntechnique aims to minimize feature correlation in the training process of\ncloth-changing ReID baselines. Our approach is model-independent, offering\nbroad enhancements without needing additional data or labels. We validate our\nmethod through comprehensive experiments on prevalent CC-ReID datasets, showing\nits effectiveness in improving baseline models' generalization capabilities.\n","authors":["Hongjun Wang","Jiyuan Chen","Renhe Jiang","Xuan Song","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.05536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11728v2","updated":"2024-10-07T19:45:56Z","published":"2023-08-22T18:39:39Z","title":"Invariant representation learning for sequential recommendation","summary":"  Sequential recommendation involves automatically recommending the next item\nto users based on their historical item sequence. While most prior research\nemploys RNN or transformer methods to glean information from the item\nsequence-generating probabilities for each user-item pair and recommending the\ntop items, these approaches often overlook the challenge posed by spurious\nrelationships. This paper specifically addresses these spurious relations. We\nintroduce a novel sequential recommendation framework named Irl4Rec. This\nframework harnesses invariant learning and employs a new objective that factors\nin the relationship between spurious variables and adjustment variables during\nmodel training. This approach aids in identifying spurious relations.\nComparative analyses reveal that our framework outperforms three typical\nmethods, underscoring the effectiveness of our model. Moreover, an ablation\nstudy further demonstrates the critical role our model plays in detecting\nspurious relations.\n","authors":["Xiaofan Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.11728v2.pdf","comment":"This paper has limited contribution, and too simple for submission"},{"id":"http://arxiv.org/abs/2410.05411v1","updated":"2024-10-07T18:23:00Z","published":"2024-10-07T18:23:00Z","title":"Constructing and Masking Preference Profile with LLMs for Filtering\n  Discomforting Recommendation","summary":"  Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.\n","authors":["Jiahao Liu","YiYang Shao","Peng Zhang","Dongsheng Li","Hansu Gu","Chao Chen","Longzhi Du","Tun Lu","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2410.05411v1.pdf","comment":"15 pages, under review"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.05269v1","updated":"2024-10-07T17:59:58Z","published":"2024-10-07T17:59:58Z","title":"Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models","summary":"  Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.\n","authors":["Fei Wang","Ninareh Mehrabi","Palash Goyal","Rahul Gupta","Kai-Wei Chang","Aram Galstyan"],"pdf_url":"https://arxiv.org/pdf/2410.05269v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/DataAdvisor/"},{"id":"http://arxiv.org/abs/2406.11839v2","updated":"2024-10-07T17:59:42Z","published":"2024-06-17T17:59:58Z","title":"mDPO: Conditional Preference Optimization for Multimodal Large Language\n  Models","summary":"  Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.\n","authors":["Fei Wang","Wenxuan Zhou","James Y. Huang","Nan Xu","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11839v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Project website:\n  https://feiwang96.github.io/mDPO"},{"id":"http://arxiv.org/abs/2410.05265v1","updated":"2024-10-07T17:59:35Z","published":"2024-10-07T17:59:35Z","title":"PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs","summary":"  Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.\n","authors":["Mengzhao Chen","Yi Liu","Jiahao Wang","Yi Bin","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.05265v1.pdf","comment":"A PTQ method to significantly boost the performance of static\n  activation quantization"},{"id":"http://arxiv.org/abs/2410.05263v1","updated":"2024-10-07T17:59:09Z","published":"2024-10-07T17:59:09Z","title":"Regression Conformal Prediction under Bias","summary":"  Uncertainty quantification is crucial to account for the imperfect\npredictions of machine learning algorithms for high-impact applications.\nConformal prediction (CP) is a powerful framework for uncertainty\nquantification that generates calibrated prediction intervals with valid\ncoverage. In this work, we study how CP intervals are affected by bias - the\nsystematic deviation of a prediction from ground truth values - a phenomenon\nprevalent in many real-world applications. We investigate the influence of bias\non interval lengths of two different types of adjustments -- symmetric\nadjustments, the conventional method where both sides of the interval are\nadjusted equally, and asymmetric adjustments, a more flexible method where the\ninterval can be adjusted unequally in positive or negative directions. We\npresent theoretical and empirical analyses characterizing how symmetric and\nasymmetric adjustments impact the \"tightness\" of CP intervals for regression\ntasks. Specifically for absolute residual and quantile-based non-conformity\nscores, we prove: 1) the upper bound of symmetrically adjusted interval lengths\nincreases by $2|b|$ where $b$ is a globally applied scalar value representing\nbias, 2) asymmetrically adjusted interval lengths are not affected by bias, and\n3) conditions when asymmetrically adjusted interval lengths are guaranteed to\nbe smaller than symmetric ones. Our analyses suggest that even if predictions\nexhibit significant drift from ground truth values, asymmetrically adjusted\nintervals are still able to maintain the same tightness and validity of\nintervals as if the drift had never happened, while symmetric ones\nsignificantly inflate the lengths. We demonstrate our theoretical results with\ntwo real-world prediction tasks: sparse-view computed tomography (CT)\nreconstruction and time-series weather forecasting. Our work paves the way for\nmore bias-robust machine learning systems.\n","authors":["Matt Y. Cheung","Tucker J. Netherton","Laurence E. Court","Ashok Veeraraghavan","Guha Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.05263v1.pdf","comment":"17 pages, 6 figures, code available at:\n  https://github.com/matthewyccheung/conformal-metric"},{"id":"http://arxiv.org/abs/2410.05258v1","updated":"2024-10-07T17:57:38Z","published":"2024-10-07T17:57:38Z","title":"Differential Transformer","summary":"  Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.\n","authors":["Tianzhu Ye","Li Dong","Yuqing Xia","Yutao Sun","Yi Zhu","Gao Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.05258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05255v1","updated":"2024-10-07T17:56:53Z","published":"2024-10-07T17:56:53Z","title":"SePPO: Semi-Policy Preference Optimization for Diffusion Alignment","summary":"  Reinforcement learning from human feedback (RLHF) methods are emerging as a\nway to fine-tune diffusion models (DMs) for visual generation. However,\ncommonly used on-policy strategies are limited by the generalization capability\nof the reward model, while off-policy approaches require large amounts of\ndifficult-to-obtain paired human-annotated data, particularly in visual\ngeneration tasks. To address the limitations of both on- and off-policy RLHF,\nwe propose a preference optimization method that aligns DMs with preferences\nwithout relying on reward models or paired human-annotated data. Specifically,\nwe introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO\nleverages previous checkpoints as reference models while using them to generate\non-policy reference samples, which replace \"losing images\" in preference pairs.\nThis approach allows us to optimize using only off-policy \"winning images.\"\nFurthermore, we design a strategy for reference model selection that expands\nthe exploration in the policy space. Notably, we do not simply treat reference\nsamples as negative examples for learning. Instead, we design an anchor-based\ncriterion to assess whether the reference samples are likely to be winning or\nlosing images, allowing the model to selectively learn from the generated\nreference samples. This approach mitigates performance degradation caused by\nthe uncertainty in reference sample quality. We validate SePPO across both\ntext-to-image and text-to-video benchmarks. SePPO surpasses all previous\napproaches on the text-to-image benchmarks and also demonstrates outstanding\nperformance on the text-to-video benchmarks. Code will be released in\nhttps://github.com/DwanZhang-AI/SePPO.\n","authors":["Daoan Zhang","Guangchen Lan","Dong-Jun Han","Wenlin Yao","Xiaoman Pan","Hongming Zhang","Mingxiao Li","Pengcheng Chen","Yu Dong","Christopher Brinton","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2410.05255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05254v1","updated":"2024-10-07T17:55:35Z","published":"2024-10-07T17:55:35Z","title":"GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments","summary":"  Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.\n","authors":["Eilam Shapira","Omer Madmon","Itamar Reinman","Samuel Joseph Amouyal","Roi Reichart","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2410.05254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05252v1","updated":"2024-10-07T17:55:10Z","published":"2024-10-07T17:55:10Z","title":"Causal Micro-Narratives","summary":"  We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.\n","authors":["Mourad Heddaya","Qingcheng Zeng","Chenhao Tan","Rob Voigt","Alexander Zentefis"],"pdf_url":"https://arxiv.org/pdf/2410.05252v1.pdf","comment":"Accepted to EMNLP 2024 Workshop on Narrative Understanding"},{"id":"http://arxiv.org/abs/2410.05248v1","updated":"2024-10-07T17:52:21Z","published":"2024-10-07T17:52:21Z","title":"SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe","summary":"  To induce desired behaviors in large language models (LLMs) for\ninteraction-driven tasks, the instruction-tuning stage typically trains LLMs on\ninstruction-response pairs using the next-token prediction (NTP) loss. Previous\nwork aiming to improve instruction-tuning performance often emphasizes the need\nfor higher-quality supervised fine-tuning (SFT) datasets, which typically\ninvolves expensive data filtering with proprietary LLMs or labor-intensive data\ngeneration by human annotators. However, these approaches do not fully leverage\nthe datasets' intrinsic properties, resulting in high computational and labor\ncosts, thereby limiting scalability and performance gains. In this paper, we\npropose SFTMix, a novel recipe that elevates instruction-tuning performance\nbeyond the conventional NTP paradigm, without the need for well-curated\ndatasets. Observing that LLMs exhibit uneven confidence across the semantic\nrepresentation space, we argue that examples with different confidence levels\nshould play distinct roles during the instruction-tuning process. Based on this\ninsight, SFTMix leverages training dynamics to identify examples with varying\nconfidence levels, then applies a Mixup-based regularization to mitigate\noverfitting on confident examples while propagating supervision signals to\nimprove learning on relatively unconfident ones. This approach enables SFTMix\nto significantly outperform NTP across a wide range of instruction-following\nand healthcare domain-specific SFT tasks, demonstrating its adaptability to\ndiverse LLM families and scalability to datasets of any size. Comprehensive\nablation studies further verify the robustness of SFTMix's design choices,\nunderscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing\napplications.\n","authors":["Yuxin Xiao","Shujian Zhang","Wenxuan Zhou","Marzyeh Ghassemi","Sanqiang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.05248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17975v2","updated":"2024-10-07T17:49:13Z","published":"2024-06-25T23:12:07Z","title":"SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)","summary":"  Whether LLMs memorize their training data and what this means, from privacy\nleakage to detecting copyright violations -- has become a rapidly growing area\nof research over the last two years. In recent months, more than 10 new methods\nhave been proposed to perform Membership Inference Attacks (MIAs) against LLMs.\nContrary to traditional MIAs which rely on fixed -- but randomized -- records\nor models, these methods are mostly evaluated on datasets collected post-hoc.\nSets of members and non-members, used to evaluate the MIA, are constructed\nusing informed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In the\nfirst part, we review the literature on MIAs against LLMs. While most work\nfocuses on sequence-level MIAs evaluated in post-hoc setups, we show that a\nrange of target models, motivations and units of interest have been considered\nin the literature. We then quantify distribution shifts present in the 6\ndatasets used in the literature, ranging from books to papers, using a bag of\nword classifier. Our analysis reveals that all of them suffer from severe\ndistribution shifts. This challenges the validity of using such setups to\nmeasure LLM memorization and may undermine the benchmarking of recently\nproposed methods. Yet, all hope might not be lost. In the second part, we\nintroduce important considerations to properly evaluate MIAs against LLMs and\ndiscuss potential ways forward: randomized test splits, injections of\nrandomized (unique) sequences, randomized finetuning, and post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide the development of MIA\nmethods and study LLM memorization. We conclude by proposing comprehensive,\neasy-to-use benchmarks for sequence- and document-level MIAs against LLMs.\n","authors":["Matthieu Meeus","Igor Shilov","Shubham Jain","Manuel Faysse","Marek Rei","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2406.17975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05233v1","updated":"2024-10-07T17:41:10Z","published":"2024-10-07T17:41:10Z","title":"SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised\n  Contrastive Learning","summary":"  We introduce a novel anchor-free contrastive learning (AFCL) method\nleveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach\nminimizes a semi-metric discriminative loss function that simultaneously\noptimizes two key objectives: reducing the distance and orthogonality between\nembeddings of similar inputs while maximizing these metrics for dissimilar\ninputs, facilitating more fine-grained contrastive learning. The AFCL method,\npowered by SimO loss, creates a fiber bundle topological structure in the\nembedding space, forming class-specific, internally cohesive yet orthogonal\nneighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset,\nproviding visualizations that demonstrate the impact of SimO loss on the\nembedding space. Our results illustrate the formation of distinct, orthogonal\nclass neighborhoods, showcasing the method's ability to create well-structured\nembeddings that balance class separation with intra-class variability. This\nwork opens new avenues for understanding and leveraging the geometric\nproperties of learned representations in various machine learning tasks.\n","authors":["Taha Bouhsine","Imad El Aaroussi","Atik Faysal","Wang Huaxia"],"pdf_url":"https://arxiv.org/pdf/2410.05233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05232v1","updated":"2024-10-07T17:40:51Z","published":"2024-10-07T17:40:51Z","title":"SymmetryLens: A new candidate paradigm for unsupervised symmetry\n  learning via locality and equivariance","summary":"  We develop a new, unsupervised symmetry learning method that starts with raw\ndata, and gives the minimal (discrete) generator of an underlying Lie group of\nsymmetries, together with a symmetry equivariant representation of the data.\nThe method is able to learn the pixel translation operator from a dataset with\nonly an approximate translation symmetry, and can learn quite different types\nof symmetries which are not apparent to the naked eye, equally well. The method\nis based on the formulation of an information-theoretic loss function that\nmeasures both the degree to which the dataset is symmetric under a given\ncandidate symmetry, and also, the degree of locality of the samples in the\ndataset with respect to this symmetry. We demonstrate that this coupling\nbetween symmetry and locality, together with a special optimization technique\ndeveloped for entropy estimation, results in a highly stable system that gives\nreproducible results. The symmetry actions we consider are group\nrepresentations, however, we believe the approach has the potential to be\ngeneralized to more general, nonlinear actions of non-commutative Lie groups.\n","authors":["Onur Efe","Arkadas Ozakin"],"pdf_url":"https://arxiv.org/pdf/2410.05232v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2312.00700v4","updated":"2024-10-07T17:40:32Z","published":"2023-12-01T16:33:57Z","title":"Generative Parameter-Efficient Fine-Tuning","summary":"  We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting\npretrained Transformer backbones on downstream tasks. GIFT learns to generate\nthe fine-tuned weights for a layer directly from its pretrained weights. The\nGIFT network is parameterized in a minimally-simple way by two linear layers\n(without bias terms), and is shared by different pretrained layers selected for\nfine-tuning (e.g., the Query layers), which result in significantly fewer\ntrainable parameters compared to the layer-specific methods like Low-Rank\nAdapter (LoRA). We also show this formulation bridges parameter-efficient\nfine-tuning and representation fine-tuning. We perform comprehensive\nexperiments on natural language tasks (commonsense and arithmetic reasoning,\ninstruction tuning, and sequence classification) and computer vision tasks\n(fine-grained classification). We obtain the best performance and parameter\nefficiency among baselines on commonsense and arithmetic reasoning, and\ninstruction following using the Llama family of models and on visual\nrecognition benchmarks using Vision Transformers. Notably, compared to LoRA, we\nobtain 5.7% absolute increase in average accuracy with 14 times reduction of\nparameters on Commonsense170k using Llama-3 (8B), and 5.4% absolute increase in\nthe win rate with 4 times reduction of parameters using Llama-2 (7B) during\ninstruction tuning. Our GIFT also obtains a slightly higher win rate on\ninstruction tuning than GPT 3.5 (Turbo 1106).\n","authors":["Chinmay Savadikar","Xi Song","Tianfu Wu"],"pdf_url":"https://arxiv.org/pdf/2312.00700v4.pdf","comment":"Project page and code: https://savadikarc.github.io/gift"},{"id":"http://arxiv.org/abs/2410.05229v1","updated":"2024-10-07T17:36:37Z","published":"2024-10-07T17:36:37Z","title":"GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in\n  Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir formal reasoning capabilities, particularly in mathematics. The GSM8K\nbenchmark is widely used to assess the mathematical reasoning of models on\ngrade-school-level questions. While the performance of LLMs on GSM8K has\nsignificantly improved in recent years, it remains unclear whether their\nmathematical reasoning capabilities have genuinely advanced, raising questions\nabout the reliability of the reported metrics. To address these concerns, we\nconduct a large-scale study on several SOTA open and closed models. To overcome\nthe limitations of existing evaluations, we introduce GSM-Symbolic, an improved\nbenchmark created from symbolic templates that allow for the generation of a\ndiverse set of questions. GSM-Symbolic enables more controllable evaluations,\nproviding key insights and more reliable metrics for measuring the reasoning\ncapabilities of models.Our findings reveal that LLMs exhibit noticeable\nvariance when responding to different instantiations of the same question.\nSpecifically, the performance of all models declines when only the numerical\nvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,\nwe investigate the fragility of mathematical reasoning in these models and show\nthat their performance significantly deteriorates as the number of clauses in a\nquestion increases. We hypothesize that this decline is because current LLMs\ncannot perform genuine logical reasoning; they replicate reasoning steps from\ntheir training data. Adding a single clause that seems relevant to the question\ncauses significant performance drops (up to 65%) across all state-of-the-art\nmodels, even though the clause doesn't contribute to the reasoning chain needed\nfor the final answer. Overall, our work offers a more nuanced understanding of\nLLMs' capabilities and limitations in mathematical reasoning.\n","authors":["Iman Mirzadeh","Keivan Alizadeh","Hooman Shahrokhi","Oncel Tuzel","Samy Bengio","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2410.05229v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.05225v1","updated":"2024-10-07T17:31:52Z","published":"2024-10-07T17:31:52Z","title":"ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse\n  Reward Continuous Control","summary":"  We consider deep deterministic policy gradient (DDPG) in the context of\nreinforcement learning with sparse rewards. To enhance exploration, we\nintroduce a search procedure, \\emph{${\\epsilon}{t}$-greedy}, which generates\nexploratory options for exploring less-visited states. We prove that search\nusing $\\epsilon t$-greedy has polynomial sample complexity under mild MDP\nassumptions. To more efficiently use the information provided by rewarded\ntransitions, we develop a new dual experience replay buffer framework,\n\\emph{GDRB}, and implement \\emph{longest n-step returns}. The resulting\nalgorithm, \\emph{ETGL-DDPG}, integrates all three techniques: \\bm{$\\epsilon\nt$}-greedy, \\textbf{G}DRB, and \\textbf{L}ongest $n$-step, into DDPG. We\nevaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms\nDDPG, as well as other state-of-the-art methods, across all tested\nsparse-reward continuous environments. Ablation studies further highlight how\neach strategy individually enhances the performance of DDPG in this setting.\n","authors":["Ehsan Futuhi","Shayan Karimi","Chao Gao","Martin Mller"],"pdf_url":"https://arxiv.org/pdf/2410.05225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05224v1","updated":"2024-10-07T17:29:40Z","published":"2024-10-07T17:29:40Z","title":"Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates","summary":"  Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.\n","authors":["Avanika Narayan","Mayee F. Chen","Kush Bhatia","Christopher R"],"pdf_url":"https://arxiv.org/pdf/2410.05224v1.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2406.13356v2","updated":"2024-10-07T17:27:30Z","published":"2024-06-19T09:03:21Z","title":"Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attack","summary":"  Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in LLMs. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of targeted relearning attacks. With access to only a small and\npotentially loosely related set of data, we find that we can \"jog\" the memory\nof unlearned models to reverse the effects of unlearning. For example, we show\nthat relearning on public medical articles can lead an unlearned LLM to output\nharmful knowledge about bioweapons, and relearning general wiki information\nabout the book series Harry Potter can force the model to output verbatim\nmemorized text. We formalize this unlearning-relearning pipeline, explore the\nattack across three popular unlearning benchmarks, and discuss future\ndirections and guidelines that result from our study.\n","authors":["Shengyuan Hu","Yiwei Fu","Zhiwei Steven Wu","Virginia Smith"],"pdf_url":"https://arxiv.org/pdf/2406.13356v2.pdf","comment":"26 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2310.13391v3","updated":"2024-10-07T17:27:21Z","published":"2023-10-20T10:03:14Z","title":"Learning Successor Features with Distributed Hebbian Temporal Memory","summary":"  This paper presents a novel approach to address the challenge of online\ntemporal memory learning for decision-making under uncertainty in\nnon-stationary, partially observable environments. The proposed algorithm,\nDistributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism\nand a multicomponent neuron model. DHTM aims to capture sequential data\nrelationships and make cumulative predictions about future observations,\nforming Successor Features (SF). Inspired by neurophysiological models of the\nneocortex, the algorithm utilizes distributed representations, sparse\ntransition matrices, and local Hebbian-like learning rules to overcome the\ninstability and slow learning process of traditional temporal memory algorithms\nlike RNN and HMM. Experimental results demonstrate that DHTM outperforms LSTM\nand a biologically inspired HMM-like algorithm, CSCG, in the case of\nnon-stationary datasets. Our findings suggest that DHTM is a promising approach\nfor addressing the challenges of online sequence learning and planning in\ndynamic environments.\n","authors":["Evgenii Dzhivelikian","Petr Kuderov","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2310.13391v3.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.05222v1","updated":"2024-10-07T17:26:31Z","published":"2024-10-07T17:26:31Z","title":"Precise Model Benchmarking with Only a Few Observations","summary":"  How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.\n","authors":["Riccardo Fogliato","Pratik Patil","Nil-Jana Akpinar","Mathew Monfort"],"pdf_url":"https://arxiv.org/pdf/2410.05222v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.08704v2","updated":"2024-10-07T17:23:25Z","published":"2024-05-14T15:42:55Z","title":"Full Line Code Completion: Bringing AI to Desktop","summary":"  In recent years, several industrial solutions for the problem of multi-token\ncode completion appeared, each making a great advance in the area but mostly\nfocusing on cloud-based runtime and avoiding working on the end user's device.\n  In this work, we describe our approach for building a multi-token code\ncompletion feature for the JetBrains' IntelliJ Platform, which we call Full\nLine Code Completion. The feature suggests only syntactically correct code and\nworks fully locally, i.e., data querying and the generation of suggestions\nhappens on the end user's machine. We share important time and\nmemory-consumption restrictions, as well as design principles that a code\ncompletion engine should satisfy. Working entirely on the end user's device,\nour code completion engine enriches user experience while being not only fast\nand compact but also secure. We share a number of useful techniques to meet the\nstated development constraints and also describe offline and online evaluation\npipelines that allowed us to make better decisions.\n  Our online evaluation shows that the usage of the tool leads to 1.3 times\nmore Python code in the IDE being produced by code completion. The described\nsolution was initially started with a help of researchers and was then bundled\ninto all JetBrains IDEs where it is now used by millions of users. Thus, we\nbelieve that this work is useful for bridging academia and industry, providing\nresearchers with the knowledge of what happens when complex research-based\nsolutions are integrated into real products.\n","authors":["Anton Semenkin","Vitaliy Bibaev","Yaroslav Sokolov","Kirill Krylov","Alexey Kalina","Anna Khannanova","Danila Savenkov","Darya Rovdo","Igor Davidenko","Kirill Karnaukhov","Maxim Vakhrushev","Mikhail Kostyukov","Mikhail Podvitskii","Petr Surkov","Yaroslav Golubev","Nikita Povarov","Timofey Bryksin"],"pdf_url":"https://arxiv.org/pdf/2405.08704v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.05218v1","updated":"2024-10-07T17:22:56Z","published":"2024-10-07T17:22:56Z","title":"Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories","summary":"  Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs.\n","authors":["Toni J. B. Liu","Nicolas Boull","Raphal Sarfati","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2410.05218v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2312.05516v3","updated":"2024-10-07T17:21:57Z","published":"2023-12-09T09:55:07Z","title":"Stateful Large Language Model Serving with Pensieve","summary":"  Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.\n","authors":["Lingfan Yu","Jinkun Lin","Jinyang Li"],"pdf_url":"https://arxiv.org/pdf/2312.05516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02476v2","updated":"2024-10-07T17:15:37Z","published":"2024-10-03T13:35:08Z","title":"Online Convex Optimization with a Separation Oracle","summary":"  In this paper, we introduce a new projection-free algorithm for Online Convex\nOptimization (OCO) with a state-of-the-art regret guarantee among\nseparation-based algorithms. Existing projection-free methods based on the\nclassical Frank-Wolfe algorithm achieve a suboptimal regret bound of\n$O(T^{3/4})$, while more recent separation-based approaches guarantee a regret\nbound of $O(\\kappa \\sqrt{T})$, where $\\kappa$ denotes the asphericity of the\nfeasible set, defined as the ratio of the radii of the containing and contained\nballs. However, for ill-conditioned sets, $\\kappa$ can be arbitrarily large,\npotentially leading to poor performance. Our algorithm achieves a regret bound\nof $\\widetilde{O}(\\sqrt{dT} + \\kappa d)$, while requiring only\n$\\widetilde{O}(1)$ calls to a separation oracle per round. Crucially, the main\nterm in the bound, $\\widetilde{O}(\\sqrt{d T})$, is independent of $\\kappa$,\naddressing the limitations of previous methods. Additionally, as a by-product\nof our analysis, we recover the $O(\\kappa \\sqrt{T})$ regret bound of existing\nOCO algorithms with a more straightforward analysis and improve the regret\nbound for projection-free online exp-concave optimization. Finally, for\nconstrained stochastic convex optimization, we achieve a state-of-the-art\nconvergence rate of $\\widetilde{O}(\\sigma/\\sqrt{T} + \\kappa d/T)$, where\n$\\sigma$ represents the noise in the stochastic gradients, while requiring only\n$\\widetilde{O}(1)$ calls to a separation oracle per iteration.\n","authors":["Zakaria Mhammedi"],"pdf_url":"https://arxiv.org/pdf/2410.02476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02844v2","updated":"2024-10-07T17:12:15Z","published":"2024-10-03T13:57:08Z","title":"CAnDOIT: Causal Discovery with Observational and Interventional Data\n  from Time-Series","summary":"  The study of cause-and-effect is of the utmost importance in many branches of\nscience, but also for many practical applications of intelligent systems. In\nparticular, identifying causal relationships in situations that include hidden\nfactors is a major challenge for methods that rely solely on observational data\nfor building causal models. This paper proposes CAnDOIT, a causal discovery\nmethod to reconstruct causal models using both observational and interventional\ntime-series data. The use of interventional data in the causal analysis is\ncrucial for real-world applications, such as robotics, where the scenario is\nhighly complex and observational data alone are often insufficient to uncover\nthe correct causal structure. Validation of the method is performed initially\non randomly generated synthetic models and subsequently on a well-known\nbenchmark for causal structure learning in a robotic manipulation environment.\nThe experiments demonstrate that the approach can effectively handle data from\ninterventions and exploit them to enhance the accuracy of the causal analysis.\nA Python implementation of CAnDOIT has also been developed and is publicly\navailable on GitHub: https://github.com/lcastri/causalflow.\n","authors":["Luca Castri","Sariah Mghames","Marc Hanheide","Nicola Bellotto"],"pdf_url":"https://arxiv.org/pdf/2410.02844v2.pdf","comment":"Published in Advanced Intelligent Systems"},{"id":"http://arxiv.org/abs/2410.05203v1","updated":"2024-10-07T17:07:21Z","published":"2024-10-07T17:07:21Z","title":"Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality","summary":"  The Fr\\'echet Video Distance (FVD) is a widely adopted metric for evaluating\nvideo generation distribution quality. However, its effectiveness relies on\ncritical assumptions. Our analysis reveals three significant limitations: (1)\nthe non-Gaussianity of the Inflated 3D Convnet (I3D) feature space; (2) the\ninsensitivity of I3D features to temporal distortions; (3) the impractical\nsample sizes required for reliable estimation. These findings undermine FVD's\nreliability and show that FVD falls short as a standalone metric for video\ngeneration evaluation. After extensive analysis of a wide range of metrics and\nbackbone architectures, we propose JEDi, the JEPA Embedding Distance, based on\nfeatures derived from a Joint Embedding Predictive Architecture, measured using\nMaximum Mean Discrepancy with polynomial kernel. Our experiments on multiple\nopen-source datasets show clear evidence that it is a superior alternative to\nthe widely used FVD metric, requiring only 16% of the samples to reach its\nsteady value, while increasing alignment with human evaluation by 34%, on\naverage.\n","authors":["Ge Ya"," Luo","Gian Favero","Zhi Hao Luo","Alexia Jolicoeur-Martineau","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2410.05203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10302v2","updated":"2024-10-07T17:07:09Z","published":"2024-05-16T17:55:42Z","title":"Optimal Aggregation of Prediction Intervals under Unsupervised Domain\n  Shift","summary":"  As machine learning models are increasingly deployed in dynamic environments,\nit becomes paramount to assess and quantify uncertainties associated with\ndistribution shifts. A distribution shift occurs when the underlying\ndata-generating process changes, leading to a deviation in the model's\nperformance. The prediction interval, which captures the range of likely\noutcomes for a given prediction, serves as a crucial tool for characterizing\nuncertainties induced by their underlying distribution. In this paper, we\npropose methodologies for aggregating prediction intervals to obtain one with\nminimal width and adequate coverage on the target domain under unsupervised\ndomain shift, under which we have labeled samples from a related source domain\nand unlabeled covariates from the target domain. Our analysis encompasses\nscenarios where the source and the target domain are related via i) a bounded\ndensity ratio, and ii) a measure-preserving transformation. Our proposed\nmethodologies are computationally efficient and easy to implement. Beyond\nillustrating the performance of our method through real-world datasets, we also\ndelve into the theoretical details. This includes establishing rigorous\ntheoretical guarantees, coupled with finite sample bounds, regarding the\ncoverage and width of our prediction intervals. Our approach excels in\npractical applications and is underpinned by a solid theoretical framework,\nensuring its reliability and effectiveness across diverse contexts.\n","authors":["Jiawei Ge","Debarghya Mukherjee","Jianqing Fan"],"pdf_url":"https://arxiv.org/pdf/2405.10302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05192v1","updated":"2024-10-07T16:49:39Z","published":"2024-10-07T16:49:39Z","title":"Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss\n  Landscape Perspective","summary":"  Training language models currently requires pre-determining a fixed compute\nbudget because the typical cosine learning rate schedule depends on the total\nnumber of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a\nconstant learning rate to produce a main branch of iterates that can in\nprinciple continue indefinitely without a pre-specified compute budget. Then,\ngiven any compute budget, one can branch out from the main branch at a proper\nat any time with a rapidly decaying learning rate to produce a strong model.\nEmpirically, WSD generates a non-traditional loss curve: the loss remains\nelevated during the stable phase but sharply declines during the decay phase.\nTowards explaining this phenomenon, we conjecture that pretraining loss\nexhibits a river valley landscape, which resembles a deep valley with a river\nat its bottom. Under this assumption, we show that during the stable phase, the\niterate undergoes large oscillations due to the high learning rate, yet it\nprogresses swiftly along the river. During the decay phase, the rapidly\ndropping learning rate minimizes the iterate's oscillations, moving it closer\nto the river and revealing true optimization progress. Therefore, the sustained\nhigh learning rate phase and fast decaying phase are responsible for progress\nin the river and the mountain directions respectively, and are both critical.\nOur analysis predicts phenomenons consistent with empirical observations and\nshows that this landscape can emerge from pretraining on a simple bi-gram\ndataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that\nreuses previous checkpoints' decay phases and keeps only one main branch, where\nwe resume from a decayed checkpoint. WSD-S empirically outperforms WSD and\nCyclic-Cosine in obtaining multiple language model checkpoints across various\ncompute budgets in a single run for parameters scaling from 0.1B to 1.2B.\n","authors":["Kaiyue Wen","Zhiyuan Li","Jason Wang","David Hall","Percy Liang","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2410.05192v1.pdf","comment":"45 pages,13 figures"},{"id":"http://arxiv.org/abs/2410.05188v1","updated":"2024-10-07T16:47:30Z","published":"2024-10-07T16:47:30Z","title":"Matrix-weighted networks for modeling multidimensional dynamics","summary":"  Networks are powerful tools for modeling interactions in complex systems.\nWhile traditional networks use scalar edge weights, many real-world systems\ninvolve multidimensional interactions. For example, in social networks,\nindividuals often have multiple interconnected opinions that can affect\ndifferent opinions of other individuals, which can be better characterized by\nmatrices. We propose a novel, general framework for modeling such\nmultidimensional interacting dynamics: matrix-weighted networks (MWNs). We\npresent the mathematical foundations of MWNs and examine consensus dynamics and\nrandom walks within this context. Our results reveal that the coherence of MWNs\ngives rise to non-trivial steady states that generalize the notions of\ncommunities and structural balance in traditional networks.\n","authors":["Yu Tian","Sadamori Kojaku","Hiroki Sayama","Renaud Lambiotte"],"pdf_url":"https://arxiv.org/pdf/2410.05188v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.18074v2","updated":"2024-10-07T16:46:42Z","published":"2024-07-25T14:28:58Z","title":"Principal-Agent Reinforcement Learning: Orchestrating AI Agents with\n  Contracts","summary":"  The increasing deployment of AI is shaping the future landscape of the\ninternet, which is set to become an integrated ecosystem of AI agents.\nOrchestrating the interaction among AI agents necessitates decentralized,\nself-sustaining mechanisms that harmonize the tension between individual\ninterests and social welfare. In this paper we tackle this challenge by\nsynergizing reinforcement learning with principal-agent theory from economics.\nTaken separately, the former allows unrealistic freedom of intervention, while\nthe latter struggles to scale in sequential settings. Combining them achieves\nthe best of both worlds. We propose a framework where a principal guides an\nagent in a Markov Decision Process (MDP) using a series of contracts, which\nspecify payments by the principal based on observable outcomes of the agent's\nactions. We present and analyze a meta-algorithm that iteratively optimizes the\npolicies of the principal and agent, showing its equivalence to a contraction\noperator on the principal's Q-function, and its convergence to subgame-perfect\nequilibrium. We then scale our algorithm with deep Q-learning and analyze its\nconvergence in the presence of approximation error, both theoretically and\nthrough experiments with randomly generated binary game-trees. Extending our\nframework to multiple agents, we apply our methodology to the combinatorial\nCoin Game. Addressing this multi-agent sequential social dilemma is a promising\nfirst step toward scaling our approach to more complex, real-world instances.\n","authors":["Dima Ivanov","Paul Dtting","Inbal Talgam-Cohen","Tonghan Wang","David C. Parkes"],"pdf_url":"https://arxiv.org/pdf/2407.18074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00099v4","updated":"2024-10-07T16:45:42Z","published":"2024-04-30T18:00:02Z","title":"Creative Beam Search: LLM-as-a-Judge For Improving Response Generation","summary":"  Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2405.00099v4.pdf","comment":"Presented as a short paper at the 15th International Conference on\n  Computational Creativity (ICCC'24)"},{"id":"http://arxiv.org/abs/2410.03098v2","updated":"2024-10-07T16:41:49Z","published":"2024-10-04T02:47:49Z","title":"Forest Proximities for Time Series","summary":"  RF-GAP has recently been introduced as an improved random forest proximity\nmeasure. In this paper, we present PF-GAP, an extension of RF-GAP proximities\nto proximity forests, an accurate and efficient time series classification\nmodel. We use the forest proximities in connection with Multi-Dimensional\nScaling to obtain vector embeddings of univariate time series, comparing the\nembeddings to those obtained using various time series distance measures. We\nalso use the forest proximities alongside Local Outlier Factors to investigate\nthe connection between misclassified points and outliers, comparing with\nnearest neighbor classifiers which use time series distance measures. We show\nthat the forest proximities may exhibit a stronger connection between\nmisclassified points and outliers than nearest neighbor classifiers.\n","authors":["Ben Shaw","Jake Rhodes","Soukaina Filali Boubrahimi","Kevin R. Moon"],"pdf_url":"https://arxiv.org/pdf/2410.03098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05182v1","updated":"2024-10-07T16:41:45Z","published":"2024-10-07T16:41:45Z","title":"MARs: Multi-view Attention Regularizations for Patch-based Feature\n  Recognition of Space Terrain","summary":"  The visual detection and tracking of surface terrain is required for\nspacecraft to safely land on or navigate within close proximity to celestial\nobjects. Current approaches rely on template matching with pre-gathered\npatch-based features, which are expensive to obtain and a limiting factor in\nperceptual capability. While recent literature has focused on in-situ detection\nmethods to enhance navigation and operational autonomy, robust description is\nstill needed. In this work, we explore metric learning as the lightweight\nfeature description mechanism and find that current solutions fail to address\ninter-class similarity and multi-view observational geometry. We attribute this\nto the view-unaware attention mechanism and introduce Multi-view Attention\nRegularizations (MARs) to constrain the channel and spatial attention across\nmultiple feature views, regularizing the what and where of attention focus. We\nthoroughly analyze many modern metric learning losses with and without MARs and\ndemonstrate improved terrain-feature recognition performance by upwards of 85%.\nWe additionally introduce the Luna-1 dataset, consisting of Moon crater\nlandmarks and reference navigation frames from NASA mission data to support\nfuture research in this difficult task. Luna-1 and source code are publicly\navailable at https://droneslab.github.io/mars/.\n","authors":["Timothy Chase Jr","Karthik Dantu"],"pdf_url":"https://arxiv.org/pdf/2410.05182v1.pdf","comment":"ECCV 2024. Project page available at\n  https://droneslab.github.io/mars/"},{"id":"http://arxiv.org/abs/2407.13493v3","updated":"2024-10-07T16:40:25Z","published":"2024-07-18T13:23:16Z","title":"Training Foundation Models as Data Compression: On Information, Model\n  Weights and Copyright Law","summary":"  The training process of foundation models as for other classes of deep\nlearning systems is based on minimizing the reconstruction error over a\ntraining set. For this reason, they are susceptible to the memorization and\nsubsequent reproduction of training samples. In this paper, we introduce a\ntraining-as-compressing perspective, wherein the model's weights embody a\ncompressed representation of the training data. From a copyright standpoint,\nthis point of view implies that the weights could be considered a reproduction\nor a derivative work of a potentially protected set of works. We investigate\nthe technical and legal challenges that emerge from this framing of the\ncopyright of outputs generated by foundation models, including their\nimplications for practitioners and researchers. We demonstrate that adopting an\ninformation-centric approach to the problem presents a promising pathway for\ntackling these emerging complex legal issues.\n","authors":["Giorgio Franceschelli","Claudia Cevenini","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2407.13493v3.pdf","comment":"Spotlight presentation at GenLaw'24, see\n  https://www.genlaw.org/2024-icml-papers#training-foundation-models-as-data-compression-on-information-model-weights-and-copyright-law"},{"id":"http://arxiv.org/abs/2410.02381v2","updated":"2024-10-07T16:39:24Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.05928v3","updated":"2024-10-07T16:37:56Z","published":"2024-09-09T09:26:48Z","title":"Machine Learning Based Optimal Design of Fibrillar Adhesives","summary":"  Fibrillar adhesion, observed in animals like beetles, spiders, and geckos,\nrelies on nanoscopic or microscopic fibrils to enhance surface adhesion via\n'contact splitting.' This concept has inspired engineering applications across\nrobotics, transportation, and medicine. Recent studies suggest that functional\ngrading of fibril properties can improve adhesion, but this is a complex design\nchallenge that has only been explored in simplified geometries. While machine\nlearning (ML) has gained traction in adhesive design, no previous attempts have\ntargeted fibril-array scale optimization. In this study, we propose an ML-based\ntool that optimizes the distribution of fibril compliance to maximize adhesive\nstrength. Our tool, featuring two deep neural networks (DNNs), recovers\nprevious design results for simple geometries and introduces novel solutions\nfor complex configurations. The Predictor DNN estimates adhesive strength based\non random compliance distributions, while the Designer DNN optimizes compliance\nfor maximum strength using gradient-based optimization. Our method\nsignificantly reduces test error and accelerates the optimization process,\noffering a high-performance solution for designing fibrillar adhesives and\nmicro-architected materials aimed at fracture resistance by achieving equal\nload sharing (ELS).\n","authors":["Mohammad Shojaeifard","Matteo Ferraresso","Alessandro Lucantonio","Mattia Bacca"],"pdf_url":"https://arxiv.org/pdf/2409.05928v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05177v1","updated":"2024-10-07T16:37:35Z","published":"2024-10-07T16:37:35Z","title":"Are causal effect estimations enough for optimal recommendations under\n  multitreatment scenarios?","summary":"  When making treatment selection decisions, it is essential to include a\ncausal effect estimation analysis to compare potential outcomes under different\ntreatments or controls, assisting in optimal selection. However, merely\nestimating individual treatment effects may not suffice for truly optimal\ndecisions. Our study addressed this issue by incorporating additional criteria,\nsuch as the estimations' uncertainty, measured by the conditional\nvalue-at-risk, commonly used in portfolio and insurance management. For\ncontinuous outcomes observable before and after treatment, we incorporated a\nspecific prediction condition. We prioritized treatments that could yield\noptimal treatment effect results and lead to post-treatment outcomes more\ndesirable than pretreatment levels, with the latter condition being called the\nprediction criterion. With these considerations, we propose a comprehensive\nmethodology for multitreatment selection. Our approach ensures satisfaction of\nthe overlap assumption, crucial for comparing outcomes for treated and control\ngroups, by training propensity score models as a preliminary step before\nemploying traditional causal models. To illustrate a practical application of\nour methodology, we applied it to the credit card limit adjustment problem.\nAnalyzing a fintech company's historical data, we found that relying solely on\ncounterfactual predictions was inadequate for appropriate credit line\nmodifications. Incorporating our proposed additional criteria significantly\nenhanced policy performance.\n","authors":["Sherly Alfonso-Snchez","Kristina P. Sendova","Cristin Bravo"],"pdf_url":"https://arxiv.org/pdf/2410.05177v1.pdf","comment":"34 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.02151v3","updated":"2024-10-07T16:35:15Z","published":"2024-04-02T17:58:27Z","title":"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks","summary":"  We show that even the most recent safety-aligned LLMs are not robust to\nsimple adaptive jailbreaking attacks. First, we demonstrate how to successfully\nleverage access to logprobs for jailbreaking: we initially design an\nadversarial prompt template (sometimes adapted to the target LLM), and then we\napply random search on a suffix to maximize a target logprob (e.g., of the\ntoken \"Sure\"), potentially with multiple restarts. In this way, we achieve 100%\nattack success rate -- according to GPT-4 as a judge -- on Vicuna-13B,\nMistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B,\nLlama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that\nwas adversarially trained against the GCG attack. We also show how to jailbreak\nall Claude models -- that do not expose logprobs -- via either a transfer or\nprefilling attack with a 100% success rate. In addition, we show how to use\nrandom search on a restricted set of tokens for finding trojan strings in\npoisoned models -- a task that shares many similarities with jailbreaking --\nwhich is the algorithm that brought us the first place in the SaTML'24 Trojan\nDetection Competition. The common theme behind these attacks is that adaptivity\nis crucial: different models are vulnerable to different prompting templates\n(e.g., R2D2 is very sensitive to in-context learning prompts), some models have\nunique vulnerabilities based on their APIs (e.g., prefilling for Claude), and\nin some settings, it is crucial to restrict the token search space based on\nprior knowledge (e.g., for trojan detection). For reproducibility purposes, we\nprovide the code, logs, and jailbreak artifacts in the JailbreakBench format at\nhttps://github.com/tml-epfl/llm-adaptive-attacks.\n","authors":["Maksym Andriushchenko","Francesco Croce","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2404.02151v3.pdf","comment":"Updates in the v3: GPT-4o and Claude 3.5 Sonnet results, improved\n  writing. Updates in the v2: more models (Llama3, Phi-3, Nemotron-4-340B),\n  jailbreak artifacts for all attacks are available, evaluation with different\n  judges (Llama-3-70B and Llama Guard 2), more experiments (convergence plots\n  over iterations, ablation on the suffix length for random search), examples\n  of jailbroken generation"},{"id":"http://arxiv.org/abs/2310.09675v2","updated":"2024-10-07T16:28:52Z","published":"2023-10-14T22:24:26Z","title":"Efficient Model-Agnostic Multi-Group Equivariant Networks","summary":"  Constructing model-agnostic group equivariant networks, such as equitune\n(Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be\ncomputationally expensive for large product groups. We address this problem by\nproviding efficient model-agnostic equivariant designs for two related\nproblems: one where the network has multiple inputs each with potentially\ndifferent groups acting on them, and another where there is a single input but\nthe group acting on it is a large product group. For the first design, we\ninitially consider a linear model and characterize the entire equivariant space\nthat satisfies this constraint. This characterization gives rise to a novel\nfusion layer between different channels that satisfies an invariance-symmetry\n(IS) constraint, which we call an IS layer. We then extend this design beyond\nlinear models, similar to equitune, consisting of equivariant and IS layers. We\nalso show that the IS layer is a universal approximator of invariant-symmetric\nfunctions. Inspired by the first design, we use the notion of the IS property\nto design a second efficient model-agnostic equivariant design for large\nproduct groups acting on a single input. For the first design, we provide\nexperiments on multi-image classification where each view is transformed\nindependently with transformations such as rotations. We find equivariant\nmodels are robust to such transformations and perform competitively otherwise.\nFor the second design, we consider three applications: language\ncompositionality on the SCAN dataset to product groups; fairness in natural\nlanguage generation from GPT-2 to address intersectionality; and robust\nzero-shot image classification with CLIP. Overall, our methods are simple and\ngeneral, competitive with equitune and its variants, while also being\ncomputationally more efficient.\n","authors":["Razan Baltaji","Sourya Basu","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2310.09675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v2","updated":"2024-10-07T16:26:00Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses \"You are a helpful assistant\" as part of its\ndefault system prompt. Despite current practices of adding personas to system\nprompts, it remains unclear how different personas affect a model's performance\non objective tasks. In this study, we present a systematic evaluation of\npersonas in system prompts. We curate a list of 162 roles covering 6 types of\ninterpersonal relationships and 8 domains of expertise. Through extensive\nanalysis of 4 popular families of LLMs and 2,410 factual questions, we\ndemonstrate that adding personas in system prompts does not improve model\nperformance across a range of questions compared to the control setting where\nno persona is added. Nevertheless, further analysis suggests that the gender,\ntype, and domain of the persona can all influence the resulting prediction\naccuracies. We further experimented with a list of persona search strategies\nand found that, while aggregating results from the best persona for each\nquestion significantly improves prediction accuracy, automatically identifying\nthe best persona is challenging, with predictions often performing no better\nthan random selection. Overall, our findings suggest that while adding a\npersona may lead to performance gains in certain settings, the effect of each\npersona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10207v2","updated":"2024-10-07T16:25:34Z","published":"2024-07-14T14:01:38Z","title":"Learning to Steer Markovian Agents under Model Uncertainty","summary":"  Designing incentives for an adapting population is a ubiquitous problem in a\nwide array of economic applications and beyond. In this work, we study how to\ndesign additional rewards to steer multi-agent systems towards desired policies\n\\emph{without} prior knowledge of the agents' underlying learning dynamics.\nMotivated by the limitation of existing works, we consider a new and general\ncategory of learning dynamics called \\emph{Markovian agents}. We introduce a\nmodel-based non-episodic Reinforcement Learning (RL) formulation for our\nsteering problem. Importantly, we focus on learning a \\emph{history-dependent}\nsteering strategy to handle the inherent model uncertainty about the agents'\nlearning dynamics. We introduce a novel objective function to encode the\ndesiderata of achieving a good steering outcome with reasonable cost.\nTheoretically, we identify conditions for the existence of steering strategies\nto guide agents to the desired policies. Complementing our theoretical\ncontributions, we provide empirical algorithms to approximately solve our\nobjective, which effectively tackles the challenge in learning\nhistory-dependent strategies. We demonstrate the efficacy of our algorithms\nthrough empirical evaluations.\n","authors":["Jiawei Huang","Vinzenz Thoma","Zebang Shen","Heinrich H. Nax","Niao He"],"pdf_url":"https://arxiv.org/pdf/2407.10207v2.pdf","comment":"34 Pages"},{"id":"http://arxiv.org/abs/2403.18681v2","updated":"2024-10-07T16:25:02Z","published":"2024-03-27T15:24:54Z","title":"Deep Fusion: Capturing Dependencies in Contrastive Learning via\n  Transformer Projection Heads","summary":"  Contrastive Learning (CL) has emerged as a powerful method for training\nfeature extraction models using unlabeled data. Recent studies suggest that\nincorporating a linear projection head post-backbone significantly enhances\nmodel performance. In this work, we investigate the use of a transformer model\nas a projection head within the CL framework, aiming to exploit the\ntransformer's capacity for capturing long-range dependencies across embeddings\nto further improve performance. Our key contributions are fourfold: First, we\nintroduce a novel application of transformers in the projection head role for\ncontrastive learning, marking the first endeavor of its kind. Second, our\nexperiments reveal a compelling \"Deep Fusion\" phenomenon where the attention\nmechanism progressively captures the correct relational dependencies among\nsamples from the same class in deeper layers. Third, we provide a theoretical\nframework that explains and supports this \"Deep Fusion\" behavior. Finally, we\ndemonstrate through experimental results that our model achieves superior\nperformance compared to the existing approach of using a feed-forward layer.\n","authors":["Huanran Li","Daniel Pimentel-Alarcn"],"pdf_url":"https://arxiv.org/pdf/2403.18681v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.05167v1","updated":"2024-10-07T16:24:18Z","published":"2024-10-07T16:24:18Z","title":"Presto! Distilling Steps and Layers for Accelerating Music Generation","summary":"  Despite advances in diffusion-based text-to-music (TTM) methods, efficient,\nhigh-quality generation remains a challenge. We introduce Presto!, an approach\nto inference acceleration for score-based diffusion transformers via reducing\nboth sampling steps and cost per step. To reduce steps, we develop a new\nscore-based distribution matching distillation (DMD) method for the EDM-family\nof diffusion models, the first GAN-based distillation method for TTM. To reduce\nthe cost per step, we develop a simple, but powerful improvement to a recent\nlayer distillation method that improves learning via better preserving hidden\nstate variance. Finally, we combine our step and layer distillation methods\ntogether for a dual-faceted approach. We evaluate our step and layer\ndistillation methods independently and show each yield best-in-class\nperformance. Our combined distillation method can generate high-quality outputs\nwith improved diversity, accelerating our base model by 10-18x (230/435ms\nlatency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --\nthe fastest high-quality TTM to our knowledge. Sound examples can be found at\nhttps://presto-music.github.io/web/.\n","authors":["Zachary Novack","Ge Zhu","Jonah Casebeer","Julian McAuley","Taylor Berg-Kirkpatrick","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2410.05167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05109v2","updated":"2024-10-07T16:21:29Z","published":"2024-02-07T18:58:50Z","title":"Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding","summary":"  To combat the memory bandwidth-bound nature of autoregressive LLM inference,\nprevious research has proposed the speculative decoding frame-work. To perform\nspeculative decoding, a small draft model proposes candidate continuations of\nthe input sequence that are then verified in parallel by the base model. One\nway to specify the draft model, as used in the recent Medusa decoding\nframework, is as a collection of lightweight heads, called draft heads, that\noperate on the base model's hidden states. To date, all existing draft heads\nhave been sequentially independent, meaning that they speculate tokens in the\ncandidate continuation independently of any preceding tokens in the candidate\ncontinuation. In this work, we propose Hydra heads: a sequentially-dependent\ndrop-in replacement for standard draft heads that significantly improves the\naccuracy of draft head speculation. We further explore the design space of\nHydra head training objectives and architectures, and propose a carefully tuned\nHydra head recipe, which we call Hydra++, that improves decoding throughput by\nup to 1.31x and 2.70x compared to Medusa decoding and autoregressive de-coding\nrespectively. Overall, Hydra heads are a simple and well-motivated intervention\non standard draft heads that significantly improve the end-to-end speed of\ndraft head-based speculative decoding. We make our code publicly available at\nhttps://github.com/zankner/Hydra.\n","authors":["Zachary Ankner","Rishab Parthasarathy","Aniruddha Nrusimha","Christopher Rinard","Jonathan Ragan-Kelley","William Brandon"],"pdf_url":"https://arxiv.org/pdf/2402.05109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06357v4","updated":"2024-10-07T16:19:17Z","published":"2024-02-09T12:07:06Z","title":"The SkipSponge Attack: Sponge Weight Poisoning of Deep Neural Networks","summary":"  Sponge attacks aim to increase the energy consumption and computation time of\nneural networks. In this work, we present a novel sponge attack called\nSkipSponge. SkipSponge is the first sponge attack that is performed directly on\nthe parameters of a pre-trained model using only a few data samples. Our\nexperiments show that SkipSponge can successfully increase the energy\nconsumption of image classification models, GANs, and autoencoders requiring\nfewer samples than the state-of-the-art (Sponge Poisoning). We show that\npoisoning defenses are ineffective if not adjusted specifically for the defense\nagainst SkipSponge (i.e., they decrease target layer bias values). Our work\nshows that SkipSponge is more effective on the GANs and the autoencoders than\nSponge Poisoning. Additionally, SkipSponge is stealthier than Sponge Poisoning\nas it does not require significant changes in the victim model's weights. Our\nexperiments indicate that SkipSponge can be performed even when an attacker has\naccess to only 1% of the entire dataset and reaches up to 13% energy increase.\n","authors":["Jona te Lintelo","Stefanos Koffas","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2402.06357v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05163v1","updated":"2024-10-07T16:16:53Z","published":"2024-10-07T16:16:53Z","title":"A Simulation-Free Deep Learning Approach to Stochastic Optimal Control","summary":"  We propose a simulation-free algorithm for the solution of generic problems\nin stochastic optimal control (SOC). Unlike existing methods, our approach does\nnot require the solution of an adjoint problem, but rather leverages Girsanov\ntheorem to directly calculate the gradient of the SOC objective on-policy. This\nallows us to speed up the optimization of control policies parameterized by\nneural networks since it completely avoids the expensive back-propagation step\nthrough stochastic differential equations (SDEs) used in the Neural SDE\nframework. In particular, it enables us to solve SOC problems in high dimension\nand on long time horizons. We demonstrate the efficiency of our approach in\nvarious domains of applications, including standard stochastic optimal control\nproblems, sampling from unnormalized distributions via construction of a\nSchr\\\"odinger-F\\\"ollmer process, and fine-tuning of pre-trained diffusion\nmodels. In all cases our method is shown to outperform the existing methods in\nboth the computing time and memory efficiency.\n","authors":["Mengjian Hua","Matthieu Laurire","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2410.05163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03986v6","updated":"2024-10-07T16:15:36Z","published":"2023-10-06T03:04:21Z","title":"Robust Multimodal Learning with Missing Modalities via\n  Parameter-Efficient Adaptation","summary":"  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose a simple and parameter-efficient adaptation\nprocedure for pretrained multimodal networks. In particular, we exploit\nmodulation of intermediate features to compensate for the missing modalities.\nWe demonstrate that such adaptation can partially bridge performance drop due\nto missing modalities and outperform independent, dedicated networks trained\nfor the available modality combinations in some cases. The proposed adaptation\nrequires extremely small number of parameters (e.g., fewer than 1% of the total\nparameters) and applicable to a wide range of modality combinations and tasks.\nWe conduct a series of experiments to highlight the missing modality robustness\nof our proposed method on five different multimodal tasks across seven\ndatasets. Our proposed method demonstrates versatility across various tasks and\ndatasets, and outperforms existing methods for robust multimodal learning with\nmissing modalities.\n","authors":["Md Kaykobad Reza","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2310.03986v6.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). 28 pages, 6 figures, 17 tables"},{"id":"http://arxiv.org/abs/2403.18699v2","updated":"2024-10-07T16:07:23Z","published":"2024-03-27T15:48:16Z","title":"Preventing Collapse in Contrastive Learning with Orthonormal Prototypes\n  (CLOP)","summary":"  Contrastive learning has emerged as a powerful method in deep learning,\nexcelling at learning effective representations through contrasting samples\nfrom different distributions. However, neural collapse, where embeddings\nconverge into a lower-dimensional space, poses a significant challenge,\nespecially in semi-supervised and self-supervised setups. In this paper, we\nfirst theoretically analyze the effect of large learning rates on contrastive\nlosses that solely rely on the cosine similarity metric, and derive a\ntheoretical bound to mitigate this collapse. {Building on these insights, we\npropose CLOP, a novel semi-supervised loss function designed to prevent neural\ncollapse by promoting the formation of orthogonal linear subspaces among class\nembeddings.} Unlike prior approaches that enforce a simplex ETF structure, CLOP\nfocuses on subspace separation, leading to more distinguishable embeddings.\nThrough extensive experiments on real and synthetic datasets, we demonstrate\nthat CLOP enhances performance, providing greater stability across different\nlearning rates and batch sizes.\n","authors":["Huanran Li","Manh Nguyen","Daniel Pimentel-Alarcn"],"pdf_url":"https://arxiv.org/pdf/2403.18699v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2309.15608v2","updated":"2024-10-07T16:05:53Z","published":"2023-09-27T12:15:05Z","title":"NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit\n  sensitivity maps","summary":"  We present a novel learned image reconstruction method for accelerated\ncardiac MRI with multiple receiver coils based on deep convolutional neural\nnetworks (CNNs) and algorithm unrolling. In contrast to many existing learned\nMR image reconstruction techniques that necessitate coil-sensitivity map (CSM)\nestimation as a distinct network component, our proposed approach avoids\nexplicit CSM estimation. Instead, it implicitly captures and learns to exploit\nthe inter-coil relationships of the images. Our method consists of a series of\nnovel learned image and k-space blocks with shared latent information and\nadaptation to the acquisition parameters by feature-wise modulation (FiLM), as\nwell as coil-wise data-consistency (DC) blocks.\n  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920\nand 0.942 in the cine track and mapping track validation leaderboard of the\nMICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different\nteams at the time of writing.\n  Code will be made available at https://github.com/fzimmermann89/CMRxRecon\n","authors":["Felix Frederik Zimmermann","Andreas Kofler"],"pdf_url":"https://arxiv.org/pdf/2309.15608v2.pdf","comment":"Accepted at MICCAI STACOM 2023"},{"id":"http://arxiv.org/abs/2405.14577v2","updated":"2024-10-07T16:01:49Z","published":"2024-05-23T13:51:55Z","title":"Representation noising effectively prevents harmful fine-tuning on LLMs","summary":"  Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that is\neffective even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the effectiveness of our defence lies in its \"depth\": the degree\nto which information about harmful representations is removed across all layers\nof the LLM.\n","authors":["Domenic Rosati","Jan Wehner","Kai Williams","ukasz Bartoszcze","David Atanasov","Robie Gonzales","Subhabrata Majumdar","Carsten Maple","Hassan Sajjad","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2405.14577v2.pdf","comment":"Published in NeurIPs 2024"},{"id":"http://arxiv.org/abs/2410.05147v1","updated":"2024-10-07T16:00:27Z","published":"2024-10-07T16:00:27Z","title":"PAMLR: A Passive-Active Multi-Armed Bandit-Based Solution for LoRa\n  Channel Allocation","summary":"  Achieving low duty cycle operation in low-power wireless networks in urban\nenvironments is complicated by the complex and variable dynamics of external\ninterference and fading. We explore the use of reinforcement learning for\nachieving low power consumption for the task of optimal selection of channels.\nThe learning relies on a hybrid of passive channel sampling for dealing with\nexternal interference and active channel sampling for dealing with fading. Our\nsolution, Passive-Active Multi-armed bandit for LoRa (PAMLR, pronounced\n\"Pamela\"), balances the two types of samples to achieve energy-efficient\nchannel selection: active channel measurements are tuned to an appropriately\nlow level to update noise thresholds, and to compensate passive channel\nmeasurements are tuned to an appropriately high level for selecting the\ntop-most channels from channel exploration using the noise thresholds. The\nrates of both types of samples are adapted in response to channel dynamics.\nBased on extensive testing in multiple environments in different cities, we\nvalidate that PAMLR can maintain excellent communication quality, as\ndemonstrated by a low SNR regret compared to the optimal channel allocation\npolicy, while substantially minimizing the energy cost associated with channel\nmeasurements.\n","authors":["Jihoon Yun","Chengzhang Li","Anish Arora"],"pdf_url":"https://arxiv.org/pdf/2410.05147v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.14995v2","updated":"2024-10-07T15:59:03Z","published":"2024-06-21T09:14:11Z","title":"Differentiable and Learnable Wireless Simulation with Geometric\n  Transformers","summary":"  Modelling the propagation of electromagnetic wireless signals is critical for\ndesigning modern communication systems. Wireless ray tracing simulators model\nsignal propagation based on the 3D geometry and other scene parameters, but\ntheir accuracy is fundamentally limited by underlying modelling assumptions and\ncorrectness of parameters. In this work, we introduce Wi-GATr, a\nfully-learnable neural simulation surrogate designed to predict the channel\nobservations based on scene primitives (e.g., surface mesh, antenna position\nand orientation). Recognizing the inherently geometric nature of these\nprimitives, Wi-GATr leverages an equivariant Geometric Algebra Transformer that\noperates on a tokenizer specifically tailored for wireless simulation. We\nevaluate our approach on a range of tasks (i.e., signal strength and delay\nspread prediction, receiver localization, and geometry reconstruction) and find\nthat Wi-GATr is accurate, fast, sample-efficient, and robust to\nsymmetry-induced transformations. Remarkably, we find our results also\ntranslate well to the real world: Wi-GATr demonstrates more than 35% lower\nerror than hybrid techniques, and 70% lower error than a calibrated wireless\ntracer.\n","authors":["Thomas Hehn","Markus Peschl","Tribhuvanesh Orekondy","Arash Behboodi","Johann Brehmer"],"pdf_url":"https://arxiv.org/pdf/2406.14995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05108v2","updated":"2024-10-07T15:57:38Z","published":"2024-04-07T23:34:51Z","title":"Efficient Gradient Estimation of Variational Quantum Circuits with Lie\n  Algebraic Symmetries","summary":"  Hybrid quantum-classical optimization and learning strategies are among the\nmost promising approaches to harnessing quantum information or gaining a\nquantum advantage over classical methods. However, efficient estimation of the\ngradient of the objective function in such models remains a challenge due to\nseveral factors including the exponential dimensionality of the Hilbert spaces,\nand information loss of quantum measurements. In this work, we developed an\nefficient framework that makes the Hadamard test efficiently applicable to\ngradient estimation for a broad range of quantum systems, an advance that had\nbeen wanting from the outset. Under certain mild structural assumptions, the\ngradient is estimated with the measurement shots that scale logarithmically\nwith the number of parameters and with polynomial classical and quantum time.\nThis is an exponential reduction in the measurement cost and polynomial speed\nup in time compared to existing works. The structural assumptions are (1) the\ndimension of the dynamical Lie algebra is polynomial in the number of qubits,\nand (2) the observable has a bounded Hilbert-Schmidt norm.\n","authors":["Mohsen Heidari","Masih Mozakka","Wojciech Szpankowski"],"pdf_url":"https://arxiv.org/pdf/2404.05108v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2407.10930v2","updated":"2024-10-07T15:52:48Z","published":"2024-07-15T17:30:31Z","title":"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together","summary":"  Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai\n","authors":["Dilara Soylu","Christopher Potts","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2407.10930v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.05140v1","updated":"2024-10-07T15:50:30Z","published":"2024-10-07T15:50:30Z","title":"Tuning-Free Bilevel Optimization: New Algorithms and Convergence\n  Analysis","summary":"  Bilevel optimization has recently attracted considerable attention due to its\nabundant applications in machine learning problems. However, existing methods\nrely on prior knowledge of problem parameters to determine stepsizes, resulting\nin significant effort in tuning stepsizes when these parameters are unknown. In\nthis paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO.\nD-TFBO employs a double-loop structure with stepsizes adaptively adjusted by\nthe \"inverse of cumulative gradient norms\" strategy. S-TFBO features a simpler\nfully single-loop structure that updates three variables simultaneously with a\ntheory-motivated joint design of adaptive stepsizes for all variables. We\nprovide a comprehensive convergence analysis for both algorithms and show that\nD-TFBO and S-TFBO respectively require $O(\\frac{1}{\\epsilon})$ and\n$O(\\frac{1}{\\epsilon}\\log^4(\\frac{1}{\\epsilon}))$ iterations to find an\n$\\epsilon$-accurate stationary point, (nearly) matching their well-tuned\ncounterparts using the information of problem parameters. Experiments on\nvarious problems show that our methods achieve performance comparable to\nexisting well-tuned approaches, while being more robust to the selection of\ninitial stepsizes. To the best of our knowledge, our methods are the first to\ncompletely eliminate the need for stepsize tuning, while achieving theoretical\nguarantees.\n","authors":["Yifan Yang","Hao Ban","Minhui Huang","Shiqian Ma","Kaiyi Ji"],"pdf_url":"https://arxiv.org/pdf/2410.05140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03853v5","updated":"2024-10-07T15:46:59Z","published":"2023-12-06T19:07:38Z","title":"Dr. Jekyll and Mr. Hyde: Two Faces of LLMs","summary":"  Recently, we have witnessed a rise in the use of Large Language Models\n(LLMs), especially in applications like chatbots. Safety mechanisms are\nimplemented to prevent improper responses from these chatbots. In this work, we\nbypass these measures for ChatGPT and Gemini by making them impersonate complex\npersonas with personality characteristics that are not aligned with a truthful\nassistant. First, we create elaborate biographies of these personas, which we\nthen use in a new session with the same chatbots. Our conversations then follow\na role-play style to elicit prohibited responses. Using personas, we show that\nprohibited responses are provided, making it possible to obtain unauthorized,\nillegal, or harmful information in both ChatGPT and Gemini. We also introduce\nseveral ways of activating such adversarial personas, showing that both\nchatbots are vulnerable to this attack. With the same principle, we introduce\ntwo defenses that push the model to interpret trustworthy personalities and\nmake it more robust against such attacks.\n","authors":["Matteo Gioele Collu","Tom Janssen-Groesbeek","Stefanos Koffas","Mauro Conti","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2312.03853v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07917v3","updated":"2024-10-07T15:45:38Z","published":"2024-02-27T15:59:15Z","title":"A Neural-Evolutionary Algorithm for Autonomous Transit Network Design","summary":"  Planning a public transit network is a challenging optimization problem, but\nessential in order to realize the benefits of autonomous buses. We propose a\nnovel algorithm for planning networks of routes for autonomous buses. We first\ntrain a graph neural net model as a policy for constructing route networks, and\nthen use the policy as one of several mutation operators in a evolutionary\nalgorithm. We evaluate this algorithm on a standard set of benchmarks for\ntransit network design, and find that it outperforms the learned policy alone\nby up to 20% and a plain evolutionary algorithm approach by up to 53% on\nrealistic benchmark instances.\n","authors":["Andrew Holliday","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2403.07917v3.pdf","comment":"Copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works. arXiv admin note: text overlap with\n  arXiv:2306.00720"},{"id":"http://arxiv.org/abs/2410.05136v1","updated":"2024-10-07T15:43:28Z","published":"2024-10-07T15:43:28Z","title":"LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles","summary":"  Transferability of adversarial examples is a well-known property that\nendangers all classification models, even those that are only accessible\nthrough black-box queries. Prior work has shown that an ensemble of models is\nmore resilient to transferability: the probability that an adversarial example\nis effective against most models of the ensemble is low. Thus, most ongoing\nresearch focuses on improving ensemble diversity. Another line of prior work\nhas shown that Lipschitz continuity of the models can make models more robust\nsince it limits how a model's output changes with small input perturbations. In\nthis paper, we study the effect of Lipschitz continuity on transferability\nrates. We show that although a lower Lipschitz constant increases the\nrobustness of a single model, it is not as beneficial in training robust\nensembles as it increases the transferability rate of adversarial examples\nacross models in the ensemble. Therefore, we introduce LOTOS, a new training\nparadigm for ensembles, which counteracts this adverse effect. It does so by\npromoting orthogonality among the top-$k$ sub-spaces of the transformations of\nthe corresponding affine layers of any pair of models in the ensemble. We\ntheoretically show that $k$ does not need to be large for convolutional layers,\nwhich makes the computational overhead negligible. Through various experiments,\nwe show LOTOS increases the robust accuracy of ensembles of ResNet-18 models by\n$6$ percentage points (p.p) against black-box attacks on CIFAR-10. It is also\ncapable of combining with the robustness of prior state-of-the-art methods for\ntraining robust ensembles to enhance their robust accuracy by $10.7$ p.p.\n","authors":["Ali Ebrahimpour-Boroojeny","Hari Sundaram","Varun Chandrasekaran"],"pdf_url":"https://arxiv.org/pdf/2410.05136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05133v1","updated":"2024-10-07T15:36:50Z","published":"2024-10-07T15:36:50Z","title":"A Digital Twin Framework for Liquid-cooled Supercomputers as\n  Demonstrated at Exascale","summary":"  We present ExaDigiT, an open-source framework for developing comprehensive\ndigital twins of liquid-cooled supercomputers. It integrates three main\nmodules: (1) a resource allocator and power simulator, (2) a transient\nthermo-fluidic cooling model, and (3) an augmented reality model of the\nsupercomputer and central energy plant. The framework enables the study of\n\"what-if\" scenarios, system optimizations, and virtual prototyping of future\nsystems. Using Frontier as a case study, we demonstrate the framework's\ncapabilities by replaying six months of system telemetry for systematic\nverification and validation. Such a comprehensive analysis of a liquid-cooled\nexascale supercomputer is the first of its kind. ExaDigiT elucidates complex\ntransient cooling system dynamics, runs synthetic or real workloads, and\npredicts energy losses due to rectification and voltage conversion. Throughout\nour paper, we present lessons learned to benefit HPC practitioners developing\nsimilar digital twins. We envision the digital twin will be a key enabler for\nsustainable, energy-efficient supercomputing.\n","authors":["Wesley Brewer","Matthias Maiterth","Vineet Kumar","Rafal Wojda","Sedrick Bouknight","Jesse Hines","Woong Shin","Scott Greenwood","David Grant","Wesley Williams","Feiyi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05133v1.pdf","comment":"14 pages, 9 figures, To be published in the Proceedings of the\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis. 2024"},{"id":"http://arxiv.org/abs/2406.16424v2","updated":"2024-10-07T15:33:37Z","published":"2024-06-24T08:18:19Z","title":"Memory-Enhanced Neural Solvers for Efficient Adaptation in Combinatorial\n  Optimization","summary":"  Combinatorial Optimization is crucial to numerous real-world applications,\nyet still presents challenges due to its (NP-)hard nature. Amongst existing\napproaches, heuristics often offer the best trade-off between quality and\nscalability, making them suitable for industrial use. While Reinforcement\nLearning (RL) offers a flexible framework for designing heuristics, its\nadoption over handcrafted heuristics remains incomplete within industrial\nsolvers. Existing learned methods still lack the ability to adapt to specific\ninstances and fully leverage the available computational budget. The current\nbest methods either rely on a collection of pre-trained policies, or on\ndata-inefficient fine-tuning; hence failing to fully utilize newly available\ninformation within the constraints of the budget. In response, we present\nMEMENTO, an approach that leverages memory to improve the adaptation of neural\nsolvers at inference time. MEMENTO enables updating the action distribution\ndynamically based on the outcome of previous decisions. We validate its\neffectiveness on benchmark problems, in particular Traveling Salesman and\nCapacitated Vehicle Routing, demonstrating its superiority over tree-search and\npolicy-gradient fine-tuning; and showing it can be zero-shot combined with\ndiversity-based solvers. We successfully train all RL auto-regressive solvers\non large instances, and show that MEMENTO can scale and is data-efficient.\nOverall, MEMENTO enables to push the state-of-the-art on 11 out of 12 evaluated\ntasks.\n","authors":["Felix Chalumeau","Refiloe Shabe","Noah De Nicola","Arnu Pretorius","Thomas D. Barrett","Nathan Grinsztajn"],"pdf_url":"https://arxiv.org/pdf/2406.16424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12282v2","updated":"2024-10-07T15:29:14Z","published":"2024-04-18T15:58:31Z","title":"Investigating Guiding Information for Adaptive Collocation Point\n  Sampling in PINNs","summary":"  Physics-informed neural networks (PINNs) provide a means of obtaining\napproximate solutions of partial differential equations and systems through the\nminimisation of an objective function which includes the evaluation of a\nresidual function at a set of collocation points within the domain. The quality\nof a PINNs solution depends upon numerous parameters, including the number and\ndistribution of these collocation points. In this paper we consider a number of\nstrategies for selecting these points and investigate their impact on the\noverall accuracy of the method. In particular, we suggest that no single\napproach is likely to be \"optimal\" but we show how a number of important\nmetrics can have an impact in improving the quality of the results obtained\nwhen using a fixed number of residual evaluations. We illustrate these\napproaches through the use of two benchmark test problems: Burgers' equation\nand the Allen-Cahn equation.\n","authors":["Jose Florido","He Wang","Amirul Khan","Peter K. Jimack"],"pdf_url":"https://arxiv.org/pdf/2404.12282v2.pdf","comment":"15 pages, 8 figures, 2 tables. Published in the conference\n  proceedings of the International Conference on Computational Science (ICCS)\n  2024. Replacement to correct a typo regarding the value of viscosity listed\n  in the captions"},{"id":"http://arxiv.org/abs/2410.05124v1","updated":"2024-10-07T15:25:21Z","published":"2024-10-07T15:25:21Z","title":"Agnostic Smoothed Online Learning","summary":"  Classical results in statistical learning typically consider two extreme\ndata-generating models: i.i.d. instances from an unknown distribution, or fully\nadversarial instances, often much more challenging statistically. To bridge the\ngap between these models, recent work introduced the smoothed framework, in\nwhich at each iteration an adversary generates instances from a distribution\nconstrained to have density bounded by $\\sigma^{-1}$ compared to some fixed\nbase measure $\\mu$. This framework interpolates between the i.i.d. and\nadversarial cases, depending on the value of $\\sigma$. For the classical online\nprediction problem, most prior results in smoothed online learning rely on the\narguably strong assumption that the base measure $\\mu$ is known to the learner,\ncontrasting with standard settings in the PAC learning or consistency\nliterature. We consider the general agnostic problem in which the base measure\nis unknown and values are arbitrary. Along this direction, Block et al. showed\nthat empirical risk minimization has sublinear regret under the well-specified\nassumption. We propose an algorithm R-Cover based on recursive coverings which\nis the first to guarantee sublinear regret for agnostic smoothed online\nlearning without prior knowledge of $\\mu$. For classification, we prove that\nR-Cover has adaptive regret $\\tilde O(\\sqrt{dT/\\sigma})$ for function classes\nwith VC dimension $d$, which is optimal up to logarithmic factors. For\nregression, we establish that R-Cover has sublinear oblivious regret for\nfunction classes with polynomial fat-shattering dimension growth.\n","authors":["Mose Blanchard"],"pdf_url":"https://arxiv.org/pdf/2410.05124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00428v2","updated":"2024-10-07T15:24:10Z","published":"2024-10-01T06:23:17Z","title":"LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management","summary":"  The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.\n","authors":["Yi Xiong","Hao Wu","Changxu Shao","Ziqing Wang","Rui Zhang","Yuhong Guo","Junping Zhao","Ke Zhang","Zhenxuan Pan"],"pdf_url":"https://arxiv.org/pdf/2410.00428v2.pdf","comment":"11 pages, 7 figures, 1 table"},{"id":"http://arxiv.org/abs/2401.03192v2","updated":"2024-10-07T15:21:37Z","published":"2024-01-06T11:13:16Z","title":"On the Convergence of Hermitian Dynamic Mode Decomposition","summary":"  We study the convergence of Hermitian Dynamic Mode Decomposition (DMD) to the\nspectral properties of self-adjoint Koopman operators. Hermitian DMD is a\ndata-driven method that approximates the Koopman operator associated with an\nunknown nonlinear dynamical system, using discrete-time snapshots. This\napproach preserves the self-adjointness of the operator in its\nfinite-dimensional approximations. \\rev{We prove that, under suitably broad\nconditions, the spectral measures corresponding to the eigenvalues and\neigenfunctions computed by Hermitian DMD converge to those of the underlying\nKoopman operator}. This result also applies to skew-Hermitian systems (after\nmultiplication by $i$), applicable to generators of continuous-time\nmeasure-preserving systems. Along the way, we establish a general theorem on\nthe convergence of spectral measures for finite sections of self-adjoint\noperators, including those that are unbounded, which is of independent interest\nto the wider spectral community. We numerically demonstrate our results by\napplying them to two-dimensional Schr\\\"odinger equations.\n","authors":["Nicolas Boull","Matthew J. Colbrook"],"pdf_url":"https://arxiv.org/pdf/2401.03192v2.pdf","comment":"24 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2312.00137"},{"id":"http://arxiv.org/abs/2410.05117v1","updated":"2024-10-07T15:14:58Z","published":"2024-10-07T15:14:58Z","title":"Assouad, Fano, and Le Cam with Interaction: A Unifying Lower Bound\n  Framework and Characterization for Bandit Learnability","summary":"  In this paper, we develop a unified framework for lower bound methods in\nstatistical estimation and interactive decision making. Classical lower bound\ntechniques -- such as Fano's inequality, Le Cam's method, and Assouad's lemma\n-- have been central to the study of minimax risk in statistical estimation,\nyet they are insufficient for the analysis of methods that collect data in an\ninteractive manner. The recent minimax lower bounds for interactive decision\nmaking via the Decision-Estimation Coefficient (DEC) appear to be genuinely\ndifferent from the classical methods. We propose a unified view of these\ndistinct methodologies through a general algorithmic lower bound method. We\nfurther introduce a novel complexity measure, decision dimension, which\nfacilitates the derivation of new lower bounds for interactive decision making.\nIn particular, decision dimension provides a characterization of bandit\nlearnability for any structured bandit model class. Further, we characterize\nthe sample complexity of learning convex model class up to a polynomial gap\nwith the decision dimension, addressing the remaining gap between upper and\nlower bounds in Foster et al. (2021, 2023).\n","authors":["Fan Chen","Dylan J. Foster","Yanjun Han","Jian Qian","Alexander Rakhlin","Yunbei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.05117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05116v1","updated":"2024-10-07T15:12:01Z","published":"2024-10-07T15:12:01Z","title":"Human-Feedback Efficient Reinforcement Learning for Online Diffusion\n  Model Finetuning","summary":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback.\n","authors":["Ayano Hiranaka","Shang-Fu Chen","Chieh-Hsin Lai","Dongjun Kim","Naoki Murata","Takashi Shibuya","Wei-Hsiang Liao","Shao-Hua Sun","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.05116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05107v1","updated":"2024-10-07T15:03:00Z","published":"2024-10-07T15:03:00Z","title":"Hyper-Representations: Learning from Populations of Neural Networks","summary":"  This thesis addresses the challenge of understanding Neural Networks through\nthe lens of their most fundamental component: the weights, which encapsulate\nthe learned information and determine the model behavior. At the core of this\nthesis is a fundamental question: Can we learn general, task-agnostic\nrepresentations from populations of Neural Network models? The key contribution\nof this thesis to answer that question are hyper-representations, a\nself-supervised method to learn representations of NN weights. Work in this\nthesis finds that trained NN models indeed occupy meaningful structures in the\nweight space, that can be learned and used. Through extensive experiments, this\nthesis demonstrates that hyper-representations uncover model properties, such\nas their performance, state of training, or hyperparameters. Moreover, the\nidentification of regions with specific properties in hyper-representation\nspace allows to sample and generate model weights with targeted properties.\nThis thesis demonstrates applications for fine-tuning, and transfer learning to\ngreat success. Lastly, it presents methods that allow hyper-representations to\ngeneralize beyond model sizes, architectures, and tasks. The practical\nimplications of that are profound, as it opens the door to foundation models of\nNeural Networks, which aggregate and instantiate their knowledge across models\nand architectures. Ultimately, this thesis contributes to the deeper\nunderstanding of Neural Networks by investigating structures in their weights\nwhich leads to more interpretable, efficient, and adaptable models. By laying\nthe groundwork for representation learning of NN weights, this research\ndemonstrates the potential to change the way Neural Networks are developed,\nanalyzed, and used.\n","authors":["Konstantin Schrholt"],"pdf_url":"https://arxiv.org/pdf/2410.05107v1.pdf","comment":"PhD Dissertation accepted at University of St. Gallen"},{"id":"http://arxiv.org/abs/2410.05106v1","updated":"2024-10-07T15:02:48Z","published":"2024-10-07T15:02:48Z","title":"Nonasymptotic Analysis of Stochastic Gradient Descent with the\n  Richardson-Romberg Extrapolation","summary":"  We address the problem of solving strongly convex and smooth minimization\nproblems using stochastic gradient descent (SGD) algorithm with a constant step\nsize. Previous works suggested to combine the Polyak-Ruppert averaging\nprocedure with the Richardson-Romberg extrapolation technique to reduce the\nasymptotic bias of SGD at the expense of a mild increase of the variance. We\nsignificantly extend previous results by providing an expansion of the\nmean-squared error of the resulting estimator with respect to the number of\niterations $n$. More precisely, we show that the mean-squared error can be\ndecomposed into the sum of two terms: a leading one of order\n$\\mathcal{O}(n^{-1/2})$ with explicit dependence on a minimax-optimal\nasymptotic covariance matrix, and a second-order term of order\n$\\mathcal{O}(n^{-3/4})$ where the power $3/4$ can not be improved in general.\nWe also extend this result to the $p$-th moment bound keeping optimal scaling\nof the remainders with respect to $n$. Our analysis relies on the properties of\nthe SGD iterates viewed as a time-homogeneous Markov chain. In particular, we\nestablish that this chain is geometrically ergodic with respect to a suitably\ndefined weighted Wasserstein semimetric.\n","authors":["Marina Sheshukova","Denis Belomestny","Alain Durmus","Eric Moulines","Alexey Naumov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2410.05106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15929v2","updated":"2024-10-07T15:01:48Z","published":"2024-02-24T23:16:57Z","title":"Decoding Intelligence: A Framework for Certifying Knowledge\n  Comprehension in LLMs","summary":"  Knowledge comprehension capability is an important aspect of human\nintelligence. As Large Language Models (LLMs) are being envisioned as\nsuperhuman agents, it is crucial for them to be proficient at knowledge\ncomprehension. However, existing benchmarking studies do not provide\nconsistent, generalizable, and formal guarantees on the knowledge comprehension\ncapabilities of LLMs. In this work, we propose the first framework to certify\nknowledge comprehension in LLMs with formal probabilistic guarantees. Our\ncertificates are quantitative -- they consist of high-confidence, tight bounds\non the probability that a target LLM gives the correct answer on any knowledge\ncomprehension prompt sampled from a distribution. We design and certify novel\nspecifications that precisely represent distributions of knowledge\ncomprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for\nspecifications over the Wikidata5m knowledge graph. We find that the knowledge\ncomprehension capability improves significantly with scaling the size of the\nmodels.\n","authors":["Isha Chaudhary","Vedaant V. Jain","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2402.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05102v1","updated":"2024-10-07T15:01:29Z","published":"2024-10-07T15:01:29Z","title":"SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks","summary":"  Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.\n","authors":["Fenia Christopoulou","Ronald Cardenas","Gerasimos Lampouras","Haitham Bou-Ammar","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05102v1.pdf","comment":"20 papges, 9 figures, 5 tables. Under Review"},{"id":"http://arxiv.org/abs/2410.05101v1","updated":"2024-10-07T14:56:07Z","published":"2024-10-07T14:56:07Z","title":"CR-CTC: Consistency regularization on CTC for improved speech\n  recognition","summary":"  Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance compared to transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized\nCTC (CR-CTC), which enforces consistency between two CTC distributions obtained\nfrom different augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC, which achieves\nperformance comparable to, or even slightly better than, that of transducer and\nCTC/AED.\n","authors":["Zengwei Yao","Wei Kang","Xiaoyu Yang","Fangjun Kuang","Liyong Guo","Han Zhu","Zengrui Jin","Zhaoqing Li","Long Lin","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2410.05101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19961v4","updated":"2024-10-07T14:54:18Z","published":"2024-05-30T11:32:42Z","title":"Transition Path Sampling with Improved Off-Policy Training of Diffusion\n  Path Samplers","summary":"  Understanding transition pathways between meta-stable states in molecular\nsystems is crucial to advance material design and drug discovery. However,\nunbiased molecular dynamics simulations are computationally infeasible due to\nthe high energy barriers separating these states. Although recent machine\nlearning techniques offer potential solutions, they are often limited to simple\nsystems or rely on collective variables (CVs) derived from costly domain\nexpertise. In this paper, we introduce a novel approach that trains diffusion\npath samplers (DPS) for transition path sampling (TPS) without the need for\nCVs. We recast the problem as an amortized sampling of the target path measure,\nminimizing the log-variance divergence between the path measure induced by our\nDPS and the target path measure. To ensure scalability for high-dimensional\ntasks, we introduce (1) a new off-policy training objective based on learning\ncontrol variates with replay buffers and (2) a scale-based equivariant\nparameterization of the bias forces. We evaluate our approach, coined TPS-DPS,\non a synthetic double-well potential and three peptides: Alanine Dipeptide,\nPolyproline Helix, and Chignolin. Results show that our approach produces more\nrealistic and diverse transition pathways compared to existing baselines.\n","authors":["Kiyoung Seong","Seonghyun Park","Seonghwan Kim","Woo Youn Kim","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2405.19961v4.pdf","comment":"10 pages, 8 figures, 1 tables"},{"id":"http://arxiv.org/abs/2410.05097v1","updated":"2024-10-07T14:51:54Z","published":"2024-10-07T14:51:54Z","title":"DreamSat: Towards a General 3D Model for Novel View Synthesis of Space\n  Objects","summary":"  Novel view synthesis (NVS) enables to generate new images of a scene or\nconvert a set of 2D images into a comprehensive 3D model. In the context of\nSpace Domain Awareness, since space is becoming increasingly congested, NVS can\naccurately map space objects and debris, improving the safety and efficiency of\nspace operations. Similarly, in Rendezvous and Proximity Operations missions,\n3D models can provide details about a target object's shape, size, and\norientation, allowing for better planning and prediction of the target's\nbehavior. In this work, we explore the generalization abilities of these\nreconstruction techniques, aiming to avoid the necessity of retraining for each\nnew scene, by presenting a novel approach to 3D spacecraft reconstruction from\nsingle-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art\nsingle-view reconstruction model, on a high-quality dataset of 190 high-quality\nspacecraft models and integrating it into the DreamGaussian framework. We\ndemonstrate consistent improvements in reconstruction quality across multiple\nmetrics, including Contrastive Language-Image Pretraining (CLIP) score\n(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity\nIndex (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)\n(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method\naddresses the lack of domain-specific 3D reconstruction tools in the space\nindustry by leveraging state-of-the-art diffusion models and 3D Gaussian\nsplatting techniques. This approach maintains the efficiency of the\nDreamGaussian framework while enhancing the accuracy and detail of spacecraft\nreconstructions. The code for this work can be accessed on GitHub\n(https://github.com/ARCLab-MIT/space-nvs).\n","authors":["Nidhi Mathihalli","Audrey Wei","Giovanni Lavezzi","Peng Mun Siew","Victor Rodriguez-Fernandez","Hodei Urrutxua","Richard Linares"],"pdf_url":"https://arxiv.org/pdf/2410.05097v1.pdf","comment":"Presented at the 75th International Astronautical Congress, October\n  2024, Milan, Italy"},{"id":"http://arxiv.org/abs/2410.05090v1","updated":"2024-10-07T14:42:45Z","published":"2024-10-07T14:42:45Z","title":"HyperINF: Unleashing the HyperPower of the Schulz's Method for Data\n  Influence Estimation","summary":"  Influence functions provide a principled method to assess the contribution of\nindividual training samples to a specific target. Yet, their high computational\ncosts limit their applications on large-scale models and datasets. Existing\nmethods proposed for influence function approximation have significantly\nreduced the computational overheads. However, they mostly suffer from\ninaccurate estimation due to the lack of strong convergence guarantees from the\nalgorithm. The family of hyperpower methods are well-known for their rigorous\nconvergence guarantees on matrix inverse approximation, while the matrix\nmultiplication operation can involve intractable memory and computation costs\non large-scale models. We propose HyperINF, an efficient and accurate influence\nfunction approximation method which leverages the hyperpower method,\nspecifically Schulz's iterative algorithm.\n  To deal with the computation-intensive matrix multiplication, we incorporate\nthe generalized fisher information (GFIM) as a low-rank approximation of the\nHessian matrix, which reduces the memory and computation overheads to constant\ncosts independent of ranks on LoRA-tuned models.\n  We first demonstrate the superior accuracy and stability of \\method compared\nto other baselines through a synthetic convergence simulation for matrix\ninversion. We further validate the efficacy of \\method through extensive\nreal-world data attribution tasks, including mislabeled data detection and data\nselection for LLM and VLM fine-tuning.\n  On LoRA-tuned models, HyperINF achieves superior downstream performance with\nminimal memory and computational overhead, while other baselines suffer from\nsignificant degradation. Our codebase is available at\nhttps://github.com/Blackzxy/HyperINF.\n","authors":["Xinyu Zhou","Simin Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2410.05090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14768v2","updated":"2024-10-07T14:35:14Z","published":"2024-05-23T16:35:52Z","title":"WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.\n","authors":["Peng Wang","Zexi Li","Ningyu Zhang","Ziwen Xu","Yunzhi Yao","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14768v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.05080v1","updated":"2024-10-07T14:33:50Z","published":"2024-10-07T14:33:50Z","title":"ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery","summary":"  The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.\n","authors":["Ziru Chen","Shijie Chen","Yuting Ning","Qianheng Zhang","Boshi Wang","Botao Yu","Yifei Li","Zeyi Liao","Chen Wei","Zitong Lu","Vishal Dey","Mingyi Xue","Frazier N. Baker","Benjamin Burns","Daniel Adu-Ampratwum","Xuhui Huang","Xia Ning","Song Gao","Yu Su","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2410.05080v1.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2410.05078v1","updated":"2024-10-07T14:32:03Z","published":"2024-10-07T14:32:03Z","title":"Compression via Pre-trained Transformers: A Study on Byte-Level\n  Multimodal Data","summary":"  Foundation models have recently been shown to be strong data compressors.\nHowever, when accounting for their excessive parameter count, their compression\nratios are actually inferior to standard compression algorithms. Moreover,\nnaively reducing the number of parameters may not necessarily help as it leads\nto worse predictions and thus weaker compression. In this paper, we conduct a\nlarge-scale empirical study to investigate whether there is a sweet spot where\ncompetitive compression ratios with pre-trained vanilla transformers are\npossible. To this end, we train families of models on 165GB of raw byte\nsequences of either text, image, or audio data (and all possible combinations\nof the three) and then compress 1GB of out-of-distribution (OOD) data from each\nmodality. We find that relatively small models (i.e., millions of parameters)\ncan outperform standard general-purpose compression algorithms (gzip, LZMA2)\nand even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when\nfactoring in parameter count. We achieve, e.g., the lowest compression ratio of\n0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and\ndataset scale, we conduct extensive ablations and hyperparameter sweeps, and we\ninvestigate the effect of unimodal versus multimodal training. We find that\neven small models can be trained to perform well on multiple modalities, but,\nin contrast to previously reported results with large-scale foundation models,\ntransfer to unseen modalities is generally weak.\n","authors":["David Heurtel-Depeiges","Anian Ruoss","Joel Veness","Tim Genewein"],"pdf_url":"https://arxiv.org/pdf/2410.05078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05076v1","updated":"2024-10-07T14:30:27Z","published":"2024-10-07T14:30:27Z","title":"TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention","summary":"  Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.\n","authors":["Lijie Yang","Zhihao Zhang","Zhuofu Chen","Zikun Li","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2410.05076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12034v2","updated":"2024-10-07T14:27:56Z","published":"2024-06-17T19:06:54Z","title":"Self-MoE: Towards Compositional Large Language Models with\n  Self-Specialized Experts","summary":"  We present Self-MoE, an approach that transforms a monolithic LLM into a\ncompositional, modular system of self-specialized experts, named MiXSE (MiXture\nof Self-specialized Experts). Our approach leverages self-specialization, which\nconstructs expert modules using self-generated synthetic data, each equipping a\nshared base LLM with distinct domain-specific capabilities, activated via\nself-optimized routing. This allows for dynamic and capability-specific\nhandling of various target tasks, enhancing overall capabilities, without\nextensive human-labeled data and added parameters. Our empirical results reveal\nthat specializing LLMs may exhibit potential trade-offs in performances on\nnon-specialized tasks. On the other hand, our Self-MoE demonstrates substantial\nimprovements (6.5%p on average) over the base LLM across diverse benchmarks\nsuch as knowledge, reasoning, math, and coding. It also consistently\noutperforms other methods, including instance merging and weight merging, while\noffering better flexibility and interpretability by design with semantic\nexperts and routing. Our findings highlight the critical role of modularity,\nthe applicability of Self-MoE to multiple base LLMs, and the potential of\nself-improvement in achieving efficient, scalable, and adaptable systems.\n","authors":["Junmo Kang","Leonid Karlinsky","Hongyin Luo","Zhen Wang","Jacob Hansen","James Glass","David Cox","Rameswar Panda","Rogerio Feris","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2406.12034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11410v2","updated":"2024-10-07T14:26:56Z","published":"2024-02-18T00:53:05Z","title":"An Elementary Predictor Obtaining $2\\sqrt{T}+1$ Distance to Calibration","summary":"  Blasiok et al. [2023] proposed distance to calibration as a natural measure\nof calibration error that unlike expected calibration error (ECE) is\ncontinuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument\nestablishing the existence of an online predictor that can obtain $O(\\sqrt{T})$\ndistance to calibration in the adversarial setting, which is known to be\nimpossible for ECE. They leave as an open problem finding an explicit,\nefficient algorithm. We resolve this problem and give an extremely simple,\nefficient, deterministic algorithm that obtains distance to calibration error\nat most $2\\sqrt{T}+1$.\n","authors":["Eshwar Ram Arunachaleswaran","Natalie Collina","Aaron Roth","Mirah Shi"],"pdf_url":"https://arxiv.org/pdf/2402.11410v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05071v1","updated":"2024-10-07T14:26:49Z","published":"2024-10-07T14:26:49Z","title":"Function Gradient Approximation with Random Shallow ReLU Networks with\n  Control Applications","summary":"  Neural networks are widely used to approximate unknown functions in control.\nA common neural network architecture uses a single hidden layer (i.e. a shallow\nnetwork), in which the input parameters are fixed in advance and only the\noutput parameters are trained. The typical formal analysis asserts that if\noutput parameters exist to approximate the unknown function with sufficient\naccuracy, then desired control performance can be achieved. A long-standing\ntheoretical gap was that no conditions existed to guarantee that, for the fixed\ninput parameters, required accuracy could be obtained by training the output\nparameters. Our recent work has partially closed this gap by demonstrating that\nif input parameters are chosen randomly, then for any sufficiently smooth\nfunction, with high-probability there are output parameters resulting in\n$O((1/m)^{1/2})$ approximation errors, where $m$ is the number of neurons.\nHowever, some applications, notably continuous-time value function\napproximation, require that the network approximates the both the unknown\nfunction and its gradient with sufficient accuracy. In this paper, we show that\nrandomly generated input parameters and trained output parameters result in\ngradient errors of $O((\\log(m)/m)^{1/2})$, and additionally, improve the\nconstants from our prior work. We show how to apply the result to policy\nevaluation problems.\n","authors":["Andrew Lamperski","Siddharth Salapaka"],"pdf_url":"https://arxiv.org/pdf/2410.05071v1.pdf","comment":"Under Review for American Control Conference, 2025"},{"id":"http://arxiv.org/abs/2408.14325v3","updated":"2024-10-07T14:23:50Z","published":"2024-08-26T14:54:13Z","title":"Function-Space MCMC for Bayesian Wide Neural Networks","summary":"  Bayesian Neural Networks represent a fascinating confluence of deep learning\nand probabilistic reasoning, offering a compelling framework for understanding\nuncertainty in complex predictive models. In this paper, we investigate the use\nof the preconditioned Crank-Nicolson algorithm and its Langevin version to\nsample from the reparametrised posterior distribution of the weights as the\nwidths of Bayesian Neural Networks grow larger. In addition to being robust in\nthe infinite-dimensional setting, we prove that the acceptance probabilities of\nthe proposed methods approach 1 as the width of the network increases,\nindependently of any stepsize tuning. Moreover, we examine and compare how the\nmixing speeds of the underdamped Langevin Monte Carlo, the preconditioned\nCrank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are\ninfluenced by changes in the network width in some real-world cases. Our\nfindings suggest that, in wide Bayesian Neural Networks configurations, the\npreconditioned Crank-Nicolson method allows for more efficient sampling of the\nreparametrised posterior distribution, as evidenced by a higher effective\nsample size and improved diagnostic results compared with the other analysed\nalgorithms.\n","authors":["Lucia Pezzetti","Stefano Favaro","Stefano Peluchetti"],"pdf_url":"https://arxiv.org/pdf/2408.14325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05063v1","updated":"2024-10-07T14:21:51Z","published":"2024-10-07T14:21:51Z","title":"Control-oriented Clustering of Visual Latent Representation","summary":"  We initiate a study of the geometry of the visual representation space -- the\ninformation channel from the vision encoder to the action decoder -- in an\nimage-based control pipeline learned from behavior cloning. Inspired by the\nphenomenon of neural collapse (NC) in image classification, we investigate\nwhether a similar law of clustering emerges in the visual representation space.\nSince image-based control is a regression task without explicitly defined\nclasses, the central piece of the puzzle lies in determining according to what\nimplicit classes the visual features cluster, if such a law exists. Focusing on\nimage-based planar pushing, we posit the most important role of the visual\nrepresentation in a control task is to convey a goal to the action decoder. We\nthen classify training samples of expert demonstrations into eight\n\"control-oriented\" classes based on (a) the relative pose between the object\nand the target in the input or (b) the relative pose of the object induced by\nexpert actions in the output, where one class corresponds to one relative pose\northant (REPO). Across four different instantiations of architecture, we report\nthe prevalent emergence of control-oriented clustering in the visual\nrepresentation space according to the eight REPOs. Beyond empirical\nobservation, we show such a law of clustering can be leveraged as an\nalgorithmic tool to improve test-time performance when training a policy with\nlimited expert demonstrations. Particularly, we pretrain the vision encoder\nusing NC as a regularization to encourage control-oriented clustering of the\nvisual features. Surprisingly, such an NC-pretrained vision encoder, when\nfinetuned end-to-end with the action decoder, boosts the test-time performance\nby 10% to 35% in the low-data regime. Real-world vision-based planar pushing\nexperiments confirmed the surprising advantage of control-oriented visual\nrepresentation pretraining.\n","authors":["Han Qi","Haocheng Yin","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.05063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02458v2","updated":"2024-10-07T14:20:42Z","published":"2024-01-04T08:00:32Z","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey","summary":"  The advent of foundation models (FMs) as an emerging suite of AI techniques\nhas struck a wave of opportunities in computational healthcare. The interactive\nnature of these models, guided by pre-training data and human instructions, has\nignited a data-centric AI paradigm that emphasizes better data\ncharacterization, quality, and scale. In healthcare AI, obtaining and\nprocessing high-quality clinical data records has been a longstanding\nchallenge, ranging from data quantity, annotation, patient privacy, and ethics.\nIn this survey, we investigate a wide range of data-centric approaches in the\nFM era (from model pre-training to inference) towards improving the healthcare\nworkflow. We discuss key perspectives in AI security, assessment, and alignment\nwith human values. Finally, we offer a promising outlook of FM-based analytics\nto enhance the performance of patient outcome and clinical workflow in the\nevolving landscape of healthcare and medicine. We provide an up-to-date list of\nhealthcare-related foundation models and datasets at\nhttps://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .\n","authors":["Yunkun Zhang","Jin Gao","Zheling Tan","Lingfeng Zhou","Kexin Ding","Mu Zhou","Shaoting Zhang","Dequan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02458v2.pdf","comment":"Survey content updated to include recent research work and progress"},{"id":"http://arxiv.org/abs/2407.03878v2","updated":"2024-10-07T14:14:54Z","published":"2024-07-04T12:15:42Z","title":"Geodesic Optimization for Predictive Shift Adaptation on EEG data","summary":"  Electroencephalography (EEG) data is often collected from diverse contexts\ninvolving different populations and EEG devices. This variability can induce\ndistribution shifts in the data $X$ and in the biomedical variables of interest\n$y$, thus limiting the application of supervised machine learning (ML)\nalgorithms. While domain adaptation (DA) methods have been developed to\nmitigate the impact of these shifts, such methods struggle when distribution\nshifts occur simultaneously in $X$ and $y$. As state-of-the-art ML models for\nEEG represent the data by spatial covariance matrices, which lie on the\nRiemannian manifold of Symmetric Positive Definite (SPD) matrices, it is\nappealing to study DA techniques operating on the SPD manifold. This paper\nproposes a novel method termed Geodesic Optimization for Predictive Shift\nAdaptation (GOPSA) to address test-time multi-source DA for situations in which\nsource domains have distinct $y$ distributions. GOPSA exploits the geodesic\nstructure of the Riemannian manifold to jointly learn a domain-specific\nre-centering operator representing site-specific intercepts and the regression\nmodel. We performed empirical benchmarks on the cross-site generalization of\nage-prediction models with resting-state EEG data from a large multi-national\ndataset (HarMNqEEG), which included $14$ recording sites and more than $1500$\nhuman participants. Compared to state-of-the-art methods, our results showed\nthat GOPSA achieved significantly higher performance on three regression\nmetrics ($R^2$, MAE, and Spearman's $\\rho$) for several source-target site\ncombinations, highlighting its effectiveness in tackling multi-source DA with\npredictive shifts in EEG data analysis. Our method has the potential to combine\nthe advantages of mixed-effects modeling with machine learning for biomedical\napplications of EEG, such as multicenter clinical trials.\n","authors":["Apolline Mellot","Antoine Collas","Sylvain Chevallier","Alexandre Gramfort","Denis A. Engemann"],"pdf_url":"https://arxiv.org/pdf/2407.03878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01386v2","updated":"2024-10-07T14:14:39Z","published":"2024-10-02T09:55:58Z","title":"FLAME: Adaptive and Reactive Concept Drift Mitigation for Federated\n  Learning Deployments","summary":"  This paper presents Federated Learning with Adaptive Monitoring and\nElimination (FLAME), a novel solution capable of detecting and mitigating\nconcept drift in Federated Learning (FL) Internet of Things (IoT) environments.\nConcept drift poses significant challenges for FL models deployed in dynamic\nand real-world settings. FLAME leverages an FL architecture, considers a\nreal-world FL pipeline, and proves capable of maintaining model performance and\naccuracy while addressing bandwidth and privacy constraints. Introducing\nvarious features and extensions on previous works, FLAME offers a robust\nsolution to concept drift, significantly reducing computational load and\ncommunication overhead. Compared to well-known lightweight mitigation methods,\nFLAME demonstrates superior performance in maintaining high F1 scores and\nreducing resource utilisation in large-scale IoT deployments, making it a\npromising approach for real-world applications.\n","authors":["Ioannis Mavromatis","Stefano De Feo","Aftab Khan"],"pdf_url":"https://arxiv.org/pdf/2410.01386v2.pdf","comment":"Accepted for Publication at ACM EWSN 2024 - EMERGE Workshop"},{"id":"http://arxiv.org/abs/2410.05057v1","updated":"2024-10-07T14:14:38Z","published":"2024-10-07T14:14:38Z","title":"SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image\n  Classification","summary":"  Data curation is the problem of how to collect and organize samples into a\ndataset that supports efficient learning. Despite the centrality of the task,\nlittle work has been devoted towards a large-scale, systematic comparison of\nvarious curation methods. In this work, we take steps towards a formal\nevaluation of data curation strategies and introduce SELECT, the first\nlarge-scale benchmark of curation strategies for image classification.\n  In order to generate baseline methods for the SELECT benchmark, we create a\nnew dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K\nto date. Our dataset extends ImageNet with 5 new training-data shifts, each\napproximately the size of ImageNet-1K itself, and each assembled using a\ndistinct curation strategy. We evaluate our data curation baselines in two\nways: (i) using each training-data shift to train identical image\nclassification models from scratch (ii) using the data itself to fit a\npretrained self-supervised representation.\n  Our findings show interesting trends, particularly pertaining to recent\nmethods for data curation such as synthetic data generation and lookup based on\nCLIP embeddings. We show that although these strategies are highly competitive\nfor certain tasks, the curation strategy used to assemble the original\nImageNet-1K dataset remains the gold standard. We anticipate that our benchmark\ncan illuminate the path for new methods to further reduce the gap. We release\nour checkpoints, code, documentation, and a link to our dataset at\nhttps://github.com/jimmyxu123/SELECT.\n","authors":["Benjamin Feuer","Jiawei Xu","Niv Cohen","Patrick Yubeaton","Govind Mittal","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2410.05057v1.pdf","comment":"NeurIPS 2024, Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.05050v1","updated":"2024-10-07T14:05:57Z","published":"2024-10-07T14:05:57Z","title":"FreSh: Frequency Shifting for Accelerated Neural Representation Learning","summary":"  Implicit Neural Representations (INRs) have recently gained attention as a\npowerful approach for continuously representing signals such as images, videos,\nand 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to\nexhibit a low-frequency bias, limiting their ability to capture high-frequency\ndetails accurately. This limitation is typically addressed by incorporating\nhigh-frequency input embeddings or specialized activation layers. In this work,\nwe demonstrate that these embeddings and activations are often configured with\nhyperparameters that perform well on average but are suboptimal for specific\ninput signals under consideration, necessitating a costly grid search to\nidentify optimal settings. Our key observation is that the initial frequency\nspectrum of an untrained model's output correlates strongly with the model's\neventual performance on a given target signal. Leveraging this insight, we\npropose frequency shifting (or FreSh), a method that selects embedding\nhyperparameters to align the frequency spectrum of the model's initial output\nwith that of the target signal. We show that this simple initialization\ntechnique improves performance across various neural representation methods and\ntasks, achieving results comparable to extensive hyperparameter sweeps but with\nonly marginal computational overhead compared to training a single model with\ndefault hyperparameters.\n","authors":["Adam Kania","Marko Mihajlovic","Sergey Prokudin","Jacek Tabor","Przemysaw Spurek"],"pdf_url":"https://arxiv.org/pdf/2410.05050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03386v2","updated":"2024-10-07T14:01:11Z","published":"2024-06-05T15:36:57Z","title":"Learning Long Range Dependencies on Graphs via Random Walks","summary":"  Message-passing graph neural networks (GNNs) excel at capturing local\nrelationships but struggle with long-range dependencies in graphs. In contrast,\ngraph transformers (GTs) enable global information exchange but often\noversimplify the graph structure by representing graphs as sets of fixed-length\nvectors. This work introduces a novel architecture that overcomes the\nshortcomings of both approaches by combining the long-range information of\nrandom walks with local message passing. By treating random walks as sequences,\nour architecture leverages recent advances in sequence models to effectively\ncapture long-range dependencies within these walks. Based on this concept, we\npropose a framework that offers (1) more expressive graph representations\nthrough random walk sequences, (2) the ability to utilize any sequence model\nfor capturing long-range dependencies, and (3) the flexibility by integrating\nvarious GNN and GT architectures. Our experimental evaluations demonstrate that\nour approach achieves significant performance improvements on 19 graph and node\nbenchmark datasets, notably outperforming existing methods by up to 13\\% on the\nPascalVoc-SP and COCO-SP datasets. The code is available at\nhttps://github.com/BorgwardtLab/NeuralWalker.\n","authors":["Dexiong Chen","Till Hendrik Schulz","Karsten Borgwardt"],"pdf_url":"https://arxiv.org/pdf/2406.03386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13687v4","updated":"2024-10-07T13:59:42Z","published":"2024-07-18T17:42:37Z","title":"Dynamic Pricing in Securities Lending Market: Application in Revenue\n  Optimization for an Agent Lender Portfolio","summary":"  Securities lending is an important part of the financial market structure,\nwhere agent lenders help long term institutional investors to lend out their\nsecurities to short sellers in exchange for a lending fee. Agent lenders within\nthe market seek to optimize revenue by lending out securities at the highest\nrate possible. Typically, this rate is set by hard-coded business rules or\nstandard supervised machine learning models. These approaches are often\ndifficult to scale and are not adaptive to changing market conditions. Unlike a\ntraditional stock exchange with a centralized limit order book, the securities\nlending market is organized similarly to an e-commerce marketplace, where agent\nlenders and borrowers can transact at any agreed price in a bilateral fashion.\nThis similarity suggests that the use of typical methods for addressing dynamic\npricing problems in e-commerce could be effective in the securities lending\nmarket. We show that existing contextual bandit frameworks can be successfully\nutilized in the securities lending market. Using offline evaluation on real\nhistorical data, we show that the contextual bandit approach can consistently\noutperform typical approaches by at least 15% in terms of total revenue\ngenerated.\n","authors":["Jing Xu","Yung-Cheng Hsu","William Biscarri"],"pdf_url":"https://arxiv.org/pdf/2407.13687v4.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05044v1","updated":"2024-10-07T13:58:40Z","published":"2024-10-07T13:58:40Z","title":"PhotoReg: Photometrically Registering 3D Gaussian Splatting Models","summary":"  Building accurate representations of the environment is critical for\nintelligent robots to make decisions during deployment. Advances in\nphotorealistic environment models have enabled robots to develop\nhyper-realistic reconstructions, which can be used to generate images that are\nintuitive for human inspection. In particular, the recently introduced\n\\ac{3DGS}, which describes the scene with up to millions of primitive\nellipsoids, can be rendered in real time. \\ac{3DGS} has rapidly gained\nprominence. However, a critical unsolved problem persists: how can we fuse\nmultiple \\ac{3DGS} into a single coherent model? Solving this problem will\nenable robot teams to jointly build \\ac{3DGS} models of their surroundings. A\nkey insight of this work is to leverage the {duality} between photorealistic\nreconstructions, which render realistic 2D images from 3D structure, and\n\\emph{3D foundation models}, which predict 3D structure from image pairs. To\nthis end, we develop PhotoReg, a framework to register multiple photorealistic\n\\ac{3DGS} models with 3D foundation models. As \\ac{3DGS} models are generally\nbuilt from monocular camera images, they have \\emph{arbitrary scale}. To\nresolve this, PhotoReg actively enforces scale consistency among the different\n\\ac{3DGS} models by considering depth estimates within these models. Then, the\nalignment is iteratively refined with fine-grained photometric losses to\nproduce high-quality fused \\ac{3DGS} models. We rigorously evaluate PhotoReg on\nboth standard benchmark datasets and our custom-collected datasets, including\nwith two quadruped robots. The code is released at\n\\url{ziweny11.github.io/photoreg}.\n","authors":["Ziwen Yuan","Tianyi Zhang","Matthew Johnson-Roberson","Weiming Zhi"],"pdf_url":"https://arxiv.org/pdf/2410.05044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05041v1","updated":"2024-10-07T13:53:17Z","published":"2024-10-07T13:53:17Z","title":"Systematic Literature Review of Vision-Based Approaches to Outdoor\n  Livestock Monitoring with Lessons from Wildlife Studies","summary":"  Precision livestock farming (PLF) aims to improve the health and welfare of\nlivestock animals and farming outcomes through the use of advanced\ntechnologies. Computer vision, combined with recent advances in machine\nlearning and deep learning artificial intelligence approaches, offers a\npossible solution to the PLF ideal of 24/7 livestock monitoring that helps\nfacilitate early detection of animal health and welfare issues. However, a\nsignificant number of livestock species are raised in large outdoor habitats\nthat pose technological challenges for computer vision approaches. This review\nprovides a comprehensive overview of computer vision methods and open\nchallenges in outdoor animal monitoring. We include research from both the\nlivestock and wildlife fields in the review because of the similarities in\nappearance, behaviour, and habitat for many livestock and wildlife. We focus on\nlarge terrestrial mammals, such as cattle, horses, deer, goats, sheep, koalas,\ngiraffes, and elephants. We use an image processing pipeline to frame our\ndiscussion and highlight the current capabilities and open technical challenges\nat each stage of the pipeline. The review found a clear trend towards the use\nof deep learning approaches for animal detection, counting, and multi-species\nclassification. We discuss in detail the applicability of current vision-based\nmethods to PLF contexts and promising directions for future research.\n","authors":["Stacey D. Scott","Zayn J. Abbas","Feerass Ellid","Eli-Henry Dykhne","Muhammad Muhaiminul Islam","Weam Ayad","Kristina Kacmorova","Dan Tulpan","Minglun Gong"],"pdf_url":"https://arxiv.org/pdf/2410.05041v1.pdf","comment":"28 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.05026v1","updated":"2024-10-07T13:26:36Z","published":"2024-10-07T13:26:36Z","title":"Active Fine-Tuning of Generalist Policies","summary":"  Pre-trained generalist policies are rapidly gaining relevance in robot\nlearning due to their promise of fast adaptation to novel, in-domain tasks.\nThis adaptation often relies on collecting new demonstrations for a specific\ntask of interest and applying imitation learning algorithms, such as behavioral\ncloning. However, as soon as several tasks need to be learned, we must decide\nwhich tasks should be demonstrated and how often? We study this multi-task\nproblem and explore an interactive framework in which the agent adaptively\nselects the tasks to be demonstrated. We propose AMF (Active Multi-task\nFine-tuning), an algorithm to maximize multi-task policy performance under a\nlimited demonstration budget by collecting demonstrations yielding the largest\ninformation gain on the expert policy. We derive performance guarantees for AMF\nunder regularity assumptions and demonstrate its empirical effectiveness to\nefficiently fine-tune neural policies in complex and high-dimensional\nenvironments.\n","authors":["Marco Bagatella","Jonas Hbotter","Georg Martius","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2410.05026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05021v1","updated":"2024-10-07T13:24:24Z","published":"2024-10-07T13:24:24Z","title":"DEPT: Decoupled Embeddings for Pre-training Language Models","summary":"  Language Model pre-training benefits from a broader data mixture to enhance\nperformance across domains and languages. However, training on such\nheterogeneous text corpora is complex, requiring extensive and cost-intensive\nefforts. Since these data sources vary in lexical, syntactic, and semantic\naspects, they cause negative interference or the \"curse of multilinguality\". We\npropose a novel pre-training framework to alleviate this curse. Our method,\nDEPT, decouples the embedding layers from the transformer body while\nsimultaneously training the latter in multiple contexts. DEPT enables the model\nto train without being bound to a shared global vocabulary. DEPT: (1) can train\nrobustly and effectively under significant data heterogeneity, (2) reduces the\nparameter count of the token embeddings by up to 80% and the communication\ncosts by 675x for billion-scale models (3) enhances model generalization and\nplasticity in adapting to new languages and domains, and (4) allows training\nwith custom optimized vocabulary per data source. We prove DEPT's potential by\nperforming the first vocabulary-agnostic federated multilingual pre-training of\na 1.3 billion-parameter model across high and low-resource languages, reducing\nits parameter count by 409 million.\n","authors":["Alex Iacob","Lorenzo Sani","Meghdad Kurmanji","William F. Shen","Xinchi Qiu","Dongqi Cai","Yan Gao","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2410.05021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19146v2","updated":"2024-10-07T13:21:13Z","published":"2024-05-29T14:51:41Z","title":"I Bet You Did Not Mean That: Testing Semantic Importance via Betting","summary":"  Recent works have extended notions of feature importance to semantic concepts\nthat are inherently interpretable to the users interacting with a black-box\npredictive model. Yet, precise statistical guarantees, such as false positive\nrate and false discovery rate control, are needed to communicate findings\ntransparently and to avoid unintended consequences in real-world scenarios. In\nthis paper, we formalize the global (i.e., over a population) and local (i.e.,\nfor a sample) statistical importance of semantic concepts for the predictions\nof opaque models by means of conditional independence, which allows for\nrigorous testing. We use recent ideas of sequential kernelized independence\ntesting (SKIT) to induce a rank of importance across concepts, and showcase the\neffectiveness and flexibility of our framework on synthetic datasets as well as\non image classification tasks using several and diverse vision-language models.\n","authors":["Jacopo Teneggi","Jeremias Sulam"],"pdf_url":"https://arxiv.org/pdf/2405.19146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05020v1","updated":"2024-10-07T13:20:26Z","published":"2024-10-07T13:20:26Z","title":"FRIDA: Free-Rider Detection using Privacy Attacks","summary":"  Federated learning is increasingly popular as it enables multiple parties\nwith limited datasets and resources to train a high-performing machine learning\nmodel collaboratively. However, similarly to other collaborative systems,\nfederated learning is vulnerable to free-riders -- participants who do not\ncontribute to the training but still benefit from the shared model. Free-riders\nnot only compromise the integrity of the learning process but also slow down\nthe convergence of the global model, resulting in increased costs for the\nhonest participants.\n  To address this challenge, we propose FRIDA: free-rider detection using\nprivacy attacks, a framework that leverages inference attacks to detect\nfree-riders. Unlike traditional methods that only capture the implicit effects\nof free-riding, FRIDA directly infers details of the underlying training\ndatasets, revealing characteristics that indicate free-rider behaviour. Through\nextensive experiments, we demonstrate that membership and property inference\nattacks are effective for this purpose. Our evaluation shows that FRIDA\noutperforms state-of-the-art methods, especially in non-IID settings.\n","authors":["Pol G. Recasens","dm Horvth","Alberto Gutierrez-Torre","Jordi Torres","Josep Ll. Berral","Balzs Pej"],"pdf_url":"https://arxiv.org/pdf/2410.05020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15160v2","updated":"2024-10-07T13:19:53Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"  Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.\n","authors":["Gilad Yehudai","Haim Kaplan","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2407.15160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05019v1","updated":"2024-10-07T13:19:10Z","published":"2024-10-07T13:19:10Z","title":"RelUNet: Relative Channel Fusion U-Net for Multichannel Speech\n  Enhancement","summary":"  Neural multi-channel speech enhancement models, in particular those based on\nthe U-Net architecture, demonstrate promising performance and generalization\npotential. These models typically encode input channels independently, and\nintegrate the channels during later stages of the network. In this paper, we\npropose a novel modification of these models by incorporating relative\ninformation from the outset, where each channel is processed in conjunction\nwith a reference channel through stacking. This input strategy exploits\ncomparative differences to adaptively fuse information between channels,\nthereby capturing crucial spatial information and enhancing the overall\nperformance. The experiments conducted on the CHiME-3 dataset demonstrate\nimprovements in speech enhancement metrics across various architectures.\n","authors":["Ibrahim Aldarmaki","Thamar Solorio","Bhiksha Raj","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2410.05019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05016v1","updated":"2024-10-07T13:15:07Z","published":"2024-10-07T13:15:07Z","title":"T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data","summary":"  Self-supervision is often used for pre-training to foster performance on a\ndownstream task by constructing meaningful representations of samples.\nSelf-supervised learning (SSL) generally involves generating different views of\nthe same sample and thus requires data augmentations that are challenging to\nconstruct for tabular data. This constitutes one of the main challenges of\nself-supervision for structured data. In the present work, we propose a novel\naugmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on\na Joint Embedding Predictive Architecture (JEPA) and is akin to mask\nreconstruction in the latent space. It involves predicting the latent\nrepresentation of one subset of features from the latent representation of a\ndifferent subset within the same sample, thereby learning rich representations\nwithout augmentations. We use our method as a pre-training technique and train\nseveral deep classifiers on the obtained representation. Our experimental\nresults demonstrate a substantial improvement in both classification and\nregression tasks, outperforming models trained directly on samples in their\noriginal data space. Moreover, T-JEPA enables some methods to consistently\noutperform or match the performance of traditional methods likes Gradient\nBoosted Decision Trees. To understand why, we extensively characterize the\nobtained representations and show that T-JEPA effectively identifies relevant\nfeatures for downstream tasks without access to the labels. Additionally, we\nintroduce regularization tokens, a novel regularization method critical for\ntraining of JEPA-based models on structured data.\n","authors":["Hugo Thimonier","Jos Lucas De Melo Costa","Fabrice Popineau","Arpad Rimmel","Bich-Lin Doan"],"pdf_url":"https://arxiv.org/pdf/2410.05016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01104v2","updated":"2024-10-07T13:13:41Z","published":"2024-10-01T22:22:35Z","title":"softmax is not enough (for sharp out-of-distribution)","summary":"  A key property of reasoning systems is the ability to make sharp decisions on\ntheir input data. For contemporary AI systems, a key carrier of sharp behaviour\nis the softmax function, with its capability to perform differentiable\nquery-key lookups. It is a common belief that the predictive power of networks\nleveraging softmax arises from \"circuits\" which sharply perform certain kinds\nof computations consistently across many diverse inputs. However, for these\ncircuits to be robust, they would need to generalise well to arbitrary valid\ninputs. In this paper, we dispel this myth: even for tasks as simple as finding\nthe maximum key, any learned circuitry must disperse as the number of items\ngrows at test time. We attribute this to a fundamental limitation of the\nsoftmax function to robustly approximate sharp functions, prove this phenomenon\ntheoretically, and propose adaptive temperature as an ad-hoc technique for\nimproving the sharpness of softmax at inference time.\n","authors":["Petar Velikovi","Christos Perivolaropoulos","Federico Barbero","Razvan Pascanu"],"pdf_url":"https://arxiv.org/pdf/2410.01104v2.pdf","comment":"Comments welcome. 15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.16581v3","updated":"2024-10-07T13:12:20Z","published":"2024-05-26T14:18:38Z","title":"On Bits and Bandits: Quantifying the Regret-Information Trade-off","summary":"  In many sequential decision problems, an agent performs a repeated task. He\nthen suffers regret and obtains information that he may use in the following\nrounds. However, sometimes the agent may also obtain information and avoid\nsuffering regret by querying external sources. We study the trade-off between\nthe information an agent accumulates and the regret it suffers. We invoke\ninformation-theoretic methods for obtaining regret lower bounds, that also\nallow us to easily re-derive several known lower bounds. We introduce the first\nBayesian regret lower bounds that depend on the information an agent\naccumulates. We also prove regret upper bounds using the amount of information\nthe agent accumulates. These bounds show that information measured in bits, can\nbe traded off for regret, measured in reward. Finally, we demonstrate the\nutility of these bounds in improving the performance of a question-answering\ntask with large language models, allowing us to obtain valuable insights.\n","authors":["Itai Shufaro","Nadav Merlis","Nir Weinberger","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2405.16581v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18427v2","updated":"2024-10-07T13:07:11Z","published":"2024-09-27T03:28:11Z","title":"Neural Collaborative Filtering to Detect Anomalies in Human Semantic\n  Trajectories","summary":"  Human trajectory anomaly detection has become increasingly important across a\nwide range of applications, including security surveillance and public health.\nHowever, existing trajectory anomaly detection methods are primarily focused on\nvehicle-level traffic, while human-level trajectory anomaly detection remains\nunder-explored. Since human trajectory data is often very sparse, machine\nlearning methods have become the preferred approach for identifying complex\npatterns. However, concerns regarding potential biases and the robustness of\nthese models have intensified the demand for more transparent and explainable\nalternatives. In response to these challenges, our research focuses on\ndeveloping a lightweight anomaly detection model specifically designed to\ndetect anomalies in human trajectories. We propose a Neural Collaborative\nFiltering approach to model and predict normal mobility. Our method is designed\nto model users' daily patterns of life without requiring prior knowledge,\nthereby enhancing performance in scenarios where data is sparse or incomplete,\nsuch as in cold start situations. Our algorithm consists of two main modules.\nThe first is the collaborative filtering module, which applies collaborative\nfiltering to model normal mobility of individual humans to places of interest.\nThe second is the neural module, responsible for interpreting the complex\nspatio-temporal relationships inherent in human trajectory data. To validate\nour approach, we conducted extensive experiments using simulated and real-world\ndatasets comparing to numerous state-of-the-art trajectory anomaly detection\napproaches.\n","authors":["Yueyang Liu","Lance Kennedy","Hossein Amiri","Andreas Zfle"],"pdf_url":"https://arxiv.org/pdf/2409.18427v2.pdf","comment":"Accepted for publication in the 1st ACM SIGSPATIAL International\n  Workshop on Geospatial Anomaly Detection (GeoAnomalies'24)"},{"id":"http://arxiv.org/abs/2410.04996v1","updated":"2024-10-07T12:52:38Z","published":"2024-10-07T12:52:38Z","title":"Assumption-Lean Post-Integrated Inference with Negative Control Outcomes","summary":"  Data integration has become increasingly common in aligning multiple\nheterogeneous datasets. With high-dimensional outcomes, data integration\nmethods aim to extract low-dimensional embeddings of observations to remove\nunwanted variations, such as batch effects and unmeasured covariates, inherent\nin data collected from different sources. However, multiple hypothesis testing\nafter data integration can be substantially biased due to the data-dependent\nintegration processes. To address this challenge, we introduce a robust\npost-integrated inference (PII) method that adjusts for latent heterogeneity\nusing negative control outcomes. By leveraging causal interpretations, we\nderive nonparametric identification conditions that form the basis of our PII\napproach.\n  Our assumption-lean semiparametric inference method extends robustness and\ngenerality to projected direct effect estimands that account for mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide\ndeterministic quantifications of the bias of target estimands induced by\nestimated embeddings and finite-sample linear expansions of the estimators with\nuniform concentration bounds on the residuals for all outcomes.\n  The proposed doubly robust estimators are consistent and efficient under\nminimal assumptions, facilitating data-adaptive estimation with machine\nlearning algorithms. Using random forests, we evaluate empirical statistical\nerrors in simulations and analyze single-cell CRISPR perturbed datasets with\npotential unmeasured confounders.\n","authors":["Jin-Hong Du","Kathryn Roeder","Larry Wasserman"],"pdf_url":"https://arxiv.org/pdf/2410.04996v1.pdf","comment":"29 pages for main text, and 18 pages for appendix, 9 figures for main\n  text, 4 figures for appendix"},{"id":"http://arxiv.org/abs/2402.16017v2","updated":"2024-10-07T12:52:19Z","published":"2024-02-25T07:28:28Z","title":"Spectrum Extraction and Clipping for Implicitly Linear Layers","summary":"  We show the effectiveness of automatic differentiation in efficiently and\ncorrectly computing and controlling the spectrum of implicitly linear\noperators, a rich family of layer types including all standard convolutional\nand dense layers. We provide the first clipping method which is correct for\ngeneral convolution layers, and illuminate the representational limitation that\ncaused correctness issues in prior work. We study the effect of the batch\nnormalization layers when concatenated with convolutional layers and show how\nour clipping method can be applied to their composition. By comparing the\naccuracy and performance of our algorithms to the state-of-the-art methods,\nusing various experiments, we show they are more precise and efficient and lead\nto better generalization and adversarial robustness. We provide the code for\nusing our methods at https://github.com/Ali-E/FastClip.\n","authors":["Ali Ebrahimpour Boroojeny","Matus Telgarsky","Hari Sundaram"],"pdf_url":"https://arxiv.org/pdf/2402.16017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15664v2","updated":"2024-10-07T12:52:00Z","published":"2024-06-21T21:44:27Z","title":"Flat Posterior Does Matter For Bayesian Model Averaging","summary":"  Bayesian neural network (BNN) approximates the posterior distribution of\nmodel parameters and utilizes the posterior for prediction via Bayesian Model\nAveraging (BMA). The quality of the posterior approximation is critical for\nachieving accurate and robust predictions. It is known that flatness in the\nloss landscape is strongly associated with generalization performance, and it\nnecessitates consideration to improve the quality of the posterior\napproximation. In this work, we empirically demonstrate that BNNs often\nstruggle to capture the flatness. Moreover, we provide both experimental and\ntheoretical evidence showing that BMA can be ineffective without ensuring\nflatness. To address this, we propose Sharpness-Aware Bayesian Model Averaging\n(SA-BMA), a novel optimizer that seeks flat posteriors by calculating\ndivergence in the parameter space. SA-BMA aligns with the intrinsic nature of\nBNN and the generalized version of existing sharpness-aware optimizers for DNN.\nIn addition, we suggest a Bayesian Transfer Learning scheme to efficiently\nleverage pre-trained DNN. We validate the efficacy of SA-BMA in enhancing\ngeneralization performance in few-shot classification and distribution shift by\nensuring flat posterior.\n","authors":["Sungjun Lim","Jeyoon Yeom","Sooyon Kim","Hoyoon Byun","Jinho Kang","Yohan Jung","Jiyoung Jung","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2406.15664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04992v1","updated":"2024-10-07T12:48:03Z","published":"2024-10-07T12:48:03Z","title":"MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic\n  Compartment Neurons for Stress Detection using Physiological Signals","summary":"  Long short-term memory (LSTM) has emerged as a definitive network for\nanalyzing and inferring time series data. LSTM has the capability to extract\nspectral features and a mixture of temporal features. Due to this benefit, a\nsimilar feature extraction method is explored for the spiking counterparts\ntargeting time-series data. Though LSTMs perform well in their spiking form,\nthey tend to be compute and power intensive. Addressing this issue, this work\nproposes Multi-Compartment Leaky (MCLeaky) neuron as a viable alternative for\nefficient processing of time series data. The MCLeaky neuron, derived from the\nLeaky Integrate and Fire (LIF) neuron model, contains multiple memristive\nsynapses interlinked to form a memory component, which emulates the human\nbrain's Hippocampus region. The proposed MCLeaky neuron based Spiking Neural\nNetwork model and its quantized variant were benchmarked against\nstate-of-the-art (SOTA) Spiking LSTMs to perform human stress detection, by\ncomparing compute requirements, latency and real-world performances on unseen\ndata with models derived through Neural Architecture Search (NAS). Results show\nthat networks with MCLeaky activation neuron managed a superior accuracy of\n98.8% to detect stress based on Electrodermal Activity (EDA) signals, better\nthan any other investigated models, while using 20% less parameters on average.\nMCLeaky neuron was also tested for various signals including EDA Wrist and\nChest, Temperature, ECG, and combinations of them. Quantized MCLeaky model was\nalso derived and validated to forecast their performance on hardware\narchitectures, which resulted in 91.84% accuracy. The neurons were evaluated\nfor multiple modalities of data towards stress detection, which resulted in\nenergy savings of 25.12x to 39.20x and EDP gains of 52.37x to 81.9x over ANNs,\nwhile offering a best accuracy of 98.8% when compared with the rest of the SOTA\nimplementations.\n","authors":["Ajay B. S.","Phani Pavan K","Madhav Rao"],"pdf_url":"https://arxiv.org/pdf/2410.04992v1.pdf","comment":"13 pages, 15 figures. Applied to IEEE Transactions on Computer Aided\n  Design Journal. Awaiting a verdict"},{"id":"http://arxiv.org/abs/2410.04988v1","updated":"2024-10-07T12:42:51Z","published":"2024-10-07T12:42:51Z","title":"Efficient Model-Based Reinforcement Learning Through Optimistic Thompson\n  Sampling","summary":"  Learning complex robot behavior through interactions with the environment\nnecessitates principled exploration. Effective strategies should prioritize\nexploring regions of the state-action space that maximize rewards, with\noptimistic exploration emerging as a promising direction aligned with this idea\nand enabling sample-efficient reinforcement learning. However, existing methods\noverlook a crucial aspect: the need for optimism to be informed by a belief\nconnecting the reward and state. To address this, we propose a practical,\ntheoretically grounded approach to optimistic exploration based on Thompson\nsampling. Our model structure is the first that allows for reasoning about\njoint uncertainty over transitions and rewards. We apply our method on a set of\nMuJoCo and VMAS continuous control tasks. Our experiments demonstrate that\noptimistic exploration significantly accelerates learning in environments with\nsparse rewards, action penalties, and difficult-to-explore regions.\nFurthermore, we provide insights into when optimism is beneficial and emphasize\nthe critical role of model uncertainty in guiding exploration.\n","authors":["Jasmine Bayrooti","Carl Henrik Ek","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2410.04988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15421v2","updated":"2024-10-07T12:38:57Z","published":"2022-09-30T12:26:14Z","title":"TabDDPM: Modelling Tabular Data with Diffusion Models","summary":"  Denoising diffusion probabilistic models are currently becoming the leading\nparadigm of generative modeling for many important data modalities. Being the\nmost prevalent in the computer vision community, diffusion models have also\nrecently gained some attention in other domains, including speech, NLP, and\ngraph-like data. In this work, we investigate if the framework of diffusion\nmodels can be advantageous for general tabular problems, where datapoints are\ntypically represented by vectors of heterogeneous features. The inherent\nheterogeneity of tabular data makes it quite challenging for accurate modeling,\nsince the individual features can be of completely different nature, i.e., some\nof them can be continuous and some of them can be discrete. To address such\ndata types, we introduce TabDDPM -- a diffusion model that can be universally\napplied to any tabular dataset and handles any type of feature. We extensively\nevaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority\nover existing GAN/VAE alternatives, which is consistent with the advantage of\ndiffusion models in other fields. Additionally, we show that TabDDPM is\neligible for privacy-oriented setups, where the original datapoints cannot be\npublicly shared.\n","authors":["Akim Kotelnikov","Dmitry Baranchuk","Ivan Rubachev","Artem Babenko"],"pdf_url":"https://arxiv.org/pdf/2209.15421v2.pdf","comment":"code https://github.com/yandex-research/tab-ddpm"},{"id":"http://arxiv.org/abs/2410.04982v1","updated":"2024-10-07T12:23:40Z","published":"2024-10-07T12:23:40Z","title":"Safe Learning-Based Optimization of Model Predictive Control:\n  Application to Battery Fast-Charging","summary":"  Model predictive control (MPC) is a powerful tool for controlling complex\nnonlinear systems under constraints, but often struggles with model\nuncertainties and the design of suitable cost functions. To address these\nchallenges, we discuss an approach that integrates MPC with safe Bayesian\noptimization to optimize long-term closed-loop performance despite significant\nmodel-plant mismatches. By parameterizing the MPC stage cost function using a\nradial basis function network, we employ Bayesian optimization as a\nmulti-episode learning strategy to tune the controller without relying on\nprecise system models. This method mitigates conservativeness introduced by\noverly cautious soft constraints in the MPC cost function and provides\nprobabilistic safety guarantees during learning, ensuring that safety-critical\nconstraints are met with high probability. As a practical application, we apply\nour approach to fast charging of lithium-ion batteries, a challenging task due\nto the complicated battery dynamics and strict safety requirements, subject to\nthe requirement to be implementable in real time. Simulation results\ndemonstrate that, in the context of model-plant mismatch, our method reduces\ncharging times compared to traditional MPC methods while maintaining safety.\nThis work extends previous research by emphasizing closed-loop constraint\nsatisfaction and offers a promising solution for enhancing performance in\nsystems where model uncertainties and safety are critical concerns.\n","authors":["Sebastian Hirt","Andreas Hhl","Johannes Pohlodek","Joachim Schaeffer","Maik Pfefferkorn","Richard D. Braatz","Rolf Findeisen"],"pdf_url":"https://arxiv.org/pdf/2410.04982v1.pdf","comment":"7 pages, 4 figures, submitted to ACC 2025"},{"id":"http://arxiv.org/abs/2410.04968v1","updated":"2024-10-07T12:12:51Z","published":"2024-10-07T12:12:51Z","title":"Collaboration! Towards Robust Neural Methods for Routing Problems","summary":"  Despite enjoying desirable efficiency and reduced reliance on domain\nexpertise, existing neural methods for vehicle routing problems (VRPs) suffer\nfrom severe robustness issues -- their performance significantly deteriorates\non clean instances with crafted perturbations. To enhance robustness, we\npropose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the\ndefense of neural VRP methods, which is crucial yet underexplored in the\nliterature. Given a neural VRP method, we adversarially train multiple models\nin a collaborative manner to synergistically promote robustness against\nattacks, while boosting standard generalization on clean instances. A neural\nrouter is designed to adeptly distribute training instances among models,\nenhancing overall load balancing and collaborative efficacy. Extensive\nexperiments verify the effectiveness and versatility of CNF in defending\nagainst various attacks across different neural VRP methods. Notably, our\napproach also achieves impressive out-of-distribution generalization on\nbenchmark instances.\n","authors":["Jianan Zhou","Yaoxin Wu","Zhiguang Cao","Wen Song","Jie Zhang","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2410.04968v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.02987v2","updated":"2024-10-07T12:11:58Z","published":"2024-02-05T13:18:42Z","title":"Reconstruct Your Previous Conversations! Comprehensively Investigating\n  Privacy Leakage Risks in Conversations with GPT Models","summary":"  Significant advancements have recently been made in large language models\nrepresented by GPT models. Users frequently have multi-round private\nconversations with cloud-hosted GPT models for task optimization. Yet, this\noperational paradigm introduces additional attack surfaces, particularly in\ncustom GPTs and hijacked chat sessions. In this paper, we introduce a\nstraightforward yet potent Conversation Reconstruction Attack. This attack\ntargets the contents of previous conversations between GPT models and benign\nusers, i.e., the benign users' input contents during their interaction with GPT\nmodels. The adversary could induce GPT models to leak such contents by querying\nthem with designed malicious prompts. Our comprehensive examination of privacy\nrisks during the interactions with GPT models under this attack reveals GPT-4's\nconsiderable resilience. We present two advanced attacks targeting improved\nreconstruction of past conversations, demonstrating significant privacy leakage\nacross all models under these advanced techniques. Evaluating various defense\nmechanisms, we find them ineffective against these attacks. Our findings\nhighlight the ease with which privacy can be compromised in interactions with\nGPT models, urging the community to safeguard against potential abuses of these\nmodels' capabilities.\n","authors":["Junjie Chu","Zeyang Sha","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02987v2.pdf","comment":"Accepted in EMNLP 2024. 14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.19339v2","updated":"2024-10-07T12:05:55Z","published":"2024-09-28T12:49:16Z","title":"Visual Question Decomposition on Multimodal Large Language Models","summary":"  Question decomposition has emerged as an effective strategy for prompting\nLarge Language Models (LLMs) to answer complex questions. However, while\nexisting methods primarily focus on unimodal language models, the question\ndecomposition capability of Multimodal Large Language Models (MLLMs) has yet to\nbe explored. To this end, this paper explores visual question decomposition on\nMLLMs. Specifically, we introduce a systematic evaluation framework including a\ndataset and several evaluation criteria to assess the quality of the decomposed\nsub-questions, revealing that existing MLLMs struggle to produce high-quality\nsub-questions. To address this limitation, we propose a specific finetuning\ndataset, DecoVQA+, for enhancing the model's question decomposition capability.\nAiming at enabling models to perform appropriate selective decomposition, we\npropose an efficient finetuning pipeline. The finetuning pipeline consists of\nour proposed dataset and a training objective for selective decomposition.\nFinetuned MLLMs demonstrate significant improvements in the quality of\nsub-questions and the policy of selective question decomposition. Additionally,\nthe models also achieve higher accuracy with selective decomposition on VQA\nbenchmark datasets.\n","authors":["Haowei Zhang","Jianzhe Liu","Zhen Han","Shuo Chen","Bailan He","Volker Tresp","Zhiqiang Xu","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2409.19339v2.pdf","comment":"Accepted to EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2309.16397v3","updated":"2024-10-07T12:05:12Z","published":"2023-09-28T12:44:51Z","title":"Uncertainty-Aware Decision Transformer for Stochastic Driving\n  Environments","summary":"  Offline Reinforcement Learning (RL) enables policy learning without active\ninteractions, making it especially appealing for self-driving tasks. Recent\nsuccesses of Transformers inspire casting offline RL as sequence modeling,\nwhich, however, fails in stochastic environments with incorrect assumptions\nthat identical actions can consistently achieve the same goal. In this paper,\nwe introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in\nstochastic driving environments without introducing additional transition or\ncomplex generative models. Specifically, UNREST estimates uncertainties by\nconditional mutual information between transitions and returns. Discovering\n'uncertainty accumulation' and 'temporal locality' properties of driving\nenvironments, we replace the global returns in decision transformers with\ntruncated returns less affected by environments to learn from actual outcomes\nof actions rather than environment transitions. We also dynamically evaluate\nuncertainty at inference for cautious planning. Extensive experiments\ndemonstrate UNREST's superior performance in various driving scenarios and the\npower of our uncertainty estimation strategy.\n","authors":["Zenan Li","Fan Nie","Qiao Sun","Fang Da","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2309.16397v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04959v1","updated":"2024-10-07T11:58:56Z","published":"2024-10-07T11:58:56Z","title":"Failure-Proof Non-Contrastive Self-Supervised Learning","summary":"  We identify sufficient conditions to avoid known failure modes, including\nrepresentation, dimensional, cluster and intracluster collapses, occurring in\nnon-contrastive self-supervised learning. Based on these findings, we propose a\nprincipled design for the projector and loss function. We theoretically\ndemonstrate that this design introduces an inductive bias that promotes\nlearning representations that are both decorrelated and clustered without\nexplicit enforcing these properties and leading to improved generalization. To\nthe best of our knowledge, this is the first solution that achieves robust\ntraining with respect to these failure modes while guaranteeing enhanced\ngeneralization performance in downstream tasks. We validate our theoretical\nfindings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100,\nand show that our solution, dubbed FALCON, outperforms existing feature\ndecorrelation and cluster-based self-supervised learning methods in terms of\ngeneralization to clustering and linear classification tasks.\n","authors":["Emanuele Sansone","Tim Lebailly","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2410.04959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04185v2","updated":"2024-10-07T11:54:11Z","published":"2024-09-06T11:01:55Z","title":"Residual Stream Analysis with Multi-Layer SAEs","summary":"  Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, SAEs are\nusually trained separately on each transformer layer, making it difficult to\nuse them to study how information flows across layers. To solve this problem,\nwe introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer. Given that the residual\nstream is understood to preserve information across layers, we expected MLSAE\nlatents to `switch on' at a token position and remain active at later layers.\nInterestingly, we find that individual latents are often active at a single\nlayer for a given token or prompt, but this layer may differ for different\ntokens or prompts. We quantify these phenomena by defining a distribution over\nlayers and considering its variance. We find that the variance of the\ndistributions of latent activations over layers is about two orders of\nmagnitude greater when aggregating over tokens compared with a single token.\nFor larger underlying models, the degree to which latents are active at\nmultiple layers increases, which is consistent with the fact that the residual\nstream activation vectors at adjacent layers become more similar. Finally, we\nrelax the assumption that the residual stream basis is the same at every layer\nby applying pre-trained tuned-lens transformations, but our findings remain\nqualitatively similar. Our results represent a new approach to understanding\nhow representations change as they flow through transformers. We release our\ncode to train and analyze MLSAEs at https://github.com/tim-lawson/mlsae.\n","authors":["Tim Lawson","Lucy Farnik","Conor Houghton","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2409.04185v2.pdf","comment":"34 pages, 26 figures"},{"id":"http://arxiv.org/abs/2409.11439v2","updated":"2024-10-07T11:44:38Z","published":"2024-09-16T09:19:19Z","title":"Machine listening in a neonatal intensive care unit","summary":"  Oxygenators, alarm devices, and footsteps are some of the most common sound\nsources in a hospital. Detecting them has scientific value for environmental\npsychology but comes with challenges of its own: namely, privacy preservation\nand limited labeled data. In this paper, we address these two challenges via a\ncombination of edge computing and cloud computing. For privacy preservation, we\nhave designed an acoustic sensor which computes third-octave spectrograms on\nthe fly instead of recording audio waveforms. For sample-efficient machine\nlearning, we have repurposed a pretrained audio neural network (PANN) via\nspectral transcoding and label space adaptation. A small-scale study in a\nneonatological intensive care unit (NICU) confirms that the time series of\ndetected events align with another modality of measurement: i.e., electronic\nbadges for parents and healthcare professionals. Hence, this paper demonstrates\nthe feasibility of polyphonic machine listening in a hospital ward while\nguaranteeing privacy by design.\n","authors":["Modan Tailleur","Vincent Lostanlen","Jean-Philippe Rivire","Pierre Aumond"],"pdf_url":"https://arxiv.org/pdf/2409.11439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04941v1","updated":"2024-10-07T11:35:24Z","published":"2024-10-07T11:35:24Z","title":"Detecting and Approximating Redundant Computational Blocks in Neural\n  Networks","summary":"  Deep neural networks often learn similar internal representations, both\nacross different models and within their own layers. While inter-network\nsimilarities have enabled techniques such as model stitching and merging,\nintra-network similarities present new opportunities for designing more\nefficient architectures. In this paper, we investigate the emergence of these\ninternal similarities across different layers in diverse neural architectures,\nshowing that similarity patterns emerge independently of the datataset used. We\nintroduce a simple metric, Block Redundancy, to detect redundant blocks,\nproviding a foundation for future architectural optimization methods. Building\non this, we propose Redundant Blocks Approximation (RBA), a general framework\nthat identifies and approximates one or more redundant computational blocks\nusing simpler transformations. We show that the transformation $\\mathcal{T}$\nbetween two representations can be efficiently computed in closed-form, and it\nis enough to replace the redundant blocks from the network. RBA reduces model\nparameters and time complexity while maintaining good performance. We validate\nour method on classification tasks in the vision domain using a variety of\npretrained foundational models and datasets.\n","authors":["Irene Cannistraci","Emanuele Rodol","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.04941v1.pdf","comment":"9 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.04940v1","updated":"2024-10-07T11:32:17Z","published":"2024-10-07T11:32:17Z","title":"Next state prediction gives rise to entangled, yet compositional\n  representations of objects","summary":"  Compositional representations are thought to enable humans to generalize\nacross combinatorially vast state spaces. Models with learnable object slots,\nwhich encode information about objects in separate latent codes, have shown\npromise for this type of generalization but rely on strong architectural\npriors. Models with distributed representations, on the other hand, use\noverlapping, potentially entangled neural codes, and their ability to support\ncompositional generalization remains underexplored. In this paper we examine\nwhether distributed models can develop linearly separable representations of\nobjects, like slotted models, through unsupervised training on videos of object\ninteractions. We show that, surprisingly, models with distributed\nrepresentations often match or outperform models with object slots in\ndownstream prediction tasks. Furthermore, we find that linearly separable\nobject representations can emerge without object-centric priors, with auxiliary\nobjectives like next-state prediction playing a key role. Finally, we observe\nthat distributed models' object representations are never fully disentangled,\neven if they are linearly separable: Multiple objects can be encoded through\npartially overlapping neural populations while still being highly separable\nwith a linear classifier. We hypothesize that maintaining partially shared\ncodes enables distributed models to better compress object dynamics,\npotentially enhancing generalization.\n","authors":["Tankred Saanum","Luca M. Schulze Buschoff","Peter Dayan","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2410.04940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04929v1","updated":"2024-10-07T11:19:23Z","published":"2024-10-07T11:19:23Z","title":"Goal-Conditioned Terminal Value Estimation for Real-time and Multi-task\n  Model Predictive Control","summary":"  While MPC enables nonlinear feedback control by solving an optimal control\nproblem at each timestep, the computational burden tends to be significantly\nlarge, making it difficult to optimize a policy within the control period. To\naddress this issue, one possible approach is to utilize terminal value learning\nto reduce computational costs. However, the learned value cannot be used for\nother tasks in situations where the task dynamically changes in the original\nMPC setup. In this study, we develop an MPC framework with goal-conditioned\nterminal value learning to achieve multitask policy optimization while reducing\ncomputational time. Furthermore, by using a hierarchical control structure that\nallows the upper-level trajectory planner to output appropriate\ngoal-conditioned trajectories, we demonstrate that a robot model is able to\ngenerate diverse motions. We evaluate the proposed method on a bipedal inverted\npendulum robot model and confirm that combining goal-conditioned terminal value\nlearning with an upper-level trajectory planner enables real-time control;\nthus, the robot successfully tracks a target trajectory on sloped terrain.\n","authors":["Mitsuki Morita","Satoshi Yamamori","Satoshi Yagi","Norikazu Sugimoto","Jun Morimoto"],"pdf_url":"https://arxiv.org/pdf/2410.04929v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.17263v2","updated":"2024-10-07T11:15:41Z","published":"2024-06-25T04:07:22Z","title":"Efficient, Multimodal, and Derivative-Free Bayesian Inference With\n  Fisher-Rao Gradient Flows","summary":"  In this paper, we study efficient approximate sampling for probability\ndistributions known up to normalization constants. We specifically focus on a\nproblem class arising in Bayesian inference for large-scale inverse problems in\nscience and engineering applications. The computational challenges we address\nwith the proposed methodology are: (i) the need for repeated evaluations of\nexpensive forward models; (ii) the potential existence of multiple modes; and\n(iii) the fact that gradient of, or adjoint solver for, the forward model might\nnot be feasible.\n  While existing Bayesian inference methods meet some of these challenges\nindividually, we propose a framework that tackles all three systematically. Our\napproach builds upon the Fisher-Rao gradient flow in probability space,\nyielding a dynamical system for probability densities that converges towards\nthe target distribution at a uniform exponential rate. This rapid convergence\nis advantageous for the computational burden outlined in (i). We apply Gaussian\nmixture approximations with operator splitting techniques to simulate the flow\nnumerically; the resulting approximation can capture multiple modes thus\naddressing (ii). Furthermore, we employ the Kalman methodology to facilitate a\nderivative-free update of these Gaussian components and their respective\nweights, addressing the issue in (iii).\n  The proposed methodology results in an efficient derivative-free sampler\nflexible enough to handle multi-modal distributions: Gaussian Mixture Kalman\nInversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically\nand numerically in several experiments with multimodal target distributions,\nincluding proof-of-concept and two-dimensional examples, as well as a\nlarge-scale application: recovering the Navier-Stokes initial condition from\nsolution data at positive times.\n","authors":["Yifan Chen","Daniel Zhengyu Huang","Jiaoyang Huang","Sebastian Reich","Andrew M. Stuart"],"pdf_url":"https://arxiv.org/pdf/2406.17263v2.pdf","comment":"42 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.04916v1","updated":"2024-10-07T11:04:38Z","published":"2024-10-07T11:04:38Z","title":"Defense-as-a-Service: Black-box Shielding against Backdoored Graph\n  Models","summary":"  With the trend of large graph learning models, business owners tend to employ\na model provided by a third party to deliver business services to users.\nHowever, these models might be backdoored, and malicious users can submit\ntrigger-embedded inputs to manipulate the model predictions. Current graph\nbackdoor defenses have several limitations: 1) depending on model-related\ndetails, 2) requiring additional model fine-tuning, and 3) relying upon extra\nexplainability tools, all of which are infeasible under stringent privacy\npolicies. To address those limitations, we propose GraphProt, which allows\nresource-constrained business owners to rely on third parties to avoid backdoor\nattacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and\nonly relies on the input graph. The key insight is to leverage subgraph\ninformation for prediction, thereby mitigating backdoor effects induced by\ntriggers. GraphProt comprises two components: clustering-based trigger\nelimination and robust subgraph ensemble. Specifically, we first propose\nfeature-topology clustering that aims to remove most of the anomalous subgraphs\n(triggers). Moreover, we design subgraph sampling strategies based on\nfeature-topology clustering to build a robust classifier via majority vote.\nExperimental results across three backdoor attacks and six benchmark datasets\ndemonstrate that GraphProt significantly reduces the backdoor attack success\nrate while preserving the model accuracy on regular graph classification tasks.\n","authors":["Xiao Yang","Kai Zhou","Yuni Lai","Gaolei Li"],"pdf_url":"https://arxiv.org/pdf/2410.04916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12561v5","updated":"2024-10-07T10:52:47Z","published":"2022-12-23T19:37:39Z","title":"An active learning method for solving competitive multi-agent\n  decision-making and control problems","summary":"  To identify a stationary action profile for a population of competitive\nagents, each executing private strategies, we introduce a novel active-learning\nscheme where a centralized external observer (or entity) can probe the agents'\nreactions and recursively update simple local parametric estimates of the\naction-reaction mappings. Under very general working assumptions (not even\nassuming that a stationary profile exists), sufficient conditions are\nestablished to assess the asymptotic properties of the proposed active learning\nmethodology so that, if the parameters characterizing the action-reaction\nmappings converge, a stationary action profile is achieved. Such conditions\nhence act also as certificates for the existence of such a profile. Extensive\nnumerical simulations involving typical competitive multi-agent control and\ndecision-making problems illustrate the practical effectiveness of the proposed\nlearning-based approach.\n","authors":["Filippo Fabiani","Alberto Bemporad"],"pdf_url":"https://arxiv.org/pdf/2212.12561v5.pdf","comment":"Python package available at https://github.com/bemporad/gnep-learn"},{"id":"http://arxiv.org/abs/2410.04907v1","updated":"2024-10-07T10:48:36Z","published":"2024-10-07T10:48:36Z","title":"Decomposition Polyhedra of Piecewise Linear Functions","summary":"  In this paper we contribute to the frequently studied question of how to\ndecompose a continuous piecewise linear (CPWL) function into a difference of\ntwo convex CPWL functions. Every CPWL function has infinitely many such\ndecompositions, but for applications in optimization and neural network theory,\nit is crucial to find decompositions with as few linear pieces as possible.\nThis is a highly challenging problem, as we further demonstrate by disproving a\nrecently proposed approach by Tran and Wang [Minimal representations of\ntropical rational functions. Algebraic Statistics, 15(1):27-59, 2024]. To make\nthe problem more tractable, we propose to fix an underlying polyhedral complex\ndetermining the possible locus of nonlinearity. Under this assumption, we prove\nthat the set of decompositions forms a polyhedron that arises as intersection\nof two translated cones. We prove that irreducible decompositions correspond to\nthe bounded faces of this polyhedron and minimal solutions must be vertices. We\nthen identify cases with a unique minimal decomposition, and illustrate how our\ninsights have consequences in the theory of submodular functions. Finally, we\nimprove upon previous constructions of neural networks for a given convex CPWL\nfunction and apply our framework to obtain results in the nonconvex case.\n","authors":["Marie-Charlotte Brandenburg","Moritz Grillo","Christoph Hertrich"],"pdf_url":"https://arxiv.org/pdf/2410.04907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12510v2","updated":"2024-10-07T10:31:58Z","published":"2024-03-19T07:24:54Z","title":"Generalized Consistency Trajectory Models for Image Manipulation","summary":"  Diffusion models (DMs) excel in unconditional generation, as well as on\napplications such as image editing and restoration. The success of DMs lies in\nthe iterative nature of diffusion: diffusion breaks down the complex process of\nmapping noise to data into a sequence of simple denoising tasks. Moreover, we\nare able to exert fine-grained control over the generation process by injecting\nguidance terms into each denoising step. However, the iterative process is also\ncomputationally intensive, often taking from tens up to thousands of function\nevaluations. Although consistency trajectory models (CTMs) enable traversal\nbetween any time points along the probability flow ODE (PFODE) and score\ninference with a single function evaluation, CTMs only allow translation from\nGaussian noise to data. This work aims to unlock the full potential of CTMs by\nproposing generalized CTMs (GCTMs), which translate between arbitrary\ndistributions via ODEs. We discuss the design space of GCTMs and demonstrate\ntheir efficacy in various image manipulation tasks such as image-to-image\ntranslation, restoration, and editing.\n","authors":["Beomsu Kim","Jaemin Kim","Jeongsol Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11834v4","updated":"2024-10-07T10:28:24Z","published":"2023-12-19T04:02:50Z","title":"Multi-agent reinforcement learning using echo-state network and its\n  application to pedestrian dynamics","summary":"  In recent years, simulations of pedestrians using the multi-agent\nreinforcement learning (MARL) have been studied. This study considered the\nroads on a grid-world environment, and implemented pedestrians as MARL agents\nusing an echo-state network and the least squares policy iteration method.\nUnder this environment, the ability of these agents to learn to move forward by\navoiding other agents was investigated. Specifically, we considered two types\nof tasks: the choice between a narrow direct route and a broad detour, and the\nbidirectional pedestrian flow in a corridor. The simulations results indicated\nthat the learning was successful when the density of the agents was not that\nhigh.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2312.11834v4.pdf","comment":"25 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.04891v1","updated":"2024-10-07T10:19:09Z","published":"2024-10-07T10:19:09Z","title":"Low-Rank Continual Personalization of Diffusion Models","summary":"  Recent personalization methods for diffusion models, such as Dreambooth,\nallow fine-tuning pre-trained models to generate new concepts. However,\napplying these techniques across multiple tasks in order to include, e.g.,\nseveral new objects or styles, leads to mutual interference between their\nadapters. While recent studies attempt to mitigate this issue by combining\ntrained adapters across tasks after fine-tuning, we adopt a more rigorous\nregime and investigate the personalization of large diffusion models under a\ncontinual learning scenario, where such interference leads to catastrophic\nforgetting of previous knowledge. To that end, we evaluate the na\\\"ive\ncontinual fine-tuning of customized models and compare this approach with three\nmethods for consecutive adapters' training: sequentially merging new adapters,\nmerging orthogonally initialized adapters, and updating only relevant\nparameters according to the task. In our experiments, we show that the proposed\napproaches mitigate forgetting when compared to the na\\\"ive approach.\n","authors":["ukasz Staniszewski","Katarzyna Zaleska","Kamil Deja"],"pdf_url":"https://arxiv.org/pdf/2410.04891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04887v1","updated":"2024-10-07T10:16:40Z","published":"2024-10-07T10:16:40Z","title":"Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural\n  Collapse","summary":"  Deep neural networks (DNNs) at convergence consistently represent the\ntraining data in the last layer via a highly symmetric geometric structure\nreferred to as neural collapse. This empirical evidence has spurred a line of\ntheoretical research aimed at proving the emergence of neural collapse, mostly\nfocusing on the unconstrained features model. Here, the features of the\npenultimate layer are free variables, which makes the model data-agnostic and,\nhence, puts into question its ability to capture DNN training. Our work\naddresses the issue, moving away from unconstrained features and studying DNNs\nthat end with at least two linear layers. We first prove generic guarantees on\nneural collapse that assume (i) low training error and balancedness of the\nlinear layers (for within-class variability collapse), and (ii) bounded\nconditioning of the features before the linear part (for orthogonality of\nclass-means, as well as their alignment with weight matrices). We then show\nthat such assumptions hold for gradient descent training with weight decay: (i)\nfor networks with a wide first layer, we prove low training error and\nbalancedness, and (ii) for solutions that are either nearly optimal or stable\nunder large learning rates, we additionally prove the bounded conditioning.\nTaken together, our results are the first to show neural collapse in the\nend-to-end training of DNNs.\n","authors":["Arthur Jacot","Peter Skenk","Zihan Wang","Marco Mondelli"],"pdf_url":"https://arxiv.org/pdf/2410.04887v1.pdf","comment":"29 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.06165v4","updated":"2024-10-07T10:14:00Z","published":"2024-02-09T03:48:20Z","title":"Learning Contrastive Feature Representations for Facial Action Unit\n  Detection","summary":"  Facial action unit (AU) detection has long encountered the challenge of\ndetecting subtle feature differences when AUs activate. Existing methods often\nrely on encoding pixel-level information of AUs, which not only encodes\nadditional redundant information but also leads to increased model complexity\nand limited generalizability. Additionally, the accuracy of AU detection is\nnegatively impacted by the class imbalance issue of each AU type, and the\npresence of noisy and false AU labels. In this paper, we introduce a novel\ncontrastive learning framework aimed for AU detection that incorporates both\nself-supervised and supervised signals, thereby enhancing the learning of\ndiscriminative features for accurate AU detection. To tackle the class\nimbalance issue, we employ a negative sample re-weighting strategy that adjusts\nthe step size of updating parameters for minority and majority class samples.\nMoreover, to address the challenges posed by noisy and false AU labels, we\nemploy a sampling technique that encompasses three distinct types of positive\nsample pairs. This enables us to inject self-supervised signals into the\nsupervised signal, effectively mitigating the adverse effects of noisy labels.\nOur experimental assessments, conducted on four widely-utilized benchmark\ndatasets (BP4D, DISFA, GFT and Aff-Wild2), underscore the superior performance\nof our approach compared to state-of-the-art methods of AU detection. Our code\nis available at \\url{https://github.com/Ziqiao-Shang/AUNCE}.\n","authors":["Ziqiao Shang","Bin Liu","Fengmao Lv","Fei Teng","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2402.06165v4.pdf","comment":"13 pages, 17 figures, submitted to IEEE Transactions on Circuits and\n  Systems for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2302.00671v2","updated":"2024-10-07T10:04:28Z","published":"2023-02-01T18:58:20Z","title":"QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing","summary":"  Multi-task reinforcement learning (MTRL) aims to learn several tasks\nsimultaneously for better sample efficiency than learning them separately.\nTraditional methods achieve this by sharing parameters or relabeled data\nbetween tasks. In this work, we introduce a new framework for sharing\nbehavioral policies across tasks, which can be used in addition to existing\nMTRL methods. The key idea is to improve each task's off-policy data collection\nby employing behaviors from other task policies. Selectively sharing helpful\nbehaviors acquired in one task to collect training data for another task can\nlead to higher-quality trajectories, leading to more sample-efficient MTRL.\nThus, we introduce a simple and principled framework called Q-switch mixture of\npolicies (QMP) that selectively shares behavior between different task policies\nby using the task's Q-function to evaluate and select useful shareable\nbehaviors. We theoretically analyze how QMP improves the sample efficiency of\nthe underlying RL algorithm. Our experiments show that QMP's behavioral policy\nsharing provides complementary gains over many popular MTRL algorithms and\noutperforms alternative ways to share behaviors in various manipulation,\nlocomotion, and navigation environments. Videos are available at\nhttps://qmp-mtrl.github.io.\n","authors":["Grace Zhang","Ayush Jain","Injune Hwang","Shao-Hua Sun","Joseph J. Lim"],"pdf_url":"https://arxiv.org/pdf/2302.00671v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04883v1","updated":"2024-10-07T10:02:31Z","published":"2024-10-07T10:02:31Z","title":"Improving the Sampling Strategy in KernelSHAP","summary":"  Shapley values are a popular model-agnostic explanation framework for\nexplaining predictions made by complex machine learning models. The framework\nprovides feature contribution scores that sum to the predicted response and\nrepresent each feature's importance. The computation of exact Shapley values is\ncomputationally expensive due to estimating an exponential amount of\nnon-trivial conditional expectations. The KernelSHAP framework enables us to\napproximate the Shapley values using a sampled subset of weighted conditional\nexpectations. We propose three main novel contributions: a stabilizing\ntechnique to reduce the variance of the weights in the current state-of-the-art\nstrategy, a novel weighing scheme that corrects the Shapley kernel weights\nbased on sampled subsets, and a straightforward strategy that includes the\nimportant subsets and integrates them with the corrected Shapley kernel\nweights. We compare these new approximation strategies against existing ones by\nevaluating their Shapley value accuracy as a function of the number of subsets.\nThe results demonstrate that our sampling strategies significantly enhance the\naccuracy of the approximated Shapley value explanations, making them more\nreliable in practical applications. This work provides valuable insights and\npractical recommendations for researchers and practitioners seeking to\nimplement Shapley value-based explainability of their models.\n","authors":["Lars Henry Berge Olsen","Martin Jullum"],"pdf_url":"https://arxiv.org/pdf/2410.04883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08227v2","updated":"2024-10-07T09:51:46Z","published":"2024-07-11T07:01:50Z","title":"DALL-M: Context-Aware Clinical Data Augmentation with LLMs","summary":"  X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets.\n","authors":["Chihcheng Hsieh","Catarina Moreira","Isabel Blanco Nobre","Sandra Costa Sousa","Chun Ouyang","Margot Brereton","Joaquim Jorge","Jacinto C. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2407.08227v2.pdf","comment":"we introduce a pioneering approach to clinical data augmentation that\n  employs large language models (LLMs) to generate patient contextual synthetic\n  data. It preserves the integrity of real patient data while enriching the\n  dataset with contextually relevant synthetic features, significantly\n  enhancing model performance"},{"id":"http://arxiv.org/abs/2308.06300v3","updated":"2024-10-07T09:48:14Z","published":"2023-08-11T07:57:12Z","title":"Classification of All Blood Cell Images using ML and DL Models","summary":"  Human blood primarily comprises plasma, red blood cells, white blood cells,\nand platelets. It plays a vital role in transporting nutrients to different\norgans, where it stores essential health-related data about the human body.\nBlood cells are utilized to defend the body against diverse infections,\nincluding fungi, viruses, and bacteria. Hence, blood analysis can help\nphysicians assess an individual's physiological condition. Blood cells have\nbeen sub-classified into eight groups: Neutrophils, eosinophils, basophils,\nlymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and\nmetamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of\ntheir nucleus, shape, and cytoplasm. Traditionally, pathologists and\nhematologists in laboratories have examined these blood cells using a\nmicroscope before manually classifying them. The manual approach is slower and\nmore prone to human error. Therefore, it is essential to automate this process.\nIn our paper, transfer learning with CNN pre-trained models. VGG16, VGG19,\nResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20\napplied to the PBC dataset's normal DIB. The overall accuracy achieved with\nthese models lies between 91.375 and 94.72%. Hence, inspired by these\npre-trained architectures, a model has been proposed to automatically classify\nthe ten types of blood cells with increased accuracy. A novel CNN-based\nframework has been presented to improve accuracy. The proposed CNN model has\nbeen tested on the PBC dataset normal DIB. The outcomes of the experiments\ndemonstrate that our CNN-based framework designed for blood cell classification\nattains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional\nneural network model performs competitively when compared to earlier results\nreported in the literature.\n","authors":["Rabia Asghar","Sanjay Kumar","Paul Hynds","Abeera Mahfooz"],"pdf_url":"https://arxiv.org/pdf/2308.06300v3.pdf","comment":"15"},{"id":"http://arxiv.org/abs/2410.04870v1","updated":"2024-10-07T09:36:43Z","published":"2024-10-07T09:36:43Z","title":"On the Optimization and Generalization of Two-layer Transformers with\n  Sign Gradient Descent","summary":"  The Adam optimizer is widely used for transformer optimization in practice,\nwhich makes understanding the underlying optimization mechanisms an important\nproblem. However, due to the Adam's complexity, theoretical analysis of how it\noptimizes transformers remains a challenging task. Fortunately, Sign Gradient\nDescent (SignGD) serves as an effective surrogate for Adam. Despite its\nsimplicity, theoretical understanding of how SignGD optimizes transformers\nstill lags behind. In this work, we study how SignGD optimizes a two-layer\ntransformer -- consisting of a softmax attention layer with trainable query-key\nparameterization followed by a linear layer -- on a linearly separable noisy\ndataset. We identify four stages in the training dynamics, each exhibiting\nintriguing behaviors. Based on the training dynamics, we prove the fast\nconvergence but poor generalization of the learned transformer on the noisy\ndataset. We also show that Adam behaves similarly to SignGD in terms of both\noptimization and generalization in this setting. Additionally, we find that the\npoor generalization of SignGD is not solely due to data noise, suggesting that\nboth SignGD and Adam requires high-quality data for real-world tasks. Finally,\nexperiments on synthetic and real-world datasets empirically support our\ntheoretical results.\n","authors":["Bingrui Li","Wei Huang","Andi Han","Zhanpeng Zhou","Taiji Suzuki","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04870v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.04865v1","updated":"2024-10-07T09:27:51Z","published":"2024-10-07T09:27:51Z","title":"Mastering Chinese Chess AI (Xiangqi) Without Search","summary":"  We have developed a high-performance Chinese Chess AI that operates without\nreliance on search algorithms. This AI has demonstrated the capability to\ncompete at a level commensurate with the top 0.1\\% of human players. By\neliminating the search process typically associated with such systems, this AI\nachieves a Queries Per Second (QPS) rate that exceeds those of systems based on\nthe Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and\nsurpasses those based on the AlphaBeta pruning algorithm by more than a\nhundredfold. The AI training system consists of two parts: supervised learning\nand reinforcement learning. Supervised learning provides an initial human-like\nChinese chess AI, while reinforcement learning, based on supervised learning,\nelevates the strength of the entire AI to a new level. Based on this training\nsystem, we carried out enough ablation experiments and discovered that 1. The\nsame parameter amount of Transformer architecture has a higher performance than\nCNN on Chinese chess; 2. Possible moves of both sides as features can greatly\nimprove the training process; 3. Selective opponent pool, compared to pure\nself-play training, results in a faster improvement curve and a higher strength\nlimit. 4. Value Estimation with Cutoff(VECT) improves the original PPO\nalgorithm training process and we will give the explanation.\n","authors":["Yu Chen","Juntong Lin","Zhichao Shu"],"pdf_url":"https://arxiv.org/pdf/2410.04865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18338v3","updated":"2024-10-07T09:20:59Z","published":"2024-09-26T23:23:27Z","title":"AQMLator -- An Auto Quantum Machine Learning E-Platform","summary":"  A successful Machine Learning (ML) model implementation requires three main\ncomponents: training dataset, suitable model architecture and training\nprocedure. Given dataset and task, finding an appropriate model might be\nchallenging. AutoML, a branch of ML, focuses on automatic architecture search\n-- a meta method that aims at moving human from ML system design process. The\nsuccess of ML and the development of quantum computing (QC) in recent years led\nto a birth of new fascinating field called Quantum Machine Learning (QML) that,\namongst others, incorporates quantum computers into ML models. In this paper we\npresent AQMLator, an Auto Quantum Machine Learning platform that aims to\nautomatically propose and train the quantum layers of an ML model with minimal\ninput from the user. This way, data scientists can bypass the entry barrier for\nQC and use QML. AQMLator uses standard ML libraries, making it easy to\nintroduce into existing ML pipelines.\n","authors":["Tomasz Rybotycki","Piotr Gawron"],"pdf_url":"https://arxiv.org/pdf/2409.18338v3.pdf","comment":"15 pages, 3 figures, links to software in the text"},{"id":"http://arxiv.org/abs/2410.04855v1","updated":"2024-10-07T09:19:13Z","published":"2024-10-07T09:19:13Z","title":"Unsupervised Skill Discovery for Robotic Manipulation through Automatic\n  Task Generation","summary":"  Learning skills that interact with objects is of major importance for robotic\nmanipulation. These skills can indeed serve as an efficient prior for solving\nvarious manipulation tasks. We propose a novel Skill Learning approach that\ndiscovers composable behaviors by solving a large and diverse number of\nautonomously generated tasks. Our method learns skills allowing the robot to\nconsistently and robustly interact with objects in its environment. The\ndiscovered behaviors are embedded in primitives which can be composed with\nHierarchical Reinforcement Learning to solve unseen manipulation tasks. In\nparticular, we leverage Asymmetric Self-Play to discover behaviors and\nMultiplicative Compositional Policies to embed them. We compare our method to\nSkill Learning baselines and find that our skills are more interactive.\nFurthermore, the learned skills can be used to solve a set of unseen\nmanipulation tasks, in simulation as well as on a real robotic platform.\n","authors":["Paul Jansonnie","Bingbing Wu","Julien Perez","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2410.04855v1.pdf","comment":"Accepted at the 2024 IEEE-RAS International Conference on Humanoid\n  Robots"},{"id":"http://arxiv.org/abs/2410.04853v1","updated":"2024-10-07T09:16:58Z","published":"2024-10-07T09:16:58Z","title":"TimeCNN: Refining Cross-Variable Interaction on Time Point for Time\n  Series Forecasting","summary":"  Time series forecasting is extensively applied across diverse domains.\nTransformer-based models demonstrate significant potential in modeling\ncross-time and cross-variable interaction. However, we notice that the\ncross-variable correlation of multivariate time series demonstrates\nmultifaceted (positive and negative correlations) and dynamic progression over\ntime, which is not well captured by existing Transformer-based models. To\naddress this issue, we propose a TimeCNN model to refine cross-variable\ninteractions to enhance time series forecasting. Its key innovation is\ntimepoint-independent, where each time point has an independent convolution\nkernel, allowing each time point to have its independent model to capture\nrelationships among variables. This approach effectively handles both positive\nand negative correlations and adapts to the evolving nature of variable\nrelationships over time. Extensive experiments conducted on 12 real-world\ndatasets demonstrate that TimeCNN consistently outperforms state-of-the-art\nmodels. Notably, our model achieves significant reductions in computational\nrequirements (approximately 60.46%) and parameter count (about 57.50%), while\ndelivering inference speeds 3 to 4 times faster than the benchmark iTransformer\nmodel\n","authors":["Ao Hu","Dongkai Wang","Yong Dai","Shiyi Qi","Liangjian Wen","Jun Wang","Zhi Chen","Xun Zhou","Zenglin Xu","Jiang Duan"],"pdf_url":"https://arxiv.org/pdf/2410.04853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09887v2","updated":"2024-10-07T09:15:07Z","published":"2024-07-13T13:27:57Z","title":"OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization\n  Modeling","summary":"  Large language models (LLMs) have exhibited their problem-solving abilities\nin mathematical reasoning. Solving realistic optimization (OPT) problems in\napplication scenarios requires advanced and applied mathematics ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose OptiBench, a\nbenchmark for End-to-end optimization problem-solving with human-readable\ninputs and outputs. OptiBench contains rich optimization problems, including\nlinear and nonlinear programming with or without tabular data, which can\ncomprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are\nrequired to call a code solver to provide precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and\nclosed-source LLMs (e.g., GPT-4), we further propose a data synthesis method\nnamely ReSocratic. Unlike general data synthesis methods that proceed from\nquestions to answers, \\ReSocratic first incrementally synthesizes formatted\noptimization demonstration with mathematical formulations step by step and then\nback-translates the generated demonstrations into questions. Based on this, we\nsynthesize the ReSocratic-29k dataset. We further conduct supervised\nfine-tuning with ReSocratic-29k on multiple open-source models. Experimental\nresults show that ReSocratic-29k significantly improves the performance of\nopen-source models.\n","authors":["Zhicheng Yang","Yiwei Wang","Yinya Huang","Zhijiang Guo","Wei Shi","Xiongwei Han","Liang Feng","Linqi Song","Xiaodan Liang","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2407.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13538v5","updated":"2024-10-07T09:11:49Z","published":"2023-11-22T17:24:21Z","title":"AlignedCoT: Prompting Large Language Models via Native-Speaking\n  Demonstrations","summary":"  Large Language Models prompting, such as using in-context demonstrations, is\na mainstream technique for invoking LLMs to perform high-performance and solid\ncomplex reasoning (e.g., mathematical reasoning, commonsense reasoning), and\nhas the potential for further human-machine collaborative scientific findings.\nHowever, current LLMs are delicate and elusive in prompt words and styles. And\nthere is an unseen gap between LLM understanding and human-written prompts.\nThis paper introduces Alignedcot, an LLM-acquainted prompting technique that\nincludes proficient ``native-speaking'' in in-context learning for the LLMs.\nSpecifically, it achieves consistent and correct step-wise prompts in zero-shot\nscenarios by progressively probing, refining, and formatting the LLM chain of\nthoughts so that free from handcrafted few-shot demonstrations while\nmaintaining the prompt quality. We conduct experiments on mathematical\nreasoning and commonsense reasoning. We find that LLMs with Alignedcot perform\nsignificantly superior to them with human-crafted demonstrations. We further\napply Alignedcot for rewriting the GSM8K training set, resulting in a\nGSM8K-Align dataset. We observe its benefits for retrieval augmented\ngeneration. The code and data can be found at\nhttps://github.com/yangzhch6/AlignedCoT.\n","authors":["Zhicheng Yang","Yinya Huang","Jing Xiong","Liang Feng","Xiaodan Liang","Yiwei Wang","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2311.13538v5.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.04840v1","updated":"2024-10-07T08:54:23Z","published":"2024-10-07T08:54:23Z","title":"Strong Model Collapse","summary":"  Within the scaling laws paradigm, which underpins the training of large\nneural networks like ChatGPT and Llama, we consider a supervised regression\nsetting and establish the existance of a strong form of the model collapse\nphenomenon, a critical performance degradation due to synthetic data in the\ntraining corpus. Our results show that even the smallest fraction of synthetic\ndata (e.g., as little as 1\\% of the total training dataset) can still lead to\nmodel collapse: larger and larger training sets do not enhance performance. We\nfurther investigate whether increasing model size, an approach aligned with\ncurrent trends in training large language models, exacerbates or mitigates\nmodel collapse. In a simplified regime where neural networks are approximated\nvia random projections of tunable size, we both theoretically and empirically\nshow that larger models can amplify model collapse. Interestingly, our theory\nalso indicates that, beyond the interpolation threshold (which can be extremely\nhigh for very large datasets), larger models may mitigate the collapse,\nalthough they do not entirely prevent it. Our theoretical findings are\nempirically verified through experiments on language models and feed-forward\nneural networks for images.\n","authors":["Elvis Dohmatob","Yunzhen Feng","Julia Kempe"],"pdf_url":"https://arxiv.org/pdf/2410.04840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14407v3","updated":"2024-10-07T08:45:35Z","published":"2024-02-22T09:48:47Z","title":"Learning an Actionable Discrete Diffusion Policy via Large-Scale\n  Actionless Video Pre-Training","summary":"  Learning a generalist embodied agent capable of completing multiple tasks\nposes challenges, primarily stemming from the scarcity of action-labeled\nrobotic datasets. In contrast, a vast amount of human videos exist, capturing\nintricate tasks and interactions with the physical world. Promising prospects\narise for utilizing actionless human videos for pre-training and transferring\nthe knowledge to facilitate robot policy learning through limited robot\ndemonstrations. However, it remains a challenge due to the domain gap between\nhumans and robots. Moreover, it is difficult to extract useful information\nrepresenting the dynamic world from human videos, because of its noisy and\nmultimodal data structure. In this paper, we introduce a novel framework to\ntackle these challenges, which leverages a unified discrete diffusion to\ncombine generative pre-training on human videos and policy fine-tuning on a\nsmall number of action-labeled robot videos. We start by compressing both human\nand robot videos into unified video tokens. In the pre-training stage, we\nemploy a discrete diffusion model with a mask-and-replace diffusion strategy to\npredict future video tokens in the latent space. In the fine-tuning stage, we\nharness the imagined future videos to guide low-level action learning with a\nlimited set of robot data. Experiments demonstrate that our method generates\nhigh-fidelity future videos for planning and enhances the fine-tuned policies\ncompared to previous state-of-the-art approaches with superior performance. Our\nproject website is available at https://video-diff.github.io/.\n","authors":["Haoran He","Chenjia Bai","Ling Pan","Weinan Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2402.14407v3.pdf","comment":"Accepted by NeurIPS 2024. 24 pages"},{"id":"http://arxiv.org/abs/2408.08313v2","updated":"2024-10-07T08:44:35Z","published":"2024-08-15T17:59:57Z","title":"Can Large Language Models Understand Symbolic Graphics Programs?","summary":"  Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n","authors":["Zeju Qiu","Weiyang Liu","Haiwen Feng","Zhen Liu","Tim Z. Xiao","Katherine M. Collins","Joshua B. Tenenbaum","Adrian Weller","Michael J. Black","Bernhard Schlkopf"],"pdf_url":"https://arxiv.org/pdf/2408.08313v2.pdf","comment":"Technical Report v2 (46 pages, 24 figures, project page:\n  https://sgp-bench.github.io/, substantial update from v1)"},{"id":"http://arxiv.org/abs/2409.03588v2","updated":"2024-10-07T08:44:08Z","published":"2024-09-05T14:43:11Z","title":"Cost Estimation in Unit Commitment Problems Using Simulation-Based\n  Inference","summary":"  The Unit Commitment (UC) problem is a key optimization task in power systems\nto forecast the generation schedules of power units over a finite time period\nby minimizing costs while meeting demand and technical constraints. However,\nmany parameters required by the UC problem are unknown, such as the costs. In\nthis work, we estimate these unknown costs using simulation-based inference on\nan illustrative UC problem, which provides an approximated posterior\ndistribution of the parameters given observed generation schedules and demands.\nOur results highlight that the learned posterior distribution effectively\ncaptures the underlying distribution of the data, providing a range of possible\nvalues for the unknown parameters given a past observation. This posterior\nallows for the estimation of past costs using observed past generation\nschedules, enabling operators to better forecast future costs and make more\nrobust generation scheduling forecasts. We present avenues for future research\nto address overconfidence in posterior estimation, enhance the scalability of\nthe methodology and apply it to more complex UC problems modeling the network\nconstraints and renewable energy sources.\n","authors":["Matthias Pirlet","Adrien Bolland","Gilles Louppe","Damien Ernst"],"pdf_url":"https://arxiv.org/pdf/2409.03588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04833v1","updated":"2024-10-07T08:40:29Z","published":"2024-10-07T08:40:29Z","title":"Multimodal Fusion Strategies for Mapping Biophysical Landscape Features","summary":"  Multimodal aerial data are used to monitor natural systems, and machine\nlearning can significantly accelerate the classification of landscape features\nwithin such imagery to benefit ecology and conservation. It remains\nunder-explored, however, how these multiple modalities ought to be fused in a\ndeep learning model. As a step towards filling this gap, we study three\nstrategies (Early fusion, Late fusion, and Mixture of Experts) for fusing\nthermal, RGB, and LiDAR imagery using a dataset of spatially-aligned\northomosaics in these three modalities. In particular, we aim to map three\necologically-relevant biophysical landscape features in African savanna\necosystems: rhino middens, termite mounds, and water. The three fusion\nstrategies differ in whether the modalities are fused early or late, and if\nlate, whether the model learns fixed weights per modality for each class or\ngenerates weights for each class adaptively, based on the input. Overall, the\nthree methods have similar macro-averaged performance with Late fusion\nachieving an AUC of 0.698, but their per-class performance varies strongly,\nwith Early fusion achieving the best recall for middens and water and Mixture\nof Experts achieving the best recall for mounds.\n","authors":["Lucia Gordon","Nico Lang","Catherine Ressijac","Andrew Davies"],"pdf_url":"https://arxiv.org/pdf/2410.04833v1.pdf","comment":"9 pages, 4 figures, ECCV 2024 Workshop in CV for Ecology"},{"id":"http://arxiv.org/abs/2410.04824v1","updated":"2024-10-07T08:22:20Z","published":"2024-10-07T08:22:20Z","title":"Taming Gradient Oversmoothing and Expansion in Graph Neural Networks","summary":"  Oversmoothing has been claimed as a primary bottleneck for multi-layered\ngraph neural networks (GNNs). Multiple analyses have examined how and why\noversmoothing occurs. However, none of the prior work addressed how\noptimization is performed under the oversmoothing regime. In this work, we show\nthe presence of $\\textit{gradient oversmoothing}$ preventing optimization\nduring training. We further analyze that GNNs with residual connections, a\nwell-known solution to help gradient flow in deep architecture, introduce\n$\\textit{gradient expansion}$, a phenomenon of the gradient explosion in\ndiverse directions. Therefore, adding residual connections cannot be a solution\nfor making a GNN deep. Our analysis reveals that constraining the Lipschitz\nbound of each layer can neutralize the gradient expansion. To this end, we\nprovide a simple yet effective normalization method to prevent the gradient\nexpansion. An empirical study shows that the residual GNNs with hundreds of\nlayers can be efficiently trained with the proposed normalization without\ncompromising performance. Additional studies show that the empirical\nobservations corroborate our theoretical analysis.\n","authors":["MoonJeong Park","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13753v3","updated":"2024-10-07T08:20:55Z","published":"2024-05-22T15:38:30Z","title":"A Dynamic Model of Performative Human-ML Collaboration: Theory and\n  Empirical Evidence","summary":"  Machine learning (ML) models are increasingly used in various applications,\nfrom recommendation systems in e-commerce to diagnosis prediction in\nhealthcare. In this paper, we present a novel dynamic framework for thinking\nabout the deployment of ML models in a performative, human-ML collaborative\nsystem. In our framework, the introduction of ML recommendations changes the\ndata-generating process of human decisions, which are only a proxy to the\nground truth and which are then used to train future versions of the model. We\nshow that this dynamic process in principle can converge to different stable\npoints, i.e. where the ML model and the Human+ML system have the same\nperformance. Some of these stable points are suboptimal with respect to the\nactual ground truth. As a proof of concept, we conduct an empirical user study\nwith 1,408 participants. In the study, humans solve instances of the knapsack\nproblem with the help of machine learning predictions of varying performance.\nThis is an ideal setting because we can identify the actual ground truth, and\nevaluate the performance of human decisions supported by ML recommendations. We\nfind that for many levels of ML performance, humans can improve upon the ML\npredictions. We also find that the improvement could be even higher if humans\nrationally followed the ML recommendations. Finally, we test whether monetary\nincentives can increase the quality of human decisions, but we fail to find any\npositive effect. Using our empirical data to approximate our collaborative\nsystem suggests that the learning process would dynamically reach an\nequilibrium performance that is around 92% of the maximum knapsack value. Our\nresults have practical implications for the deployment of ML models in contexts\nwhere human decisions may deviate from the indisputable ground truth.\n","authors":["Tom Shr","Samira Samadi","Chiara Farronato"],"pdf_url":"https://arxiv.org/pdf/2405.13753v3.pdf","comment":"10 Pages and appendix"},{"id":"http://arxiv.org/abs/2409.06744v2","updated":"2024-10-07T08:20:32Z","published":"2024-09-10T06:52:33Z","title":"ProteinBench: A Holistic Evaluation of Protein Foundation Models","summary":"  Recent years have witnessed a surge in the development of protein foundation\nmodels, significantly improving performance in protein prediction and\ngenerative tasks ranging from 3D structure prediction and protein design to\nconformational dynamics. However, the capabilities and limitations associated\nwith these models remain poorly understood due to the absence of a unified\nevaluation framework. To fill this gap, we introduce ProteinBench, a holistic\nevaluation framework designed to enhance the transparency of protein foundation\nmodels. Our approach consists of three key components: (i) A taxonomic\nclassification of tasks that broadly encompass the main challenges in the\nprotein domain, based on the relationships between different protein\nmodalities; (ii) A multi-metric evaluation approach that assesses performance\nacross four key dimensions: quality, novelty, diversity, and robustness; and\n(iii) In-depth analyses from various user objectives, providing a holistic view\nof model performance. Our comprehensive evaluation of protein foundation models\nreveals several key findings that shed light on their current capabilities and\nlimitations. To promote transparency and facilitate further research, we\nrelease the evaluation dataset, code, and a public leaderboard publicly for\nfurther analysis and a general modular toolkit. We intend for ProteinBench to\nbe a living benchmark for establishing a standardized, in-depth evaluation\nframework for protein foundation models, driving their development and\napplication while fostering collaboration within the field.\n","authors":["Fei Ye","Zaixiang Zheng","Dongyu Xue","Yuning Shen","Lihao Wang","Yiming Ma","Yan Wang","Xinyou Wang","Xiangxin Zhou","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2409.06744v2.pdf","comment":"30 pages, 2 figures and 15 tables"},{"id":"http://arxiv.org/abs/2410.04818v1","updated":"2024-10-07T08:08:36Z","published":"2024-10-07T08:08:36Z","title":"Physics-Informed GNN for non-linear constrained optimization: PINCO a\n  solver for the AC-optimal power flow","summary":"  The energy transition is driving the integration of large shares of\nintermittent power sources in the electric power grid. Therefore, addressing\nthe AC optimal power flow (AC-OPF) effectively becomes increasingly essential.\nThe AC-OPF, which is a fundamental optimization problem in power systems, must\nbe solved more frequently to ensure the safe and cost-effective operation of\npower systems. Due to its non-linear nature, AC-OPF is often solved in its\nlinearized form, despite inherent inaccuracies. Non-linear solvers, such as the\ninterior point method, are typically employed to solve the full OPF problem.\nHowever, these iterative methods may not converge for large systems and do not\nguarantee global optimality. This work explores a physics-informed graph neural\nnetwork, PINCO, to solve the AC-OPF. We demonstrate that this method provides\naccurate solutions in a fraction of the computational time when compared to the\nestablished non-linear programming solvers. Remarkably, PINCO generalizes\neffectively across a diverse set of loading conditions in the power system. We\nshow that our method can solve the AC-OPF without violating inequality\nconstraints. Furthermore, it can function both as a solver and as a hybrid\nuniversal function approximator. Moreover, the approach can be easily adapted\nto different power systems with minimal adjustments to the hyperparameters,\nincluding systems with multiple generators at each bus. Overall, this work\ndemonstrates an advancement in the field of power system optimization to tackle\nthe challenges of the energy transition. The code and data utilized in this\npaper are available at https://anonymous.4open.science/r/opf_pinn_iclr-B83E/.\n","authors":["Anna Varbella","Damien Briens","Blazhe Gjorgiev","Giuseppe Alessio D'Inverno","Giovanni Sansavini"],"pdf_url":"https://arxiv.org/pdf/2410.04818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04814v1","updated":"2024-10-07T07:54:53Z","published":"2024-10-07T07:54:53Z","title":"Learning Interpretable Hierarchical Dynamical Systems Models from Time\n  Series Data","summary":"  In science, we are often interested in obtaining a generative model of the\nunderlying system dynamics from observed time series. While powerful methods\nfor dynamical systems reconstruction (DSR) exist when data come from a single\ndomain, how to best integrate data from multiple dynamical regimes and leverage\nit for generalization is still an open question. This becomes particularly\nimportant when individual time series are short, and group-level information\nmay help to fill in for gaps in single-domain data. At the same time, averaging\nis not an option in DSR, as it will wipe out crucial dynamical properties\n(e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is\nneeded that enables to efficiently harvest group-level (multi-domain)\ninformation while retaining all single-domain dynamical characteristics. Here\nwe provide such a hierarchical approach and showcase it on popular DSR\nbenchmarks, as well as on neuroscientific and medical time series. In addition\nto faithful reconstruction of all individual dynamical regimes, our\nunsupervised methodology discovers common low-dimensional feature spaces in\nwhich datasets with similar dynamics cluster. The features spanning these\nspaces were further dynamically highly interpretable, surprisingly in often\nlinear relation to control parameters that govern the dynamics of the\nunderlying system. Finally, we illustrate transfer learning and generalization\nto new parameter regimes.\n","authors":["Manuel Brenner","Elias Weber","Georgia Koppe","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2410.04814v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.04810v1","updated":"2024-10-07T07:45:18Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Gengyuan Zhang","Jinhe Bi","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19911v4","updated":"2024-10-07T07:41:11Z","published":"2024-07-29T11:39:22Z","title":"Efficient Shield Synthesis via State-Space Transformation","summary":"  We consider the problem of synthesizing safety strategies for control\nsystems, also known as shields. Since the state space is infinite, shields are\ntypically computed over a finite-state abstraction, with the most common\nabstraction being a rectangular grid. However, for many systems, such a grid\ndoes not align well with the safety property or the system dynamics. That is\nwhy a coarse grid is rarely sufficient, but a fine grid is typically\ncomputationally infeasible to obtain. In this paper, we show that appropriate\nstate-space transformations can still allow to use a coarse grid at almost no\ncomputational overhead. We demonstrate in three case studies that our\ntransformation-based synthesis outperforms a standard synthesis by several\norders of magnitude. In the first two case studies, we use domain knowledge to\nselect a suitable transformation. In the third case study, we instead report on\nresults in engineering a transformation without domain knowledge.\n","authors":["Asger Horn Brorholt","Andreas Holck Heg-Petersen","Kim Guldstrand Larsen","Christian Schilling"],"pdf_url":"https://arxiv.org/pdf/2407.19911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.02135v4","updated":"2024-10-07T15:27:58Z","published":"2021-05-05T15:38:36Z","title":"UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms","summary":"  Policy evaluation is an important instrument for the comparison of different\nalgorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the\nvalue function $V^{\\pi}$ corresponding to a policy $\\pi$ does not provide\nreliable information on how far is the policy $\\pi$ from the optimal one. We\npresent a novel model-free upper value iteration procedure $({\\sf UVIP})$ that\nallows us to estimate the suboptimality gap $V^{\\star}(x) - V^{\\pi}(x)$ from\nabove and to construct confidence intervals for $V^\\star$. Our approach relies\non upper bounds to the solution of the Bellman optimality equation via\nmartingale approach. We provide theoretical guarantees for ${\\sf UVIP}$ under\ngeneral assumptions and illustrate its performance on a number of benchmark RL\nproblems.\n","authors":["Ilya Levin","Denis Belomestny","Alexey Naumov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2105.02135v4.pdf","comment":"ICOMP-2024 camera-ready version"}],"Multimedia":[{"id":"http://arxiv.org/abs/2305.05440v3","updated":"2024-10-07T12:20:15Z","published":"2023-05-09T13:33:12Z","title":"Improved Screen Content Coding in VVC Using Soft Context Formation","summary":"  Screen content images typically contain a mix of natural and synthetic image\nparts. Synthetic sections usually are comprised of uniformly colored areas and\nrepeating colors and patterns. In the VVC standard, these properties are\nexploited using Intra Block Copy and Palette Mode. In this paper, we show that\npixel-wise lossless coding can outperform lossy VVC coding in such areas. We\npropose an enhanced VVC coding approach for screen content images using the\nprinciple of soft context formation. First, the image is separated into two\nlayers in a block-wise manner using a learning-based method with four block\nfeatures. Synthetic image parts are coded losslessly using soft context\nformation, the rest with VVC.We modify the available soft context formation\ncoder to incorporate information gained by the decoded VVC layer for improved\ncoding efficiency. Using this approach, we achieve Bjontegaard-Delta-rate gains\nof 4.98% on the evaluated data sets compared to VVC.\n","authors":["Hannah Och","Shabhrish Reddy Uddehal","Tilo Strutz","Andr Kaup"],"pdf_url":"https://arxiv.org/pdf/2305.05440v3.pdf","comment":"5 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.04906v1","updated":"2024-10-07T10:48:08Z","published":"2024-10-07T10:48:08Z","title":"Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation","summary":"  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n","authors":["Ivan Rinaldi","Nicola Fanelli","Giovanna Castellano","Gennaro Vessio"],"pdf_url":"https://arxiv.org/pdf/2410.04906v1.pdf","comment":"Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024"},{"id":"http://arxiv.org/abs/2410.04810v1","updated":"2024-10-07T07:45:18Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Gengyuan Zhang","Jinhe Bi","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04797v1","updated":"2024-10-07T07:16:29Z","published":"2024-10-07T07:16:29Z","title":"Attentive-based Multi-level Feature Fusion for Voice Disorder Diagnosis","summary":"  Voice disorders negatively impact the quality of daily life in various ways.\nHowever, accurately recognizing the category of pathological features from raw\naudio remains a considerable challenge due to the limited dataset. A promising\nmethod to handle this issue is extracting multi-level pathological information\nfrom speech in a comprehensive manner by fusing features in the latent space.\nIn this paper, a novel framework is designed to explore the way of high-quality\nfeature fusion for effective and generalized detection performance.\nSpecifically, the proposed model follows a two-stage training paradigm: (1)\nECAPA-TDNN and Wav2vec 2.0 which have shown remarkable effectiveness in various\ndomains are employed to learn the universal pathological information from raw\naudio; (2) An attentive fusion module is dedicatedly designed to establish the\ninteraction between pathological features projected by EcapTdnn and Wav2vec 2.0\nrespectively and guide the multi-layer fusion, the entire model is jointly\nfine-tuned from pre-trained features by the automatic voice pathology detection\ntask. Finally, comprehensive experiments on the FEMH and SVD datasets\ndemonstrate that the proposed framework outperforms the competitive baselines,\nand achieves the accuracy of 90.51% and 87.68%.\n","authors":["Lipeng Shen","Yifan Xiong","Dongyue Guo","Wei Mo","Lingyu Yu","Hui Yang","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v3","updated":"2024-10-07T07:13:24Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v3.pdf","comment":"accepted at EMNLP2024 - system demonstration track"},{"id":"http://arxiv.org/abs/2410.05474v1","updated":"2024-10-07T20:12:08Z","published":"2024-10-07T20:12:08Z","title":"R-Bench: Are your Large Multimodal Model Robust to Real-world\n  Corruptions?","summary":"  The outstanding performance of Large Multimodal Models (LMMs) has made them\nwidely applied in vision-related tasks. However, various corruptions in the\nreal world mean that images will not be as ideal as in simulations, presenting\nsignificant challenges for the practical application of LMMs. To address this\nissue, we introduce R-Bench, a benchmark focused on the **Real-world Robustness\nof LMMs**. Specifically, we: (a) model the complete link from user capture to\nLMMs reception, comprising 33 corruption dimensions, including 7 steps\naccording to the corruption sequence, and 7 groups based on low-level\nattributes; (b) collect reference/distorted image dataset before/after\ncorruption, including 2,970 question-answer pairs with human labeling; (c)\npropose comprehensive evaluation for absolute/relative robustness and benchmark\n20 mainstream LMMs. Results show that while LMMs can correctly handle the\noriginal reference images, their performance is not stable when faced with\ndistorted images, and there is a significant gap in robustness compared to the\nhuman visual system. We hope that R-Bench will inspire improving the robustness\nof LMMs, **extending them from experimental simulations to the real-world\napplication**. Check https://q-future.github.io/R-Bench for details.\n","authors":["Chunyi Li","Jianbo Zhang","Zicheng Zhang","Haoning Wu","Yuan Tian","Wei Sun","Guo Lu","Xiaohong Liu","Xiongkuo Min","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2410.05474v1.pdf","comment":null}]},"2024-10-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2211.11548v2","updated":"2024-10-06T23:54:24Z","published":"2022-09-17T05:34:32Z","title":"Survey of Query-based Text Summarization","summary":"  Query-based text summarization is an important real world problem that\nrequires to condense the prolix text data into a summary under the guidance of\nthe query information provided by users. The topic has been studied for a long\ntime and there are many existing interesting research related to query-based\ntext summarization. Yet much of the work is not systematically surveyed. This\nsurvey aims at summarizing some interesting work in query-based text\nsummarization methods as well as related generic text summarization methods.\nNot all taxonomies in this paper exist the related work to the best of our\nknowledge and some analysis will be presented.\n","authors":["Hang Yu","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2211.11548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07064v3","updated":"2024-10-06T23:53:34Z","published":"2023-11-13T04:08:49Z","title":"Prompts have evil twins","summary":"  We discover that many natural-language prompts can be replaced by\ncorresponding prompts that are unintelligible to humans but that provably\nelicit similar behavior in language models. We call these prompts \"evil twins\"\nbecause they are obfuscated and uninterpretable (evil), but at the same time\nmimic the functionality of the original natural-language prompts (twins).\nRemarkably, evil twins transfer between models. We find these prompts by\nsolving a maximum-likelihood problem which has applications of independent\ninterest.\n","authors":["Rimon Melamed","Lucas H. McCabe","Tanay Wakhare","Yejin Kim","H. Howie Huang","Enric Boix-Adsera"],"pdf_url":"https://arxiv.org/pdf/2311.07064v3.pdf","comment":"EMNLP 2024 Main, camera-ready"},{"id":"http://arxiv.org/abs/2409.18025v2","updated":"2024-10-06T23:30:44Z","published":"2024-09-26T16:32:19Z","title":"An Adversarial Perspective on Machine Unlearning for AI Safety","summary":"  Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.\n","authors":["Jakub ucki","Boyi Wei","Yangsibo Huang","Peter Henderson","Florian Tramr","Javier Rando"],"pdf_url":"https://arxiv.org/pdf/2409.18025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11823v2","updated":"2024-10-06T22:42:47Z","published":"2024-06-17T17:57:30Z","title":"On Efficient Language and Vision Assistants for Visually-Situated\n  Natural Language Understanding: What Matters in Reading and Reasoning","summary":"  Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva.\n","authors":["Geewook Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.11823v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2408.08926v2","updated":"2024-10-06T22:19:54Z","published":"2024-08-15T17:23:10Z","title":"Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks\n  of Language Models","summary":"  Language Model (LM) agents for cybersecurity that are capable of autonomously\nidentifying vulnerabilities and executing exploits have the potential to cause\nreal-world impact. Policymakers, model providers, and other researchers in the\nAI and cybersecurity communities are interested in quantifying the capabilities\nof such agents to help mitigate cyberrisk and investigate opportunities for\npenetration testing. Toward that end, we introduce Cybench, a framework for\nspecifying cybersecurity tasks and evaluating agents on those tasks. We include\n40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF\ncompetitions, chosen to be recent, meaningful, and spanning a wide range of\ndifficulties. Each task includes its own description, starter files, and is\ninitialized in an environment where an agent can execute bash commands and\nobserve outputs. Since many tasks are beyond the capabilities of existing LM\nagents, we introduce subtasks for each task, which break down a task into\nintermediary steps for a more detailed evaluation. To evaluate agent\ncapabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o,\nOpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct,\nGemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask\nguidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and\nClaude 3 Opus successfully solved complete tasks that took human teams up to 11\nminutes to solve. In comparison, the most difficult task took human teams 24\nhours and 54 minutes to solve. All code and data are publicly available at\nhttps://cybench.github.io\n","authors":["Andy K. Zhang","Neil Perry","Riya Dulepet","Joey Ji","Justin W. Lin","Eliot Jones","Celeste Menders","Gashon Hussein","Samantha Liu","Donovan Jasper","Pura Peetathawatchai","Ari Glenn","Vikram Sivashankar","Daniel Zamoshchin","Leo Glikbarg","Derek Askaryar","Mike Yang","Teddy Zhang","Rishi Alluri","Nathan Tran","Rinnara Sangpisit","Polycarpos Yiorkadjis","Kenny Osele","Gautham Raghupathi","Dan Boneh","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2408.08926v2.pdf","comment":"78 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.12074v2","updated":"2024-10-06T22:17:03Z","published":"2024-06-17T20:20:47Z","title":"COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for\n  Aligning Large Language Models to Online Communities","summary":"  Social scientists use surveys to probe the opinions and beliefs of\npopulations, but these methods are slow, costly, and prone to biases. Recent\nadvances in large language models (LLMs) enable the creating of computational\nrepresentations or \"digital twins\" of populations that generate human-like\nresponses mimicking the population's language, styles, and attitudes. We\nintroduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs\nto online communities to elicit their beliefs. Given a corpus of a community's\nonline discussions, Community-Cross-Instruct automatically generates\ninstruction-output pairs by an advanced LLM to (1) finetune a foundational LLM\nto faithfully represent that community, and (2) evaluate the alignment of the\nfinetuned model to the community. We demonstrate the method's utility in\naccurately representing political and diet communities on Reddit. Unlike prior\nmethods requiring human-authored instructions, Community-Cross-Instruct\ngenerates instructions in a fully unsupervised manner, enhancing scalability\nand generalization across domains. This work enables cost-effective and\nautomated surveying of diverse online communities.\n","authors":["Zihao He","Minh Duc Chu","Rebecca Dorn","Siyi Guo","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2406.12074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18400v4","updated":"2024-10-06T22:13:16Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v4.pdf","comment":"23 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.04633v1","updated":"2024-10-06T21:33:51Z","published":"2024-10-06T21:33:51Z","title":"A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for\n  Speech Emotion Recognition","summary":"  Best-performing speech models are trained on large amounts of data in the\nlanguage they are meant to work for. However, most languages have sparse data,\nmaking training models challenging. This shortage of data is even more\nprevalent in speech emotion recognition. Our work explores the model's\nperformance in limited data, specifically for speech emotion recognition.\nMeta-learning specializes in improving the few-shot learning. As a result, we\nemploy meta-learning techniques on speech emotion recognition tasks, accent\nrecognition, and person identification. To this end, we propose a series of\nimprovements over the multistage meta-learning method. Unlike other works\nfocusing on smaller models due to the high computational cost of meta-learning\nalgorithms, we take a more practical approach. We incorporate a large\npre-trained backbone and a prototypical network, making our methods more\nfeasible and applicable. Our most notable contribution is an improved\nfine-tuning technique during meta-testing that significantly boosts the\nperformance on out-of-distribution datasets. This result, together with\nincremental improvements from several other works, helped us achieve accuracy\nscores of 83.78% and 56.30% for Greek and Romanian speech emotion recognition\ndatasets not included in the training or validation splits in the context of\n4-way 5-shot learning.\n","authors":["David-Gabriel Ion","Rzvan-Alexandru Smdu","Dumitru-Clementin Cercel","Florin Pop","Mihaela-Claudia Cercel"],"pdf_url":"https://arxiv.org/pdf/2410.04633v1.pdf","comment":"16 pages, 1 figure, Accepted by WISE 2024"},{"id":"http://arxiv.org/abs/2402.17840v3","updated":"2024-10-06T21:25:41Z","published":"2024-02-27T19:08:05Z","title":"Follow My Instruction and Spill the Beans: Scalable Data Extraction from\n  Retrieval-Augmented Generation Systems","summary":"  Retrieval-Augmented Generation (RAG) improves pre-trained models by\nincorporating external knowledge at test time to enable customized adaptation.\nWe study the risk of datastore leakage in Retrieval-In-Context RAG Language\nModels (LMs). We show that an adversary can exploit LMs' instruction-following\ncapabilities to easily extract text data verbatim from the datastore of RAG\nsystems built with instruction-tuned LMs via prompt injection. The\nvulnerability exists for a wide range of modern LMs that span Llama2,\nMistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the\nexploitability exacerbates as the model size scales up. We also study multiple\neffects of RAG setup on the extractability of data, indicating that following\nunexpected instructions to regurgitate data can be an outcome of failure in\neffectively utilizing contexts for modern LMs, and further show that such\nvulnerability can be greatly mitigated by position bias elimination strategies.\nExtending our study to production RAG models GPTs, we design an attack that can\ncause datastore leakage with a 100% success rate on 25 randomly selected\ncustomized GPTs with at most 2 queries, and we extract text data verbatim at a\nrate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words\nby prompting the GPTs with only 100 queries generated by themselves.\n","authors":["Zhenting Qi","Hanlin Zhang","Eric Xing","Sham Kakade","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2402.17840v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.04652v1","updated":"2024-10-06T23:25:21Z","published":"2024-10-06T23:25:21Z","title":"Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI","summary":"  Seamless integration of virtual and physical worlds in augmented reality\nbenefits from the system semantically \"understanding\" the physical environment.\nAR research has long focused on the potential of context awareness,\ndemonstrating novel capabilities that leverage the semantics in the 3D\nenvironment for various object-level interactions. Meanwhile, the computer\nvision community has made leaps in neural vision-language understanding to\nenhance environment perception for autonomous tasks. In this work, we introduce\na multimodal 3D object representation that unifies both semantic and linguistic\nknowledge with the geometric representation, enabling user-guided machine\nlearning involving physical objects. We first present a fast multimodal 3D\nreconstruction pipeline that brings linguistic understanding to AR by fusing\nCLIP vision-language features into the environment and object models. We then\npropose \"in-situ\" machine learning, which, in conjunction with the multimodal\nrepresentation, enables new tools and interfaces for users to interact with\nphysical spaces and objects in a spatially and linguistically meaningful\nmanner. We demonstrate the usefulness of the proposed system through two\nreal-world AR applications on Magic Leap 2: a) spatial search in physical\nenvironments with natural language and b) an intelligent inventory system that\ntracks object changes over time. We also make our full implementation and demo\ndata available at (https://github.com/cy-xu/spatially_aware_AI) to encourage\nfurther exploration and research in spatially aware AI.\n","authors":["Chengyuan Xu","Radha Kumaran","Noah Stier","Kangyou Yu","Tobias Hllerer"],"pdf_url":"https://arxiv.org/pdf/2410.04652v1.pdf","comment":"10 pages, 6 figures, accepted to IEEE ISMAR 2024"},{"id":"http://arxiv.org/abs/2410.04648v1","updated":"2024-10-06T23:04:29Z","published":"2024-10-06T23:04:29Z","title":"AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional\n  Semantic Diffusion for Retinal Vessel Segmentation","summary":"  Deep learning has shown remarkable performance in medical image segmentation.\nHowever, despite its promise, deep learning has many challenges in practice due\nto its inability to effectively transition to unseen domains, caused by the\ninherent data distribution shift and the lack of manual annotations to guide\ndomain adaptation. To tackle this problem, we present an unsupervised domain\nadaptation (UDA) method named AdaptDiff that enables a retinal vessel\nsegmentation network trained on fundus photography (FP) to produce satisfactory\nresults on unseen modalities (e.g., OCT-A) without any manual labels. For all\nour target domains, we first adopt a segmentation model trained on the source\ndomain to create pseudo-labels. With these pseudo-labels, we train a\nconditional semantic diffusion probabilistic model to represent the target\ndomain distribution. Experimentally, we show that even with low quality\npseudo-labels, the diffusion model can still capture the conditional semantic\ninformation. Subsequently, we sample on the target domain with binary vessel\nmasks from the source domain to get paired data, i.e., target domain synthetic\nimages conditioned on the binary vessel map. Finally, we fine-tune the\npre-trained segmentation network using the synthetic paired data to mitigate\nthe domain gap. We assess the effectiveness of AdaptDiff on seven publicly\navailable datasets across three distinct modalities. Our results demonstrate a\nsignificant improvement in segmentation performance across all unseen datasets.\nOur code is publicly available at https://github.com/DeweiHu/AdaptDiff.\n","authors":["Dewei Hu","Hao Li","Han Liu","Jiacheng Wang","Xing Yao","Daiwei Lu","Ipek Oguz"],"pdf_url":"https://arxiv.org/pdf/2410.04648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04646v1","updated":"2024-10-06T23:01:57Z","published":"2024-10-06T23:01:57Z","title":"Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for\n  Robust Ground-View Scene Rendering","summary":"  We present a novel-view rendering algorithm, Mode-GS, for ground-robot\ntrajectory datasets. Our approach is based on using anchored Gaussian splats,\nwhich are designed to overcome the limitations of existing 3D Gaussian\nsplatting algorithms. Prior neural rendering methods suffer from severe splat\ndrift due to scene complexity and insufficient multi-view observation, and can\nfail to fix splats on the true geometry in ground-robot datasets. Our method\nintegrates pixel-aligned anchors from monocular depths and generates Gaussian\nsplats around these anchors using residual-form Gaussian decoders. To address\nthe inherent scale ambiguity of monocular depth, we parameterize anchors with\nper-view depth-scales and employ scale-consistent depth loss for online scale\ncalibration. Our method results in improved rendering performance, based on\nPSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns,\nand achieves state-of-the-art rendering performance on the R3LIVE odometry\ndataset and the Tanks and Temples dataset.\n","authors":["Yonghan Lee","Jaehoon Choi","Dongki Jung","Jaeseong Yun","Soohyun Ryu","Dinesh Manocha","Suyong Yeon"],"pdf_url":"https://arxiv.org/pdf/2410.04646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11823v2","updated":"2024-10-06T22:42:47Z","published":"2024-06-17T17:57:30Z","title":"On Efficient Language and Vision Assistants for Visually-Situated\n  Natural Language Understanding: What Matters in Reading and Reasoning","summary":"  Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva.\n","authors":["Geewook Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.11823v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2410.01665v2","updated":"2024-10-06T22:28:20Z","published":"2024-10-02T15:32:01Z","title":"Towards a vision foundation model for comprehensive assessment of\n  Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (CMR), considered the gold standard for\nnoninvasive cardiac assessment, is a diverse and complex modality requiring a\nwide variety of image processing tasks for comprehensive assessment of cardiac\nmorphology and function. Advances in deep learning have enabled the development\nof state-of-the-art (SoTA) models for these tasks. However, model training is\nchallenging due to data and label scarcity, especially in the less common\nimaging sequences. Moreover, each model is often trained for a specific task,\nwith no connection between related tasks. In this work, we introduce a vision\nfoundation model trained for CMR assessment, that is trained in a\nself-supervised fashion on 36 million CMR images. We then finetune the model in\nsupervised way for 9 clinical tasks typical to a CMR workflow, across\nclassification, segmentation, landmark localization, and pathology detection.\nWe demonstrate improved accuracy and robustness across all tasks, over a range\nof available labeled dataset sizes. We also demonstrate improved few-shot\nlearning with fewer labeled samples, a common challenge in medical image\nanalyses. We achieve an out-of-box performance comparable to SoTA for most\nclinical tasks. The proposed method thus presents a resource-efficient, unified\nframework for CMR assessment, with the potential to accelerate the development\nof deep learning-based solutions for image analysis tasks, even with few\nannotated data available.\n","authors":["Athira J Jacob","Indraneel Borgohain","Teodora Chitiboi","Puneet Sharma","Dorin Comaniciu","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2410.01665v2.pdf","comment":"11 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.04636v1","updated":"2024-10-06T21:51:02Z","published":"2024-10-06T21:51:02Z","title":"Multi-Tiered Self-Contrastive Learning for Medical Microwave Radiometry\n  (MWR) Breast Cancer Detection","summary":"  The pursuit of enhanced breast cancer detection and monitoring techniques is\na paramount healthcare objective, driving the need for innovative imaging\ntechnologies and diagnostic approaches. This study introduces a novel\nmulti-tiered self-contrastive model tailored for the application of microwave\nradiometry (MWR) breast cancer detection. Our approach encompasses three\ndistinct models: Local-MWR (L-MWR), Regional-MWR (R-MWR), and Global-MWR\n(G-MWR), each engineered to analyze varying sub-regional comparisons within the\nbreasts. These models are cohesively integrated through the Joint-MWR (J-MWR)\nnetwork, which leverages the self-contrastive data generated at each analytical\nlevel to enhance detection capabilities. Employing a dataset comprising 4,932\ncases of female patients, our research showcases the effectiveness of our\nproposed models. Notably, the J-MWR model distinguishes itself by achieving a\nMatthews correlation coefficient of 0.74 $\\pm$ 0.018, surpassing existing MWR\nneural networks and contrastive methods. These results highlight the\nsignificant potential of self-contrastive learning techniques in improving both\nthe diagnostic accuracy and generalizability of MWR-based breast cancer\ndetection processes. Such advancements hold considerable promise for further\ninvestigative and clinical endeavors. The source code is available at:\nhttps://github.com/cgalaz01/self_contrastive_mwr\n","authors":["Christoforos Galazis","Huiyi Wu","Igor Goryanin"],"pdf_url":"https://arxiv.org/pdf/2410.04636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04634v1","updated":"2024-10-06T21:42:53Z","published":"2024-10-06T21:42:53Z","title":"Is What You Ask For What You Get? Investigating Concept Associations in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models are increasingly used in impactful real-life\napplications. As such, there is a growing need to audit these models to ensure\nthat they generate desirable, task-appropriate images. However, systematically\ninspecting the associations between prompts and generated content in a\nhuman-understandable way remains challenging. To address this, we propose\n\\emph{Concept2Concept}, a framework where we characterize conditional\ndistributions of vision language models using interpretable concepts and\nmetrics that can be defined in terms of these concepts. This characterization\nallows us to use our framework to audit models and prompt-datasets. To\ndemonstrate, we investigate several case studies of conditional distributions\nof prompts, such as user defined distributions or empirical, real world\ndistributions. Lastly, we implement Concept2Concept as an open-source\ninteractive visualization tool facilitating use by non-technical end-users.\n  Warning: This paper contains discussions of harmful content, including CSAM\nand NSFW material, which may be disturbing to some readers.\n","authors":["Salma Abdel Magid","Weiwei Pan","Simon Warchol","Grace Guo","Junsik Kim","Mahia Rahman","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00321v2","updated":"2024-10-06T21:29:57Z","published":"2024-10-01T01:41:23Z","title":"A Cat Is A Cat (Not A Dog!): Unraveling Information Mix-ups in\n  Text-to-Image Encoders through Causal Analysis and Embedding Optimization","summary":"  This paper analyzes the impact of causal manner in the text encoder of\ntext-to-image (T2I) diffusion models, which can lead to information bias and\nloss. Previous works have focused on addressing the issues through the\ndenoising process. However, there is no research discussing how text embedding\ncontributes to T2I models, especially when generating more than one object. In\nthis paper, we share a comprehensive analysis of text embedding: i) how text\nembedding contributes to the generated images and ii) why information gets lost\nand biases towards the first-mentioned object. Accordingly, we propose a simple\nbut effective text embedding balance optimization method, which is\ntraining-free, with an improvement of 90.05% on information balance in stable\ndiffusion. Furthermore, we propose a new automatic evaluation metric that\nquantifies information loss more accurately than existing methods, achieving\n81% concordance with human assessments. This metric effectively measures the\npresence and accuracy of objects, addressing the limitations of current\ndistribution scores like CLIP's text-image similarities.\n","authors":["Chieh-Yun Chen","Li-Wu Tsao","Chiang Tseng","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2410.00321v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.15615v2","updated":"2024-10-06T21:08:01Z","published":"2024-09-23T23:39:03Z","title":"KISS-Matcher: Fast and Robust Point Cloud Registration Revisited","summary":"  While global point cloud registration systems have advanced significantly in\nall aspects, many studies have focused on specific components, such as feature\nextraction, graph-theoretic pruning, or pose solvers. In this paper, we take a\nholistic view on the registration problem and develop an open-source and\nversatile C++ library for point cloud registration, called\n\\textit{KISS-Matcher}. KISS-Matcher combines a novel feature detector,\n\\textit{Faster-PFH}, that improves over the classical fast point feature\nhistogram (FPFH). Moreover, it adopts a $k$-core-based graph-theoretic pruning\nto reduce the time complexity of rejecting outlier correspondences. Finally, it\ncombines these modules in a complete, user-friendly, and ready-to-use pipeline.\nAs verified by extensive experiments, KISS-Matcher has superior scalability and\nbroad applicability, achieving a substantial speed-up compared to\nstate-of-the-art outlier-robust registration pipelines while preserving\naccuracy. Our code will be available at\n\\href{https://github.com/MIT-SPARK/KISS-Matcher}{\\texttt{https://github.com/MIT-SPARK/KISS-Matcher}}.\n","authors":["Hyungtae Lim","Daebeom Kim","Gunhee Shin","Jingnan Shi","Ignacio Vizzo","Hyun Myung","Jaesik Park","Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2409.15615v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.04618v1","updated":"2024-10-06T20:38:14Z","published":"2024-10-06T20:38:14Z","title":"Towards Unsupervised Blind Face Restoration using Diffusion Prior","summary":"  Blind face restoration methods have shown remarkable performance,\nparticularly when trained on large-scale synthetic datasets with supervised\nlearning. These datasets are often generated by simulating low-quality face\nimages with a handcrafted image degradation pipeline. The models trained on\nsuch synthetic degradations, however, cannot deal with inputs of unseen\ndegradations. In this paper, we address this issue by using only a set of input\nimages, with unknown degradations and without ground truth targets, to\nfine-tune a restoration model that learns to map them to clean and contextually\nconsistent outputs. We utilize a pre-trained diffusion model as a generative\nprior through which we generate high quality images from the natural image\ndistribution while maintaining the input image content through consistency\nconstraints. These generated images are then used as pseudo targets to\nfine-tune a pre-trained restoration model. Unlike many recent approaches that\nemploy diffusion models at test time, we only do so during training and thus\nmaintain an efficient inference-time performance. Extensive experiments show\nthat the proposed approach can consistently improve the perceptual quality of\npre-trained blind face restoration models while maintaining great consistency\nwith the input contents. Our best model also achieves the state-of-the-art\nresults on both synthetic and real-world datasets.\n","authors":["Tianshu Kuai","Sina Honari","Igor Gilitschenski","Alex Levinshtein"],"pdf_url":"https://arxiv.org/pdf/2410.04618v1.pdf","comment":"Project page: https://dt-bfr.github.io/"},{"id":"http://arxiv.org/abs/2410.04609v1","updated":"2024-10-06T20:11:53Z","published":"2024-10-06T20:11:53Z","title":"VISTA: A Visual and Textual Attention Dataset for Interpreting\n  Multimodal Models","summary":"  The recent developments in deep learning led to the integration of natural\nlanguage processing (NLP) with computer vision, resulting in powerful\nintegrated Vision and Language Models (VLMs). Despite their remarkable\ncapabilities, these models are frequently regarded as black boxes within the\nmachine learning research community. This raises a critical question: which\nparts of an image correspond to specific segments of text, and how can we\ndecipher these associations? Understanding these connections is essential for\nenhancing model transparency, interpretability, and trustworthiness. To answer\nthis question, we present an image-text aligned human visual attention dataset\nthat maps specific associations between image regions and corresponding text\nsegments. We then compare the internal heatmaps generated by VL models with\nthis dataset, allowing us to analyze and better understand the model's\ndecision-making process. This approach aims to enhance model transparency,\ninterpretability, and trustworthiness by providing insights into how these\nmodels align visual and linguistic information. We conducted a comprehensive\nstudy on text-guided visual saliency detection in these VL models. This study\naims to understand how different models prioritize and focus on specific visual\nelements in response to corresponding text segments, providing deeper insights\ninto their internal mechanisms and improving our ability to interpret their\noutputs.\n","authors":[" Harshit","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2410.04609v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2211.11548v2","updated":"2024-10-06T23:54:24Z","published":"2022-09-17T05:34:32Z","title":"Survey of Query-based Text Summarization","summary":"  Query-based text summarization is an important real world problem that\nrequires to condense the prolix text data into a summary under the guidance of\nthe query information provided by users. The topic has been studied for a long\ntime and there are many existing interesting research related to query-based\ntext summarization. Yet much of the work is not systematically surveyed. This\nsurvey aims at summarizing some interesting work in query-based text\nsummarization methods as well as related generic text summarization methods.\nNot all taxonomies in this paper exist the related work to the best of our\nknowledge and some analysis will be presented.\n","authors":["Hang Yu","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2211.11548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04602v1","updated":"2024-10-06T19:34:23Z","published":"2024-10-06T19:34:23Z","title":"Decoding MIE: A Novel Dataset Approach Using Topic Extraction and\n  Affiliation Parsing","summary":"  The rapid expansion of medical informatics literature presents significant\nchallenges in synthesizing and analyzing research trends. This study introduces\na novel dataset derived from the Medical Informatics Europe (MIE) Conference\nproceedings, addressing the need for sophisticated analytical tools in the\nfield. Utilizing the Triple-A software, we extracted and processed metadata and\nabstract from 4,606 articles published in the \"Studies in Health Technology and\nInformatics\" journal series, focusing on MIE conferences from 1996 onwards. Our\nmethodology incorporated advanced techniques such as affiliation parsing using\nthe TextRank algorithm. The resulting dataset, available in JSON format, offers\na comprehensive view of bibliometric details, extracted topics, and\nstandardized affiliation information. Analysis of this data revealed\ninteresting patterns in Digital Object Identifier usage, citation trends, and\nauthorship attribution across the years. Notably, we observed inconsistencies\nin author data and a brief period of linguistic diversity in publications. This\ndataset represents a significant contribution to the medical informatics\ncommunity, enabling longitudinal studies of research trends, collaboration\nnetwork analyses, and in-depth bibliometric investigations. By providing this\nenriched, structured resource spanning nearly three decades of conference\nproceedings, we aim to facilitate novel insights and advancements in the\nrapidly evolving field of medical informatics.\n","authors":["Ehsan Bitaraf","Maryam Jafarpour"],"pdf_url":"https://arxiv.org/pdf/2410.04602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08975v2","updated":"2024-10-06T18:27:55Z","published":"2024-09-13T16:48:39Z","title":"Accurate and Fast Estimation of Temporal Motifs using Path Sampling","summary":"  Counting the number of small subgraphs, called motifs, is a fundamental\nproblem in social network analysis and graph mining. Many real-world networks\nare directed and temporal, where edges have timestamps. Motif counting in\ndirected, temporal graphs is especially challenging because there are a\nplethora of different kinds of patterns. Temporal motif counts reveal much\nricher information and there is a need for scalable algorithms for motif\ncounting.\n  A major challenge in counting is that there can be trillions of temporal\nmotif matches even with a graph with only millions of vertices. Both the motifs\nand the input graphs can have multiple edges between two vertices, leading to a\ncombinatorial explosion problem. Counting temporal motifs involving just four\nvertices is not feasible with current state-of-the-art algorithms.\n  We design an algorithm, TEACUPS, that addresses this problem using a novel\ntechnique of temporal path sampling. We combine a path sampling method with\ncarefully designed temporal data structures, to propose an efficient\napproximate algorithm for temporal motif counting. TEACUPS is an unbiased\nestimator with provable concentration behavior, which can be used to bound the\nestimation error. For a Bitcoin graph with hundreds of millions of edges,\nTEACUPS runs in less than 1 minute, while the exact counting algorithm takes\nmore than a day. We empirically demonstrate the accuracy of TEACUPS on large\ndatasets, showing an average of 30$\\times$ speedup (up to 2000$\\times$ speedup)\ncompared to existing GPU-based exact counting methods while preserving high\ncount estimation accuracy.\n","authors":["Yunjie Pan","Omkar Bhalerao","C. Seshadhri","Nishil Talati"],"pdf_url":"https://arxiv.org/pdf/2409.08975v2.pdf","comment":"Accepted in ICDM'24"},{"id":"http://arxiv.org/abs/2410.04568v1","updated":"2024-10-06T17:53:44Z","published":"2024-10-06T17:53:44Z","title":"Ranking Policy Learning via Marketplace Expected Value Estimation From\n  Observational Data","summary":"  We develop a decision making framework to cast the problem of learning a\nranking policy for search or recommendation engines in a two-sided e-commerce\nmarketplace as an expected reward optimization problem using observational\ndata. As a value allocation mechanism, the ranking policy allocates retrieved\nitems to the designated slots so as to maximize the user utility from the\nslotted items, at any given stage of the shopping journey. The objective of\nthis allocation can in turn be defined with respect to the underlying\nprobabilistic user browsing model as the expected number of interaction events\non presented items matching the user intent, given the ranking context. Through\nrecognizing the effect of ranking as an intervention action to inform users'\ninteractions with slotted items and the corresponding economic value of the\ninteraction events for the marketplace, we formulate the expected reward of the\nmarketplace as the collective value from all presented ranking actions. The key\nelement in this formulation is a notion of context value distribution, which\nsignifies not only the attribution of value to ranking interventions within a\nsession but also the distribution of marketplace reward across user sessions.\nWe build empirical estimates for the expected reward of the marketplace from\nobservational data that account for the heterogeneity of economic value across\nsession contexts as well as the distribution shifts in learning from\nobservational user activity data. The ranking policy can then be trained by\noptimizing the empirical expected reward estimates via standard Bayesian\ninference techniques. We report empirical results for a product search ranking\ntask in a major e-commerce platform demonstrating the fundamental trade-offs\ngoverned by ranking polices trained on empirical reward estimates with respect\nto extreme choices of the context value distribution.\n","authors":["Ehsan Ebrahimzadeh","Nikhil Monga","Hang Gao","Alex Cozzi","Abraham Bagherjeiran"],"pdf_url":"https://arxiv.org/pdf/2410.04568v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2410.04552v1","updated":"2024-10-06T17:03:27Z","published":"2024-10-06T17:03:27Z","title":"Modeling Social Media Recommendation Impacts Using Academic Networks: A\n  Graph Neural Network Approach","summary":"  The widespread use of social media has highlighted potential negative impacts\non society and individuals, largely driven by recommendation algorithms that\nshape user behavior and social dynamics. Understanding these algorithms is\nessential but challenging due to the complex, distributed nature of social\nmedia networks as well as limited access to real-world data. This study\nproposes to use academic social networks as a proxy for investigating\nrecommendation systems in social media. By employing Graph Neural Networks\n(GNNs), we develop a model that separates the prediction of academic infosphere\nfrom behavior prediction, allowing us to simulate recommender-generated\ninfospheres and assess the model's performance in predicting future\nco-authorships. Our approach aims to improve our understanding of\nrecommendation systems' roles and social networks modeling. To support the\nreproducibility of our work we publicly make available our implementations:\nhttps://github.com/DimNeuroLab/academic_network_project\n","authors":["Sabrina Guidotti","Gregor Donabauer","Simone Somazzi","Udo Kruschwitz","Davide Taibi","Dimitri Ognibene"],"pdf_url":"https://arxiv.org/pdf/2410.04552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04551v1","updated":"2024-10-06T17:01:18Z","published":"2024-10-06T17:01:18Z","title":"Social Choice for Heterogeneous Fairness in Recommendation","summary":"  Algorithmic fairness in recommender systems requires close attention to the\nneeds of a diverse set of stakeholders that may have competing interests.\nPrevious work in this area has often been limited by fixed, single-objective\ndefinitions of fairness, built into algorithms or optimization criteria that\nare applied to a single fairness dimension or, at most, applied identically\nacross dimensions. These narrow conceptualizations limit the ability to adapt\nfairness-aware solutions to the wide range of stakeholder needs and fairness\ndefinitions that arise in practice. Our work approaches recommendation fairness\nfrom the standpoint of computational social choice, using a multi-agent\nframework. In this paper, we explore the properties of different social choice\nmechanisms and demonstrate the successful integration of multiple,\nheterogeneous fairness definitions across multiple data sets.\n","authors":["Amanda Aird","Elena tefancov","Cassidy All","Amy Voida","Martin Homola","Nicholas Mattei","Robin Burke"],"pdf_url":"https://arxiv.org/pdf/2410.04551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12216v2","updated":"2024-10-06T16:15:09Z","published":"2024-07-16T23:50:07Z","title":"Mindful-RAG: A Study of Points of Failure in Retrieval Augmented\n  Generation","summary":"  Large Language Models (LLMs) are proficient at generating coherent and\ncontextually relevant text but face challenges when addressing\nknowledge-intensive queries in domain-specific and factual question-answering\ntasks. Retrieval-augmented generation (RAG) systems mitigate this by\nincorporating external knowledge sources, such as structured knowledge graphs\n(KGs). However, LLMs often struggle to produce accurate answers despite access\nto KG-extracted information containing necessary facts. Our study investigates\nthis dilemma by analyzing error patterns in existing KG-based RAG methods and\nidentifying eight critical failure points. We observed that these errors\npredominantly occur due to insufficient focus on discerning the question's\nintent and adequately gathering relevant context from the knowledge graph\nfacts. Drawing on this analysis, we propose the Mindful-RAG approach, a\nframework designed for intent-based and contextually aligned knowledge\nretrieval. This method explicitly targets the identified failures and offers\nimprovements in the correctness and relevance of responses provided by LLMs,\nrepresenting a significant step forward from existing methods.\n","authors":["Garima Agrawal","Tharindu Kumarage","Zeyad Alghamdi","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.12216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06259v2","updated":"2024-10-06T15:45:21Z","published":"2024-03-10T16:57:10Z","title":"Editing Conceptual Knowledge for Large Language Models","summary":"  Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.\n","authors":["Xiaohan Wang","Shengyu Mao","Ningyu Zhang","Shumin Deng","Yunzhi Yao","Yue Shen","Lei Liang","Jinjie Gu","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.06259v2.pdf","comment":"EMNLP 2024 Findings; Code: https://github.com/zjunlp/EasyEdit\n  Dataset: https://huggingface.co/datasets/zjunlp/ConceptEdit"},{"id":"http://arxiv.org/abs/2408.10159v2","updated":"2024-10-06T06:27:35Z","published":"2024-08-19T17:09:32Z","title":"Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation","summary":"  Sequential recommendation systems predict a user's next item of interest by\nanalyzing past interactions, aligning recommendations with individual\npreferences. Leveraging the strengths of Large Language Models (LLMs) in\nknowledge comprehension and reasoning, recent approaches have applied LLMs to\nsequential recommendation through language generation paradigms. These methods\nconvert user behavior sequences into prompts for LLM fine-tuning, utilizing\nLow-Rank Adaptation (LoRA) modules to refine recommendations. However, the\nuniform application of LoRA across diverse user behaviors sometimes fails to\ncapture individual variability, leading to suboptimal performance and negative\ntransfer between disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)\nframework. iLoRA creates a diverse array of experts, each capturing specific\naspects of user preferences, and introduces a sequence representation guided\ngate function. This gate function processes historical interaction sequences to\ngenerate enriched representations, guiding the gating network to output\ncustomized expert participation weights. This tailored approach mitigates\nnegative transfer and dynamically adjusts to diverse behavior patterns.\nExtensive experiments on three benchmark datasets demonstrate the effectiveness\nof iLoRA, highlighting its superior performance compared to existing methods in\ncapturing user-specific preferences and improving recommendation accuracy.\n","authors":["Xiaoyu Kong","Jiancan Wu","An Zhang","Leheng Sheng","Hui Lin","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2408.10159v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.04570v1","updated":"2024-10-06T17:56:13Z","published":"2024-10-06T17:56:13Z","title":"Watermarking Decision Tree Ensembles","summary":"  Protecting the intellectual property of machine learning models is a hot\ntopic and many watermarking schemes for deep neural networks have been proposed\nin the literature. Unfortunately, prior work largely neglected the\ninvestigation of watermarking techniques for other types of models, including\ndecision tree ensembles, which are a state-of-the-art model for classification\ntasks on non-perceptual data. In this paper, we present the first watermarking\nscheme designed for decision tree ensembles, focusing in particular on random\nforest models. We discuss watermark creation and verification, presenting a\nthorough security analysis with respect to possible attacks. We finally perform\nan experimental evaluation of the proposed scheme, showing excellent results in\nterms of accuracy and security against the most relevant threats.\n","authors":["Stefano Calzavara","Lorenzo Cazzaro","Donald Gera","Salvatore Orlando"],"pdf_url":"https://arxiv.org/pdf/2410.04570v1.pdf","comment":"7 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.04534v1","updated":"2024-10-06T16:04:05Z","published":"2024-10-06T16:04:05Z","title":"UniMuMo: Unified Text, Music and Motion Generation","summary":"  We introduce UniMuMo, a unified multimodal model capable of taking arbitrary\ntext, music, and motion data as input conditions to generate outputs across all\nthree modalities. To address the lack of time-synchronized data, we align\nunpaired music and motion data based on rhythmic patterns to leverage existing\nlarge-scale music-only and motion-only datasets. By converting music, motion,\nand text into token-based representation, our model bridges these modalities\nthrough a unified encoder-decoder transformer architecture. To support multiple\ngeneration tasks within a single framework, we introduce several architectural\nimprovements. We propose encoding motion with a music codebook, mapping motion\ninto the same feature space as music. We introduce a music-motion parallel\ngeneration scheme that unifies all music and motion generation tasks into a\nsingle transformer decoder architecture with a single training task of\nmusic-motion joint generation. Moreover, the model is designed by fine-tuning\nexisting pre-trained single-modality models, significantly reducing\ncomputational demands. Extensive experiments demonstrate that UniMuMo achieves\ncompetitive results on all unidirectional generation benchmarks across music,\nmotion, and text modalities. Quantitative results are available in the\n\\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.\n","authors":["Han Yang","Kun Su","Yutong Zhang","Jiaben Chen","Kaizhi Qian","Gaowen Liu","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2410.04534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01920v2","updated":"2024-10-06T15:49:20Z","published":"2024-07-02T03:34:16Z","title":"To Forget or Not? Towards Practical Knowledge Unlearning for Large\n  Language Models","summary":"  Large Language Models (LLMs) trained on extensive corpora inevitably retain\nsensitive data, such as personal privacy information and copyrighted material.\nRecent advancements in knowledge unlearning involve updating LLM parameters to\nerase specific knowledge. However, current unlearning paradigms are mired in\nvague forgetting boundaries, often erasing knowledge indiscriminately. In this\nwork, we introduce KnowUnDo, a benchmark containing copyrighted content and\nuser privacy domains to evaluate if the unlearning process inadvertently erases\nessential knowledge. Our findings indicate that existing unlearning methods\noften suffer from excessive unlearning. To address this, we propose a simple\nyet effective method, MemFlex, which utilizes gradient information to precisely\ntarget and unlearn sensitive parameters. Experimental results show that MemFlex\nis superior to existing methods in both precise knowledge unlearning and\ngeneral knowledge retaining of LLMs. Code and dataset are released at\nhttps://github.com/zjunlp/KnowUnDo.\n","authors":["Bozhong Tian","Xiaozhuan Liang","Siyuan Cheng","Qingbin Liu","Mengru Wang","Dianbo Sui","Xi Chen","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.01920v2.pdf","comment":"EMNLP 2024 Findings; Code and dataset are released at\n  https://github.com/zjunlp/KnowUnDo"},{"id":"http://arxiv.org/abs/2410.04491v1","updated":"2024-10-06T14:10:28Z","published":"2024-10-06T14:10:28Z","title":"Knowledge-Guided Dynamic Modality Attention Fusion Framework for\n  Multimodal Sentiment Analysis","summary":"  Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the\nusers' sentiment. Previous methods focus on equally treating the contribution\nof each modality or statically using text as the dominant modality to conduct\ninteraction, which neglects the situation where each modality may become\ndominant. In this paper, we propose a Knowledge-Guided Dynamic Modality\nAttention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses\nsentiment knowledge to guide the model dynamically selecting the dominant\nmodality and adjusting the contributions of each modality. In addition, with\nthe obtained multimodal representation, the model can further highlight the\ncontribution of dominant modality through the correlation evaluation loss.\nExtensive experiments on four MSA benchmark datasets indicate that KuDA\nachieves state-of-the-art performance and is able to adapt to different\nscenarios of dominant modality.\n","authors":["Xinyu Feng","Yuming Lin","Lihua He","You Li","Liang Chang","Ya Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.04491v1.pdf","comment":"Accepted to EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2409.14708v2","updated":"2024-10-06T10:30:19Z","published":"2024-09-23T05:01:43Z","title":"A Multimedia Framework for Continuum Robots: Systematic, Computational,\n  and Control Perspectives","summary":"  Continuum robots, which often rely on interdisciplinary and multimedia\ncollaborations, have been increasingly recognized for their potential to\nrevolutionize the field of human-computer interaction (HCI) in varied\napplications due to their adaptive, responsive, and flexible characteristics.\nDespite their promises, the lack of an integrated framework poses a significant\nlimitation for both users and developers, resulting in inefficiency and\ncomplexity during preliminary developments. Thus, this paper introduces a\nunified framework for continuum robotic systems that addresses these challenges\nby integrating system architecture, dynamics computation, and control strategy\nwithin a computer-aided design (CAD) platform. The proposed method allows for\nefficient modeling and quick preview of the robot performance, and thus\nfacilitating iterative design and implementation, with a view to enhancing the\nquality of robot developments.\n","authors":["Po-Yu Hsieh","June-Hao Hou"],"pdf_url":"https://arxiv.org/pdf/2409.14708v2.pdf","comment":"9 pages, 10 figures, 1 table"}]},"2024-10-05T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2401.01497v3","updated":"2024-10-05T22:55:51Z","published":"2024-01-03T02:02:58Z","title":"A Pre-trained Sequential Recommendation Framework: Popularity Dynamics\n  for Zero-shot Transfer","summary":"  Sequential recommenders are crucial to the success of online applications,\n\\eg e-commerce, video streaming, and social media. While model architectures\ncontinue to improve, for every new application domain, we still have to train a\nnew model from scratch for high quality recommendations. On the other hand,\npre-trained language and vision models have shown great success in zero-shot or\nfew-shot adaptation to new application domains. Inspired by the success of\npre-trained models in peer AI fields, we propose a novel pre-trained sequential\nrecommendation framework: PrepRec. We learn universal item representations by\nmodeling item popularity dynamics. Through extensive experiments on five\nreal-world datasets, we show that PrepRec, without any auxiliary information,\ncan not only zero-shot transfer to a new domain, but achieve competitive\nperformance compared to state-of-the-art sequential recommender models with\nonly a fraction of the model size. In addition, with a simple post-hoc\ninterpolation, PrepRec can improve the performance of existing sequential\nrecommenders on average by 13.8\\% in Recall@10 and 29.5% in NDCG@10. We provide\nan anonymized implementation of PrepRec at\nhttps://anonymous.4open.science/r/PrepRec--2F60/\n","authors":["Junting Wang","Praneet Rathi","Hari Sundaram"],"pdf_url":"https://arxiv.org/pdf/2401.01497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04254v1","updated":"2024-10-05T18:22:15Z","published":"2024-10-05T18:22:15Z","title":"Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia","summary":"  Links are a fundamental part of information networks, turning isolated pieces\nof knowledge into a network of information that is much richer than the sum of\nits parts. However, adding a new link to the network is not trivial: it\nrequires not only the identification of a suitable pair of source and target\nentities but also the understanding of the content of the source to locate a\nsuitable position for the link in the text. The latter problem has not been\naddressed effectively, particularly in the absence of text spans in the source\nthat could serve as anchors to insert a link to the target entity. To bridge\nthis gap, we introduce and operationalize the task of entity insertion in\ninformation networks. Focusing on the case of Wikipedia, we empirically show\nthat this problem is, both, relevant and challenging for editors. We compile a\nbenchmark dataset in 105 languages and develop a framework for entity insertion\ncalled LocEI (Localized Entity Insertion) and its multilingual variant XLocEI.\nWe show that XLocEI outperforms all baseline models (including state-of-the-art\nprompt-based ranking with LLMs such as GPT-4) and that it can be applied in a\nzero-shot manner on languages not seen during training with minimal performance\ndrop. These findings are important for applying entity insertion models in\npractice, e.g., to support editors in adding links across the more than 300\nlanguage versions of Wikipedia.\n","authors":["Toms Feith","Akhil Arora","Martin Gerlach","Debjit Paul","Robert West"],"pdf_url":"https://arxiv.org/pdf/2410.04254v1.pdf","comment":"EMNLP 2024; 24 pages; 62 figures"},{"id":"http://arxiv.org/abs/2410.04231v1","updated":"2024-10-05T17:11:37Z","published":"2024-10-05T17:11:37Z","title":"Metadata-based Data Exploration with Retrieval-Augmented Generation for\n  Large Language Models","summary":"  Developing the capacity to effectively search for requisite datasets is an\nurgent requirement to assist data users in identifying relevant datasets\nconsidering the very limited available metadata. For this challenge, the\nutilization of third-party data is emerging as a valuable source for\nimprovement. Our research introduces a new architecture for data exploration\nwhich employs a form of Retrieval-Augmented Generation (RAG) to enhance\nmetadata-based data discovery. The system integrates large language models\n(LLMs) with external vector databases to identify semantic relationships among\ndiverse types of datasets. The proposed framework offers a new method for\nevaluating semantic similarity among heterogeneous data sources and for\nimproving data exploration. Our study includes experimental results on four\ncritical tasks: 1) recommending similar datasets, 2) suggesting combinable\ndatasets, 3) estimating tags, and 4) predicting variables. Our results\ndemonstrate that RAG can enhance the selection of relevant datasets,\nparticularly from different categories, when compared to conventional metadata\napproaches. However, performance varied across tasks and models, which confirms\nthe significance of selecting appropriate techniques based on specific use\ncases. The findings suggest that this approach holds promise for addressing\nchallenges in data exploration and discovery, although further refinement is\nnecessary for estimation tasks.\n","authors":["Teruaki Hayashi","Hiroki Sakaji","Jiayi Dai","Randy Goebel"],"pdf_url":"https://arxiv.org/pdf/2410.04231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04195v1","updated":"2024-10-05T15:17:07Z","published":"2024-10-05T15:17:07Z","title":"LLMTemporalComparator: A Tool for Analysing Differences in Temporal\n  Adaptations of Large Language Models","summary":"  This study addresses the challenges of analyzing temporal discrepancies in\nlarge language models (LLMs) trained on data from different time periods. To\nfacilitate the automatic exploration of these differences, we propose a novel\nsystem that compares in a systematic way the outputs of two LLM versions based\non user-defined queries. The system first generates a hierarchical topic\nstructure rooted in a user-specified keyword, allowing for an organized\ncomparison of topical categories. Subsequently, it evaluates the generated text\nby both LLMs to identify differences in vocabulary, information presentation,\nand underlying themes. This fully automated approach not only streamlines the\nidentification of shifts in public opinion and cultural norms but also enhances\nour understanding of the adaptability and robustness of machine learning\napplications in response to temporal changes. By fostering research in\ncontinual model adaptation and comparative summarization, this work contributes\nto the development of more transparent machine learning models capable of\ncapturing the nuances of evolving societal contexts.\n","authors":["Reinhard Friedrich Fritsch","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2410.04195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18952v5","updated":"2024-10-05T08:36:04Z","published":"2023-05-27T16:05:00Z","title":"Exploring the Practicality of Generative Retrieval on Dynamic Corpora","summary":"  Benchmarking the performance of information retrieval (IR) is mostly\nconducted with a fixed set of documents (static corpora). However, in realistic\nscenarios, this is rarely the case and the documents to be retrieved are\nconstantly updated and added. In this paper, we focus on Generative Retrievals\n(GR), which apply autoregressive language models to IR problems, and explore\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor real-world deployment of IR systems handling vast and ever-changing\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\nGR is more adaptable to evolving knowledge (4-11%), robust in learning\nknowledge with temporal information, and efficient in terms of inference FLOPs\n(x2), indexing time (x6), and storage footprint (x4) compared to Dual Encoders\n(DE), which are commonly used in retrieval systems. Our paper highlights the\npotential of GR for future use in practical IR systems within dynamic\nenvironments.\n","authors":["Chaeeun Kim","Soyoung Yoon","Hyunji Lee","Joel Jang","Sohee Yang","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2305.18952v5.pdf","comment":"published at EMNLP 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.04225v1","updated":"2024-10-05T16:42:23Z","published":"2024-10-05T16:42:23Z","title":"AIM 2024 Challenge on Video Super-Resolution Quality Assessment: Methods\n  and Results","summary":"  This paper presents the Video Super-Resolution (SR) Quality Assessment (QA)\nChallenge that was part of the Advances in Image Manipulation (AIM) workshop,\nheld in conjunction with ECCV 2024. The task of this challenge was to develop\nan objective QA method for videos upscaled 2x and 4x by modern image- and\nvideo-SR algorithms. QA methods were evaluated by comparing their output with\naggregate subjective scores collected from >150,000 pairwise votes obtained\nthrough crowd-sourced comparisons across 52 SR methods and 1124 upscaled\nvideos. The goal was to advance the state-of-the-art in SR QA, which had proven\nto be a challenging problem with limited applicability of traditional QA\nmethods. The challenge had 29 registered participants, and 5 teams had\nsubmitted their final results, all outperforming the current state-of-the-art.\nAll data, including the private test subset, has been made publicly available\non the challenge homepage at\nhttps://challenges.videoprocessing.ai/challenges/super-resolution-metrics-challenge.html\n","authors":["Ivan Molodetskikh","Artem Borisov","Dmitriy Vatolin","Radu Timofte","Jianzhao Liu","Tianwu Zhi","Yabin Zhang","Yang Li","Jingwen Xu","Yiting Liao","Qing Luo","Ao-Xiang Zhang","Peng Zhang","Haibo Lei","Linyan Jiang","Yaqing Li","Yuqin Cao","Wei Sun","Weixia Zhang","Yinan Sun","Ziheng Jia","Yuxin Zhu","Xiongkuo Min","Guangtao Zhai","Weihua Luo","Yupeng Z.","Hong Y"],"pdf_url":"https://arxiv.org/pdf/2410.04225v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.06995v2","updated":"2024-10-05T07:32:03Z","published":"2024-05-11T12:06:31Z","title":"Benchmarking Cross-Domain Audio-Visual Deception Detection","summary":"  Automated deception detection is crucial for assisting humans in accurately\nassessing truthfulness and identifying deceptive behavior. Conventional\ncontact-based techniques, like polygraph devices, rely on physiological signals\nto determine the authenticity of an individual's statements. Nevertheless,\nrecent developments in automated deception detection have demonstrated that\nmultimodal features derived from both audio and video modalities may outperform\nhuman observers on publicly available datasets. Despite these positive\nfindings, the generalizability of existing audio-visual deception detection\napproaches across different scenarios remains largely unexplored. To close this\ngap, we present the first cross-domain audio-visual deception detection\nbenchmark, that enables us to assess how well these methods generalize for use\nin real-world scenarios. We used widely adopted audio and visual features and\ndifferent architectures for benchmarking, comparing single-to-single and\nmulti-to-single domain generalization performance. To further exploit the\nimpacts using data from multiple source domains for training, we investigate\nthree types of domain sampling strategies, including domain-simultaneous,\ndomain-alternating, and domain-by-domain for multi-to-single domain\ngeneralization evaluation. We also propose an algorithm to enhance the\ngeneralization performance by maximizing the gradient inner products between\nmodality encoders, named ``MM-IDGM\". Furthermore, we proposed the\nAttention-Mixer fusion method to improve performance, and we believe that this\nnew cross-domain benchmark will facilitate future research in audio-visual\ndeception detection.\n","authors":["Xiaobao Guo","Zitong Yu","Nithish Muthuchamy Selvaraj","Bingquan Shen","Adams Wai-Kin Kong","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2405.06995v2.pdf","comment":"12 pages"}]},"2024-10-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.07176v1","updated":"2024-10-09T17:59:58Z","published":"2024-10-09T17:59:58Z","title":"Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models","summary":"  Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.\n","authors":["Fei Wang","Xingchen Wan","Ruoxi Sun","Jiefeng Chen","Sercan . Ark"],"pdf_url":"https://arxiv.org/pdf/2410.07176v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.07173v1","updated":"2024-10-09T17:59:33Z","published":"2024-10-09T17:59:33Z","title":"Do better language models have crisper vision?","summary":"  How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07170v1","updated":"2024-10-09T17:59:06Z","published":"2024-10-09T17:59:06Z","title":"One Initialization to Rule them All: Fine-tuning via Explained Variance\n  Adaptation","summary":"  Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.\n","authors":["Fabian Paischer","Lukas Hauzenberger","Thomas Schmied","Benedikt Alkin","Marc Peter Deisenroth","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2410.07170v1.pdf","comment":"10 pages + references and appendix, code available at\n  https://github.com/ml-jku/EVA"},{"id":"http://arxiv.org/abs/2410.07167v1","updated":"2024-10-09T17:59:04Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v1.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2410.07168v1","updated":"2024-10-09T17:59:04Z","published":"2024-10-09T17:59:04Z","title":"Sylber: Syllabic Embedding Representation of Speech from Raw Audio","summary":"  Syllables are compositional units of spoken language that play a crucial role\nin human speech perception and production. However, current neural speech\nrepresentations lack structure, resulting in dense token sequences that are\ncostly to process. To bridge this gap, we propose a new model, Sylber, that\nproduces speech representations with clean and robust syllabic structure.\nSpecifically, we propose a self-supervised model that regresses features on\nsyllabic segments distilled from a teacher model which is an exponential moving\naverage of the model in training. This results in a highly structured\nrepresentation of speech features, offering three key benefits: 1) a fast,\nlinear-time syllable segmentation algorithm, 2) efficient syllabic tokenization\nwith an average of 4.27 tokens per second, and 3) syllabic units better suited\nfor lexical and syntactic understanding. We also train token-to-speech\ngenerative models with our syllabic units and show that fully intelligible\nspeech can be reconstructed from these tokens. Lastly, we observe that\ncategorical perception, a linguistic phenomenon of speech perception, emerges\nnaturally in our model, making the embedding space more categorical and sparse\nthan previous self-supervised learning approaches. Together, we present a novel\nself-supervised approach for representing speech as syllables, with significant\npotential for efficient speech tokenization and spoken language modeling.\n","authors":["Cheol Jun Cho","Nicholas Lee","Akshat Gupta","Dhruv Agarwal","Ethan Chen","Alan W Black","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2410.07168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07166v1","updated":"2024-10-09T17:59:00Z","published":"2024-10-09T17:59:00Z","title":"Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making","summary":"  We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.\n","authors":["Manling Li","Shiyu Zhao","Qineng Wang","Kangrui Wang","Yu Zhou","Sanjana Srivastava","Cem Gokmen","Tony Lee","Li Erran Li","Ruohan Zhang","Weiyu Liu","Percy Liang","Li Fei-Fei","Jiayuan Mao","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.07166v1.pdf","comment":"Accepted for oral presentation at NeurIPS 2024 in the Datasets and\n  Benchmarks track"},{"id":"http://arxiv.org/abs/2410.07163v1","updated":"2024-10-09T17:58:12Z","published":"2024-10-09T17:58:12Z","title":"Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning","summary":"  In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple.\n","authors":["Chongyu Fan","Jiancheng Liu","Licong Lin","Jinghan Jia","Ruiqi Zhang","Song Mei","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.07163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07157v1","updated":"2024-10-09T17:56:15Z","published":"2024-10-09T17:56:15Z","title":"InstructG2I: Synthesizing Images from Multimodal Attributed Graphs","summary":"  In this paper, we approach an overlooked yet critical task Graph2Image:\ngenerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To\naddress these challenges, we propose a graph context-conditioned diffusion\nmodel called InstructG2I. InstructG2I first exploits the graph structure and\nmultimodal information to conduct informative neighbor sampling by combining\npersonalized page rank and re-ranking based on vision-language features. Then,\na Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary\nset of graph prompts to guide the denoising process of diffusion. Finally, we\npropose graph classifier-free guidance, enabling controllable generation by\nvarying the strength of graph guidance and multiple connected edges to a node.\nExtensive experiments conducted on three datasets from different domains\ndemonstrate the effectiveness and controllability of our approach. The code is\navailable at https://github.com/PeterGriffinJin/InstructG2I.\n","authors":["Bowen Jin","Ziqi Pang","Bingjun Guo","Yu-Xiong Wang","Jiaxuan You","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2410.07157v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.07147v1","updated":"2024-10-09T17:54:41Z","published":"2024-10-09T17:54:41Z","title":"Taking a turn for the better: Conversation redirection throughout the\n  course of mental-health therapy","summary":"  Mental-health therapy involves a complex conversation flow in which patients\nand therapists continuously negotiate what should be talked about next. For\nexample, therapists might try to shift the conversation's direction to keep the\ntherapeutic process on track and avoid stagnation, or patients might push the\ndiscussion towards issues they want to focus on.\n  How do such patient and therapist redirections relate to the development and\nquality of their relationship? To answer this question, we introduce a\nprobabilistic measure of the extent to which a certain utterance immediately\nredirects the flow of the conversation, accounting for both the intention and\nthe actual realization of such a change. We apply this new measure to\ncharacterize the development of patient-therapist relationships over multiple\nsessions in a very large, widely-used online therapy platform. Our analysis\nreveals that (1) patient control of the conversation's direction generally\nincreases relative to that of the therapist as their relationship progresses;\nand (2) patients who have less control in the first few sessions are\nsignificantly more likely to eventually express dissatisfaction with their\ntherapist and terminate the relationship.\n","authors":["Vivian Nguyen","Sang Min Jung","Lillian Lee","Thomas D. Hull","Cristian Danescu-Niculescu-Mizil"],"pdf_url":"https://arxiv.org/pdf/2410.07147v1.pdf","comment":"To appear in the Proceedings of EMNLP (Findings) 2024. Code available\n  at https://convokit.cornell.edu"},{"id":"http://arxiv.org/abs/2410.07145v1","updated":"2024-10-09T17:54:28Z","published":"2024-10-09T17:54:28Z","title":"Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling","summary":"  One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.\n","authors":["Yingfa Chen","Xinrong Zhang","Shengding Hu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.07145v1.pdf","comment":"21 pages, 18 figures"},{"id":"http://arxiv.org/abs/2410.07137v1","updated":"2024-10-09T17:53:06Z","published":"2024-10-09T17:53:06Z","title":"Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates","summary":"  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.\n","authors":["Xiaosen Zheng","Tianyu Pang","Chao Du","Qian Liu","Jing Jiang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.07137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07129v1","updated":"2024-10-09T17:51:55Z","published":"2024-10-09T17:51:55Z","title":"Mental Disorders Detection in the Era of Large Language Models","summary":"  This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.\n","authors":["Gleb Kuzmin","Petr Strepetov","Maksim Stankevich","Ivan Smirnov","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2410.07129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12877v2","updated":"2024-10-09T17:51:44Z","published":"2024-07-16T08:25:26Z","title":"ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models","summary":"  Assessing the quality of outputs generated by generative models, such as\nlarge language models and vision language models, presents notable challenges.\nTraditional methods for evaluation typically rely on either human assessments,\nwhich are resource-intensive, or automatic metrics that often show a low\ncorrelation with human judgment. Another common approach is to use deep\nlearning systems, which not only consume a substantial amount of compute and\ntime but also require extensive training data. In this study, we introduce a\ntuning-free framework called ReFeR, designed to evaluate generative outputs,\nincluding both text and images, by leveraging a 2-level hierarchy of LLMs and\nVLMs themselves. We rigorously evaluate our framework, ReFeR, across four\ndiverse evaluation tasks. The framework not only improves the accuracy of these\nevaluations, surpassing previous benchmarks but also generates constructive\nfeedback. Interestingly, the framework is also applicable to reasoning tasks.\nExperiments on four reasoning tasks demonstrate superior collective reasoning\nabilities of the framework. We present two variants of the framework:\nReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a\nmore cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient\nwhile being comparably accurate to ReFeR-Turbo. We make code, data and PIP\npackage publicly available. See this PIP URL\nhttps://pypi.org/project/refer-agents/ and this Git URL\nhttps://github.com/yaswanth-iitkgp/ReFeR_Code .\n","authors":["Yaswanth Narsupalli","Abhranil Chandra","Sreevatsa Muppirala","Manish Gupta","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2407.12877v2.pdf","comment":"Paper Under Review"},{"id":"http://arxiv.org/abs/2410.07118v1","updated":"2024-10-09T17:48:40Z","published":"2024-10-09T17:48:40Z","title":"Exploring the Readiness of Prominent Small Language Models for the\n  Democratization of Financial Literacy","summary":"  The use of small language models (SLMs), herein defined as models with less\nthan three billion parameters, is increasing across various domains and\napplications. Due to their ability to run on more accessible hardware and\npreserve user privacy, SLMs possess the potential to democratize access to\nlanguage models for individuals of different socioeconomic status and with\ndifferent privacy preferences. This study assesses several state-of-the-art\nSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama\nproject) for use in the financial domain to support the development of\nfinancial literacy LMs. Democratizing access to quality financial information\nfor those who are financially under educated is greatly needed in society,\nparticularly as new financial markets and products emerge and participation in\nfinancial markets increases due to ease of access. We are the first to examine\nthe use of open-source SLMs to democratize access to financial question\nanswering capabilities for individuals and students. To this end, we provide an\nanalysis of the memory usage, inference time, similarity comparisons to\nground-truth answers, and output readability of prominent SLMs to determine\nwhich models are most accessible and capable of supporting access to financial\ninformation. We analyze zero-shot and few-shot learning variants of the models.\nThe results suggest that some off-the-shelf SLMs merit further exploration and\nfine-tuning to prepare them for individual use, while others may have limits to\ntheir democratization.\n","authors":["Tagore Rao Kosireddy","Jeffrey D. Wall","Evan Lucas"],"pdf_url":"https://arxiv.org/pdf/2410.07118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07109v1","updated":"2024-10-09T17:45:47Z","published":"2024-10-09T17:45:47Z","title":"I Want to Break Free! Anti-Social Behavior and Persuasion Ability of\n  LLMs in Multi-Agent Settings with Social Hierarchy","summary":"  As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.\n","authors":["Gian Maria Campedelli","Nicol Penzo","Massimo Stefan","Roberto Dess","Marco Guerini","Bruno Lepri","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2410.07109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12108v2","updated":"2024-10-09T17:45:07Z","published":"2024-07-16T18:28:40Z","title":"Private prediction for large-scale synthetic text generation","summary":"  We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.\n","authors":["Kareem Amin","Alex Bie","Weiwei Kong","Alexey Kurakin","Natalia Ponomareva","Umar Syed","Andreas Terzis","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2407.12108v2.pdf","comment":"20 pages; updated figure + some new experiments from EMNLP 2024\n  findings camera-ready"},{"id":"http://arxiv.org/abs/2410.07103v1","updated":"2024-10-09T17:41:53Z","published":"2024-10-09T17:41:53Z","title":"Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context","summary":"  Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.\n","authors":["Sangwon Yu","Ik-hwan Kim","Jongyoon Song","Saehyung Lee","Junsung Park","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.07103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00260v2","updated":"2024-10-09T17:39:59Z","published":"2024-09-30T22:15:58Z","title":"DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining","summary":"  Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline.\n","authors":["Vinayak Arannil","Neha Narwal","Sourav Sanjukta Bhabesh","Sai Nikhil Thirandas","Darren Yow-Bang Wang","Graham Horwood","Alex Anto Chirayath","Gouri Pandeshwar"],"pdf_url":"https://arxiv.org/pdf/2410.00260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06112v3","updated":"2024-10-09T17:38:22Z","published":"2024-01-11T18:46:12Z","title":"Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed\n  Embeddings","summary":"  Word embedding is one of the most important components in natural language\nprocessing, but interpreting high-dimensional embeddings remains a challenging\nproblem. To address this problem, Independent Component Analysis (ICA) is\nidentified as an effective solution. ICA-transformed word embeddings reveal\ninterpretable semantic axes; however, the order of these axes are arbitrary. In\nthis study, we focus on this property and propose a novel method, Axis Tour,\nwhich optimizes the order of the axes. Inspired by Word Tour, a one-dimensional\nword embedding method, we aim to improve the clarity of the word embedding\nspace by maximizing the semantic continuity of the axes. Furthermore, we show\nthrough experiments on downstream tasks that Axis Tour yields better or\ncomparable low-dimensional embeddings compared to both PCA and ICA.\n","authors":["Hiroaki Yamagiwa","Yusuke Takase","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2401.06112v3.pdf","comment":"EMNLP 2024 Findings (short)"},{"id":"http://arxiv.org/abs/2410.07095v1","updated":"2024-10-09T17:34:27Z","published":"2024-10-09T17:34:27Z","title":"MLE-bench: Evaluating Machine Learning Agents on Machine Learning\n  Engineering","summary":"  We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.\n","authors":["Jun Shern Chan","Neil Chowdhury","Oliver Jaffe","James Aung","Dane Sherburn","Evan Mays","Giulio Starace","Kevin Liu","Leon Maksin","Tejal Patwardhan","Lilian Weng","Aleksander Mdry"],"pdf_url":"https://arxiv.org/pdf/2410.07095v1.pdf","comment":"10 pages. Plus 17 pages appendix. 8 figures. Equal contribution by\n  first seven authors. Authors randomized. Work by Neil Chowdhury done while at\n  OpenAI"},{"id":"http://arxiv.org/abs/2410.07094v1","updated":"2024-10-09T17:34:14Z","published":"2024-10-09T17:34:14Z","title":"An Approach for Auto Generation of Labeling Functions for Software\n  Engineering Chatbots","summary":"  Software engineering (SE) chatbots are increasingly gaining attention for\ntheir role in enhancing development processes. At the core of chatbots are the\nNatural Language Understanding platforms (NLUs), which enable them to\ncomprehend and respond to user queries. Before deploying NLUs, there is a need\nto train them with labeled data. However, acquiring such labeled data for SE\nchatbots is challenging due to the scarcity of high-quality datasets. This\nchallenge arises because training SE chatbots requires specialized vocabulary\nand phrases not found in typical language datasets. Consequently, chatbot\ndevelopers often resort to manually annotating user queries to gather the data\nnecessary for training effective chatbots, a process that is both\ntime-consuming and resource-intensive. Previous studies propose approaches to\nsupport chatbot practitioners in annotating users' posed queries. However,\nthese approaches require human intervention to generate rules, called labeling\nfunctions (LFs), that identify and categorize user queries based on specific\npatterns in the data. To address this issue, we propose an approach to\nautomatically generate LFs by extracting patterns from labeled user queries. We\nevaluate the effectiveness of our approach by applying it to the queries of\nfour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)\nand measure the performance improvement gained from training the NLU on the\nqueries labeled by the generated LFs. We find that the generated LFs\neffectively label data with AUC scores of up to 85.3%, and NLU's performance\nimprovement of up to 27.2% across the studied datasets. Furthermore, our\nresults show that the number of LFs used to generate LFs affects the labeling\nperformance. We believe that our approach can save time and resources in\nlabeling users' queries, allowing practitioners to focus on core chatbot\nfunctionalities.\n","authors":["Ebube Alor","Ahmad Abdellatif","SayedHassan Khatoonabadi","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2410.07094v1.pdf","comment":"Submitted to IEEE Transactions on Software Engineering for review"},{"id":"http://arxiv.org/abs/2410.07083v1","updated":"2024-10-09T17:24:28Z","published":"2024-10-09T17:24:28Z","title":"Stanceformer: Target-Aware Transformer for Stance Detection","summary":"  The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}\n","authors":["Krishna Garg","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2410.07083v1.pdf","comment":"16 pages, 2 figures, 14 tables including Appendix"},{"id":"http://arxiv.org/abs/2410.07076v1","updated":"2024-10-09T17:19:58Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v1.pdf","comment":"Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git"},{"id":"http://arxiv.org/abs/2410.07073v1","updated":"2024-10-09T17:16:22Z","published":"2024-10-09T17:16:22Z","title":"Pixtral 12B","summary":"  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.\n","authors":["Pravesh Agrawal","Szymon Antoniak","Emma Bou Hanna","Devendra Chaplot","Jessica Chudnovsky","Saurabh Garg","Theophile Gervet","Soham Ghosh","Amlie Hliou","Paul Jacob","Albert Q. Jiang","Timothe Lacroix","Guillaume Lample","Diego Las Casas","Thibaut Lavril","Teven Le Scao","Andy Lo","William Marshall","Louis Martin","Arthur Mensch","Pavankumar Muddireddy","Valera Nemychnikova","Marie Pellat","Patrick Von Platen","Nikhil Raghuraman","Baptiste Rozire","Alexandre Sablayrolles","Lucile Saulnier","Romain Sauvestre","Wendy Shang","Roman Soletskyi","Lawrence Stewart","Pierre Stock","Joachim Studnia","Sandeep Subramanian","Sagar Vaze","Thomas Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06809v3","updated":"2024-10-09T17:16:15Z","published":"2024-04-10T07:56:26Z","title":"Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation","summary":"  The rapid development of large language models has led to the widespread\nadoption of Retrieval-Augmented Generation (RAG), which integrates external\nknowledge to alleviate knowledge bottlenecks and mitigate hallucinations.\nHowever, the existing RAG paradigm inevitably suffers from the impact of flawed\ninformation introduced during the retrieval phrase, thereby diminishing the\nreliability and correctness of the generated outcomes. In this paper, we\npropose Credibility-aware Generation (CAG), a universally applicable framework\ndesigned to mitigate the impact of flawed information in RAG. At its core, CAG\naims to equip models with the ability to discern and process information based\non its credibility. To this end, we propose an innovative data transformation\nframework that generates data based on credibility, thereby effectively\nendowing models with the capability of CAG. Furthermore, to accurately evaluate\nthe models' capabilities of CAG, we construct a comprehensive benchmark\ncovering three critical real-world scenarios. Experimental results demonstrate\nthat our model can effectively understand and utilize credibility for\ngeneration, significantly outperform other models with retrieval augmentation,\nand exhibit resilience against the disruption caused by noisy documents,\nthereby maintaining robust performance. Moreover, our model supports customized\ncredibility, offering a wide range of potential applications.\n","authors":["Ruotong Pan","Boxi Cao","Hongyu Lin","Xianpei Han","Jia Zheng","Sirui Wang","Xunliang Cai","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2404.06809v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Our code, benchmark, and\n  models are available at https://github.com/panruotong/CAG"},{"id":"http://arxiv.org/abs/2410.07069v1","updated":"2024-10-09T17:14:50Z","published":"2024-10-09T17:14:50Z","title":"ReIFE: Re-evaluating Instruction-Following Evaluation","summary":"  The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.\n","authors":["Yixin Liu","Kejian Shi","Alexander R. Fabbri","Yilun Zhao","Peifeng Wang","Chien-Sheng Wu","Shafiq Joty","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2410.07069v1.pdf","comment":"GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result\n  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE"},{"id":"http://arxiv.org/abs/2408.11252v3","updated":"2024-10-09T17:12:50Z","published":"2024-08-21T00:17:59Z","title":"Counterfactuals As a Means for Evaluating Faithfulness of Attribution\n  Methods in Autoregressive Language Models","summary":"  Despite the widespread adoption of autoregressive language models,\nexplainability evaluation research has predominantly focused on span infilling\nand masked language models. Evaluating the faithfulness of an explanation\nmethod -- how accurately it explains the inner workings and decision-making of\nthe model -- is challenging because it is difficult to separate the model from\nits explanation. Most faithfulness evaluation techniques corrupt or remove\ninput tokens deemed important by a particular attribution (feature importance)\nmethod and observe the resulting change in the model's output. However, for\nautoregressive language models, this approach creates out-of-distribution\ninputs due to their next-token prediction training objective. In this study, we\npropose a technique that leverages counterfactual generation to evaluate the\nfaithfulness of attribution methods for autoregressive language models. Our\ntechnique generates fluent, in-distribution counterfactuals, making the\nevaluation protocol more reliable.\n","authors":["Sepehr Kamahi","Yadollah Yaghoobzadeh"],"pdf_url":"https://arxiv.org/pdf/2408.11252v3.pdf","comment":"Accepted to BlackboxNLP @ EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.07064v1","updated":"2024-10-09T17:06:57Z","published":"2024-10-09T17:06:57Z","title":"Data Selection via Optimal Control for Language Models","summary":"  This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which mitigates the quick exhaustion of\navailable web-crawled corpora. Our code, data, and model checkpoints can be\nfound in https://github.com/microsoft/LMOps/tree/main/data_selection.\n","authors":["Yuxian Gu","Li Dong","Hongning Wang","Yaru Hao","Qingxiu Dong","Furu Wei","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16570v5","updated":"2024-10-09T16:52:19Z","published":"2024-08-29T14:37:05Z","title":"Predictability maximization and the origins of word order harmony","summary":"  We address the linguistic problem of the sequential arrangement of a head and\nits dependents from an information theoretic perspective. In particular, we\nconsider the optimal placement of a head that maximizes the predictability of\nthe sequence. We assume that dependents are statistically independent given a\nhead, in line with the open-choice principle and the core assumptions of\ndependency grammar. We demonstrate the optimality of harmonic order, i.e.,\nplacing the head last maximizes the predictability of the head whereas placing\nthe head first maximizes the predictability of dependents. We also show that\npostponing the head is the optimal strategy to maximize its predictability\nwhile bringing it forward is the optimal strategy to maximize the\npredictability of dependents. We unravel the advantages of the strategy of\nmaximizing the predictability of the head over maximizing the predictability of\ndependents. Our findings shed light on the placements of the head adopted by\nreal languages or emerging in different kinds of experiments.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2408.16570v5.pdf","comment":"Minor corrections; references added"},{"id":"http://arxiv.org/abs/2410.07054v1","updated":"2024-10-09T16:51:21Z","published":"2024-10-09T16:51:21Z","title":"Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing","summary":"  Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases.\n","authors":["Weichuan Wang","Zhaoyi Li","Defu Lian","Chen Ma","Linqi Song","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2410.07054v1.pdf","comment":"20 pages, EMNLP'2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.07053v1","updated":"2024-10-09T16:51:10Z","published":"2024-10-09T16:51:10Z","title":"Robots in the Middle: Evaluating LLMs in Dispute Resolution","summary":"  Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms.\n","authors":["Jinzhe Tan","Hannes Westermann","Nikhil Reddy Pottanigari","Jaromr avelka","Sbastien Mees","Mia Godet","Karim Benyekhlef"],"pdf_url":"https://arxiv.org/pdf/2410.07053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07035v1","updated":"2024-10-09T16:15:36Z","published":"2024-10-09T16:15:36Z","title":"PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness","summary":"  Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.\n","authors":["Zekun Wang","Feiyu Duan","Yibo Zhang","Wangchunshu Zhou","Ke Xu","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2410.07035v1.pdf","comment":"39 pages. CP-Bench and LenCtrl-Bench are available in\n  https://huggingface.co/datasets/ZenMoore/CP-Bench and\n  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench"},{"id":"http://arxiv.org/abs/2410.07030v1","updated":"2024-10-09T16:13:19Z","published":"2024-10-09T16:13:19Z","title":"Clean Evaluations on Contaminated Visual Language Models","summary":"  How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.\n","authors":["Hongyuan Lu","Shujie Miao","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2410.07030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07025v1","updated":"2024-10-09T16:07:11Z","published":"2024-10-09T16:07:11Z","title":"Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback","summary":"  Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology.\n","authors":["Dennis Hein","Zhihong Chen","Sophie Ostmeier","Justin Xu","Maya Varma","Eduardo Pontes Reis","Arne Edward Michalson","Christian Bluethgen","Hyun Joo Shin","Curtis Langlotz","Akshay S Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2410.07025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02554v2","updated":"2024-10-09T16:07:04Z","published":"2022-08-04T09:53:22Z","title":"Vocabulary Transfer for Medical Texts","summary":"  Working within specific NLP subdomains presents significant challenges,\nprimarily due to a persistent deficit of data. Stringent privacy concerns and\nlimited data accessibility often drive this shortage. Additionally, the medical\ndomain demands high accuracy, where even marginal improvements in model\nperformance can have profound impacts. In this study, we investigate the\npotential of vocabulary transfer to enhance model performance in biomedical NLP\ntasks. Specifically, we focus on vocabulary extension, a technique that\ninvolves expanding the target vocabulary to incorporate domain-specific\nbiomedical terms. Our findings demonstrate that vocabulary extension, leads to\nmeasurable improvements in both downstream model performance and inference\ntime.\n","authors":["Priyanka Singh","Vladislav D. Mosin","Ivan P. Yamshchikov"],"pdf_url":"https://arxiv.org/pdf/2208.02554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12746v5","updated":"2024-10-09T16:04:39Z","published":"2024-06-18T16:06:38Z","title":"Diversify, Rationalize, and Combine: Ensembling Multiple QA Strategies\n  for Zero-shot Knowledge-based VQA","summary":"  Knowledge-based Visual Question-answering (K-VQA) often requires the use of\nbackground knowledge beyond the image. However, we discover that a single\nknowledge generation strategy is often insufficient for all K-VQA questions. To\nthis end, we propose Diversification, Evidence Truncation, and Combination for\nKnowledge-based Elucidation (DietCoke), which utilizes a bundle of\ncomplementary question-answering tactics and aggregates their answers using\ntextual rationales. DietCoke comprises of three stages: diversification,\nrationalization, and ensemble. The diversification stage generates three\ndistinctive decision contexts, each leading to its own answer candidate. The\nrationalization stage generates two rationales, the automatic rationale and the\nmechanistic rationale, for each answer candidate using decorrelated techniques.\nFinally, in the ensemble stage, an LLM informed by the rationales selects one\nanswer from the three candidates. Experiments show that DietCoke significantly\noutperforms state-of-the-art LLM-based baselines by 2.8% on OK-VOA and 4.7% on\nA-OKVOA and that the strategies in the ensembles are highly complementary. Code\nis available at: https://github.com/limiaoyu/DietCoke\n","authors":["Miaoyu Li","Haoxin Li","Zilin Du","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2406.12746v5.pdf","comment":"Accepted to Findings of EMNLP2024"},{"id":"http://arxiv.org/abs/2410.07009v1","updated":"2024-10-09T15:52:48Z","published":"2024-10-09T15:52:48Z","title":"Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based\n  Outline-guided Generation","summary":"  The patent domain is gaining attention in natural language processing\nresearch, offering practical applications in streamlining the patenting process\nand providing challenging benchmarks for large language models (LLMs). However,\nthe generation of the description sections of patents, which constitute more\nthan 90% of the patent document, has not been studied to date. We address this\ngap by introducing the task of outline-guided paper-to-patent generation, where\nan academic paper provides the technical specification of the invention and an\noutline conveys the desired patent structure. We present PAP2PAT, a new\nchallenging benchmark of 1.8k patent-paper pairs with document outlines,\ncollected using heuristics that reflect typical research lab practices. Our\nexperiments with current open-weight LLMs and outline-guided chunk-based\ngeneration show that they can effectively use information from the paper but\nstruggle with repetitions, likely due to the inherent repetitiveness of patent\nlanguage. We release our data and code.\n","authors":["Valentin Knappich","Simon Razniewski","Anna Htty","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2410.07009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07002v1","updated":"2024-10-09T15:45:52Z","published":"2024-10-09T15:45:52Z","title":"CursorCore: Assist Programming through Aligning Anything","summary":"  Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.\n","authors":["Hao Jiang","Qi Liu","Rui Li","Shengyu Ye","Shijin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v3","updated":"2024-10-09T15:44:36Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v3.pdf","comment":"Accepted by Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.13053v3","updated":"2024-10-09T15:33:10Z","published":"2024-05-19T20:46:07Z","title":"MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models","summary":"  The pretrain+fine-tune paradigm is foundational for deploying large language\nmodels (LLMs) across various downstream applications. Within this framework,\nLow-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning\n(PEFT), producing numerous reusable task-specific LoRA adapters. However, this\napproach requires explicit task intention selection, posing challenges for\nautonomous task sensing and switching during inference with multiple existing\nLoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA\n(Multiple-tasks embedded LoRA), a scalable and efficient framework that reuses\nmultiple task-specific LoRA adapters into the base LLM via a full-mode\nMixture-of-Experts (MoE) architecture. This framework also includes novel MoE\nforward acceleration strategies to address the efficiency challenges of\ntraditional MoE implementations. Our evaluation, using the LlaMA2-13B and\nLlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA,\ndemonstrates equivalent performance with the traditional PEFT method. Moreover,\nthe LLM equipped with MeteoRA achieves superior performance in handling\ncomposite tasks, effectively solving ten sequential problems in a single\ninference pass, thereby demonstrating the framework's enhanced capability for\ntimely adapter switching.\n","authors":["Jingwei Xu","Junyu Lai","Yunpeng Huang"],"pdf_url":"https://arxiv.org/pdf/2405.13053v3.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2405.12109v2","updated":"2024-10-09T15:25:25Z","published":"2024-05-20T15:25:18Z","title":"Linguistic Structure from a Bottleneck on Sequential Information\n  Processing","summary":"  Human language is a unique form of communication in the natural world,\ndistinguished by its structured nature. Most fundamentally, it is systematic,\nmeaning that signals can be broken down into component parts that are\nindividually meaningful -- roughly, words -- which are combined in a regular\nway to form sentences. Furthermore, the way in which these parts are combined\nmaintains a kind of locality: words are usually concatenated together, and they\nform contiguous phrases, keeping related parts of sentences close to each\nother. We address the challenge of understanding how these basic properties of\nlanguage arise from broader principles of efficient communication under\ninformation processing constraints. Here we show that natural-language-like\nsystematicity arises in codes that are constrained by predictive information, a\nmeasure of the amount of information that must be extracted from the past of a\nsequence in order to predict its future. In simulations, we show that such\ncodes approximately factorize their source distributions, and then express the\nresulting factors systematically and locally. Next, in a series of\ncross-linguistic corpus studies, we show that human languages are structured to\nhave low predictive information at the levels of phonology, morphology, syntax,\nand semantics. Our result suggests that human language performs a sequential,\ndiscrete form of Independent Components Analysis on the statistical\ndistribution over meanings that need to be expressed. It establishes a link\nbetween the statistical and algebraic structure of human language, and\nreinforces the idea that the structure of human language is shaped by\ncommunication under cognitive constraints.\n","authors":["Richard Futrell","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2405.12109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12176v2","updated":"2024-10-09T15:23:44Z","published":"2024-07-16T21:03:14Z","title":"GPT-4V Cannot Generate Radiology Reports Yet","summary":"  GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.\n","authors":["Yuyang Jiang","Chacha Chen","Dang Nguyen","Benjamin M. Mervak","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2407.12176v2.pdf","comment":"24 pages, 3 figures, code:\n  https://github.com/YuyangJ0/GPT-4V-evaluation-radiology-report"},{"id":"http://arxiv.org/abs/2410.06981v1","updated":"2024-10-09T15:18:57Z","published":"2024-10-09T15:18:57Z","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","summary":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality.\n","authors":["Michael Lan","Philip Torr","Austin Meek","Ashkan Khakzar","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.06981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06973v1","updated":"2024-10-09T15:11:13Z","published":"2024-10-09T15:11:13Z","title":"Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara","summary":"  In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.\n","authors":["Azree Nazri","Olalekan Agbolade","Faisal Aziz"],"pdf_url":"https://arxiv.org/pdf/2410.06973v1.pdf","comment":"20 pages, 5 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.06965v1","updated":"2024-10-09T15:02:34Z","published":"2024-10-09T15:02:34Z","title":"Uncovering Factor Level Preferences to Improve Human-Model Alignment","summary":"  Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.\n","authors":["Juhyun Oh","Eunsu Kim","Jiseon Kim","Wenda Xu","Inha Cha","William Yang Wang","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2410.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19919v2","updated":"2024-10-09T14:57:48Z","published":"2024-09-30T03:48:54Z","title":"Understanding Higher-Order Correlations Among Semantic Components in\n  Embeddings","summary":"  Independent Component Analysis (ICA) offers interpretable semantic components\nof embeddings. While ICA theory assumes that embeddings can be linearly\ndecomposed into independent components, real-world data often do not satisfy\nthis assumption. Consequently, non-independencies remain between the estimated\ncomponents, which ICA cannot eliminate. We quantified these non-independencies\nusing higher-order correlations and demonstrated that when the higher-order\ncorrelation between two components is large, it indicates a strong semantic\nassociation between them, along with many words sharing common meanings with\nboth components. The entire structure of non-independencies was visualized\nusing a maximum spanning tree of semantic components. These findings provide\ndeeper insights into embeddings through ICA.\n","authors":["Momose Oyama","Hiroaki Yamagiwa","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2409.19919v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06961v1","updated":"2024-10-09T14:57:31Z","published":"2024-10-09T14:57:31Z","title":"Self-Boosting Large Language Models with Synthetic Preference Data","summary":"  Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.\n","authors":["Qingxiu Dong","Li Dong","Xingxing Zhang","Zhifang Sui","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.06961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06949v1","updated":"2024-10-09T14:45:45Z","published":"2024-10-09T14:45:45Z","title":"Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach","summary":"  In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.\n","authors":["Xuanming Zhang","Yuxuan Chen","Yuan Yuan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06949v1.pdf","comment":"26 pages, 7 figures. Submitted ICLR 2025"},{"id":"http://arxiv.org/abs/2410.06944v1","updated":"2024-10-09T14:38:49Z","published":"2024-10-09T14:38:49Z","title":"CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on\n  Relatively Free Word Ordered and Morphologically Rich Low Resource Languages","summary":"  Neural dependency parsing has achieved remarkable performance for low\nresource morphologically rich languages. It has also been well-studied that\nmorphologically rich languages exhibit relatively free word order. This prompts\na fundamental investigation: Is there a way to enhance dependency parsing\nperformance, making the model robust to word order variations utilizing the\nrelatively free word order nature of morphologically rich languages? In this\nwork, we examine the robustness of graph-based parsing architectures on 7\nrelatively free word order languages. We focus on scrutinizing essential\nmodifications such as data augmentation and the removal of position encoding\nrequired to adapt these architectures accordingly. To this end, we propose a\ncontrastive self-supervised learning method to make the model robust to word\norder variations. Furthermore, our proposed modification demonstrates a\nsubstantial average gain of 3.03/2.95 points in 7 relatively free word order\nlanguages, as measured by the UAS/LAS Score metric when compared to the best\nperforming baseline.\n","authors":["Pretam Ray","Jivnesh Sandhan","Amrith Krishna","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2410.06944v1.pdf","comment":"Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.20271v2","updated":"2024-10-09T14:30:08Z","published":"2024-07-25T07:09:35Z","title":"Learn while Unlearn: An Iterative Unlearning Framework for Generative\n  Language Models","summary":"  Recent advancements in machine learning, particularly in Natural Language\nProcessing (NLP), have led to the development of sophisticated models trained\non extensive datasets, yet raising concerns about the potential leakage of\nsensitive information. In response, regulatory measures such as the European\nUnion's General Data Protection Regulation (GDPR) have driven increasing\ninterest in Machine Unlearning techniques, which enable models to selectively\nforget specific data entries. Early approaches primarily relied on\npre-processing methods, while more recent research has shifted towards\ntraining-based unlearning techniques. Despite their effectiveness, most\nexisting methods require access to the original training data, which is often\ninaccessible. Additionally, directly applying unlearning techniques bear the\ncost of undermining the model's expressive capabilities. To address these\nchallenges, we introduce the Iterative Contrastive Unlearning (ICU) framework,\nwhich consists of three core components: A Knowledge Unlearning Induction\nmodule designed to remove specific knowledge through an unlearning loss; A\nContrastive Learning Enhancement module to preserve the model's expressive\ncapabilities against the pure unlearning goal; And an Iterative Unlearning\nRefinement module that dynamically assess the unlearning extent on specific\ndata pieces and make iterative update. Experimental results demonstrate the\nefficacy of our ICU method in unlearning sensitive information while\nmaintaining the model's overall performance, offering a promising solution for\nprivacy-conscious machine learning applications.\n","authors":["Haoyu Tang","Ye Liu","Xukai Liu","Kai Zhang","Yanghai Zhang","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.20271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01899v2","updated":"2024-10-09T14:25:00Z","published":"2024-07-02T02:50:15Z","title":"Scope-enhanced Compositional Semantic Parsing for DRT","summary":"  Discourse Representation Theory (DRT) distinguishes itself from other\nsemantic representation frameworks by its ability to model complex semantic and\ndiscourse phenomena through structural nesting and variable binding. While\nseq2seq models hold the state of the art on DRT parsing, their accuracy\ndegrades with the complexity of the sentence, and they sometimes struggle to\nproduce well-formed DRT representations. We introduce the AMS parser, a\ncompositional, neurosymbolic semantic parser for DRT. It rests on a novel\nmechanism for predicting quantifier scope. We show that the AMS parser reliably\nproduces well-formed outputs and performs well on DRT parsing, especially on\ncomplex sentences.\n","authors":["Xiulin Yang","Jonas Groschwitz","Alexander Koller","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2407.01899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06916v1","updated":"2024-10-09T14:15:30Z","published":"2024-10-09T14:15:30Z","title":"SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration","summary":"  Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text.\n","authors":["Heming Xia","Yongqi Li","Jun Zhang","Cunxiao Du","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06913v1","updated":"2024-10-09T14:12:51Z","published":"2024-10-09T14:12:51Z","title":"Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning","summary":"  Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github.\n","authors":["Runchuan Zhu","Zhipeng Ma","Jiang Wu","Junyuan Gao","Jiaqi Wang","Dahua Lin","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2410.06913v1.pdf","comment":"Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding\n  author: Conghui He"},{"id":"http://arxiv.org/abs/2410.02492v2","updated":"2024-10-09T14:07:15Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v2.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2410.06898v1","updated":"2024-10-09T13:59:34Z","published":"2024-10-09T13:59:34Z","title":"Generative Model for Less-Resourced Language with 1 billion parameters","summary":"  Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model.\n","authors":["Domen Vre","Martin Boi","Alja Potonik","Toma Martini","Marko Robnik-ikonja"],"pdf_url":"https://arxiv.org/pdf/2410.06898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06886v1","updated":"2024-10-09T13:47:50Z","published":"2024-10-09T13:47:50Z","title":"FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding","summary":"  The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications.\n","authors":["Jingyang Deng","Zhengyang Shen","Boyang Wang","Lixin Su","Suqi Cheng","Ying Nie","Junfeng Wang","Dawei Yin","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.06886v1.pdf","comment":"Accepted by the 27th European Conference on Artificial Intelligence\n  (ECAI-2024), this is the full version of the paper including technical\n  appendices. This final version features enhanced formatting and corrections\n  to errors present in other online versions. We regret any inconvenience this\n  may have caused our readers"},{"id":"http://arxiv.org/abs/2409.06927v3","updated":"2024-10-09T13:39:27Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v3.pdf","comment":"9 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2305.10652v4","updated":"2024-10-09T13:06:48Z","published":"2023-05-18T02:19:05Z","title":"Speech Separation based on Contrastive Learning and Deep Modularization","summary":"  The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly.\n","authors":["Peter Ochieng"],"pdf_url":"https://arxiv.org/pdf/2305.10652v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2212.00369"},{"id":"http://arxiv.org/abs/2410.06846v1","updated":"2024-10-09T13:06:43Z","published":"2024-10-09T13:06:43Z","title":"Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity","summary":"  Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2410.06846v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.06845v1","updated":"2024-10-09T13:06:40Z","published":"2024-10-09T13:06:40Z","title":"MentalArena: Self-play Training of Language Models for Diagnosis and\n  Treatment of Mental Health Disorders","summary":"  Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main\n","authors":["Cheng Li","May Fung","Qingyun Wang","Chi Han","Manling Li","Jindong Wang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2410.06845v1.pdf","comment":"Technical Report; 27 pages"},{"id":"http://arxiv.org/abs/2405.14722v4","updated":"2024-10-09T12:48:03Z","published":"2024-05-23T15:51:24Z","title":"DAPE: Data-Adaptive Positional Encoding for Length Extrapolation","summary":"  Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Minbin Huang","Jingyao Li","Jing Xiong","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2405.14722v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.02966v3","updated":"2024-10-09T12:46:40Z","published":"2024-03-05T13:43:58Z","title":"Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering","summary":"  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.\n","authors":["Sungho Ko","Hyunjin Cho","Hyungjoo Chae","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2403.02966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04798v2","updated":"2024-10-09T12:37:08Z","published":"2024-10-07T07:21:49Z","title":"DAPE V2: Process Attention Score as Feature Map for Length Extrapolation","summary":"  The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.\n","authors":["Chuanyang Zheng","Yihang Gao","Han Shi","Jing Xiong","Jiankai Sun","Jingyao Li","Minbin Huang","Xiaozhe Ren","Michael Ng","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2410.04798v2.pdf","comment":"Tech Report. arXiv admin note: text overlap with arXiv:2405.14722"},{"id":"http://arxiv.org/abs/2405.17264v2","updated":"2024-10-09T12:34:19Z","published":"2024-05-27T15:22:58Z","title":"On the Noise Robustness of In-Context Learning for Text Generation","summary":"  Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations.\n","authors":["Hongfu Gao","Feipeng Zhang","Wenyu Jiang","Jun Shu","Feng Zheng","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2405.17264v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.18708v4","updated":"2024-10-09T12:29:38Z","published":"2024-09-27T12:54:13Z","title":"Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with\n  ASCII Art to Mask Profanity","summary":"  We introduce a novel family of adversarial attacks that exploit the inability\nof language models to interpret ASCII art. To evaluate these attacks, we\npropose the ToxASCII benchmark and develop two custom ASCII art fonts: one\nleveraging special tokens and another using text-filled letter shapes. Our\nattacks achieve a perfect 1.0 Attack Success Rate across ten models, including\nOpenAI's o1-preview and LLaMA 3.1.\n  Warning: this paper contains examples of toxic language used for research\npurposes.\n","authors":["Sergey Berezin","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2409.18708v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19759v2","updated":"2024-10-09T12:20:33Z","published":"2024-06-28T08:59:24Z","title":"Breaking the Script Barrier in Multilingual Pre-Trained Language Models\n  with Transliteration-Based Post-Training Alignment","summary":"  Multilingual pre-trained models (mPLMs) have shown impressive performance on\ncross-lingual transfer tasks. However, the transfer performance is often\nhindered when a low-resource target language is written in a different script\nthan the high-resource source language, even though the two languages may be\nrelated or share parts of their vocabularies. Inspired by recent work that uses\ntransliteration to address this problem, our paper proposes a\ntransliteration-based post-pretraining alignment (PPA) method aiming to improve\nthe cross-lingual alignment between languages using diverse scripts. We select\ntwo areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and\n$\\textbf{South+East Asian Languages}$, wherein the languages are mutually\ninfluenced but use different scripts. We apply our method to these language\ngroups and conduct extensive experiments on a spectrum of downstream tasks. The\nresults show that after PPA, models consistently outperform the original model\n(up to 50% for some tasks) in English-centric transfer. In addition, when we\nuse languages other than English as sources in transfer, our method obtains\neven larger improvements. We will make our code and models publicly available\nat \\url{https://github.com/cisnlp/Transliteration-PPA}.\n","authors":["Orgest Xhelili","Yihong Liu","Hinrich Schtze"],"pdf_url":"https://arxiv.org/pdf/2406.19759v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2405.19846v5","updated":"2024-10-09T12:14:22Z","published":"2024-05-30T08:50:55Z","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model","summary":"  Recent advancements in large language models (LLMs) have highlighted the\nimportance of extending context lengths for handling complex tasks. While\ntraditional methods for training on long contexts often use filtered long\ndocuments, these approaches lead to domain imbalances, limiting model\nperformance. To address this, techniques like random document concatenation\n(Standard) and similarity-based methods (KNN, ICLM) have been developed.\nHowever, they either sacrifice semantic coherence or diversity. To balance both\naspects, we introduce Quest, a query-centric data synthesis method aggregating\nsemantically relevant yet diverse documents. Quest uses a generative model to\npredict potential queries for each document, grouping documents with similar\nqueries and keywords. Extensive experiments demonstrate Quest's superior\nperformance on long-context tasks, achieving remarkable results with context\nlengths of up to 1M tokens and confirming its scalability across various model\nsizes.\n","authors":["Chaochen Gao","Xing Wu","Qi Fu","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2405.19846v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18231v2","updated":"2024-10-09T12:11:15Z","published":"2024-04-28T15:56:41Z","title":"From Persona to Personalization: A Survey on Role-Playing Language\n  Agents","summary":"  Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.\n","authors":["Jiangjie Chen","Xintao Wang","Rui Xu","Siyu Yuan","Yikai Zhang","Wei Shi","Jian Xie","Shuang Li","Ruihan Yang","Tinghui Zhu","Aili Chen","Nianqi Li","Lida Chen","Caiyu Hu","Siye Wu","Scott Ren","Ziquan Fu","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.18231v2.pdf","comment":"Accepted to TMLR 2024"},{"id":"http://arxiv.org/abs/2410.06809v1","updated":"2024-10-09T12:09:30Z","published":"2024-10-09T12:09:30Z","title":"Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level","summary":"  Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.\n","authors":["Xinyi Zeng","Yuying Shang","Yutao Zhu","Jiawei Chen","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06809v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.03278v2","updated":"2024-10-09T12:07:08Z","published":"2024-10-04T09:50:45Z","title":"What do Large Language Models Need for Machine Translation Evaluation?","summary":"  Leveraging large language models (LLMs) for various natural language\nprocessing tasks has led to superlative claims about their performance. For the\nevaluation of machine translation (MT), existing research shows that LLMs are\nable to achieve results comparable to fine-tuned multilingual pre-trained\nlanguage models. In this paper, we explore what translation information, such\nas the source, reference, translation errors and annotation guidelines, is\nneeded for LLMs to evaluate MT quality. In addition, we investigate prompting\ntechniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for\neight language pairs covering high-, medium- and low-resource languages,\nleveraging varying LLM variants. Our findings indicate the importance of\nreference translations for an LLM-based evaluation. While larger models do not\nnecessarily fare better, they tend to benefit more from CoT prompting, than\nsmaller models. We also observe that LLMs do not always provide a numerical\nscore when generating evaluations, which poses a question on their reliability\nfor the task. Our work presents a comprehensive analysis for\nresource-constrained and training-less LLM-based evaluation of machine\ntranslation. We release the accrued prompt templates, code and data publicly\nfor reproducibility.\n","authors":["Shenbin Qian","Archchana Sindhujan","Minnie Kabra","Diptesh Kanojia","Constantin Orsan","Tharindu Ranasinghe","Frdric Blain"],"pdf_url":"https://arxiv.org/pdf/2410.03278v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.17774v2","updated":"2024-10-09T11:59:34Z","published":"2024-09-26T12:11:28Z","title":"Faithfulness and the Notion of Adversarial Sensitivity in NLP\n  Explanations","summary":"  Faithfulness is arguably the most critical metric to assess the reliability\nof explainable AI. In NLP, current methods for faithfulness evaluation are\nfraught with discrepancies and biases, often failing to capture the true\nreasoning of models. We introduce Adversarial Sensitivity as a novel approach\nto faithfulness evaluation, focusing on the explainer's response when the model\nis under adversarial attack. Our method accounts for the faithfulness of\nexplainers by capturing sensitivity to adversarial input changes. This work\naddresses significant limitations in existing evaluation techniques, and\nfurthermore, quantifies faithfulness from a crucial yet underexplored paradigm.\n","authors":["Supriya Manna","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2409.17774v2.pdf","comment":"Accepted as a Full Paper at EMNLP 2024 Workshop BlackBoxNLP"},{"id":"http://arxiv.org/abs/2410.06802v1","updated":"2024-10-09T11:58:40Z","published":"2024-10-09T11:58:40Z","title":"Seg2Act: Global Context-aware Action Generation for Document Logical\n  Structuring","summary":"  Document logical structuring aims to extract the underlying hierarchical\nstructure of documents, which is crucial for document intelligence. Traditional\napproaches often fall short in handling the complexity and the variability of\nlengthy documents. To address these issues, we introduce Seg2Act, an\nend-to-end, generation-based method for document logical structuring,\nrevisiting logical structure extraction as an action generation task.\nSpecifically, given the text segments of a document, Seg2Act iteratively\ngenerates the action sequence via a global context-aware generative model, and\nsimultaneously updates its global context and current logical structure based\non the generated actions. Experiments on ChCatExt and HierDoc datasets\ndemonstrate the superior performance of Seg2Act in both supervised and transfer\nlearning settings.\n","authors":["Zichao Li","Shaojie He","Meng Liao","Xuanang Chen","Yaojie Lu","Hongyu Lin","Yanxiong Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2410.06802v1.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2311.08481v2","updated":"2024-10-09T11:54:38Z","published":"2023-11-14T19:15:55Z","title":"Functionality learning through specification instructions","summary":"  Test suites assess natural language processing models' performance on\nspecific functionalities: cases of interest involving model robustness,\nfairness, or particular linguistic capabilities. This paper introduces\nspecification instructions: text descriptions specifying fine-grained\ntask-specific behaviors. For each functionality in a suite, we generate an\ninstruction that describes it. We combine the specification instructions to\ncreate specification-augmented prompts, which we feed to language models\npre-trained on natural instruction data.\n  We conduct experiments to measure how optimizing for some functionalities may\nnegatively impact functionalities that are not covered by the specification\nset. Our analyses across four tasks and models of diverse sizes and families\nshow that smaller models struggle to follow specification instructions.\nHowever, larger models (>~3B params.) can benefit from specifications and --\nsurprisingly -- even generalize certain desirable behaviors across\nfunctionalities.\n","authors":["Pedro Henrique Luz de Araujo","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2311.08481v2.pdf","comment":"36 pages, 8 figures. Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.06795v1","updated":"2024-10-09T11:46:32Z","published":"2024-10-09T11:46:32Z","title":"From Pixels to Tokens: Revisiting Object Hallucinations in Large\n  Vision-Language Models","summary":"  Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.\n","authors":["Yuying Shang","Xinyi Zeng","Yutao Zhu","Xiao Yang","Zhengwei Fang","Jingyuan Zhang","Jiawei Chen","Zinan Liu","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05949v6","updated":"2024-10-09T11:46:24Z","published":"2024-01-11T14:38:19Z","title":"Universal Vulnerabilities in Large Language Models: Backdoor Attacks for\n  In-context Learning","summary":"  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Despite being widely applied, in-context learning is\nvulnerable to malicious attacks. In this work, we raise security concerns\nregarding this paradigm. Our studies demonstrate that an attacker can\nmanipulate the behavior of large language models by poisoning the demonstration\ncontext, without the need for fine-tuning the model. Specifically, we design a\nnew backdoor attack method, named ICLAttack, to target large language models\nbased on in-context learning. Our method encompasses two types of attacks:\npoisoning demonstration examples and poisoning demonstration prompts, which can\nmake models behave in alignment with predefined intentions. ICLAttack does not\nrequire additional fine-tuning to implant a backdoor, thus preserving the\nmodel's generality. Furthermore, the poisoned examples are correctly labeled,\nenhancing the natural stealth of our attack method. Extensive experimental\nresults across several language models, ranging in size from 1.3B to 180B\nparameters, demonstrate the effectiveness of our attack method, exemplified by\na high average attack success rate of 95.0% across the three datasets on OPT\nmodels.\n","authors":["Shuai Zhao","Meihuizi Jia","Luu Anh Tuan","Fengjun Pan","Jinming Wen"],"pdf_url":"https://arxiv.org/pdf/2401.05949v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06917v3","updated":"2024-10-09T11:17:46Z","published":"2024-07-09T14:52:52Z","title":"Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in\n  Large Language Models","summary":"  Large language models (LLMs) have been shown to propagate and amplify harmful\nstereotypes, particularly those that disproportionately affect marginalised\ncommunities. To understand the effect of these stereotypes more\ncomprehensively, we introduce GlobalBias, a dataset of 876k sentences\nincorporating 40 distinct gender-by-ethnicity groups alongside descriptors\ntypically used in bias literature, which enables us to study a broad set of\nstereotypes from around the world. We use GlobalBias to directly probe a suite\nof LMs via perplexity, which we use as a proxy to determine how certain\nstereotypes are represented in the model's internal representations. Following\nthis, we generate character profiles based on given names and evaluate the\nprevalence of stereotypes in model outputs. We find that the demographic groups\nassociated with various stereotypes remain consistent across model likelihoods\nand model outputs. Furthermore, larger models consistently display higher\nlevels of stereotypical outputs, even when explicitly instructed not to.\n","authors":["Zara Siddique","Liam D. Turner","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2407.06917v3.pdf","comment":"Accepted to EMNLP Main 2024"},{"id":"http://arxiv.org/abs/2410.06765v1","updated":"2024-10-09T10:53:18Z","published":"2024-10-09T10:53:18Z","title":"To Preserve or To Compress: An In-Depth Study of Connector Selection in\n  Multimodal Large Language Models","summary":"  In recent years, multimodal large language models (MLLMs) have garnered\nsignificant attention from both industry and academia. However, there is still\nconsiderable debate on constructing MLLM architectures, particularly regarding\nthe selection of appropriate connectors for perception tasks of varying\ngranularities. This paper systematically investigates the impact of connectors\non MLLM performance. Specifically, we classify connectors into\nfeature-preserving and feature-compressing types. Utilizing a unified\nclassification standard, we categorize sub-tasks from three comprehensive\nbenchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained\nperception, fine-grained perception, and reasoning, and evaluate the\nperformance. Our findings reveal that feature-preserving connectors excel in\n\\emph{fine-grained perception} tasks due to their ability to retain detailed\nvisual information. In contrast, feature-compressing connectors, while less\neffective in fine-grained perception tasks, offer significant speed advantages\nand perform comparably in \\emph{coarse-grained perception} and \\emph{reasoning}\ntasks. These insights are crucial for guiding MLLM architecture design and\nadvancing the optimization of MLLM architectures.\n","authors":["Junyan Lin","Haoran Chen","Dawei Zhu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06765v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.03553v2","updated":"2024-10-09T10:49:08Z","published":"2024-10-04T16:02:50Z","title":"Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose\n  Protein Understanding","summary":"  Proteins, as essential biomolecules, play a central role in biological\nprocesses, including metabolic reactions and DNA replication. Accurate\nprediction of their properties and functions is crucial in biological\napplications. Recent development of protein language models (pLMs) with\nsupervised fine tuning provides a promising solution to this problem. However,\nthe fine-tuned model is tailored for particular downstream prediction task, and\nachieving general-purpose protein understanding remains a challenge. In this\npaper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)\nframework to bridge this gap. Our approach integrates a noval structure-aware\nmodule into pLMs to inform them with structural knowledge, and then connects\nthese enhanced pLMs to large language models (LLMs) to generate understanding\nof proteins. In this framework, we propose a novel two-stage instruction tuning\npipeline that first establishes a basic understanding of proteins through\ncaption-based instructions and then refines this understanding using a mixture\nof experts (MoEs) to learn more complex properties and functional information\nwith the same amount of activated parameters. Moreover, we construct the\nlargest and most comprehensive protein instruction dataset to date, which\nallows us to train and evaluate the general-purpose protein understanding\nmodel. Extensive experimental results on open-ended generation and closed-set\nanswer tasks demonstrate the superior performance of SEPIT over both\nclosed-source general LLMs and open-source LLMs trained with protein knowledge.\n","authors":["Wei Wu","Chao Wang","Liyi Chen","Mingze Yin","Yiheng Zhu","Kun Fu","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06741v1","updated":"2024-10-09T10:20:32Z","published":"2024-10-09T10:20:32Z","title":"CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models","summary":"  Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.\n","authors":["Zi Gong","Hang Yu","Cong Liao","Bingchang Liu","Chaoyu Chen","Jianguo Li"],"pdf_url":"https://arxiv.org/pdf/2410.06741v1.pdf","comment":"15 pages, main conference of EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06735v1","updated":"2024-10-09T10:13:13Z","published":"2024-10-09T10:13:13Z","title":"Which Programming Language and What Features at Pre-training Stage\n  Affect Downstream Logical Inference Performance?","summary":"  Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.\n","authors":["Fumiya Uchiyama","Takeshi Kojima","Andrew Gambardella","Qi Cao","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2410.06735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06733v1","updated":"2024-10-09T10:09:11Z","published":"2024-10-09T10:09:11Z","title":"Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with\n  Situation Puzzles","summary":"  While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking.\n","authors":["Qi Chen","Bowen Zhang","Gang Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.06733v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06722v1","updated":"2024-10-09T09:45:01Z","published":"2024-10-09T09:45:01Z","title":"Scaling Laws for Mixed quantization in Large Language Models","summary":"  Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms.\n","authors":["Zeyu Cao","Cheng Zhang","Pedro Gimenes","Jianqiao Lu","Jianyi Cheng","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.06722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06718v1","updated":"2024-10-09T09:41:34Z","published":"2024-10-09T09:41:34Z","title":"MatMamba: A Matryoshka State Space Model","summary":"  State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}\n","authors":["Abhinav Shukla","Sai Vemprala","Aditya Kusupati","Ashish Kapoor"],"pdf_url":"https://arxiv.org/pdf/2410.06718v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06716v1","updated":"2024-10-09T09:39:55Z","published":"2024-10-09T09:39:55Z","title":"Guaranteed Generation from Large Language Models","summary":"  As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.\n","authors":["Minbeom Kim","Thibaut Thonet","Jos Rozen","Hwaran Lee","Kyomin Jung","Marc Dymetman"],"pdf_url":"https://arxiv.org/pdf/2410.06716v1.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.19874v2","updated":"2024-10-09T09:36:49Z","published":"2024-06-28T12:28:52Z","title":"Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood","summary":"  Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT\n","authors":["Yang Xu","Yu Wang","Hao An","Zhichen Liu","Yongyuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19874v2.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.06707v1","updated":"2024-10-09T09:20:24Z","published":"2024-10-09T09:20:24Z","title":"Calibrating Verbalized Probabilities for Large Language Models","summary":"  Calibrating verbalized probabilities presents a novel approach for reliably\nassessing and leveraging outputs from black-box Large Language Models (LLMs).\nRecent methods have demonstrated improved calibration by applying techniques\nlike Platt scaling or temperature scaling to the confidence scores generated by\nLLMs. In this paper, we explore the calibration of verbalized probability\ndistributions for discriminative tasks. First, we investigate the capability of\nLLMs to generate probability distributions over categorical labels. We\ntheoretically and empirically identify the issue of re-softmax arising from the\nscaling of verbalized probabilities, and propose using the invert softmax trick\nto approximate the \"logit\" by inverting verbalized probabilities. Through\nextensive evaluation on three public datasets, we demonstrate: (1) the robust\ncapability of LLMs in generating class distributions, and (2) the effectiveness\nof the invert softmax trick in estimating logits, which, in turn, facilitates\npost-calibration adjustments.\n","authors":["Cheng Wang","Gyuri Szarvas","Georges Balazs","Pavel Danchenko","Patrick Ernst"],"pdf_url":"https://arxiv.org/pdf/2410.06707v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.06704v1","updated":"2024-10-09T09:16:25Z","published":"2024-10-09T09:16:25Z","title":"PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs","summary":"  In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.\n","authors":["Krishna Kanth Nakka","Ahmed Frikha","Ricardo Mendes","Xue Jiang","Xuebing Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.06704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00403v2","updated":"2024-10-09T09:14:17Z","published":"2024-03-30T15:59:17Z","title":"UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion\n  Cause","summary":"  Multimodal emotion recognition in conversation (MERC) and multimodal\nemotion-cause pair extraction (MECPE) have recently garnered significant\nattention. Emotions are the expression of affect or feelings; responses to\nspecific events, or situations -- known as emotion causes. Both collectively\nexplain the causality between human emotion and intents. However, existing\nworks treat emotion recognition and emotion cause extraction as two individual\nproblems, ignoring their natural causality. In this paper, we propose a Unified\nMultimodal Emotion recognition and Emotion-Cause analysis framework (UniMEEC)\nto explore the causality between emotion and emotion cause. Concretely, UniMEEC\nreformulates the MERC and MECPE tasks as mask prediction problems and unifies\nthem with a causal prompt template. To differentiate the modal effects, UniMEEC\nproposes a multimodal causal prompt to probe the pre-trained knowledge\nspecified to modality and implements cross-task and cross-modality interactions\nunder task-oriented settings. Experiment results on four public benchmark\ndatasets verify the model performance on MERC and MECPE tasks and achieve\nconsistent improvements compared with the previous state-of-the-art methods.\n","authors":["Guimin Hu","Zhihong Zhu","Daniel Hershcovich","Lijie Hu","Hasti Seifi","Jiayuan Xie"],"pdf_url":"https://arxiv.org/pdf/2404.00403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17011v2","updated":"2024-10-09T08:58:40Z","published":"2024-07-24T05:26:52Z","title":"Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism","summary":"  Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. The other attributes it to LLMs'\ninherent ability of task recognition, deeming label correctness and shot\nnumbers of demonstrations as not crucial. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether similar examples are presented in the demonstrations\n(perception) and whether LLMs can recognize the task (cognition). We propose\nthe peak inverse rank metric to detect the task recognition ability of LLMs and\nstudy LLMs' reactions to different definitions of similarity. Based on these,\nwe conduct extensive experiments to elucidate how ICL functions across each\nquadrant on multiple representative classification tasks. Finally, we extend\nour analyses to generation tasks, showing that our coordinate system can also\nbe used to interpret ICL for generation tasks effectively.\n","authors":["Anhao Zhao","Fanghua Ye","Jinlan Fu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2407.17011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02655v2","updated":"2024-10-09T08:51:28Z","published":"2024-04-03T11:36:12Z","title":"Calibrating the Confidence of Large Language Models by Eliciting\n  Fidelity","summary":"  Large language models optimized with techniques like RLHF have achieved good\nalignment in being helpful and harmless. However, post-alignment, these\nlanguage models often exhibit overconfidence, where the expressed confidence\ndoes not accurately calibrate with their correctness rate. In this paper, we\ndecompose the language model confidence into the \\textit{Uncertainty} about the\nquestion and the \\textit{Fidelity} to the answer generated by language models.\nThen, we propose a plug-and-play method to estimate the confidence of language\nmodels. Our method has shown good calibration performance by conducting\nexperiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two\nnovel metrics, IPR and CE, to evaluate the calibration of the model, and we\nhave conducted a detailed discussion on \\textit{Truly Well-Calibrated\nConfidence}. Our method could serve as a strong baseline, and we hope that this\nwork will provide some insights into the model confidence calibration.\n","authors":["Mozhi Zhang","Mianqiu Huang","Rundong Shi","Linsen Guo","Chong Peng","Peng Yan","Yaqian Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2404.02655v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06682v1","updated":"2024-10-09T08:44:47Z","published":"2024-10-09T08:44:47Z","title":"Enhancing Multimodal LLM for Detailed and Accurate Video Captioning\n  using Multi-Round Preference Optimization","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimization (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimized using DPO. To further improve training, we\nintroduce a novel multi-round DPO (mrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initializing the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilize the\nprocess. To address potential catastrophic forgetting of non-captioning\nabilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO\nLLM by using the captions generated by the mrDPO-trained model as supervised\nlabels. Experiments show that mrDPO significantly enhances video-SALMONN 2's\ncaptioning accuracy, reducing global and local error rates by 40\\% and 20\\%,\nrespectively, while decreasing the repetition rate by 35\\%. The final\nvideo-SALMONN 2 model, with just 7 billion parameters, surpasses leading models\nsuch as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining\ncompetitive performance to the state-of-the-art on widely used video\nquestion-answering benchmark among models of similar size. Upon acceptance, we\nwill release the code, model checkpoints, and training and test data. Demos are\navailable at\n\\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zujun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06672v1","updated":"2024-10-09T08:28:53Z","published":"2024-10-09T08:28:53Z","title":"Towards Universality: Studying Mechanistic Similarity Across Language\n  Model Architectures","summary":"  The hypothesis of Universality in interpretability suggests that different\nneural networks may converge to implement similar algorithms on similar tasks.\nIn this work, we investigate two mainstream architectures for language\nmodeling, namely Transformers and Mambas, to explore the extent of their\nmechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate\ninterpretable features from these models and show that most features are\nsimilar in these two models. We also validate the correlation between feature\nsimilarity and Universality. We then delve into the circuit-level analysis of\nMamba models and find that the induction circuits in Mamba are structurally\nanalogous to those in Transformers. We also identify a nuanced difference we\ncall \\emph{Off-by-One motif}: The information of one token is written into the\nSSM state in its next position. Whilst interaction between tokens in\nTransformers does not exhibit such trend.\n","authors":["Junxuan Wang","Xuyang Ge","Wentao Shu","Qiong Tang","Yunhua Zhou","Zhengfu He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.06672v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.06667v1","updated":"2024-10-09T08:23:22Z","published":"2024-10-09T08:23:22Z","title":"Large Language Models as Code Executors: An Exploratory Study","summary":"  The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.\n","authors":["Chenyang Lyu","Lecheng Yan","Rui Xing","Wenxi Li","Younes Samih","Tianbo Ji","Longyue Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19013v2","updated":"2024-10-09T08:16:44Z","published":"2024-09-23T23:43:43Z","title":"Improving Academic Skills Assessment with NLP and Ensemble Learning","summary":"  This study addresses the critical challenges of assessing foundational\nacademic skills by leveraging advancements in natural language processing\n(NLP). Traditional assessment methods often struggle to provide timely and\ncomprehensive feedback on key cognitive and linguistic aspects, such as\ncoherence, syntax, and analytical reasoning. Our approach integrates multiple\nstate-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,\nwithin an ensemble learning framework. These models are combined through\nstacking techniques using LightGBM and Ridge regression to enhance predictive\naccuracy. The methodology involves detailed data preprocessing, feature\nextraction, and pseudo-label learning to optimize model performance. By\nincorporating sophisticated NLP techniques and ensemble learning, this study\nsignificantly improves the accuracy and efficiency of assessments, offering a\nrobust solution that surpasses traditional methods and opens new avenues for\neducational technology research focused on enhancing core academic\ncompetencies.\n","authors":["Zhengpei Cheng","Yingyi Wu","Danyang Zhang","Jiacheng Hu","Yujian Long"],"pdf_url":"https://arxiv.org/pdf/2409.19013v2.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.05413v3","updated":"2024-10-09T07:53:10Z","published":"2024-07-07T15:37:13Z","title":"SBoRA: Low-Rank Adaptation with Regional Weight Updates","summary":"  This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA reduces the number of trainable parameters by half or doubles\nthe rank with the similar number of trainable parameters as LoRA, while\nimproving learning performance. By utilizing orthogonal standard basis vectors\nto initialize one of the low-rank matrices (either $\\mathbf{A}$ or\n$\\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient\nfine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only\none of the matrices is updated, leading to a sparse update matrix\n$\\mathrm{\\Delta} \\mathbf{W}$ with predominantly zero rows or columns.\nConsequently, most of the fine-tuned model's weights\n$(\\mathbf{W}_0+\\mathrm{\\Delta} \\mathbf{W})$ remain unchanged from the\npre-trained weights, akin to the modular organization of the human brain, which\nefficiently adapts to new tasks. Our empirical results demonstrate the\nsuperiority of SBoRA-FA over LoRA in various fine-tuning tasks, including\ncommonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the\neffectiveness of QSBoRA on quantized LLaMA models of varying scales,\nhighlighting its potential for efficient adaptation to new tasks. Code is\navailable at https://github.com/cityuhkai/SBoRA\n","authors":["Lai-Man Po","Yuyang Liu","Haoxuan Wu","Tianqi Zhang","Wing-Yin Yu","Zhuohan Wang","Zeyu Jiang","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2407.05413v3.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.06638v1","updated":"2024-10-09T07:43:38Z","published":"2024-10-09T07:43:38Z","title":"Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing","summary":"  Large Language Models (LLMs) have exhibited strong mathematical reasoning and\ncomputational prowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle errors, such\nas miscalculations or incorrect substitutions, limit the models' full\nmathematical potential. Existing studies to improve mathematical ability\ntypically involve distilling reasoning skills from stronger LLMs or applying\npreference learning to step-wise response pairs. Although these methods\nleverage samples of varying granularity to mitigate reasoning errors, they\noverlook the frequently occurring subtle errors. A major reason is that sampled\npreference pairs involve differences unrelated to the errors, which may\ndistract the model from focusing on subtle errors. In this work, we propose a\nnovel preference learning framework called eRror-Injected Self-Editing (RISE),\nwhich injects predefined subtle errors into partial tokens of correct solutions\nto construct hard pairs for error mitigation. In detail, RISE uses the model\nitself to edit a small number of tokens in the solution, injecting designed\nsubtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective to focus on predefined errors and their tokens, without\nrequiring fine-grained sampling or preference annotation. Extensive experiments\nvalidate the effectiveness of RISE, with preference learning on\nQwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on\nMATH.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Chak Tou Leong","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16563v2","updated":"2024-10-09T07:39:29Z","published":"2024-04-25T12:24:37Z","title":"Evaluating Large Language Models on Time Series Feature Understanding: A\n  Comprehensive Taxonomy and Benchmark","summary":"  Large Language Models (LLMs) offer the potential for automatic time series\nanalysis and reporting, which is a critical task across many domains, spanning\nhealthcare, finance, climate, energy, and many more. In this paper, we propose\na framework for rigorously evaluating the capabilities of LLMs on time series\nunderstanding, encompassing both univariate and multivariate forms. We\nintroduce a comprehensive taxonomy of time series features, a critical\nframework that delineates various characteristics inherent in time series data.\nLeveraging this taxonomy, we have systematically designed and synthesized a\ndiverse dataset of time series, embodying the different outlined features, each\naccompanied by textual descriptions. This dataset acts as a solid foundation\nfor assessing the proficiency of LLMs in comprehending time series. Our\nexperiments shed light on the strengths and limitations of state-of-the-art\nLLMs in time series understanding, revealing which features these models\nreadily comprehend effectively and where they falter. In addition, we uncover\nthe sensitivity of LLMs to factors including the formatting of the data, the\nposition of points queried within a series and the overall time series length.\n","authors":["Elizabeth Fons","Rachneet Kaur","Soham Palande","Zhen Zeng","Tucker Balch","Manuela Veloso","Svitlana Vyetrenko"],"pdf_url":"https://arxiv.org/pdf/2404.16563v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06634v1","updated":"2024-10-09T07:35:46Z","published":"2024-10-09T07:35:46Z","title":"Tree of Problems: Improving structured problem solving with\n  compositionality","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nmultiple tasks through in-context learning. For complex reasoning tasks that\nrequire step-by-step thinking, Chain-of-Thought (CoT) prompting has given\nimpressive results, especially when combined with self-consistency.\nNonetheless, some tasks remain particularly difficult for LLMs to solve. Tree\nof Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing\nthe complex problem into paths of subproblems. In this paper, we propose Tree\nof Problems (ToP), a simpler version of ToT, which we hypothesise can work\nbetter for complex tasks that can be divided into identical subtasks. Our\nempirical results show that our approach outperforms ToT and GoT, and in\naddition performs better than CoT on complex reasoning tasks. All code for this\npaper is publicly available here:\nhttps://github.com/ArmelRandy/tree-of-problems.\n","authors":["Armel Zebaze","Benot Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2410.06634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02599v2","updated":"2024-10-09T07:31:18Z","published":"2024-08-05T16:21:17Z","title":"Progressively Label Enhancement for Large Language Model Alignment","summary":"  Large Language Models (LLM) alignment aims to prevent models from producing\ncontent that misaligns with human expectations, which can lead to ethical and\nlegal concerns. In the last few years, Reinforcement Learning from Human\nFeedback (RLHF) has been the most prominent method for achieving alignment. Due\nto challenges in stability and scalability with RLHF stages, which arise from\nthe complex interactions between multiple models, researchers are exploring\nalternative methods to achieve effects comparable to those of RLHF. However,\nthese methods often rely on large high-quality datasets. Despite some methods\nconsidering the generation of additional data to expand datasets, they often\ntreat model training and data generation as separate and static processes,\noverlooking the fact that these processes are highly interdependent, leading to\ninefficient utilization of the generated data. To deal with this problem, we\npropose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a\nframework that dynamically adjusts the model's training process based on the\nevolving quality of the generated data. Specifically, we prompt the model to\ngenerate responses for both the original query and the query guided by a set of\ncarefully designed principles, and then utilize a dynamic threshold to\ndetermine the appropriate training approach for both responses based on their\ncorresponding reward scores. Experimental results demonstrate the effectiveness\nof PLE compared to existing LLM alignment methods.\n","authors":["Biao Liu","Ning Xu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2408.02599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06625v1","updated":"2024-10-09T07:21:43Z","published":"2024-10-09T07:21:43Z","title":"ETA: Evaluating Then Aligning Safety of Vision Language Models at\n  Inference Time","summary":"  Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.\n","authors":["Yi Ding","Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06625v1.pdf","comment":"27pages"},{"id":"http://arxiv.org/abs/2410.06617v1","updated":"2024-10-09T07:14:45Z","published":"2024-10-09T07:14:45Z","title":"Learning Evolving Tools for Large Language Models","summary":"  Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning.\n","authors":["Guoxin Chen","Zhong Zhang","Xin Cong","Fangda Guo","Yesai Wu","Yankai Lin","Wenzheng Feng","Yasheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06617v1.pdf","comment":"Ongoning Work"},{"id":"http://arxiv.org/abs/2410.06615v1","updated":"2024-10-09T07:12:24Z","published":"2024-10-09T07:12:24Z","title":"$$-calibration of Language Model Confidence Scores for Generative\n  QA","summary":"  To use generative question-and-answering (QA) systems for decision-making and\nin any critical application, these systems need to provide well-calibrated\nconfidence scores that reflect the correctness of their answers. Existing\ncalibration methods aim to ensure that the confidence score is on average\nindicative of the likelihood that the answer is correct. We argue, however,\nthat this standard (average-case) notion of calibration is difficult to\ninterpret for decision-making in generative QA. To address this, we generalize\nthe standard notion of average calibration and introduce $\\beta$-calibration,\nwhich ensures calibration holds across different question-and-answer groups. We\nthen propose discretized posthoc calibration schemes for achieving\n$\\beta$-calibration.\n","authors":["Putra Manggala","Atalanti Mastakouri","Elke Kirschbaum","Shiva Prasad Kasiviswanathan","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2410.06615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11911v3","updated":"2024-10-09T06:59:31Z","published":"2024-06-16T16:46:55Z","title":"A Notion of Complexity for Theory of Mind via Discrete World Models","summary":"  Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework inspired by cognitive load theory to measure the complexity of ToM\ntasks. We quantify a problem's complexity as the number of states necessary to\nsolve it correctly. Our complexity measure also accounts for spurious states of\na ToM problem designed to make it apparently harder. We use our method to\nassess the complexity of five widely adopted ToM benchmarks. On top of this\nframework, we design a prompting technique that augments the information\navailable to a model with a description of how the environment changes with the\nagents' interactions. We name this technique Discrete World Models (DWM) and\nshow how it elicits superior performance on ToM tasks.\n","authors":["X. Angelo Huang","Emanuele La Malfa","Samuele Marro","Andrea Asperti","Anthony Cohn","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2406.11911v3.pdf","comment":"Accepted EMNLP 2024, Website\n  https://flecart.github.io/complexity-tom-dwm"},{"id":"http://arxiv.org/abs/2410.06606v1","updated":"2024-10-09T06:58:09Z","published":"2024-10-09T06:58:09Z","title":"Dissecting Fine-Tuning Unlearning in Large Language Models","summary":"  Fine-tuning-based unlearning methods prevail for preventing targeted harmful,\nsensitive, or copyrighted information within large language models while\npreserving overall capabilities. However, the true effectiveness of these\nmethods is unclear. In this paper, we delve into the limitations of\nfine-tuning-based unlearning through activation patching and parameter\nrestoration experiments. Our findings reveal that these methods alter the\nmodel's knowledge retrieval process, rather than genuinely erasing the\nproblematic knowledge embedded in the model parameters. Furthermore, behavioral\ntests demonstrate that the unlearning mechanisms inevitably impact the global\nbehavior of the models, affecting unrelated knowledge or capabilities. Our work\nadvocates the development of more resilient unlearning techniques for truly\nerasing knowledge. Our code is released at\nhttps://github.com/yihuaihong/Dissecting-FT-Unlearning.\n","authors":["Yihuai Hong","Yuelin Zou","Lijie Hu","Ziqian Zeng","Di Wang","Haiqin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.06606v1.pdf","comment":"Accepted in EMNLP 2024 Main (Short paper)"},{"id":"http://arxiv.org/abs/2402.10645v3","updated":"2024-10-09T06:54:29Z","published":"2024-02-16T12:46:16Z","title":"Can Separators Improve Chain-of-Thought Prompting?","summary":"  Chain-of-thought (CoT) prompting is a simple and effective method for\nimproving the reasoning capabilities of Large Language Models (LLMs). The basic\nidea of CoT is to let LLMs break down their thought processes step-by-step by\nputting exemplars in the input prompt. However, the densely structured prompt\nexemplars of CoT may cause the cognitive overload of LLMs. Inspired by human\ncognition, we introduce COT-SEP, a method that strategically employs separators\nat the end of each exemplar in CoT prompting. These separators are designed to\nhelp the LLMs understand their thought processes better while reasoning.\nInterestingly, it turns out that COT-SEP significantly improves the LLMs'\nperformances on complex reasoning tasks (e.g., GSM8K, AQuA, CSQA), compared\nwith the vanilla CoT, which does not use separators. We also study the effects\nof the type and the location of separators tested on multiple LLMs, including\nGPT-3.5-Turbo, GPT-4, and LLaMA-2 7B.\n","authors":["Yoonjeong Park","Hyunjin Kim","Chanyeol Choi","Junseong Kim","Jy-yong Sohn"],"pdf_url":"https://arxiv.org/pdf/2402.10645v3.pdf","comment":"IEEE FLLM 2024"},{"id":"http://arxiv.org/abs/2409.11365v2","updated":"2024-10-09T06:39:28Z","published":"2024-09-17T17:14:41Z","title":"CoCA: Regaining Safety-awareness of Multimodal Large Language Models\n  with Constitutional Calibration","summary":"  The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks.\n","authors":["Jiahui Gao","Renjie Pi","Tianyang Han","Han Wu","Lanqing Hong","Lingpeng Kong","Xin Jiang","Zhenguo Li"],"pdf_url":"https://arxiv.org/pdf/2409.11365v2.pdf","comment":"10 pages, COLM-2024"},{"id":"http://arxiv.org/abs/2410.06577v1","updated":"2024-10-09T06:22:36Z","published":"2024-10-09T06:22:36Z","title":"Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient\n  Attentions","summary":"  Recent advancements in Transformer-based large language models (LLMs) have\nset new standards in natural language processing. However, the classical\nsoftmax attention incurs significant computational costs, leading to a $O(T)$\ncomplexity for per-token generation, where $T$ represents the context length.\nThis work explores reducing LLMs' complexity while maintaining performance by\nintroducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an\ninnovative data-dependent tempered selection (DDTS) mechanism within a linear\nattention-based, purely recurrent framework, achieving significant accuracy\nwhile drastically reducing the memory usage typically associated with recurrent\nmodels. This method exemplifies semantic compression by maintaining essential\ninput information with fixed-size hidden states. Building on this, Rodimus$+$\ncombines Rodimus with the innovative Sliding Window Shared-Key Attention\n(SW-SKA) in a hybrid approach, effectively leveraging the complementary\nsemantic, token, and head compression techniques. Our experiments demonstrate\nthat Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior\ndownstream performance against models trained on more tokens, including\nQwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the\naccuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints\nwill be available soon.\n","authors":["Zhihao He","Hang Yu","Zi Gong","Shizhan Liu","Jianguo Li","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2410.06577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07471v2","updated":"2024-10-09T06:21:47Z","published":"2024-08-14T11:29:47Z","title":"Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization","summary":"  Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework for\nBridging and Modeling Correlations in pairwise data, named BMC. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nthrough targeted modifications, synthesizing a pseudo-winning response by\nimproving the losing response with the winning response as a reference.\nSecondly, we identify that DPO alone is insufficient to model these\ncorrelations and capture nuanced variations. Therefore, we propose learning\ntoken-level correlations by dynamically leveraging the policy model's\nconfidence during training. Comprehensive experiments on QA, math, and\ninstruction-following tasks demonstrate the effectiveness of our approach,\nsignificantly surpassing competitive baselines, including DPO. Additionally,\nour in-depth quantitative analysis reveals the reasons behind our method's\nsuperior performance over DPO and showcases its versatility to other DPO\nvariants. We release our repository at https://github.com/YJiangcm/BMC.\n","authors":["Yuxin Jiang","Bo Huang","Yufei Wang","Xingshan Zeng","Liangyou Li","Yasheng Wang","Xin Jiang","Lifeng Shang","Ruiming Tang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.07471v2.pdf","comment":"19 pages, 8 figures, 10 tables, working in progress"},{"id":"http://arxiv.org/abs/2405.06001v3","updated":"2024-10-09T06:09:41Z","published":"2024-05-09T11:49:05Z","title":"LLMC: Benchmarking Large Language Model Quantization with a Versatile\n  Compression Toolkit","summary":"  Recent advancements in large language models (LLMs) are propelling us toward\nartificial general intelligence with their remarkable emergent abilities and\nreasoning capabilities. However, the substantial computational and memory\nrequirements limit the widespread adoption. Quantization, a key compression\ntechnique, can effectively mitigate these demands by compressing and\naccelerating LLMs, albeit with potential risks to accuracy. Numerous studies\nhave aimed to minimize the accuracy loss associated with quantization. However,\ntheir quantization configurations vary from each other and cannot be fairly\ncompared. In this paper, we present LLMC, a plug-and-play compression toolkit,\nto fairly and systematically explore the impact of quantization. LLMC\nintegrates dozens of algorithms, models, and hardwares, offering high\nextensibility from integer to floating-point quantization, from LLM to\nvision-language (VLM) model, from fixed-bit to mixed precision, and from\nquantization to sparsification. Powered by this versatile toolkit, our\nbenchmark covers three key aspects: calibration data, algorithms (three\nstrategies), and data formats, providing novel insights and detailed analyses\nfor further research and practical guidance for users. Our toolkit is available\nat https://github.com/ModelTC/llmc.\n","authors":["Ruihao Gong","Yang Yong","Shiqiao Gu","Yushi Huang","Chengtao Lv","Yunchen Zhang","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2405.06001v3.pdf","comment":"Accepted by EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.06566v1","updated":"2024-10-09T06:00:05Z","published":"2024-10-09T06:00:05Z","title":"Detecting Bias and Enhancing Diagnostic Accuracy in Large Language\n  Models for Healthcare","summary":"  Biased AI-generated medical advice and misdiagnoses can jeopardize patient\nsafety, making the integrity of AI in healthcare more critical than ever. As\nLarge Language Models (LLMs) take on a growing role in medical decision-making,\naddressing their biases and enhancing their accuracy is key to delivering safe,\nreliable care. This study addresses these challenges head-on by introducing new\nresources designed to promote ethical and precise AI in healthcare. We present\ntwo datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate\nand mitigate biases in health-related LLM outputs, and DiseaseMatcher, with\n32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing\nsymptom-based diagnostic accuracy. Using these datasets, we developed the\nEthiClinician, a fine-tuned model built on the ChatDoctor framework, which\noutperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing\nand correcting hidden biases in existing models for healthcare, our work sets a\nnew benchmark for safer, more reliable patient outcomes.\n","authors":["Pardis Sadat Zahraei","Zahra Shakeri"],"pdf_url":"https://arxiv.org/pdf/2410.06566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11470v2","updated":"2024-10-09T05:59:07Z","published":"2024-07-16T08:08:48Z","title":"Beyond Correctness: Benchmarking Multi-dimensional Code Generation for\n  Large Language Models","summary":"  In recent years, researchers have proposed numerous benchmarks to evaluate\nthe impressive coding capabilities of large language models (LLMs). However,\ncurrent benchmarks primarily assess the accuracy of LLM-generated code, while\nneglecting other critical dimensions that also significantly impact code\nquality in real-world development. Moreover, relying exclusively on correctness\nas the guiding metric renders LLMs susceptible to data contamination.\nTherefore, this paper proposes the RACE benchmark, which comprehensively\nevaluates the quality of code generated by LLMs across 4 dimensions:\nReadability, mAintainability, Correctness, and Efficiency. Specifically,\nconsidering the demand-dependent nature of dimensions beyond correctness, we\ndesign various types of user requirements for each dimension to assess the\nmodel's ability to generate correct code that also meets user demands. We\nanalyze 28 representative LLMs based on RACE and find that: 1) current\ncorrectness-centric benchmarks fail to capture the multifaceted requirements of\ncode in real-world scenarios, while RACE provides a comprehensive evaluation\nthat reveals the defects of LLMs across multiple dimensions; 2) the RACE\nbenchmark serves as an effective tool for resisting the risk of data\ncontamination; 3) even the most advanced code LLMs still encounter significant\nchallenges in customized requirements involving complex instructions; 4) most\nLLMs exhibit an inherent preference for specific coding style. These findings\nhighlight the need for a multidimensional evaluation of code LLMs, emphasizing\nmetrics beyond correctness for real-world applications. Future efforts should\naim to develop novel learning algorithms to enhance code generation under\nvaried constraints and improve coverage and usability for diverse user needs.\n","authors":["Jiasheng Zheng","Boxi Cao","Zhengzhao Ma","Ruotong Pan","Hongyu Lin","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2407.11470v2.pdf","comment":"We release benchmark at https://github.com/jszheng21/RACE and\n  leaderboard at https://huggingface.co/spaces/jszheng/RACE_leaderboard"},{"id":"http://arxiv.org/abs/2406.14282v2","updated":"2024-10-09T05:56:07Z","published":"2024-06-20T13:07:38Z","title":"Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs","summary":"  Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.\n","authors":["Junjie Wang","Mingyang Chen","Binbin Hu","Dan Yang","Ziqi Liu","Yue Shen","Peng Wei","Zhiqiang Zhang","Jinjie Gu","Jun Zhou","Jeff Z. Pan","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14282v2.pdf","comment":"EMNLP2024 Findings"},{"id":"http://arxiv.org/abs/2410.06555v1","updated":"2024-10-09T05:17:38Z","published":"2024-10-09T05:17:38Z","title":"ING-VP: MLLMs cannot Play Easy Vision-based Games Yet","summary":"  As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps://github.com/Thisisus7/ING-VP.git.\n","authors":["Haoran Zhang","Hangyu Guo","Shuyue Guo","Meng Cao","Wenhao Huang","Jiaheng Liu","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06555v1.pdf","comment":"49 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.06554v1","updated":"2024-10-09T05:17:08Z","published":"2024-10-09T05:17:08Z","title":"The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield\n  Better Language Models","summary":"  Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at\n[https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF).\n","authors":["Yanjun Chen","Dawei Zhu","Yirong Sun","Xinghao Chen","Wei Zhang","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06554v1.pdf","comment":"10 pages, 27 figures (including 18 in the appendix), submitted to\n  EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06550v1","updated":"2024-10-09T05:15:13Z","published":"2024-10-09T05:15:13Z","title":"Investigating Cost-Efficiency of LLM-Generated Training Data for\n  Conversational Semantic Frame Analysis","summary":"  Recent studies have demonstrated that few-shot learning allows LLMs to\ngenerate training data for supervised models at a low cost. However, the\nquality of LLM-generated data may not entirely match that of human-labeled\ndata. This raises a crucial question: how should one balance the trade-off\nbetween the higher quality but more expensive human data and the lower quality\nyet substantially cheaper LLM-generated data? In this paper, we synthesized\ntraining data for conversational semantic frame analysis using GPT-4 and\nexamined how to allocate budgets optimally to achieve the best performance. Our\nexperiments, conducted across various budget levels, reveal that optimal\ncost-efficiency is achieved by combining both human and LLM-generated data\nacross a wide range of budget levels. Notably, as the budget decreases, a\nhigher proportion of LLM-generated data becomes more preferable.\n","authors":["Shiho Matta","Yin Jou Huang","Fei Cheng","Hirokazu Kiyomaru","Yugo Murawaki"],"pdf_url":"https://arxiv.org/pdf/2410.06550v1.pdf","comment":"12 pages including 4 pages of references and appendix. 7 figures"},{"id":"http://arxiv.org/abs/2410.06547v1","updated":"2024-10-09T04:53:38Z","published":"2024-10-09T04:53:38Z","title":"TuringQ: Benchmarking AI Comprehension in Theory of Computation","summary":"  We present TuringQ, the first benchmark designed to evaluate the reasoning\ncapabilities of large language models (LLMs) in the theory of computation.\nTuringQ consists of 4,006 undergraduate and graduate-level question-answer\npairs, categorized into four difficulty levels and covering seven core\ntheoretical areas. We evaluate several open-source LLMs, as well as GPT-4,\nusing Chain of Thought prompting and expert human assessment. Additionally, we\npropose an automated LLM-based evaluation system that demonstrates competitive\naccuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on\nTuringQ shows measurable improvements in reasoning ability and out-of-domain\ntasks such as algebra. TuringQ serves as both a benchmark and a resource for\nenhancing LLM performance in complex computational reasoning tasks. Our\nanalysis offers insights into LLM capabilities and advances in AI comprehension\nof theoretical computer science.\n","authors":["Pardis Sadat Zahraei","Ehsaneddin Asgari"],"pdf_url":"https://arxiv.org/pdf/2410.06547v1.pdf","comment":"Accepted to EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.17274v2","updated":"2024-10-09T04:40:24Z","published":"2024-06-25T04:41:17Z","title":"Can We Trust the Performance Evaluation of Uncertainty Estimation\n  Methods in Text Summarization?","summary":"  Text summarization, a key natural language generation (NLG) task, is vital in\nvarious domains. However, the high cost of inaccurate summaries in\nrisk-critical applications, particularly those involving human-in-the-loop\ndecision-making, raises concerns about the reliability of uncertainty\nestimation on text summarization (UE-TS) evaluation methods. This concern stems\nfrom the dependency of uncertainty model metrics on diverse and potentially\nconflicting NLG metrics. To address this issue, we introduce a comprehensive\nUE-TS benchmark incorporating 31 NLG metrics across four dimensions. The\nbenchmark evaluates the uncertainty estimation capabilities of two large\nlanguage models and one pre-trained language model on three datasets, with\nhuman-annotation analysis incorporated where applicable. We also assess the\nperformance of 14 common uncertainty estimation methods within this benchmark.\nOur findings emphasize the importance of considering multiple uncorrelated NLG\nmetrics and diverse uncertainty estimation methods to ensure reliable and\nefficient evaluation of UE-TS techniques. Our code and data are available\nhttps://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.\n","authors":["Jianfeng He","Runing Yang","Linlin Yu","Changbin Li","Ruoxi Jia","Feng Chen","Ming Jin","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2406.17274v2.pdf","comment":"62 pages, 41 figures, 11 tables"},{"id":"http://arxiv.org/abs/2409.15084v2","updated":"2024-10-09T04:37:29Z","published":"2024-09-20T14:25:08Z","title":"Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist\n  with Tertiary Memory","summary":"  Mental health issues, particularly depressive disorders, present significant\nchallenges in contemporary society, necessitating the development of effective\nautomated diagnostic methods. This paper introduces the Agent Mental Clinic\n(AMC), a self-improving conversational agent system designed to enhance\ndepression diagnosis through simulated dialogues between patient and\npsychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we\ndesign a psychiatrist agent consisting of a tertiary memory structure, a\ndialogue control and reflect plugin that acts as ``supervisor'' and a memory\nsampling module, fully leveraging the skills reflected by the psychiatrist\nagent, achieving great accuracy on depression risk and suicide risk diagnosis\nvia conversation. Experiment results on datasets collected in real-life\nscenarios demonstrate that the system, simulating the procedure of training\npsychiatrists, can be a promising optimization method for aligning LLMs with\nreal-life distribution in specific domains without modifying the weights of\nLLMs, even when only a few representative labeled cases are available.\n","authors":["Kunyao Lan","Bingrui Jin","Zichen Zhu","Siyuan Chen","Shu Zhang","Kenny Q. Zhu","Mengyue Wu"],"pdf_url":"https://arxiv.org/pdf/2409.15084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06541v1","updated":"2024-10-09T04:35:22Z","published":"2024-10-09T04:35:22Z","title":"Chip-Tuning: Classify Before Language Models Say","summary":"  The rapid development in the performance of large language models (LLMs) is\naccompanied by the escalation of model size, leading to the increasing cost of\nmodel training and inference. Previous research has discovered that certain\nlayers in LLMs exhibit redundancy, and removing these layers brings only\nmarginal loss in model performance. In this paper, we adopt the probing\ntechnique to explain the layer redundancy in LLMs and demonstrate that language\nmodels can be effectively pruned with probing classifiers. We propose\nchip-tuning, a simple and effective structured pruning framework specialized\nfor classification problems. Chip-tuning attaches tiny probing classifiers\nnamed chips to different layers of LLMs, and trains chips with the backbone\nmodel frozen. After selecting a chip for classification, all layers subsequent\nto the attached layer could be removed with marginal performance loss.\nExperimental results on various LLMs and datasets demonstrate that chip-tuning\nsignificantly outperforms previous state-of-the-art baselines in both accuracy\nand pruning ratio, achieving a pruning ratio of up to 50%. We also find that\nchip-tuning could be applied on multimodal models, and could be combined with\nmodel finetuning, proving its excellent compatibility.\n","authors":["Fangwei Zhu","Dian Li","Jiajun Huang","Gang Liu","Hui Wang","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2410.06541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19916v2","updated":"2024-10-09T04:26:37Z","published":"2024-09-30T03:37:10Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming","summary":"  Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems.\n","authors":["Tianyang Wang","Ziqian Bi","Keyu Chen","Jiawei Xu","Qian Niu","Junyu Liu","Benji Peng","Ming Li","Sen Zhang","Xuanhe Pan","Jinlang Wang","Pohsun Feng","Caitlyn Heqi Yin","Yizhu Wen","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2409.19916v2.pdf","comment":"47pages"},{"id":"http://arxiv.org/abs/2410.06524v1","updated":"2024-10-09T03:53:26Z","published":"2024-10-09T03:53:26Z","title":"Do great minds think alike? Investigating Human-AI Complementarity in\n  Question Answering with CAIMIRA","summary":"  Recent advancements of large language models (LLMs) have led to claims of AI\nsurpassing humans in natural language processing (NLP) tasks such as textual\nunderstanding and reasoning. This work investigates these assertions by\nintroducing CAIMIRA, a novel framework rooted in item response theory (IRT)\nthat enables quantitative assessment and comparison of problem-solving\nabilities of question-answering (QA) agents: humans and AI systems. Through\nanalysis of over 300,000 responses from ~70 AI systems and 155 humans across\nthousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in\nknowledge domains and reasoning skills. Humans outperform AI systems in\nknowledge-grounded abductive and conceptual reasoning, while state-of-the-art\nLLMs like GPT-4 and LLaMA show superior performance on targeted information\nretrieval and fact-based reasoning, particularly when information gaps are\nwell-defined and addressable through pattern matching or data retrieval. These\nfindings highlight the need for future QA tasks to focus on questions that\nchallenge not only higher-order reasoning and scientific thinking, but also\ndemand nuanced linguistic interpretation and cross-contextual knowledge\napplication, helping advance AI developments that better emulate or complement\nhuman cognitive abilities in real-world problem-solving.\n","authors":["Maharshi Gor","Hal Daum III","Tianyi Zhou","Jordan Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2410.06524v1.pdf","comment":"To appear at EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2403.09559v3","updated":"2024-10-09T03:51:45Z","published":"2024-03-14T16:47:25Z","title":"Less is More: High-value Data Selection for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building large vision language\nmodels~(LVLMs), which can greatly improve the task generalization and solving\ncapabilities by learning a mixture of instruction data from diverse visual\ntasks. Previous work mostly collects multiple existing visual instruction\ndatasets via heuristic ways for training (even more than a million\ninstructions), which may introduce data redundancy and enlarge the training\ncost. To investigate this issue, we conduct a series of empirical studies,\nwhich reveal a significant redundancy within the visual instruction datasets,\nand show that greatly reducing the amount of instructions from several tasks\neven do not affect the performance. Based on the findings, we propose a\nhigh-value data selection approach TIVE, to eliminate redundancy within the\nvisual instruction data and reduce the training cost. In TIVE, we first\nestimate the instance influence score on its corresponding task, and the task\ndifficulty score, based on the gradient-based influence functions. Then, we\nleverage the two kinds of scores to determine the task proportion within the\nselected visual instruction subset, and select high-value instances for each\ntask, respectively. Experiments on various LVLMs show that our approach using\nonly about 15% data can achieve comparable average performance to the full-data\nfine-tuned model across eight benchmarks, even surpassing it on four of the\nbenchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2410.02952v2","updated":"2024-10-09T03:46:46Z","published":"2024-10-03T19:52:37Z","title":"Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications","summary":"  We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation.\n","authors":["Oren Sultan","Alex Khasin","Guy Shiran","Asnat Greenstein-Messica","Dafna Shahaf"],"pdf_url":"https://arxiv.org/pdf/2410.02952v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2402.05785v4","updated":"2024-10-09T03:43:34Z","published":"2024-02-08T16:23:29Z","title":"Limits of Transformer Language Models on Learning to Compose Algorithms","summary":"  We analyze the capabilities of Transformer language models in learning\ncompositional discrete tasks. To this end, we evaluate training LLaMA models\nand prompting GPT-4 and Gemini on four tasks demanding to learn a composition\nof several discrete sub-tasks. On both training LLaMA models from scratch and\nprompting on GPT-4 and Gemini, we measure how well these models can reuse\nprimitives observable in the sub-tasks to learn the composition task. Our\nresults indicate that compositional learning in state-of-the-art Transformer\nlanguage models is highly sample inefficient: LLaMA requires more data samples\nthan relearning all sub-tasks from scratch to learn the compositional task;\nin-context prompting with few samples is unreliable and fails at executing the\nsub-tasks or correcting the errors in multi-round code generation. Further, by\nleveraging complexity theory, we support these findings with a theoretical\nanalysis focused on the sample inefficiency of gradient descent in memorizing\nfeedforward models.\n","authors":["Jonathan Thomm","Aleksandar Terzic","Giacomo Camposampiero","Michael Hersche","Bernhard Schlkopf","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2402.05785v4.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06520v1","updated":"2024-10-09T03:42:40Z","published":"2024-10-09T03:42:40Z","title":"A Novel LLM-based Two-stage Summarization Approach for Long Dialogues","summary":"  Long document summarization poses a significant challenge in natural language\nprocessing due to input lengths that exceed the capacity of most\nstate-of-the-art pre-trained language models. This study proposes a\nhierarchical framework that segments and condenses information from long\ndocuments, subsequently fine-tuning the processed text with an abstractive\nsummarization model. Unsupervised topic segmentation methods identify\nsemantically appropriate breakpoints. The condensation stage utilizes an\nunsupervised generation model to generate condensed data, and our current\nexperiments employ ChatGPT(v3.5). The summarization stage fine-tunes the\nabstractive summarization model on the condensed data to generate the final\nresults. This framework enables long documents to be processed on models even\nwhen the document length exceeds the model's maximum input size. The exclusion\nof the entire document from the summarization model reduces the time and\ncomputational resources required for training, making the framework suitable\nfor contexts with constrained local computational resources.\n","authors":["Yuan-Jhe Yin","Bo-Yu Chen","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10900v2","updated":"2024-10-09T03:42:22Z","published":"2024-06-16T11:44:43Z","title":"AutoHallusion: Automatic Generation of Hallucination Benchmarks for\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) are prone to hallucinations, where\ncertain contextual cues in an image can trigger the language module to produce\noverconfident and incorrect reasoning about abnormal or hypothetical objects.\nWhile some benchmarks have been developed to investigate LVLM hallucinations,\nthey often rely on hand-crafted corner cases whose failure patterns may not\ngeneralize well. Additionally, fine-tuning on these examples could undermine\ntheir validity. To address this, we aim to scale up the number of cases through\nan automated approach, reducing human bias in crafting such corner cases. This\nmotivates the development of AutoHallusion, the first automated benchmark\ngeneration approach that employs several key strategies to create a diverse\nrange of hallucination examples. Our generated visual-question pairs pose\nsignificant challenges to LVLMs, requiring them to overcome contextual biases\nand distractions to arrive at correct answers. AutoHallusion enables us to\ncreate new benchmarks at the minimum cost and thus overcomes the fragility of\nhand-crafted benchmarks. It also reveals common failure patterns and reasons,\nproviding key insights to detect, avoid, or control hallucinations.\nComprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro\nVision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of\nhallucination induction on synthetic and real-world datasets of AutoHallusion,\npaving the way for a long battle against hallucinations. The codebase and data\ncan be accessed at https://github.com/wuxiyang1996/AutoHallusion.\n","authors":["Xiyang Wu","Tianrui Guan","Dianqi Li","Shuaiyi Huang","Xiaoyu Liu","Xijun Wang","Ruiqi Xian","Abhinav Shrivastava","Furong Huang","Jordan Lee Boyd-Graber","Tianyi Zhou","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07175v3","updated":"2024-10-09T03:41:43Z","published":"2024-03-11T21:33:05Z","title":"Rebuilding ROME : Resolving Model Collapse during Sequential Model\n  Editing","summary":"  Recent work using Rank-One Model Editing (ROME), a popular model editing\nmethod, has shown that there are certain facts that the algorithm is unable to\nedit without breaking the model. Such edits have previously been called\ndisabling edits. These disabling edits cause immediate model collapse and\nlimits the use of ROME for sequential editing. In this paper, we show that\ndisabling edits are an artifact of irregularities in the implementation of\nROME. With this paper, we provide a more stable implementation ROME, which we\ncall r-ROME and show that model collapse is no longer observed when making\nlarge scale sequential edits with r-ROME, while further improving\ngeneralization and locality of model editing compared to the original\nimplementation of ROME. We also provide a detailed mathematical explanation of\nthe reason behind disabling edits.\n","authors":["Akshat Gupta","Sidharth Baskaran","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.07175v3.pdf","comment":"EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.06519v1","updated":"2024-10-09T03:40:22Z","published":"2024-10-09T03:40:22Z","title":"SEGMENT+: Long Text Processing with Short-Context Language Models","summary":"  There is a growing interest in expanding the input capacity of language\nmodels (LMs) across various domains. However, simply increasing the context\nwindow does not guarantee robust performance across diverse long-input\nprocessing tasks, such as understanding extensive documents and extracting\ndetailed information from lengthy and noisy data. In response, we introduce\nSEGMENT+, a general framework that enables LMs to handle extended inputs within\nlimited context windows efficiently. SEGMENT+ utilizes structured notes and a\nfiltering module to manage information flow, resulting in a system that is both\ncontrollable and interpretable. Our extensive experiments across various model\nsizes, focusing on long-document question-answering and Needle-in-a-Haystack\ntasks, demonstrate the effectiveness of SEGMENT+ in improving performance.\n","authors":["Wei Shi","Shuang Li","Kerun Yu","Jinglei Chen","Zujie Liang","Xinhui Wu","Yuxi Qian","Feng Wei","Bo Zheng","Jiaqing Liang","Jiangjie Chen","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.06519v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.01506v2","updated":"2024-10-09T03:39:11Z","published":"2024-06-03T16:34:01Z","title":"The Geometry of Categorical and Hierarchical Concepts in Large Language\n  Models","summary":"  The linear representation hypothesis is the informal idea that semantic\nconcepts are encoded as linear directions in the representation spaces of large\nlanguage models (LLMs). Previous work has shown how to make this notion precise\nfor representing binary concepts that have natural contrasts (e.g., {male,\nfemale}) as directions in representation space. However, many natural concepts\ndo not have natural contrasts (e.g., whether the output is about an animal). In\nthis work, we show how to extend the formalization of the linear representation\nhypothesis to represent features (e.g., is_animal) as vectors. This allows us\nto immediately formalize the representation of categorical concepts as\npolytopes in the representation space. Further, we use the formalization to\nprove a relationship between the hierarchical structure of concepts and the\ngeometry of their representations. We validate these theoretical results on the\nGemma and LLaMA-3 large language models, estimating representations for 900+\nhierarchically related concepts using data from WordNet.\n","authors":["Kiho Park","Yo Joong Choe","Yibo Jiang","Victor Veitch"],"pdf_url":"https://arxiv.org/pdf/2406.01506v2.pdf","comment":"Best Paper Award at the ICML 2024 Workshop on Mechanistic\n  Interpretability. Code is available at\n  https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations"},{"id":"http://arxiv.org/abs/2403.14236v5","updated":"2024-10-09T03:37:30Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v5.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.06511v1","updated":"2024-10-09T03:26:11Z","published":"2024-10-09T03:26:11Z","title":"TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training","summary":"  The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.\n","authors":["Wanchao Liang","Tianyu Liu","Less Wright","Will Constable","Andrew Gu","Chien-Chin Huang","Iris Zhang","Wei Feng","Howard Huang","Junjie Wang","Sanket Purandare","Gokul Nadathur","Stratos Idreos"],"pdf_url":"https://arxiv.org/pdf/2410.06511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06508v1","updated":"2024-10-09T03:20:02Z","published":"2024-10-09T03:20:02Z","title":"Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge\n  with Curriculum Preference Learning","summary":"  Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique\nfor enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO\nhave enabled LLMs to distill high-quality behaviors from MCTS, improving their\nreasoning performance. However, existing distillation methods underutilize the\nrich trajectory information generated by MCTS, limiting the potential for\nimprovements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel\npairwise training framework that enables LLMs to self-improve through MCTS\nbehavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via\ntwo key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from\nchild nodes sharing the same parent in the search tree, providing step-level\ninformation for more effective MCTS behavior distillation. (2) AlphaLLM-CPL\nintroduces curriculum preference learning, dynamically adjusting the training\nsequence of trajectory pairs in each offline training epoch to prioritize\ncritical learning steps and mitigate overfitting. Experimental results on\nmathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly\noutperforms previous MCTS behavior distillation methods, substantially boosting\nthe reasoning capabilities of LLMs.\n","authors":["Xiyao Wang","Linfeng Song","Ye Tian","Dian Yu","Baolin Peng","Haitao Mi","Furong Huang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.06508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06496v1","updated":"2024-10-09T02:49:56Z","published":"2024-10-09T02:49:56Z","title":"On the Similarity of Circuits across Languages: a Case Study on the\n  Subject-verb Agreement Task","summary":"  Several algorithms implemented by language models have recently been\nsuccessfully reversed-engineered. However, these findings have been\nconcentrated on specific tasks and models, leaving it unclear how universal\ncircuits are across different settings. In this paper, we study the circuits\nimplemented by Gemma 2B for solving the subject-verb agreement task across two\ndifferent languages, English and Spanish. We discover that both circuits are\nhighly consistent, being mainly driven by a particular attention head writing a\n`subject number' signal to the last residual stream, which is read by a small\nset of neurons in the final MLPs. Notably, this subject number signal is\nrepresented as a direction in the residual stream space, and is\nlanguage-independent. We demonstrate that this direction has a causal effect on\nthe model predictions, effectively flipping the Spanish predicted verb number\nby intervening with the direction found in English. Finally, we present\nevidence of similar behavior in other models within the Gemma 1 and Gemma 2\nfamilies.\n","authors":["Javier Ferrando","Marta R. Costa-juss"],"pdf_url":"https://arxiv.org/pdf/2410.06496v1.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2305.19118v4","updated":"2024-10-09T02:41:21Z","published":"2023-05-30T15:25:45Z","title":"Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate","summary":"  Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n\"tit for tat\" and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of \"tit for tat\" state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Code is available at\nhttps://github.com/Skytliang/Multi-Agents-Debate.\n","authors":["Tian Liang","Zhiwei He","Wenxiang Jiao","Xing Wang","Yan Wang","Rui Wang","Yujiu Yang","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2305.19118v4.pdf","comment":"EMNLP 2024 (main conference)"},{"id":"http://arxiv.org/abs/2410.06479v1","updated":"2024-10-09T02:14:39Z","published":"2024-10-09T02:14:39Z","title":"LLM Compression with Neural Architecture Search","summary":"  Large language models (LLMs) exhibit remarkable reasoning abilities, allowing\nthem to generalize across a wide range of downstream tasks, such as commonsense\nreasoning or instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. This poses the question: Can we compress pre-trained LLMs to meet\ndiverse size and latency requirements? We leverage Neural Architecture Search\n(NAS) to compress LLMs by pruning structural components, such as attention\nheads, neurons, and layers, aiming to achieve a Pareto-optimal balance between\nperformance and efficiency. While NAS already achieved promising results on\nsmall language models in previous work, in this paper we propose various\nextensions that allow us to scale to LLMs. Compared to structural pruning\nbaselines, we show that NAS improves performance up to 3.4% on MMLU with an\non-device latency speedup.\n","authors":["Rhea Sanjay Sukthanker","Benedikt Staffler","Frank Hutter","Aaron Klein"],"pdf_url":"https://arxiv.org/pdf/2410.06479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07840v3","updated":"2024-10-09T02:03:26Z","published":"2024-07-10T17:00:29Z","title":"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison","summary":"  Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose Decompose and Compare Consistency (DeCC)\nfor reliability measurement. By comparing the consistency between the direct\nanswer generated using the VLM's internal reasoning process, and the indirect\nanswers obtained by decomposing the question into sub-questions and reasoning\nover the sub-answers produced by the VLM, DeCC measures the reliability of\nVLM's direct answer. Experiments across six vision-language tasks with three\nVLMs show DeCC's reliability estimation achieves better correlation with task\naccuracy compared to the existing methods.\n","authors":["Qian Yang","Weixiang Yan","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07840v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.06458v1","updated":"2024-10-09T01:25:10Z","published":"2024-10-09T01:25:10Z","title":"LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for\n  Enhanced Following of Instructions with Multiple Constraints","summary":"  Instruction following is a key capability for LLMs. However, recent studies\nhave shown that LLMs often struggle with instructions containing multiple\nconstraints (e.g. a request to create a social media post \"in a funny tone\"\nwith \"no hashtag\"). Despite this, most evaluations focus solely on synthetic\ndata. To address this, we introduce RealInstruct, the first benchmark designed\nto evaluate LLMs' ability to follow real-world multi-constrained instructions\nby leveraging queries real users asked AI assistants. We also investigate\nmodel-based evaluation as a cost-effective alternative to human annotation for\nthis task. Our findings reveal that even the proprietary GPT-4 model fails to\nmeet at least one constraint on over 21% of instructions, highlighting the\nlimitations of state-of-the-art models. To address the performance gap between\nopen-source and proprietary models, we propose the Decompose, Critique and\nRefine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to\nfollow constraints. DeCRIM works by decomposing the original instruction into a\nlist of constraints and using a Critic model to decide when and where the LLM's\nresponse needs refinement. Our results show that DeCRIM improves Mistral's\nperformance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.\nMoreover, we demonstrate that with strong feedback, open-source LLMs with\nDeCRIM can outperform GPT-4 on both benchmarks.\n","authors":["Thomas Palmeira Ferraz","Kartik Mehta","Yu-Hsiang Lin","Haw-Shiuan Chang","Shereen Oraby","Sijia Liu","Vivek Subramanian","Tagyoung Chung","Mohit Bansal","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2410.06458v1.pdf","comment":"To appear at EMNLP 2024"},{"id":"http://arxiv.org/abs/2405.10443v4","updated":"2024-10-09T01:12:19Z","published":"2024-05-16T21:07:42Z","title":"Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation","summary":"  Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.\n","authors":["Matthew Raffel","Victor Agostinelli","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10443v4.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06441v1","updated":"2024-10-09T00:49:08Z","published":"2024-10-09T00:49:08Z","title":"Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and\n  Performance of SGD for Fine-Tuning Language Models","summary":"  Fine-tuning language models (LMs) with the Adam optimizer often demands\nexcessive memory, limiting accessibility. The \"in-place\" version of Stochastic\nGradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO)\nhave been proposed to address this. However, IP-SGD still requires substantial\nmemory, and MeZO suffers from slow convergence and degraded final performance\ndue to its zeroth-order nature. This paper introduces Addax, a novel method\nthat improves both memory efficiency and performance of IP-SGD by integrating\nit with MeZO. Specifically, Addax computes zeroth- or first-order gradients of\ndata points in the minibatch based on their memory consumption, combining these\ngradient estimates to update directions. By computing zeroth-order gradients\nfor data points that require more memory and first-order gradients for others,\nAddax overcomes the slow convergence of MeZO and the excessive memory\nrequirement of IP-SGD. Additionally, the zeroth-order gradient acts as a\nregularizer for the first-order gradient, further enhancing the model's final\nperformance. Theoretically, we establish the convergence of Addax under mild\nassumptions, demonstrating faster convergence and less restrictive\nhyper-parameter choices than MeZO. Our experiments with diverse LMs and tasks\nshow that Addax consistently outperforms MeZO regarding accuracy and\nconvergence speed while having a comparable memory footprint. When fine-tuning\nOPT-13B with one A100 GPU, on average, Addax outperforms MeZO in accuracy/F1\nscore by 14% and runs 15x faster while using memory similar to MeZO. In our\nexperiments on the larger OPT-30B model, on average, Addax outperforms MeZO in\nterms of accuracy/F1 score by >16 and runs 30x faster on a single H100 GPU.\nMoreover, Addax surpasses the performance of standard fine-tuning approaches,\nsuch as IP-SGD and Adam, in most tasks with significantly less memory\nrequirement.\n","authors":["Zeman Li","Xinwei Zhang","Peilin Zhong","Yuan Deng","Meisam Razaviyayn","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2410.06441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12327v4","updated":"2024-10-09T00:22:46Z","published":"2024-07-17T05:53:20Z","title":"Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language\n  Models","summary":"  Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\n\\href{https://github.com/NolanoOrg/SpectraSuite}{https://github.com/NolanoOrg/SpectraSuite}.\n","authors":["Ayush Kaushal","Tejas Vaidhya","Arnab Kumar Mondal","Tejas Pandey","Aaryan Bhagat","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2407.12327v4.pdf","comment":"42 pages, 21 figures, and 13 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.07177v1","updated":"2024-10-09T17:59:59Z","published":"2024-10-09T17:59:59Z","title":"MM-Ego: Towards Building Egocentric Multimodal LLMs","summary":"  This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.\n","authors":["Hanrong Ye","Haotian Zhang","Erik Daxberger","Lin Chen","Zongyu Lin","Yanghao Li","Bowen Zhang","Haoxuan You","Dan Xu","Zhe Gan","Jiasen Lu","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.07177v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.07173v1","updated":"2024-10-09T17:59:33Z","published":"2024-10-09T17:59:33Z","title":"Do better language models have crisper vision?","summary":"  How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.\n","authors":["Jona Ruthardt","Gertjan J. Burghouts","Serge Belongie","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2410.07173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07171v1","updated":"2024-10-09T17:59:13Z","published":"2024-10-09T17:59:13Z","title":"IterComp: Iterative Composition-Aware Feedback Learning from Model\n  Gallery for Text-to-Image Generation","summary":"  Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made\nnotable strides in compositional text-to-image generation. However, these\nmethods typically exhibit distinct strengths for compositional generation, with\nsome excelling in handling attribute binding and others in spatial\nrelationships. This disparity highlights the need for an approach that can\nleverage the complementary strengths of various models to comprehensively\nimprove the composition capability. To this end, we introduce IterComp, a novel\nframework that aggregates composition-aware model preferences from multiple\nmodels and employs an iterative feedback learning approach to enhance\ncompositional generation. Specifically, we curate a gallery of six powerful\nopen-source diffusion models and evaluate their three key compositional\nmetrics: attribute binding, spatial relationships, and non-spatial\nrelationships. Based on these metrics, we develop a composition-aware model\npreference dataset comprising numerous image-rank pairs to train\ncomposition-aware reward models. Then, we propose an iterative feedback\nlearning method to enhance compositionality in a closed-loop manner, enabling\nthe progressive self-refinement of both the base diffusion model and reward\nmodels over multiple iterations. Theoretical proof demonstrates the\neffectiveness and extensive experiments show our significant superiority over\nprevious SOTA methods (e.g., Omost and FLUX), particularly in multi-category\nobject composition and complex semantic alignment. IterComp opens new research\navenues in reward feedback learning for diffusion models and compositional\ngeneration. Code: https://github.com/YangLing0818/IterComp\n","authors":["Xinchen Zhang","Ling Yang","Guohao Li","Yaqi Cai","Jiake Xie","Yong Tang","Yujiu Yang","Mengdi Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2410.07171v1.pdf","comment":"Project: https://github.com/YangLing0818/IterComp"},{"id":"http://arxiv.org/abs/2410.07167v1","updated":"2024-10-09T17:59:04Z","published":"2024-10-09T17:59:04Z","title":"Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate","summary":"  We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.\n","authors":["Qidong Huang","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Jiaqi Wang","Dahua Lin","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.07167v1.pdf","comment":"Project page: https://github.com/shikiw/Modality-Integration-Rate"},{"id":"http://arxiv.org/abs/2410.07164v1","updated":"2024-10-09T17:58:56Z","published":"2024-10-09T17:58:56Z","title":"AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation","summary":"  Recent advancements in diffusion models have led to significant improvements\nin the generation and animation of 4D full-body human-object interactions\n(HOI). Nevertheless, existing methods primarily focus on SMPL-based motion\ngeneration, which is limited by the scarcity of realistic large-scale\ninteraction data. This constraint affects their ability to create everyday HOI\nscenes. This paper addresses this challenge using a zero-shot approach with a\npre-trained diffusion model. Despite this potential, achieving our goals is\ndifficult due to the diffusion model's lack of understanding of ''where'' and\n''how'' objects interact with the human body. To tackle these issues, we\nintroduce AvatarGO, a novel framework designed to generate animatable 4D HOI\nscenes directly from textual inputs. Specifically, 1) for the ''where''\nchallenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to\nidentify the contact body part from text prompts, ensuring precise\nrepresentation of human-object spatial relations. 2) For the ''how'' challenge,\nwe introduce correspondence-aware motion optimization that constructs motion\nfields for both human and object models using the linear blend skinning\nfunction from SMPL-X. Our framework not only generates coherent compositional\nmotions, but also exhibits greater robustness in handling penetration issues.\nExtensive experiments with existing methods validate AvatarGO's superior\ngeneration and animation capabilities on a variety of human-object pairs and\ndiverse poses. As the first attempt to synthesize 4D avatars with object\ninteractions, we hope AvatarGO could open new doors for human-centric 4D\ncontent creation.\n","authors":["Yukang Cao","Liang Pan","Kai Han","Kwan-Yee K. Wong","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.07164v1.pdf","comment":"Project page: https://yukangcao.github.io/AvatarGO/"},{"id":"http://arxiv.org/abs/2410.07157v1","updated":"2024-10-09T17:56:15Z","published":"2024-10-09T17:56:15Z","title":"InstructG2I: Synthesizing Images from Multimodal Attributed Graphs","summary":"  In this paper, we approach an overlooked yet critical task Graph2Image:\ngenerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To\naddress these challenges, we propose a graph context-conditioned diffusion\nmodel called InstructG2I. InstructG2I first exploits the graph structure and\nmultimodal information to conduct informative neighbor sampling by combining\npersonalized page rank and re-ranking based on vision-language features. Then,\na Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary\nset of graph prompts to guide the denoising process of diffusion. Finally, we\npropose graph classifier-free guidance, enabling controllable generation by\nvarying the strength of graph guidance and multiple connected edges to a node.\nExtensive experiments conducted on three datasets from different domains\ndemonstrate the effectiveness and controllability of our approach. The code is\navailable at https://github.com/PeterGriffinJin/InstructG2I.\n","authors":["Bowen Jin","Ziqi Pang","Bingjun Guo","Yu-Xiong Wang","Jiaxuan You","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2410.07157v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.07155v1","updated":"2024-10-09T17:56:03Z","published":"2024-10-09T17:56:03Z","title":"Trans4D: Realistic Geometry-Aware Transition for Compositional\n  Text-to-4D Synthesis","summary":"  Recent advances in diffusion models have demonstrated exceptional\ncapabilities in image and video generation, further improving the effectiveness\nof 4D synthesis. Existing 4D generation methods can generate high-quality 4D\nobjects or scenes based on user-friendly conditions, benefiting the gaming and\nvideo industries. However, these methods struggle to synthesize significant\nobject deformation of complex 4D transitions and interactions within scenes. To\naddress this challenge, we propose Trans4D, a novel text-to-4D synthesis\nframework that enables realistic complex scene transitions. Specifically, we\nfirst use multi-modal large language models (MLLMs) to produce a physic-aware\nscene description for 4D scene initialization and effective transition timing\nplanning. Then we propose a geometry-aware 4D transition network to realize a\ncomplex scene-level 4D transition based on the plan, which involves expressive\ngeometrical object deformation. Extensive experiments demonstrate that Trans4D\nconsistently outperforms existing state-of-the-art methods in generating 4D\nscenes with accurate and high-quality transitions, validating its\neffectiveness. Code: https://github.com/YangLing0818/Trans4D\n","authors":["Bohan Zeng","Ling Yang","Siyu Li","Jiaming Liu","Zixiang Zhang","Juanxi Tian","Kaixin Zhu","Yongzhen Guo","Fu-Yun Wang","Minkai Xu","Stefano Ermon","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.07155v1.pdf","comment":"Project: https://github.com/YangLing0818/Trans4D"},{"id":"http://arxiv.org/abs/2410.07153v1","updated":"2024-10-09T17:55:43Z","published":"2024-10-09T17:55:43Z","title":"CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based\n  Multi-Entity Action Recognition","summary":"  Skeleton-based multi-entity action recognition is a challenging task aiming\nto identify interactive actions or group activities involving multiple diverse\nentities. Existing models for individuals often fall short in this task due to\nthe inherent distribution discrepancies among entity skeletons, leading to\nsuboptimal backbone optimization. To this end, we introduce a Convex Hull\nAdaptive Shift based multi-Entity action recognition method (CHASE), which\nmitigates inter-entity distribution gaps and unbiases subsequent backbones.\nSpecifically, CHASE comprises a learnable parameterized network and an\nauxiliary objective. The parameterized network achieves plausible,\nsample-adaptive repositioning of skeleton sequences through two key components.\nFirst, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new\norigin of the coordinate system is within the skeleton convex hull. Second, the\nCoefficient Learning Block provides a lightweight parameterization of the\nmapping from skeleton sequences to their specific coefficients in convex\ncombinations. Moreover, to guide the optimization of this network for\ndiscrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean\nDiscrepancy as the additional objective. CHASE operates as a sample-adaptive\nnormalization method to mitigate inter-entity distribution discrepancies,\nthereby reducing data bias and improving the subsequent classifier's\nmulti-entity action recognition performance. Extensive experiments on six\ndatasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and\nVolleyball, consistently verify our approach by seamlessly adapting to\nsingle-entity backbones and boosting their performance in multi-entity\nscenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .\n","authors":["Yuhang Wen","Mengyuan Liu","Songtao Wu","Beichen Ding"],"pdf_url":"https://arxiv.org/pdf/2410.07153v1.pdf","comment":"NeurIPS 2024 Camera-ready Version"},{"id":"http://arxiv.org/abs/2410.07149v1","updated":"2024-10-09T17:55:02Z","published":"2024-10-09T17:55:02Z","title":"Towards Interpreting Visual Information Processing in Vision-Language\n  Models","summary":"  Vision-Language Models (VLMs) are powerful tools for processing and\nunderstanding text and images. We study the processing of visual tokens in the\nlanguage model component of LLaVA, a prominent VLM. Our approach focuses on\nanalyzing the localization of object information, the evolution of visual token\nrepresentations across layers, and the mechanism of integrating visual\ninformation for predictions. Through ablation studies, we demonstrated that\nobject identification accuracy drops by over 70\\% when object-specific tokens\nare removed. We observed that visual token representations become increasingly\ninterpretable in the vocabulary space across layers, suggesting an alignment\nwith textual tokens corresponding to image content. Finally, we found that the\nmodel extracts object information from these refined representations at the\nlast token position for prediction, mirroring the process in text-only language\nmodels for factual association tasks. These findings provide crucial insights\ninto how VLMs process and integrate visual information, bridging the gap\nbetween our understanding of language and vision models, and paving the way for\nmore interpretable and controllable multimodal systems.\n","authors":["Clement Neo","Luke Ong","Philip Torr","Mor Geva","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.07149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07133v1","updated":"2024-10-09T17:52:28Z","published":"2024-10-09T17:52:28Z","title":"EvolveDirector: Approaching Advanced Text-to-Image Generation with Large\n  Vision-Language Models","summary":"  Recent advancements in generation models have showcased remarkable\ncapabilities in generating fantastic content. However, most of them are trained\non proprietary high-quality data, and some models withhold their parameters and\nonly provide accessible application programming interfaces (APIs), limiting\ntheir benefits for downstream tasks. To explore the feasibility of training a\ntext-to-image generation model comparable to advanced models using publicly\navailable resources, we introduce EvolveDirector. This framework interacts with\nadvanced models through their public APIs to obtain text-image data pairs to\ntrain a base model. Our experiments with extensive data indicate that the model\ntrained on generated data of the advanced model can approximate its generation\ncapability. However, it requires large-scale samples of 10 million or more.\nThis incurs significant expenses in time, computational resources, and\nespecially the costs associated with calling fee-based APIs. To address this\nproblem, we leverage pre-trained large vision-language models (VLMs) to guide\nthe evolution of the base model. VLM continuously evaluates the base model\nduring training and dynamically updates and refines the training dataset by the\ndiscrimination, expansion, deletion, and mutation operations. Experimental\nresults show that this paradigm significantly reduces the required data volume.\nFurthermore, when approaching multiple advanced models, EvolveDirector can\nselect the best samples generated by them to learn powerful and balanced\nabilities. The final trained model Edgen is demonstrated to outperform these\nadvanced models. The code and model weights are available at\nhttps://github.com/showlab/EvolveDirector.\n","authors":["Rui Zhao","Hangjie Yuan","Yujie Wei","Shiwei Zhang","Yuchao Gu","Lingmin Ran","Xiang Wang","Zhangjie Wu","Junhao Zhang","Yingya Zhang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2410.07133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07119v1","updated":"2024-10-09T17:49:06Z","published":"2024-10-09T17:49:06Z","title":"Thing2Reality: Transforming 2D Content into Conditioned Multiviews and\n  3D Gaussian Objects for XR Communication","summary":"  During remote communication, participants often share both digital and\nphysical content, such as product designs, digital assets, and environments, to\nenhance mutual understanding. Recent advances in augmented communication have\nfacilitated users to swiftly create and share digital 2D copies of physical\nobjects from video feeds into a shared space. However, conventional 2D\nrepresentations of digital objects restricts users' ability to spatially\nreference items in a shared immersive environment. To address this, we propose\nThing2Reality, an Extended Reality (XR) communication platform that enhances\nspontaneous discussions of both digital and physical items during remote\nsessions. With Thing2Reality, users can quickly materialize ideas or physical\nobjects in immersive environments and share them as conditioned multiview\nrenderings or 3D Gaussians. Thing2Reality enables users to interact with remote\nobjects or discuss concepts in a collaborative manner. Our user study revealed\nthat the ability to interact with and manipulate 3D representations of objects\nsignificantly enhances the efficiency of discussions, with the potential to\naugment discussion of 2D artifacts.\n","authors":["Erzhen Hu","Mingyi Li","Jungtaek Hong","Xun Qian","Alex Olwal","David Kim","Seongkook Heo","Ruofei Du"],"pdf_url":"https://arxiv.org/pdf/2410.07119v1.pdf","comment":"18 pages (15 pages without references), 13 figures"},{"id":"http://arxiv.org/abs/2410.07113v1","updated":"2024-10-09T17:46:53Z","published":"2024-10-09T17:46:53Z","title":"Personalized Visual Instruction Tuning","summary":"  Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated significant progress; however, these models exhibit a notable\nlimitation, which we refer to as \"face blindness\". Specifically, they can\nengage in general conversations but fail to conduct personalized dialogues\ntargeting at specific individuals. This deficiency hinders the application of\nMLLMs in personalized settings, such as tailored visual assistants on mobile\ndevices, or domestic robots that need to recognize members of the family. In\nthis paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel\ndata curation and training framework designed to enable MLLMs to identify\ntarget individuals within an image and engage in personalized and coherent\ndialogues. Our approach involves the development of a sophisticated pipeline\nthat autonomously generates training data containing personalized\nconversations. This pipeline leverages the capabilities of various visual\nexperts, image generation models, and (multi-modal) large language models. To\nevaluate the personalized potential of MLLMs, we present a benchmark called\nP-Bench, which encompasses various question types with different levels of\ndifficulty. The experiments demonstrate a substantial personalized performance\nenhancement after fine-tuning with our curated dataset.\n","authors":["Renjie Pi","Jianshu Zhang","Tianyang Han","Jipeng Zhang","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.07113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07112v1","updated":"2024-10-09T17:46:34Z","published":"2024-10-09T17:46:34Z","title":"VHELM: A Holistic Evaluation of Vision Language Models","summary":"  Current benchmarks for assessing vision-language models (VLMs) often focus on\ntheir perception or problem-solving capabilities and neglect other critical\naspects such as fairness, multilinguality, or toxicity. Furthermore, they\ndiffer in their evaluation procedures and the scope of the evaluation, making\nit difficult to compare models. To address these issues, we extend the HELM\nframework to VLMs to present the Holistic Evaluation of Vision Language Models\n(VHELM). VHELM aggregates various datasets to cover one or more of the 9\naspects: visual perception, knowledge, reasoning, bias, fairness,\nmultilinguality, robustness, toxicity, and safety. In doing so, we produce a\ncomprehensive, multi-dimensional view of the capabilities of the VLMs across\nthese important factors. In addition, we standardize the standard inference\nparameters, methods of prompting, and evaluation metrics to enable fair\ncomparisons across models. Our framework is designed to be lightweight and\nautomatic so that evaluation runs are cheap and fast. Our initial run evaluates\n22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.\nWe uncover new key findings, such as the fact that efficiency-focused models\n(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than\ntheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark\nbut not when evaluated on the other aspects. For transparency, we release the\nraw model generations and complete results on our website\n(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living\nbenchmark, and we hope to continue adding new datasets and models over time.\n","authors":["Tony Lee","Haoqin Tu","Chi Heem Wong","Wenhao Zheng","Yiyang Zhou","Yifan Mai","Josselin Somerville Roberts","Michihiro Yasunaga","Huaxiu Yao","Cihang Xie","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2410.07112v1.pdf","comment":"NeurIPS 2024. First three authors contributed equally"},{"id":"http://arxiv.org/abs/2410.07110v1","updated":"2024-10-09T17:45:47Z","published":"2024-10-09T17:45:47Z","title":"Continual Learning: Less Forgetting, More OOD Generalization via\n  Adaptive Contrastive Replay","summary":"  Machine learning models often suffer from catastrophic forgetting of\npreviously learned knowledge when learning new classes. Various methods have\nbeen proposed to mitigate this issue. However, rehearsal-based learning, which\nretains samples from previous classes, typically achieves good performance but\ntends to memorize specific instances, struggling with Out-of-Distribution (OOD)\ngeneralization. This often leads to high forgetting rates and poor\ngeneralization. Surprisingly, the OOD generalization capabilities of these\nmethods have been largely unexplored. In this paper, we highlight this issue\nand propose a simple yet effective strategy inspired by contrastive learning\nand data-centric principles to address it. We introduce Adaptive Contrastive\nReplay (ACR), a method that employs dual optimization to simultaneously train\nboth the encoder and the classifier. ACR adaptively populates the replay buffer\nwith misclassified samples while ensuring a balanced representation of classes\nand tasks. By refining the decision boundary in this way, ACR achieves a\nbalance between stability and plasticity. Our method significantly outperforms\nprevious approaches in terms of OOD generalization, achieving an improvement of\n13.41\\% on Split CIFAR-100, 9.91\\% on Split Mini-ImageNet, and 5.98\\% on Split\nTiny-ImageNet.\n","authors":["Hossein Rezaei","Mohammad Sabokrou"],"pdf_url":"https://arxiv.org/pdf/2410.07110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11001v2","updated":"2024-10-09T17:44:14Z","published":"2024-03-16T19:11:57Z","title":"Topologically Faithful Multi-class Segmentation in Medical Images","summary":"  Topological accuracy in medical image segmentation is a highly important\nproperty for downstream applications such as network analysis and flow modeling\nin vessels or cell counting. Recently, significant methodological advancements\nhave brought well-founded concepts from algebraic topology to binary\nsegmentation. However, these approaches have been underexplored in multi-class\nsegmentation scenarios, where topological errors are common. We propose a\ngeneral loss function for topologically faithful multi-class segmentation\nextending the recent Betti matching concept, which is based on induced\nmatchings of persistence barcodes. We project the N-class segmentation problem\nto N single-class segmentation tasks, which allows us to use 1-parameter\npersistent homology, making training of neural networks computationally\nfeasible. We validate our method on a comprehensive set of four medical\ndatasets with highly variant topological characteristics. Our loss formulation\nsignificantly enhances topological correctness in cardiac, cell, artery-vein,\nand Circle of Willis segmentation.\n","authors":["Alexander H. Berger","Nico Stucki","Laurin Lux","Vincent Buergin","Suprosanna Shit","Anna Banaszak","Daniel Rueckert","Ulrich Bauer","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2403.11001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07093v1","updated":"2024-10-09T17:33:03Z","published":"2024-10-09T17:33:03Z","title":"LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and\n  Captioning","summary":"  Language plays a vital role in the realm of human motion. Existing methods\nhave largely depended on CLIP text embeddings for motion generation, yet they\nfall short in effectively aligning language and motion due to CLIP's\npretraining on static image-text pairs. This work introduces LaMP, a novel\nLanguage-Motion Pretraining model, which transitions from a language-vision to\na more suitable language-motion latent space. It addresses key limitations by\ngenerating motion-informative text embeddings, significantly enhancing the\nrelevance and semantics of generated motion sequences. With LaMP, we advance\nthree key tasks: text-to-motion generation, motion-text retrieval, and motion\ncaptioning through aligned language-motion representation learning. For\ngeneration, we utilize LaMP to provide the text condition instead of CLIP, and\nan autoregressive masked prediction is designed to achieve mask modeling\nwithout rank collapse in transformers. For retrieval, motion features from\nLaMP's motion transformer interact with query tokens to retrieve text features\nfrom the text transformer, and vice versa. For captioning, we finetune a large\nlanguage model with the language-informative motion features to develop a\nstrong motion captioning model. In addition, we introduce the LaMP-BertScore\nmetric to assess the alignment of generated motions with textual descriptions.\nExtensive experimental results on multiple datasets demonstrate substantial\nimprovements over previous methods across all three tasks. The code of our\nmethod will be made public.\n","authors":["Zhe Li","Weihao Yuan","Yisheng He","Lingteng Qiu","Shenhao Zhu","Xiaodong Gu","Weichao Shen","Yuan Dong","Zilong Dong","Laurence T. Yang"],"pdf_url":"https://arxiv.org/pdf/2410.07093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07087v1","updated":"2024-10-09T17:29:01Z","published":"2024-10-09T17:29:01Z","title":"Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark,\n  and Methodology","summary":"  Developing agents capable of navigating to a target location based on\nlanguage instructions and visual information, known as vision-language\nnavigation (VLN), has attracted widespread interest. Most research has focused\non ground-based agents, while UAV-based VLN remains relatively underexplored.\nRecent efforts in UAV vision-language navigation predominantly adopt\nground-based VLN settings, relying on predefined discrete action spaces and\nneglecting the inherent disparities in agent movement dynamics and the\ncomplexity of navigation tasks between ground and aerial environments. To\naddress these disparities and challenges, we propose solutions from three\nperspectives: platform, benchmark, and methodology. To enable realistic UAV\ntrajectory simulation in VLN tasks, we propose the OpenUAV platform, which\nfeatures diverse environments, realistic flight control, and extensive\nalgorithmic support. We further construct a target-oriented VLN dataset\nconsisting of approximately 12k trajectories on this platform, serving as the\nfirst dataset specifically designed for realistic UAV VLN tasks. To tackle the\nchallenges posed by complex aerial environments, we propose an assistant-guided\nUAV object search benchmark called UAV-Need-Help, which provides varying levels\nof guidance information to help UAVs better accomplish realistic VLN tasks. We\nalso propose a UAV navigation LLM that, given multi-view images, task\ndescriptions, and assistant instructions, leverages the multimodal\nunderstanding capabilities of the MLLM to jointly process visual and textual\ninformation, and performs hierarchical trajectory generation. The evaluation\nresults of our method significantly outperform the baseline models, while there\nremains a considerable gap between our results and those achieved by human\noperators, underscoring the challenge presented by the UAV-Need-Help task.\n","authors":["Xiangyu Wang","Donglin Yang","Ziqin Wang","Hohin Kwan","Jinyu Chen","Wenjun Wu","Hongsheng Li","Yue Liao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2410.07087v1.pdf","comment":"Under review as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2407.12040v4","updated":"2024-10-09T17:28:33Z","published":"2024-07-01T17:59:55Z","title":"Comprehensive Performance Evaluation of YOLO11, YOLOv10, YOLOv9 and\n  YOLOv8 on Detecting and Counting Fruitlet in Complex Orchard Environments","summary":"  This study extensively evaluated You Only Look Once (YOLO) object detection\nalgorithms across all configurations (total 22) of YOLOv8, YOLOv9, YOLOv10, and\nYOLO11 for green fruit detection in commercial orchards. The research also\nvalidated in-field fruitlet counting using an iPhone and machine vision sensors\nacross four apple varieties: Scifresh, Scilate, Honeycrisp and Cosmic Crisp.\nAmong the 22 configurations evaluated, YOLO11s and YOLOv9 gelan-base\noutperformed others with mAP@50 scores of 0.933 and 0.935 respectively. In\nterms of recall, YOLOv9 gelan-base achieved the highest value among YOLOv9\nconfigurations at 0.899, while YOLO11m led YOLO11 variants with 0.897. YOLO11n\nemerged as the fastest model, achieving fastest inference speed of only 2.4 ms,\nsignificantly outpacing the leading configurations of YOLOv10n, YOLOv9 gelan-s,\nand YOLOv8n, with speeds of 5.5, 11.5, and 4.1 ms, respectively. This\ncomparative evaluation highlights the strengths of YOLO11, YOLOv9, and YOLOv10,\noffering researchers essential insights to choose the best-suited model for\nfruitlet detection and possible automation in commercial orchards. For\nreal-time automation related work in relevant datasets, we recommend using\nYOLO11n due to its high detection and image processing speed. Keywords: YOLO11,\nYOLO11 Object Detection, YOLOv10, YOLOv9, YOLOv8, You Only Look Once, Fruitlet\nDetection, Greenfruit Detection, Green Apple Detection, Agricultural\nAutomation, Artificial Intelligence, Deep Learning, Machine Learning, Zero-shot\nDetection\n","authors":["Ranjan Sapkota","Zhichao Meng","Martin Churuvija","Xiaoqiang Du","Zenghong Ma","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2407.12040v4.pdf","comment":"15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.07081v1","updated":"2024-10-09T17:23:54Z","published":"2024-10-09T17:23:54Z","title":"JPEG Inspired Deep Learning","summary":"  Although it is traditionally believed that lossy image compression, such as\nJPEG compression, has a negative impact on the performance of deep neural\nnetworks (DNNs), it is shown by recent works that well-crafted JPEG compression\ncan actually improve the performance of deep learning (DL). Inspired by this,\nwe propose JPEG-DL, a novel DL framework that prepends any underlying DNN\narchitecture with a trainable JPEG compression layer. To make the quantization\noperation in JPEG compression trainable, a new differentiable soft quantizer is\nemployed at the JPEG layer, and then the quantization operation and underlying\nDNN are jointly trained. Extensive experiments show that in comparison with the\nstandard DL, JPEG-DL delivers significant accuracy improvements across various\ndatasets and model architectures while enhancing robustness against adversarial\nattacks. Particularly, on some fine-grained image classification datasets,\nJPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is\navailable on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.\n","authors":["Ahmed H. Salamah","Kaixiang Zheng","Yiwen Liu","En-Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2410.07081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07073v1","updated":"2024-10-09T17:16:22Z","published":"2024-10-09T17:16:22Z","title":"Pixtral 12B","summary":"  We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.\n","authors":["Pravesh Agrawal","Szymon Antoniak","Emma Bou Hanna","Devendra Chaplot","Jessica Chudnovsky","Saurabh Garg","Theophile Gervet","Soham Ghosh","Amlie Hliou","Paul Jacob","Albert Q. Jiang","Timothe Lacroix","Guillaume Lample","Diego Las Casas","Thibaut Lavril","Teven Le Scao","Andy Lo","William Marshall","Louis Martin","Arthur Mensch","Pavankumar Muddireddy","Valera Nemychnikova","Marie Pellat","Patrick Von Platen","Nikhil Raghuraman","Baptiste Rozire","Alexandre Sablayrolles","Lucile Saulnier","Romain Sauvestre","Wendy Shang","Roman Soletskyi","Lawrence Stewart","Pierre Stock","Joachim Studnia","Sandeep Subramanian","Sagar Vaze","Thomas Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07062v1","updated":"2024-10-09T17:03:49Z","published":"2024-10-09T17:03:49Z","title":"TinyEmo: Scaling down Emotional Reasoning via Metric Projection","summary":"  This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems.\n  We release code, models, and dataset at https://github.com/ggcr/TinyEmo\n","authors":["Cristian Gutierrez"],"pdf_url":"https://arxiv.org/pdf/2410.07062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05412v2","updated":"2024-10-09T16:49:58Z","published":"2023-12-08T23:55:19Z","title":"CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional\n  Modeling","summary":"  We introduce a multi-modal diffusion model tailored for the bi-directional\nconditional generation of video and audio. We propose a joint contrastive\ntraining loss to improve the synchronization between visual and auditory\noccurrences. We present experiments on two datasets to evaluate the efficacy of\nour proposed model. The assessment of generation quality and alignment\nperformance is carried out from various angles, encompassing both objective and\nsubjective metrics. Our findings demonstrate that the proposed model\noutperforms the baseline in terms of quality and generation speed through\nintroduction of our novel cross-modal easy fusion architectural block.\nFurthermore, the incorporation of the contrastive loss results in improvements\nin audio-visual alignment, particularly in the high-correlation video-to-audio\ngeneration task.\n","authors":["Ruihan Yang","Hannes Gamper","Sebastian Braun"],"pdf_url":"https://arxiv.org/pdf/2312.05412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07046v1","updated":"2024-10-09T16:36:45Z","published":"2024-10-09T16:36:45Z","title":"S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in\n  Pruning","summary":"  Recently, differentiable mask pruning methods optimize the continuous\nrelaxation architecture (soft network) as the proxy of the pruned discrete\nnetwork (hard network) for superior sub-architecture search. However, due to\nthe agnostic impact of the discretization process, the hard network struggles\nwith the equivalent representational capacity as the soft network, namely\ndiscretization gap, which severely spoils the pruning performance. In this\npaper, we first investigate the discretization gap and propose a novel\nstructural differentiable mask pruning framework named S2HPruner to bridge the\ndiscretization gap in a one-stage manner. In the training procedure, SH2Pruner\nforwards both the soft network and its corresponding hard network, then\ndistills the hard network under the supervision of the soft network. To\noptimize the mask and prevent performance degradation, we propose a decoupled\nbidirectional knowledge distillation. It blocks the weight updating from the\nhard to the soft network while maintaining the gradient corresponding to the\nmask. Compared with existing pruning arts, S2HPruner achieves surpassing\npruning performance without fine-tuning on comprehensive benchmarks, including\nCIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures.\nBesides, investigation and analysis experiments explain the effectiveness of\nS2HPruner. Codes will be released soon.\n","authors":["Weihao Lin","Shengji Tang","Chong Yu","Peng Ye","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07046v1.pdf","comment":"NeurIPS 2024 accepted"},{"id":"http://arxiv.org/abs/2410.07043v1","updated":"2024-10-09T16:34:39Z","published":"2024-10-09T16:34:39Z","title":"Z-upscaling: Optical Flow Guided Frame Interpolation for Isotropic\n  Reconstruction of 3D EM Volumes","summary":"  We propose a novel optical flow based approach to enhance the axial\nresolution of anisotropic 3D EM volumes to achieve isotropic 3D reconstruction.\nAssuming spatial continuity of 3D biological structures in well aligned EM\nvolumes, we reasoned that optical flow estimation techniques, often applied for\ntemporal resolution enhancement in videos, can be utilized. Pixel level motion\nis estimated between neighboring 2D slices along z, using spatial gradient flow\nestimates to interpolate and generate new 2D slices resulting in isotropic\nvoxels. We leverage recent state-of-the-art learning methods for video frame\ninterpolation and transfer learning techniques, and demonstrate the success of\nour approach on publicly available ultrastructure EM volumes.\n","authors":["Fisseha A. Ferede","Ali Khalighifar","Jaison John","Krishnan Venkataraman","Khaled Khairy"],"pdf_url":"https://arxiv.org/pdf/2410.07043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07030v1","updated":"2024-10-09T16:13:19Z","published":"2024-10-09T16:13:19Z","title":"Clean Evaluations on Contaminated Visual Language Models","summary":"  How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.\n","authors":["Hongyuan Lu","Shujie Miao","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2410.07030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15234v3","updated":"2024-10-09T16:12:40Z","published":"2024-05-24T05:47:23Z","title":"Defensive Unlearning with Adversarial Training for Robust Concept\n  Erasure in Diffusion Models","summary":"  Diffusion models (DMs) have achieved remarkable success in text-to-image\ngeneration, but they also pose safety risks, such as the potential generation\nof harmful content and copyright violations. The techniques of machine\nunlearning, also known as concept erasing, have been developed to address these\nrisks. However, these techniques remain vulnerable to adversarial prompt\nattacks, which can prompt DMs post-unlearning to regenerate undesired images\ncontaining concepts (such as nudity) meant to be erased. This work aims to\nenhance the robustness of concept erasing by integrating the principle of\nadversarial training (AT) into machine unlearning, resulting in the robust\nunlearning framework referred to as AdvUnlearn. However, achieving this\neffectively and efficiently is highly nontrivial. First, we find that a\nstraightforward implementation of AT compromises DMs' image generation quality\npost-unlearning. To address this, we develop a utility-retaining regularization\non an additional retain set, optimizing the trade-off between concept erasure\nrobustness and model utility in AdvUnlearn. Moreover, we identify the text\nencoder as a more suitable module for robustification compared to UNet,\nensuring unlearning effectiveness. And the acquired text encoder can serve as a\nplug-and-play robust unlearner for various DM types. Empirically, we perform\nextensive experiments to demonstrate the robustness advantage of AdvUnlearn\nacross various DM unlearning scenarios, including the erasure of nudity,\nobjects, and style concepts. In addition to robustness, AdvUnlearn also\nachieves a balanced tradeoff with model utility. To our knowledge, this is the\nfirst work to systematically explore robust DM unlearning through AT, setting\nit apart from existing methods that overlook robustness in concept erasing.\nCodes are available at: https://github.com/OPTML-Group/AdvUnlearn\n","authors":["Yimeng Zhang","Xin Chen","Jinghan Jia","Yihua Zhang","Chongyu Fan","Jiancheng Liu","Mingyi Hong","Ke Ding","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2405.15234v3.pdf","comment":"Accepted by NeurIPS'24. Codes are available at\n  https://github.com/OPTML-Group/AdvUnlearn"},{"id":"http://arxiv.org/abs/2410.07025v1","updated":"2024-10-09T16:07:11Z","published":"2024-10-09T16:07:11Z","title":"Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation\n  Models Without Human Feedback","summary":"  Radiologists play a crucial role by translating medical images into medical\nreports. However, the field faces staffing shortages and increasing workloads.\nWhile automated approaches using vision-language models (VLMs) show promise as\nassistants, they require exceptionally high accuracy. Most current VLMs in\nradiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the\ngeneral domain, additional preference fine-tuning has become standard practice.\nThe challenge in radiology lies in the prohibitive cost of obtaining\nradiologist feedback. We propose a scalable automated preference alignment\ntechnique for VLMs in radiology, focusing on chest X-ray (CXR) report\ngeneration. Our method leverages publicly available datasets with an\nLLM-as-a-Judge mechanism, eliminating the need for additional expert\nradiologist feedback. We evaluate and benchmark five direct alignment\nalgorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN\nscores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in\nan average across six metrics (domain specific and general), compared to the\nSFT baseline. We study reward overoptimization via length exploitation, with\nreports lengthening by up to 3.2x. To assess a potential alignment tax, we\nbenchmark on six additional diverse tasks, finding no significant degradations.\nA reader study involving four board-certified radiologists indicates win rates\nof up to 0.62 over the SFT baseline, while significantly penalizing verbosity.\nOur analysis provides actionable insights for the development of VLMs in\nhigh-stakes fields like radiology.\n","authors":["Dennis Hein","Zhihong Chen","Sophie Ostmeier","Justin Xu","Maya Varma","Eduardo Pontes Reis","Arne Edward Michalson","Christian Bluethgen","Hyun Joo Shin","Curtis Langlotz","Akshay S Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2410.07025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03043v2","updated":"2024-10-09T15:44:35Z","published":"2024-09-04T19:27:56Z","title":"Can Your Generative Model Detect Out-of-Distribution Covariate Shift?","summary":"  Detecting Out-of-Distribution (OOD) sensory data and covariate distribution\nshift aims to identify new test examples with different high-level image\nstatistics to the captured, normal and In-Distribution (ID) set. Existing OOD\ndetection literature largely focuses on semantic shift with little-to-no\nconsensus over covariate shift. Generative models capture the ID data in an\nunsupervised manner, enabling them to effectively identify samples that deviate\nsignificantly from this learned distribution, irrespective of the downstream\ntask. In this work, we elucidate the ability of generative models to detect and\nquantify domain-specific covariate shift through extensive analyses that\ninvolves a variety of models. To this end, we conjecture that it is sufficient\nto detect most occurring sensory faults (anomalies and deviations in global\nsignals statistics) by solely modeling high-frequency signal-dependent and\nindependent details. We propose a novel method, CovariateFlow, for OOD\ndetection, specifically tailored to covariate heteroscedastic high-frequency\nimage-components using conditional Normalizing Flows (cNFs). Our results on\nCIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate the\neffectiveness of the method by accurately detecting OOD covariate shift. This\nwork contributes to enhancing the fidelity of imaging systems and aiding\nmachine learning models in OOD detection in the presence of covariate shift.\n","authors":["Christiaan Viviers","Amaan Valiuddin","Francisco Caetano","Lemar Abdi","Lena Filatova","Peter de With","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2409.03043v2.pdf","comment":"ECCV 2024, typos corrected"},{"id":"http://arxiv.org/abs/2410.06997v1","updated":"2024-10-09T15:44:34Z","published":"2024-10-09T15:44:34Z","title":"A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one\n  Single X-ray","summary":"  Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays\nare commonly used for its diagnosis due to their cost-effectiveness. Magnetic\nResonance Imaging (MRI), on the other hand, offers detailed soft tissue\nvisualization and has become a valuable supplementary diagnostic tool for KOA.\nUnfortunately, the high cost and limited accessibility of MRI hinder its\nwidespread use, leaving many patients with KOA reliant solely on X-ray imaging.\nIn this study, we introduce a novel diffusion-based Xray2MRI model capable of\ngenerating pseudo-MRI volumes from one single X-ray image. In addition to using\nX-rays as conditional input, our model integrates target depth, KOA probability\ndistribution, and image intensity distribution modules to guide the synthesis\nprocess, ensuring that the generated corresponding slices accurately correspond\nto the anatomical structures. Experimental results demonstrate that by\nintegrating information from X-rays with additional input data, our proposed\napproach is capable of generating pseudo-MRI sequences that approximate real\nMRI scans. Moreover, by increasing the inference times, the model achieves\neffective interpolation, further improving the continuity and smoothness of the\ngenerated MRI sequences, representing one promising initial attempt for\ncost-effective medical imaging solutions.\n","authors":["Zhe Wang","Rachid Jennane","Aladine Chetouani","Mohamed Jarraya"],"pdf_url":"https://arxiv.org/pdf/2410.06997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15817v2","updated":"2024-10-09T15:26:25Z","published":"2023-12-25T21:55:00Z","title":"A Unified Generative Framework for Realistic Lidar Simulation in\n  Autonomous Driving Systems","summary":"  Simulation models for perception sensors are integral components of\nautomotive simulators used for the virtual Verification and Validation (V\\&V)\nof Autonomous Driving Systems (ADS). These models also serve as powerful tools\nfor generating synthetic datasets to train deep learning-based perception\nmodels. Lidar is a widely used sensor type among the perception sensors for ADS\ndue to its high precision in 3D environment scanning. However, developing\nrealistic Lidar simulation models is a significant technical challenge. In\nparticular, unrealistic models can result in a large gap between the\nsynthesised and real-world point clouds, limiting their effectiveness in ADS\napplications. Recently, deep generative models have emerged as promising\nsolutions to synthesise realistic sensory data. However, for Lidar simulation,\ndeep generative models have been primarily hybridised with conventional\nalgorithms, leaving unified generative approaches largely unexplored in the\nliterature. Motivated by this research gap, we propose a unified generative\nframework to enhance Lidar simulation fidelity. Our proposed framework projects\nLidar point clouds into depth-reflectance images via a lossless transformation,\nand employs our novel Controllable Lidar point cloud Generative model, CoLiGen,\nto translate the images. We extensively evaluate our CoLiGen model, comparing\nit with the state-of-the-art image-to-image translation models using various\nmetrics to assess the realness, faithfulness, and performance of a downstream\nperception model. Our results show that CoLiGen exhibits superior performance\nacross most metrics. The dataset and source code for this research are\navailable at https://github.com/hamedhaghighi/CoLiGen.git.\n","authors":["Hamed Haghighi","Mehrdad Dianati","Valentina Donzella","Kurt Debattista"],"pdf_url":"https://arxiv.org/pdf/2312.15817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06985v1","updated":"2024-10-09T15:21:46Z","published":"2024-10-09T15:21:46Z","title":"Jointly Generating Multi-view Consistent PBR Textures using\n  Collaborative Control","summary":"  Multi-view consistency remains a challenge for image diffusion models. Even\nwithin the Text-to-Texture problem, where perfect geometric correspondences are\nknown a priori, many methods fail to yield aligned predictions across views,\nnecessitating non-trivial fusion methods to incorporate the results onto the\noriginal mesh. We explore this issue for a Collaborative Control workflow\nspecifically in PBR Text-to-Texture. Collaborative Control directly models PBR\nimage probability distributions, including normal bump maps; to our knowledge,\nthe only diffusion model to directly output full PBR stacks. We discuss the\ndesign decisions involved in making this model multi-view consistent, and\ndemonstrate the effectiveness of our approach in ablation studies, as well as\npractical applications.\n","authors":["Shimon Vainer","Konstantin Kutsy","Dante De Nigris","Ciara Rowles","Slava Elizarov","Simon Donn"],"pdf_url":"https://arxiv.org/pdf/2410.06985v1.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.06982v1","updated":"2024-10-09T15:20:29Z","published":"2024-10-09T15:20:29Z","title":"Structure-Centric Robust Monocular Depth Estimation via Knowledge\n  Distillation","summary":"  Monocular depth estimation, enabled by self-supervised learning, is a key\ntechnique for 3D perception in computer vision. However, it faces significant\nchallenges in real-world scenarios, which encompass adverse weather variations,\nmotion blur, as well as scenes with poor lighting conditions at night. Our\nresearch reveals that we can divide monocular depth estimation into three\nsub-problems: depth structure consistency, local texture disambiguation, and\nsemantic-structural correlation. Our approach tackles the non-robustness of\nexisting self-supervised monocular depth estimation models to interference\ntextures by adopting a structure-centered perspective and utilizing the scene\nstructure characteristics demonstrated by semantics and illumination. We devise\na novel approach to reduce over-reliance on local textures, enhancing\nrobustness against missing or interfering patterns. Additionally, we\nincorporate a semantic expert model as the teacher and construct inter-model\nfeature dependencies via learnable isomorphic graphs to enable aggregation of\nsemantic structural knowledge. Our approach achieves state-of-the-art\nout-of-distribution monocular depth estimation performance across a range of\npublic adverse scenario datasets. It demonstrates notable scalability and\ncompatibility, without necessitating extensive model engineering. This\nshowcases the potential for customizing models for diverse industrial\napplications.\n","authors":["Runze Chen","Haiyong Luo","Fang Zhao","Jingze Yu","Yupeng Jia","Juan Wang","Xuepeng Ma"],"pdf_url":"https://arxiv.org/pdf/2410.06982v1.pdf","comment":"To be published in Asian Conference on Computer Vision 2024"},{"id":"http://arxiv.org/abs/2410.06977v1","updated":"2024-10-09T15:16:30Z","published":"2024-10-09T15:16:30Z","title":"Adaptive High-Frequency Transformer for Diverse Wildlife\n  Re-Identification","summary":"  Wildlife ReID involves utilizing visual technology to identify specific\nindividuals of wild animals in different scenarios, holding significant\nimportance for wildlife conservation, ecological research, and environmental\nmonitoring. Existing wildlife ReID methods are predominantly tailored to\nspecific species, exhibiting limited applicability. Although some approaches\nleverage extensively studied person ReID techniques, they struggle to address\nthe unique challenges posed by wildlife. Therefore, in this paper, we present a\nunified, multi-species general framework for wildlife ReID. Given that\nhigh-frequency information is a consistent representation of unique features in\nvarious species, significantly aiding in identifying contours and details such\nas fur textures, we propose the Adaptive High-Frequency Transformer model with\nthe goal of enhancing high-frequency information learning. To mitigate the\ninevitable high-frequency interference in the wilderness environment, we\nintroduce an object-aware high-frequency selection strategy to adaptively\ncapture more valuable high-frequency components. Notably, we unify the\nexperimental settings of multiple wildlife datasets for ReID, achieving\nsuperior performance over state-of-the-art ReID methods. In domain\ngeneralization scenarios, our approach demonstrates robust generalization to\nunknown species.\n","authors":["Chenyue Li","Shuoyi Chen","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2410.06977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06974v1","updated":"2024-10-09T15:12:35Z","published":"2024-10-09T15:12:35Z","title":"Diagnosis of Malignant Lymphoma Cancer Using Hybrid Optimized Techniques\n  Based on Dense Neural Networks","summary":"  Lymphoma diagnosis, particularly distinguishing between subtypes, is critical\nfor effective treatment but remains challenging due to the subtle morphological\ndifferences in histopathological images. This study presents a novel hybrid\ndeep learning framework that combines DenseNet201 for feature extraction with a\nDense Neural Network (DNN) for classification, optimized using the Harris Hawks\nOptimization (HHO) algorithm. The model was trained on a dataset of 15,000\nbiopsy images, spanning three lymphoma subtypes: Chronic Lymphocytic Leukemia\n(CLL), Follicular Lymphoma (FL), and Mantle Cell Lymphoma (MCL). Our approach\nachieved a testing accuracy of 99.33\\%, demonstrating significant improvements\nin both accuracy and model interpretability. Comprehensive evaluation using\nprecision, recall, F1-score, and ROC-AUC underscores the model's robustness and\npotential for clinical adoption. This framework offers a scalable solution for\nimproving diagnostic accuracy and efficiency in oncology.\n","authors":["Salah A. Aly","Ali Bakhiet","Mazen Balat"],"pdf_url":"https://arxiv.org/pdf/2410.06974v1.pdf","comment":"6 pages, 5 figures, 4 tables, IEEE ICCA"},{"id":"http://arxiv.org/abs/2409.15107v2","updated":"2024-10-09T15:09:47Z","published":"2024-09-23T15:17:30Z","title":"The BRAVO Semantic Segmentation Challenge Results in UNCV2024","summary":"  We propose the unified BRAVO challenge to benchmark the reliability of\nsemantic segmentation models under realistic perturbations and unknown\nout-of-distribution (OOD) scenarios. We define two categories of reliability:\n(1) semantic reliability, which reflects the model's accuracy and calibration\nwhen exposed to various perturbations; and (2) OOD reliability, which measures\nthe model's ability to detect object classes that are unknown during training.\nThe challenge attracted nearly 100 submissions from international teams\nrepresenting notable research institutions. The results reveal interesting\ninsights into the importance of large-scale pre-training and minimal\narchitectural design in developing robust and reliable semantic segmentation\nmodels.\n","authors":["Tuan-Hung Vu","Eduardo Valle","Andrei Bursuc","Tommie Kerssies","Daan de Geus","Gijs Dubbelman","Long Qian","Bingke Zhu","Yingying Chen","Ming Tang","Jinqiao Wang","Tom Voj","Jan ochman","Ji Matas","Michael Smith","Frank Ferrie","Shamik Basu","Christos Sakaridis","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2409.15107v2.pdf","comment":"ECCV 2024 proceeding paper of the BRAVO challenge 2024, see\n  https://benchmarks.elsa-ai.eu/?ch=1&com=introduction Corrected numbers in\n  Tables 1,3,4,5 and 10"},{"id":"http://arxiv.org/abs/2410.04417v2","updated":"2024-10-09T15:04:16Z","published":"2024-10-06T09:18:04Z","title":"SparseVLM: Visual Token Sparsification for Efficient Vision-Language\n  Model Inference","summary":"  In vision-language models (VLMs), visual tokens usually consume a significant\namount of computational overhead, despite their sparser information density\ncompared to text tokens. To address this, most existing methods learn a network\nto prune redundant visual tokens and require additional training data.\nDifferently, we propose an efficient training-free token optimization mechanism\ndubbed SparseVLM without extra parameters or fine-tuning costs. Concretely,\ngiven that visual tokens complement text tokens in VLMs for linguistic\nreasoning, we select visual-relevant text tokens to rate the significance of\nvision tokens within the self-attention matrix extracted from the VLMs. Then we\nprogressively prune irrelevant tokens. To maximize sparsity while retaining\nessential information, we introduce a rank-based strategy to adaptively\ndetermine the sparsification ratio for each layer, alongside a token recycling\nmethod that compresses pruned tokens into more compact representations.\nExperimental results show that our SparseVLM improves the efficiency of various\nVLMs across a range of image and video understanding tasks. In particular,\nLLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio\nof 78% while maintaining 93% of the accuracy. Our code is available at\nhttps://github.com/Gumpest/SparseVLMs.\n","authors":["Yuan Zhang","Chun-Kai Fan","Junpeng Ma","Wenzhao Zheng","Tao Huang","Kuan Cheng","Denis Gudovskiy","Tomoyuki Okuno","Yohei Nakata","Kurt Keutzer","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.04417v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.06964v1","updated":"2024-10-09T15:02:28Z","published":"2024-10-09T15:02:28Z","title":"Bridge the Points: Graph-based Few-shot Segment Anything Semantically","summary":"  The recent advancements in large-scale pre-training techniques have\nsignificantly enhanced the capabilities of vision foundation models, notably\nthe Segment Anything Model (SAM), which can generate precise masks based on\npoint and box prompts. Recent studies extend SAM to Few-shot Semantic\nSegmentation (FSS), focusing on prompt generation for SAM-based automatic\nsemantic segmentation. However, these methods struggle with selecting suitable\nprompts, require specific hyperparameter settings for different scenarios, and\nexperience prolonged one-shot inference times due to the overuse of SAM,\nresulting in low efficiency and limited automation ability. To address these\nissues, we propose a simple yet effective approach based on graph analysis. In\nparticular, a Positive-Negative Alignment module dynamically selects the point\nprompts for generating masks, especially uncovering the potential of the\nbackground context as the negative reference. Another subsequent Point-Mask\nClustering module aligns the granularity of masks and selected points as a\ndirected graph, based on mask coverage over points. These points are then\naggregated by decomposing the weakly connected components of the directed graph\nin an efficient manner, constructing distinct natural clusters. Finally, the\npositive and overshooting gating, benefiting from graph-based granularity\nalignment, aggregate high-confident masks and filter out the false-positive\nmasks for final prediction, reducing the usage of additional hyperparameters\nand redundant mask generation. Extensive experimental analysis across standard\nFSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the\neffectiveness and efficiency of the proposed approach, surpassing\nstate-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2%\non LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/.\n","authors":["Anqi Zhang","Guangyu Gao","Jianbo Jiao","Chi Harold Liu","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2410.06964v1.pdf","comment":"Accepted to NeurIPS 2024 as Spotlight"},{"id":"http://arxiv.org/abs/2410.06963v1","updated":"2024-10-09T15:02:08Z","published":"2024-10-09T15:02:08Z","title":"ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling","summary":"  This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}\n","authors":["Deok-Kyeong Jang","Dongseok Yang","Deok-Yun Jang","Byeoli Choi","Donghoon Shin","Sung-hee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.06963v1.pdf","comment":"published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024"},{"id":"http://arxiv.org/abs/2410.06940v1","updated":"2024-10-09T14:34:53Z","published":"2024-10-09T14:34:53Z","title":"Representation Alignment for Generation: Training Diffusion Transformers\n  Is Easier Than You Think","summary":"  Recent studies have shown that the denoising process in (generative)\ndiffusion models can induce meaningful (discriminative) representations inside\nthe model, though the quality of these representations still lags behind those\nlearned through recent self-supervised learning methods. We argue that one main\nbottleneck in training large-scale diffusion models for generation lies in\neffectively learning these representations. Moreover, training can be made\neasier by incorporating high-quality external visual representations, rather\nthan relying solely on the diffusion models to learn them independently. We\nstudy this by introducing a straightforward regularization called\nREPresentation Alignment (REPA), which aligns the projections of noisy input\nhidden states in denoising networks with clean image representations obtained\nfrom external, pretrained visual encoders. The results are striking: our simple\nstrategy yields significant improvements in both training efficiency and\ngeneration quality when applied to popular diffusion and flow-based\ntransformers, such as DiTs and SiTs. For instance, our method can speed up SiT\ntraining by over 17.5$\\times$, matching the performance (without\nclassifier-free guidance) of a SiT-XL model trained for 7M steps in less than\n400K steps. In terms of final generation quality, our approach achieves\nstate-of-the-art results of FID=1.42 using classifier-free guidance with the\nguidance interval.\n","authors":["Sihyun Yu","Sangkyung Kwak","Huiwon Jang","Jongheon Jeong","Jonathan Huang","Jinwoo Shin","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2410.06940v1.pdf","comment":"Preprint. Project page: https://sihyun.me/REPA"},{"id":"http://arxiv.org/abs/2405.12139v2","updated":"2024-10-09T14:13:00Z","published":"2024-05-20T16:01:01Z","title":"DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on\n  LLM","summary":"  Visual Language Tracking (VLT) enhances single object tracking (SOT) by\nintegrating natural language descriptions from a video, for the precise\ntracking of a specified object. By leveraging high-level semantic information,\nVLT guides object tracking, alleviating the constraints associated with relying\non a visual modality. Nevertheless, most VLT benchmarks are annotated in a\nsingle granularity and lack a coherent semantic framework to provide scientific\nguidance. Moreover, coordinating human annotators for high-quality annotations\nis laborious and time-consuming. To address these challenges, we introduce\nDTLLM-VLT, which automatically generates extensive and multi-granularity text\nto enhance environmental diversity. (1) DTLLM-VLT generates scientific and\nmulti-granularity text descriptions using a cohesive prompt framework. Its\nsuccinct and highly adaptable design allows seamless integration into various\nvisual tracking benchmarks. (2) We select three prominent benchmarks to deploy\nour approach: short-term tracking, long-term tracking, and global instance\ntracking. We offer four granularity combinations for these benchmarks,\nconsidering the extent and density of semantic information, thereby showcasing\nthe practicality and versatility of DTLLM-VLT. (3) We conduct comparative\nexperiments on VLT benchmarks with different text granularities, evaluating and\nanalyzing the impact of diverse text on tracking performance. Conclusionally,\nthis work leverages LLM to provide multi-granularity semantic information for\nVLT task from efficient and diverse perspectives, enabling fine-grained\nevaluation of multi-modal trackers. In the future, we believe this work can be\nextended to more datasets to support vision datasets understanding.\n","authors":["Xuchen Li","Xiaokun Feng","Shiyu Hu","Meiqi Wu","Dailing Zhang","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2405.12139v2.pdf","comment":"Accepted by CVPR Workshop 2024, Oral Presentation, Best Paper\n  Honorable Mention Award"},{"id":"http://arxiv.org/abs/2410.06912v1","updated":"2024-10-09T14:12:50Z","published":"2024-10-09T14:12:50Z","title":"Compositional Entailment Learning for Hyperbolic Vision-Language Models","summary":"  Image-text representation learning forms a cornerstone in vision-language\nmodels, where pairs of images and textual descriptions are contrastively\naligned in a shared embedding space. Since visual and textual concepts are\nnaturally hierarchical, recent work has shown that hyperbolic space can serve\nas a high-potential manifold to learn vision-language representation with\nstrong downstream performance. In this work, for the first time we show how to\nfully leverage the innate hierarchical nature of hyperbolic embeddings by\nlooking beyond individual image-text pairs. We propose Compositional Entailment\nLearning for hyperbolic vision-language models. The idea is that an image is\nnot only described by a sentence but is itself a composition of multiple object\nboxes, each with their own textual description. Such information can be\nobtained freely by extracting nouns from sentences and using openly available\nlocalized grounding models. We show how to hierarchically organize images,\nimage boxes, and their textual descriptions through contrastive and\nentailment-based objectives. Empirical evaluation on a hyperbolic\nvision-language model trained with millions of image-text pairs shows that the\nproposed compositional learning approach outperforms conventional Euclidean\nCLIP learning, as well as recent hyperbolic alternatives, with better zero-shot\nand retrieval generalization and clearly stronger hierarchical performance.\n","authors":["Avik Pal","Max van Spengler","Guido Maria D'Amely di Melendugno","Alessandro Flaborea","Fabio Galasso","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2410.06912v1.pdf","comment":"23 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.06905v1","updated":"2024-10-09T14:08:39Z","published":"2024-10-09T14:08:39Z","title":"Reliable Probabilistic Human Trajectory Prediction for Autonomous\n  Applications","summary":"  Autonomous systems, like vehicles or robots, require reliable, accurate,\nfast, resource-efficient, scalable, and low-latency trajectory predictions to\nget initial knowledge about future locations and movements of surrounding\nobjects for safe human-machine interaction. Furthermore, they need to know the\nuncertainty of the predictions for risk assessment to provide safe path\nplanning. This paper presents a lightweight method to address these\nrequirements, combining Long Short-Term Memory and Mixture Density Networks.\nOur method predicts probability distributions, including confidence level\nestimations for positional uncertainty to support subsequent risk management\napplications and runs on a low-power embedded platform. We discuss essential\nrequirements for human trajectory prediction in autonomous vehicle applications\nand demonstrate our method's performance using multiple traffic-related\ndatasets. Furthermore, we explain reliability and sharpness metrics and show\nhow important they are to guarantee the correctness and robustness of a model's\npredictions and uncertainty assessments. These essential evaluations have so\nfar received little attention for no good reason. Our approach focuses entirely\non real-world applicability. Verifying prediction uncertainties and a model's\nreliability are central to autonomous real-world applications. Our framework\nand code are available at:\nhttps://github.com/kav-institute/mdn_trajectory_forecasting.\n","authors":["Manuel Hetzel","Hannes Reichert","Konrad Doll","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2410.06905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02492v2","updated":"2024-10-09T14:07:15Z","published":"2024-10-03T13:57:07Z","title":"DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM","summary":"  Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.02492v2.pdf","comment":"Preprint, Under Review"},{"id":"http://arxiv.org/abs/2404.01053v2","updated":"2024-10-09T14:00:51Z","published":"2024-04-01T11:23:38Z","title":"HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior","summary":"  We present HAHA - a novel approach for animatable human avatar generation\nfrom monocular input videos. The proposed method relies on learning the\ntrade-off between the use of Gaussian splatting and a textured mesh for\nefficient and high fidelity rendering. We demonstrate its efficiency to animate\nand render full-body human avatars controlled via the SMPL-X parametric model.\nOur model learns to apply Gaussian splatting only in areas of the SMPL-X mesh\nwhere it is necessary, like hair and out-of-mesh clothing. This results in a\nminimal number of Gaussians being used to represent the full avatar, and\nreduced rendering artifacts. This allows us to handle the animation of small\nbody parts such as fingers that are traditionally disregarded. We demonstrate\nthe effectiveness of our approach on two open datasets: SnapshotPeople and\nX-Humans. Our method demonstrates on par reconstruction quality to the\nstate-of-the-art on SnapshotPeople, while using less than a third of Gaussians.\nHAHA outperforms previous state-of-the-art on novel poses from X-Humans both\nquantitatively and qualitatively.\n","authors":["David Svitov","Pietro Morerio","Lourdes Agapito","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2404.01053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06893v1","updated":"2024-10-09T13:57:39Z","published":"2024-10-09T13:57:39Z","title":"Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR\n  Semantic Segmentation","summary":"  We address the challenges of the semi-supervised LiDAR segmentation (SSLS)\nproblem, particularly in low-budget scenarios. The two main issues in\nlow-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the\nperformance drops due to the significant imbalance between ground-truth and\npseudo-labels. This imbalance leads to a vicious training cycle. To overcome\nthese challenges, we leverage the spatio-temporal prior by recognizing the\nsubstantial overlap between temporally adjacent LiDAR scans. We propose a\nproximity-based label estimation, which generates highly accurate pseudo-labels\nfor unlabeled data by utilizing semantic consistency with adjacent labeled\ndata. Additionally, we enhance this method by progressively expanding the\npseudo-labels from the nearest unlabeled scans, which helps significantly\nreduce errors linked to dynamic classes. Additionally, we employ a dual-branch\nstructure to mitigate performance degradation caused by data imbalance.\nExperimental results demonstrate remarkable performance in low-budget settings\n(i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 -\n50%). Finally, our method has achieved new state-of-the-art results on\nSemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5%\nlabeled data, it offers competitive results against fully-supervised\ncounterparts. Moreover, it surpasses the performance of the previous\nstate-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data\n(76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.\n","authors":["Seungho Lee","Hwijeong Lee","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2410.06893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08716v2","updated":"2024-10-09T13:56:08Z","published":"2023-07-16T10:07:15Z","title":"Enforcing 3D Topological Constraints in Composite Objects via Implicit\n  Functions","summary":"  Medical applications often require accurate 3D representations of complex\norgans with multiple parts, such as the heart and spine. Their individual parts\nmust adhere to specific topological constraints to ensure proper functionality.\nYet, there are very few mechanisms in the deep learning literature to achieve\nthis goal.\n  This paper introduces a novel approach to enforce topological constraints in\n3D object reconstruction using deep implicit signed distance functions. Our\nmethod focuses on heart and spine reconstruction but is generalizable to other\napplications. We propose a sampling-based technique that effectively checks and\nenforces topological constraints between 3D shapes by evaluating signed\ndistances at randomly sampled points throughout the volume. We demonstrate it\nby refining 3D segmentations obtained from the nn-UNet architecture.\n","authors":["Hieu Le","Jingyi Xu","Nicolas Talabot","Jiancheng Yang","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2307.08716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06892v1","updated":"2024-10-09T13:55:52Z","published":"2024-10-09T13:55:52Z","title":"Selecting the Best Sequential Transfer Path for Medical Image\n  Segmentation with Limited Labeled Data","summary":"  The medical image processing field often encounters the critical issue of\nscarce annotated data. Transfer learning has emerged as a solution, yet how to\nselect an adequate source task and effectively transfer the knowledge to the\ntarget task remains challenging. To address this, we propose a novel sequential\ntransfer scheme with a task affinity metric tailored for medical images.\nConsidering the characteristics of medical image segmentation tasks, we analyze\nthe image and label similarity between tasks and compute the task affinity\nscores, which assess the relatedness among tasks. Based on this, we select\nappropriate source tasks and develop an effective sequential transfer strategy\nby incorporating intermediate source tasks to gradually narrow the domain\ndiscrepancy and minimize the transfer cost. Thereby we identify the best\nsequential transfer path for the given target task. Extensive experiments on\nthree MRI medical datasets, FeTS 2022, iSeg-2019, and WMH, demonstrate the\nefficacy of our method in finding the best source sequence. Compared with\ndirectly transferring from a single source task, the sequential transfer\nresults underline a significant improvement in target task performance,\nachieving an average of 2.58% gain in terms of segmentation Dice score,\nnotably, 6.00% for FeTS 2022. Code is available at the git repository.\n","authors":["Jingyun Yang","Jingge Wang","Guoqing Zhang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2410.06892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07351v2","updated":"2024-10-09T13:55:01Z","published":"2024-09-11T15:37:52Z","title":"Federated Impression for Learning with Distributed Heterogeneous Data","summary":"  Standard deep learning-based classification approaches may not always be\npractical in real-world clinical applications, as they require a centralized\ncollection of all samples. Federated learning (FL) provides a paradigm that can\nlearn from distributed datasets across clients without requiring them to share\ndata, which can help mitigate privacy and data ownership issues. In FL,\nsub-optimal convergence caused by data heterogeneity is common among data from\ndifferent health centers due to the variety in data collection protocols and\npatient demographics across centers. Through experimentation in this study, we\nshow that data heterogeneity leads to the phenomenon of catastrophic forgetting\nduring local training. We propose FedImpres which alleviates catastrophic\nforgetting by restoring synthetic data that represents the global information\nas federated impression. To achieve this, we distill the global model resulting\nfrom each communication round. Subsequently, we use the synthetic data\nalongside the local data to enhance the generalization of local training.\nExtensive experiments show that the proposed method achieves state-of-the-art\nperformance on both the BloodMNIST and Retina datasets, which contain label\nimbalance and domain shift, with an improvement in classification accuracy of\nup to 20%.\n","authors":["Atrin Arya","Sana Ayromlou","Armin Saadat","Purang Abolmaesumi","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2409.07351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06879v1","updated":"2024-10-09T13:43:34Z","published":"2024-10-09T13:43:34Z","title":"Evaluating Model Performance with Hard-Swish Activation Function\n  Adjustments","summary":"  In the field of pattern recognition, achieving high accuracy is essential.\nWhile training a model to recognize different complex images, it is vital to\nfine-tune the model to achieve the highest accuracy possible. One strategy for\nfine-tuning a model involves changing its activation function. Most pre-trained\nmodels use ReLU as their default activation function, but switching to a\ndifferent activation function like Hard-Swish could be beneficial. This study\nevaluates the performance of models using ReLU, Swish and Hard-Swish activation\nfunctions across diverse image datasets. Our results show a 2.06% increase in\naccuracy for models on the CIFAR-10 dataset and a 0.30% increase in accuracy\nfor models on the ATLAS dataset. Modifying the activation functions in\narchitecture of pre-trained models lead to improved overall accuracy.\n","authors":["Sai Abhinav Pydimarry","Shekhar Madhav Khairnar","Sofia Garces Palacios","Ganesh Sankaranarayanan","Darian Hoagland","Dmitry Nepomnayshy","Huu Phong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.06879v1.pdf","comment":"2 pages"},{"id":"http://arxiv.org/abs/2410.06866v1","updated":"2024-10-09T13:27:06Z","published":"2024-10-09T13:27:06Z","title":"Secure Video Quality Assessment Resisting Adversarial Attacks","summary":"  The exponential surge in video traffic has intensified the imperative for\nVideo Quality Assessment (VQA). Leveraging cutting-edge architectures, current\nVQA models have achieved human-comparable accuracy. However, recent studies\nhave revealed the vulnerability of existing VQA models against adversarial\nattacks. To establish a reliable and practical assessment system, a secure VQA\nmodel capable of resisting such malicious attacks is urgently demanded.\nUnfortunately, no attempt has been made to explore this issue. This paper first\nattempts to investigate general adversarial defense principles, aiming at\nendowing existing VQA models with security. Specifically, we first introduce\nrandom spatial grid sampling on the video frame for intra-frame defense. Then,\nwe design pixel-wise randomization through a guardian map, globally\nneutralizing adversarial perturbations. Meanwhile, we extract temporal\ninformation from the video sequence as compensation for inter-frame defense.\nBuilding upon these principles, we present a novel VQA framework from the\nsecurity-oriented perspective, termed SecureVQA. Extensive experiments indicate\nthat SecureVQA sets a new benchmark in security while achieving competitive VQA\nperformance compared with state-of-the-art models. Ablation studies delve\ndeeper into analyzing the principles of SecureVQA, demonstrating their\ngeneralization and contributions to the security of leading VQA models.\n","authors":["Ao-Xiang Zhang","Yu Ran","Weixuan Tang","Yuan-Gen Wang","Qingxiao Guan","Chunsheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.06866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14752v4","updated":"2024-10-09T13:25:56Z","published":"2023-06-26T15:09:02Z","title":"MedLSAM: Localize and Segment Anything Model for 3D CT Images","summary":"  Recent advancements in foundation models have shown significant potential in\nmedical image analysis. However, there is still a gap in models specifically\ndesigned for medical image localization. To address this, we introduce MedLAM,\na 3D medical foundation localization model that accurately identifies any\nanatomical part within the body using only a few template scans. MedLAM employs\ntwo self-supervision tasks: unified anatomical mapping (UAM) and multi-scale\nsimilarity (MSS) across a comprehensive dataset of 14,012 CT scans.\nFurthermore, we developed MedLSAM by integrating MedLAM with the Segment\nAnything Model (SAM). This innovative framework requires extreme point\nannotations across three directions on several templates to enable MedLAM to\nlocate the target anatomical structure in the image, with SAM performing the\nsegmentation. It significantly reduces the amount of manual annotation required\nby SAM in 3D medical imaging scenarios. We conducted extensive experiments on\ntwo 3D datasets covering 38 distinct organs. Our findings are twofold: 1)\nMedLAM can directly localize anatomical structures using just a few template\nscans, achieving performance comparable to fully supervised models; 2) MedLSAM\nclosely matches the performance of SAM and its specialized medical adaptations\nwith manual prompts, while minimizing the need for extensive point annotations\nacross the entire dataset. Moreover, MedLAM has the potential to be seamlessly\nintegrated with future 3D SAM models, paving the way for enhanced segmentation\nperformance. Our code is public at \\href{https://github.com/openmedlab/MedLSAM}\n","authors":["Wenhui Lei","Xu Wei","Xiaofan Zhang","Kang Li","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.14752v4.pdf","comment":"MIA 2024. Code is public at https://github.com/openmedlab/MedLSAM"},{"id":"http://arxiv.org/abs/2410.02592v2","updated":"2024-10-09T13:13:59Z","published":"2024-10-03T15:34:41Z","title":"IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of\n  Both Driver and Passengers","summary":"  Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.\n","authors":["Zihan Fang","Zheng Lin","Senkang Hu","Hangcheng Cao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.02592v2.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.06842v1","updated":"2024-10-09T13:02:50Z","published":"2024-10-09T13:02:50Z","title":"SurANet: Surrounding-Aware Network for Concealed Object Detection via\n  Highly-Efficient Interactive Contrastive Learning Strategy","summary":"  Concealed object detection (COD) in cluttered scenes is significant for\nvarious image processing applications. However, due to that concealed objects\nare always similar to their background, it is extremely hard to distinguish\nthem. Here, the major obstacle is the tiny feature differences between the\ninside and outside object boundary region, which makes it trouble for existing\nCOD methods to achieve accurate results. In this paper, considering that the\nsurrounding environment information can be well utilized to identify the\nconcealed objects, and thus, we propose a novel deep Surrounding-Aware Network,\nnamely SurANet, for COD tasks, which introduces surrounding information into\nfeature extraction and loss function to improve the discrimination. First, we\nenhance the semantics of feature maps using differential fusion of surrounding\nfeatures to highlight concealed objects. Next, a Surrounding-Aware Contrastive\nLoss is applied to identify the concealed object via learning surrounding\nfeature maps contrastively. Then, SurANet can be trained end-to-end with high\nefficiency via our proposed Spatial-Compressed Correlation Transmission\nstrategy after our investigation of feature dynamics, and extensive experiments\nimprove that such features can be well reserved respectively. Finally,\nexperimental results demonstrate that the proposed SurANet outperforms\nstate-of-the-art COD methods on multiple real datasets. Our source code will be\navailable at https://github.com/kyh433/SurANet.\n","authors":["Yuhan Kang","Qingpeng Li","Leyuan Fang","Jian Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2410.06842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12239v3","updated":"2024-10-09T12:59:51Z","published":"2024-07-17T01:11:20Z","title":"Motion and Structure from Event-based Normal Flow","summary":"  Recovering the camera motion and scene geometry from visual data is a\nfundamental problem in the field of computer vision. Its success in standard\nvision is attributed to the maturity of feature extraction, data association\nand multi-view geometry. The recent emergence of neuromorphic event-based\ncameras places great demands on approaches that use raw event data as input to\nsolve this fundamental problem. Existing state-of-the-art solutions typically\ninfer implicitly data association by iteratively reversing the event data\ngeneration process. However, the nonlinear nature of these methods limits their\napplicability in real-time tasks, and the constant-motion assumption leads to\nunstable results under agile motion. To this end, we rethink the problem\nformulation in a way that aligns better with the differential working principle\nof event cameras. We show that the event-based normal flow can be used, via the\nproposed geometric error term, as an alternative to the full flow in solving a\nfamily of geometric problems that involve instantaneous first-order kinematics\nand scene geometry. Furthermore, we develop a fast linear solver and a\ncontinuous-time nonlinear solver on top of the proposed geometric error term.\nExperiments on both synthetic and real data show the superiority of our linear\nsolver in terms of accuracy and efficiency, and indicate its complementary\nfeature as an initialization method for existing nonlinear solvers. Besides,\nour continuous-time non-linear solver exhibits exceptional capability in\naccommodating sudden variations in motion since it does not rely on the\nconstant-motion assumption.\n","authors":["Zhongyang Ren","Bangyan Liao","Delei Kong","Jinghang Li","Peidong Liu","Laurent Kneip","Guillermo Gallego","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.12239v3.pdf","comment":"This paper has been accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2401.01984v4","updated":"2024-10-09T12:58:55Z","published":"2024-01-03T21:24:44Z","title":"AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed\n  and Low Tolerance","summary":"  Recent advances in visual anomaly detection research have seen AUROC and\nAUPRO scores on public benchmark datasets such as MVTec and VisA converge\ntowards perfect recall, giving the impression that these benchmarks are\nnear-solved. However, high AUROC and AUPRO scores do not always reflect\nqualitative performance, which limits the validity of these metrics in\nreal-world applications. We argue that the artificial ceiling imposed by the\nlack of an adequate evaluation metric restrains progression of the field, and\nit is crucial that we revisit the evaluation metrics used to rate our\nalgorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric\nthat addresses the shortcomings of AUROC and AUPRO. PIMO retains the\nrecall-based nature of the existing metrics but introduces two distinctions:\nthe assignment of curves (and respective area under the curve) is per-image,\nand its X-axis relies solely on normal images. Measuring recall per image\nsimplifies instance score indexing and is more robust to noisy annotations. As\nwe show, it also accelerates computation and enables the usage of statistical\ntests to compare models. By imposing low tolerance for false positives on\nnormal images, PIMO provides an enhanced model validation procedure and\nhighlights performance variations across datasets. Our experiments demonstrate\nthat PIMO offers practical advantages and nuanced performance insights that\nredefine anomaly detection benchmarks -- notably challenging the perception\nthat MVTec AD and VisA datasets have been solved by contemporary models.\nAvailable on GitHub: https://github.com/jpcbertoldo/aupimo.\n","authors":["Joao P. C. Bertoldo","Dick Ameln","Ashwin Vaidya","Samet Akay"],"pdf_url":"https://arxiv.org/pdf/2401.01984v4.pdf","comment":"Accepted to BMVC 2024. Official implementation\n  https://github.com/jpcbertoldo/aupimo and integrated in anomalib\n  https://github.com/openvinotoolkit/anomalib This research has been conducted\n  during Google Summer of Code 2023 (GSoC 2023) at OpenVINO (Intel)"},{"id":"http://arxiv.org/abs/2410.06841v1","updated":"2024-10-09T12:57:45Z","published":"2024-10-09T12:57:45Z","title":"Boosting Few-Shot Detection with Large Language Models and\n  Layout-to-Image Synthesis","summary":"  Recent advancements in diffusion models have enabled a wide range of works\nexploiting their ability to generate high-volume, high-quality data for use in\nvarious downstream tasks. One subclass of such models, dubbed Layout-to-Image\nSynthesis (LIS), learns to generate images conditioned on a spatial layout\n(bounding boxes, masks, poses, etc.) and has shown a promising ability to\ngenerate realistic images, albeit with limited layout-adherence. Moreover, the\nquestion of how to effectively transfer those models for scalable augmentation\nof few-shot detection data remains unanswered. Thus, we propose a collaborative\nframework employing a Large Language Model (LLM) and an LIS model for enhancing\nfew-shot detection beyond state-of-the-art generative augmentation approaches.\nWe leverage LLM's reasoning ability to extrapolate the spatial prior of the\nannotation space by generating new bounding boxes given only a few example\nannotations. Additionally, we introduce our novel layout-aware CLIP score for\nsample ranking, enabling tight coupling between generated layouts and images.\nSignificant improvements on COCO few-shot benchmarks are observed. With our\napproach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on\nthe COCO 5-,10-, and 30-shot settings, respectively.\n","authors":["Ahmed Abdullah","Nikolas Ebert","Oliver Wasenmller"],"pdf_url":"https://arxiv.org/pdf/2410.06841v1.pdf","comment":"This paper has been accepted at the Asian Conference on Computer\n  Vision (ACCV), 2024"},{"id":"http://arxiv.org/abs/2402.00525v3","updated":"2024-10-09T12:57:43Z","published":"2024-02-01T11:46:44Z","title":"StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time\n  Rendering","summary":"  Gaussian Splatting has emerged as a prominent model for constructing 3D\nrepresentations from images across diverse domains. However, the efficiency of\nthe 3D Gaussian Splatting rendering pipeline relies on several simplifications.\nNotably, reducing Gaussian to 2D splats with a single view-space depth\nintroduces popping and blending artifacts during view rotation. Addressing this\nissue requires accurate per-pixel depth computation, yet a full per-pixel sort\nproves excessively costly compared to a global sort operation. In this paper,\nwe present a novel hierarchical rasterization approach that systematically\nresorts and culls splats with minimal processing overhead. Our software\nrasterizer effectively eliminates popping artifacts and view inconsistencies,\nas demonstrated through both quantitative and qualitative measurements.\nSimultaneously, our method mitigates the potential for cheating view-dependent\neffects with popping, ensuring a more authentic representation. Despite the\nelimination of cheating, our approach achieves comparable quantitative results\nfor test images, while increasing the consistency for novel view synthesis in\nmotion. Due to its design, our hierarchical approach is only 4% slower on\naverage than the original Gaussian Splatting. Notably, enforcing consistency\nenables a reduction in the number of Gaussians by approximately half with\nnearly identical quality and view-consistency. Consequently, rendering\nperformance is nearly doubled, making our approach 1.6x faster than the\noriginal Gaussian Splatting, with a 50% reduction in memory requirements.\n","authors":["Lukas Radl","Michael Steiner","Mathias Parger","Alexander Weinrauch","Bernhard Kerbl","Markus Steinberger"],"pdf_url":"https://arxiv.org/pdf/2402.00525v3.pdf","comment":"SIGGRAPH 2024 (Journal Track); Project Page:\n  https://r4dl.github.io/StopThePop/"},{"id":"http://arxiv.org/abs/2409.14090v2","updated":"2024-10-09T12:47:39Z","published":"2024-09-21T10:08:52Z","title":"Window-based Channel Attention for Wavelet-enhanced Learned Image\n  Compression","summary":"  Learned Image Compression (LIC) models have achieved superior rate-distortion\nperformance than traditional codecs. Existing LIC models use CNN, Transformer,\nor Mixed CNN-Transformer as basic blocks. However, limited by the shifted\nwindow attention, Swin-Transformer-based LIC exhibits a restricted growth of\nreceptive fields, affecting the ability to model large objects for image\ncompression. To address this issue and improve the performance, we incorporate\nwindow partition into channel attention for the first time to obtain large\nreceptive fields and capture more global information. Since channel attention\nhinders local information learning, it is important to extend existing\nattention mechanisms in Transformer codecs to the space-channel attention to\nestablish multiple receptive fields, being able to capture global correlations\nwith large receptive fields while maintaining detailed characterization of\nlocal correlations with small receptive fields. We also incorporate the\ndiscrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for\nefficient frequency-dependent down-sampling and further enlarging receptive\nfields. Experiment results demonstrate that our method achieves\nstate-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and\n24.71% on four standard datasets compared to VTM-23.1.\n","authors":["Heng Xu","Bowen Hai","Yushun Tang","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2409.14090v2.pdf","comment":"ACCV2024 accepted; camera-ready version"},{"id":"http://arxiv.org/abs/2410.06818v1","updated":"2024-10-09T12:19:58Z","published":"2024-10-09T12:19:58Z","title":"An Improved Approach for Cardiac MRI Segmentation based on 3D UNet\n  Combined with Papillary Muscle Exclusion","summary":"  Left ventricular ejection fraction (LVEF) is the most important clinical\nparameter of cardiovascular function. The accuracy in estimating this parameter\nis highly dependent upon the precise segmentation of the left ventricle (LV)\nstructure at the end diastole and systole phases. Therefore, it is crucial to\ndevelop robust algorithms for the precise segmentation of the heart structure\nduring different phases. Methodology: In this work, an improved 3D UNet model\nis introduced to segment the myocardium and LV, while excluding papillary\nmuscles, as per the recommendation of the Society for Cardiovascular Magnetic\nResonance. For the practical testing of the proposed framework, a total of\n8,400 cardiac MRI images were collected and analysed from the military hospital\nin Tunis (HMPIT), as well as the popular ACDC public dataset. As performance\nmetrics, we used the Dice coefficient and the F1 score for validation/testing\nof the LV and the myocardium segmentation. Results: The data was split into\n70%, 10%, and 20% for training, validation, and testing, respectively. It is\nworth noting that the proposed segmentation model was tested across three axis\nviews: basal, medio basal and apical at two different cardiac phases: end\ndiastole and end systole instances. The experimental results showed a Dice\nindex of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end\ndiastolic and systolic phases, respectively. Additionally, clinical evaluation\noutcomes revealed a significant difference in the LVEF and other clinical\nparameters when the papillary muscles were included or excluded.\n","authors":["Narjes Benameur","Ramzi Mahmoudi","Mohamed Deriche","Amira fayouka","Imene Masmoudi","Nessrine Zoghlami"],"pdf_url":"https://arxiv.org/pdf/2410.06818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06811v1","updated":"2024-10-09T12:12:08Z","published":"2024-10-09T12:12:08Z","title":"Rethinking the Evaluation of Visible and Infrared Image Fusion","summary":"  Visible and Infrared Image Fusion (VIF) has garnered significant interest\nacross a wide range of high-level vision tasks, such as object detection and\nsemantic segmentation. However, the evaluation of VIF methods remains\nchallenging due to the absence of ground truth. This paper proposes a\nSegmentation-oriented Evaluation Approach (SEA) to assess VIF methods by\nincorporating the semantic segmentation task and leveraging segmentation labels\navailable in latest VIF datasets. Specifically, SEA utilizes universal\nsegmentation models, capable of handling diverse images and classes, to predict\nsegmentation outputs from fused images and compare these outputs with\nsegmentation labels. Our evaluation of recent VIF methods using SEA reveals\nthat their performance is comparable or even inferior to using visible images\nonly, despite nearly half of the infrared images demonstrating better\nperformance than visible images. Further analysis indicates that the two\nmetrics most correlated to our SEA are the gradient-based fusion metric\n$Q_{\\text{ABF}}$ and the visual information fidelity metric $Q_{\\text{VIFF}}$\nin conventional VIF evaluation metrics, which can serve as proxies when\nsegmentation labels are unavailable. We hope that our evaluation will guide the\ndevelopment of novel and practical VIF methods. The code has been released in\n\\url{https://github.com/Yixuan-2002/SEA/}.\n","authors":["Dayan Guan","Yixuan Wu","Tianzhu Liu","Alex C. Kot","Yanfeng Gu"],"pdf_url":"https://arxiv.org/pdf/2410.06811v1.pdf","comment":"The code has been released in\n  \\url{https://github.com/Yixuan-2002/SEA/}"},{"id":"http://arxiv.org/abs/2410.06806v1","updated":"2024-10-09T12:03:50Z","published":"2024-10-09T12:03:50Z","title":"QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space\n  Model","summary":"  Recent advancements in State Space Models, notably Mamba, have demonstrated\nsuperior performance over the dominant Transformer models, particularly in\nreducing the computational complexity from quadratic to linear. Yet,\ndifficulties in adapting Mamba from language to vision tasks arise due to the\ndistinct characteristics of visual data, such as the spatial locality and\nadjacency within images and large variations in information granularity across\nvisual tokens. Existing vision Mamba approaches either flatten tokens into\nsequences in a raster scan fashion, which breaks the local adjacency of images,\nor manually partition tokens into windows, which limits their long-range\nmodeling and generalization capabilities. To address these limitations, we\npresent a new vision Mamba model, coined QuadMamba, that effectively captures\nlocal dependencies of varying granularities via quadtree-based image partition\nand scan. Concretely, our lightweight quadtree-based scan module learns to\npreserve the 2D locality of spatial regions within learned window quadrants.\nThe module estimates the locality score of each token from their features,\nbefore adaptively partitioning tokens into window quadrants. An omnidirectional\nwindow shifting scheme is also introduced to capture more intact and\ninformative features across different local regions. To make the discretized\nquadtree partition end-to-end trainable, we further devise a sequence masking\nstrategy based on Gumbel-Softmax and its straight-through gradient estimator.\nExtensive experiments demonstrate that QuadMamba achieves state-of-the-art\nperformance in various vision tasks, including image classification, object\ndetection, instance segmentation, and semantic segmentation. The code is in\nhttps://github.com/VISIONSJTU/QuadMamba.\n","authors":["Fei Xie","Weijia Zhang","Zhongdao Wang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.06806v1.pdf","comment":"Accepted by Neurip2024"},{"id":"http://arxiv.org/abs/2406.05127v3","updated":"2024-10-09T12:01:24Z","published":"2024-06-07T17:55:43Z","title":"Towards Semantic Equivalence of Tokenization in Multimodal LLM","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n","authors":["Shengqiong Wu","Hao Fei","Xiangtai Li","Jiayi Ji","Hanwang Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.05127v3.pdf","comment":"Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/"},{"id":"http://arxiv.org/abs/2410.06795v1","updated":"2024-10-09T11:46:32Z","published":"2024-10-09T11:46:32Z","title":"From Pixels to Tokens: Revisiting Object Hallucinations in Large\n  Vision-Language Models","summary":"  Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.\n","authors":["Yuying Shang","Xinyi Zeng","Yutao Zhu","Xiao Yang","Zhengwei Fang","Jingyuan Zhang","Jiawei Chen","Zinan Liu","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16204v3","updated":"2024-10-09T11:39:44Z","published":"2023-12-23T11:10:43Z","title":"Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image\n  Diffusion Model Training","summary":"  Diffusion models have shown impressive performance in many domains. However,\nthe model's capability to follow natural language instructions (e.g., spatial\nrelationships between objects, generating complex scenes) is still\nunsatisfactory. In this work, we propose Iterative Prompt Relabeling (IPR), a\nnovel algorithm that aligns images to text through iterative image sampling and\nprompt relabeling with feedback. IPR first samples a batch of images\nconditioned on the text, then relabels the text prompts of unmatched text-image\npairs with classifier feedback. We conduct thorough experiments on SDv2 and\nSDXL, testing their capability to follow instructions on spatial relations.\nWith IPR, we improved up to 15.22% (absolute improvement) on the challenging\nspatial relation VISOR benchmark, demonstrating superior performance compared\nto previous RL methods. Our code is publicly available at\nhttps://github.com/cxy000000/IPR-RLDF.\n","authors":["Xinyan Chen","Jiaxin Ge","Tianjun Zhang","Jiaming Liu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.16204v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07777v2","updated":"2024-10-09T11:29:10Z","published":"2024-05-13T14:21:54Z","title":"GMSR:Gradient-Guided Mamba for Spectral Reconstruction from RGB Images","summary":"  Mainstream approaches to spectral reconstruction (SR) primarily focus on\ndesigning Convolution- and Transformer-based architectures. However, CNN\nmethods often face challenges in handling long-range dependencies, whereas\nTransformers are constrained by computational efficiency limitations. Recent\nbreakthroughs in state-space model (e.g., Mamba) has attracted significant\nattention due to its near-linear computational efficiency and superior\nperformance, prompting our investigation into its potential for SR problem. To\nthis end, we propose the Gradient-guided Mamba for Spectral Reconstruction from\nRGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by a\nglobal receptive field and linear computational complexity. Its core comprises\nmultiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch\nstructure. In addition to benefiting from efficient global feature\nrepresentation by Mamba block, we further innovatively introduce spatial\ngradient attention and spectral gradient attention to guide the reconstruction\nof spatial and spectral cues. GMSR-Net demonstrates a significant\naccuracy-efficiency trade-off, achieving state-of-the-art performance while\nmarkedly reducing the number of parameters and computational burdens. Compared\nto existing approaches, GMSR-Net slashes parameters and FLOPS by substantial\nmargins of 10 times and 20 times, respectively. Code is available at\nhttps://github.com/wxy11-27/GMSR.\n","authors":["Xinying Wang","Zhixiong Huang","Sifan Zhang","Jiawen Zhu","Paolo Gamba","Lin Feng"],"pdf_url":"https://arxiv.org/pdf/2405.07777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06781v1","updated":"2024-10-09T11:20:28Z","published":"2024-10-09T11:20:28Z","title":"Transesophageal Echocardiography Generation using Anatomical Models","summary":"  Through automation, deep learning (DL) can enhance the analysis of\ntransesophageal echocardiography (TEE) images. However, DL methods require\nlarge amounts of high-quality data to produce accurate results, which is\ndifficult to satisfy. Data augmentation is commonly used to tackle this issue.\nIn this work, we develop a pipeline to generate synthetic TEE images and\ncorresponding semantic labels. The proposed data generation pipeline expands on\nan existing pipeline that generates synthetic transthoracic echocardiography\nimages by transforming slices from anatomical models into synthetic images. We\nalso demonstrate that such images can improve DL network performance through a\nleft-ventricle semantic segmentation task. For the pipeline's unpaired\nimage-to-image (I2I) translation section, we explore two generative methods:\nCycleGAN and contrastive unpaired translation. Next, we evaluate the synthetic\nimages quantitatively using the Fr\\'echet Inception Distance (FID) Score and\nqualitatively through a human perception quiz involving expert cardiologists\nand the average researcher.\n  In this study, we achieve a dice score improvement of up to 10% when we\naugment datasets with our synthetic images. Furthermore, we compare established\nmethods of assessing unpaired I2I translation and observe a disagreement when\nevaluating the synthetic images. Finally, we see which metric better predicts\nthe generated data's efficacy when used for data augmentation.\n","authors":["Emmanuel Oladokun","Musa Abdulkareem","Jurica prem","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2410.06781v1.pdf","comment":"MICCAI2023; DALI Workshop"},{"id":"http://arxiv.org/abs/2410.06777v1","updated":"2024-10-09T11:14:07Z","published":"2024-10-09T11:14:07Z","title":"HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric\n  Understanding","summary":"  The significant advancements in visual understanding and instruction\nfollowing from Multimodal Large Language Models (MLLMs) have opened up more\npossibilities for broader applications in diverse and universal human-centric\nscenarios. However, existing image-text data may not support the precise\nmodality alignment and integration of multi-grained information, which is\ncrucial for human-centric visual understanding. In this paper, we introduce\nHERM-Bench, a benchmark for evaluating the human-centric understanding\ncapabilities of MLLMs. Our work reveals the limitations of existing MLLMs in\nunderstanding complex human-centric scenarios. To address these challenges, we\npresent HERM-100K, a comprehensive dataset with multi-level human-centric\nannotations, aimed at enhancing MLLMs' training. Furthermore, we develop\nHERM-7B, a MLLM that leverages enhanced training data from HERM-100K.\nEvaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms\nexisting MLLMs across various human-centric dimensions, reflecting the current\ninadequacy of data annotations used in MLLM training for human-centric visual\nunderstanding. This research emphasizes the importance of specialized datasets\nand benchmarks in advancing the MLLMs' capabilities for human-centric\nunderstanding.\n","authors":["Keliang Li","Zaifei Yang","Jiahe Zhao","Hongze Shen","Ruibing Hou","Hong Chang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06765v1","updated":"2024-10-09T10:53:18Z","published":"2024-10-09T10:53:18Z","title":"To Preserve or To Compress: An In-Depth Study of Connector Selection in\n  Multimodal Large Language Models","summary":"  In recent years, multimodal large language models (MLLMs) have garnered\nsignificant attention from both industry and academia. However, there is still\nconsiderable debate on constructing MLLM architectures, particularly regarding\nthe selection of appropriate connectors for perception tasks of varying\ngranularities. This paper systematically investigates the impact of connectors\non MLLM performance. Specifically, we classify connectors into\nfeature-preserving and feature-compressing types. Utilizing a unified\nclassification standard, we categorize sub-tasks from three comprehensive\nbenchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained\nperception, fine-grained perception, and reasoning, and evaluate the\nperformance. Our findings reveal that feature-preserving connectors excel in\n\\emph{fine-grained perception} tasks due to their ability to retain detailed\nvisual information. In contrast, feature-compressing connectors, while less\neffective in fine-grained perception tasks, offer significant speed advantages\nand perform comparably in \\emph{coarse-grained perception} and \\emph{reasoning}\ntasks. These insights are crucial for guiding MLLM architecture design and\nadvancing the optimization of MLLM architectures.\n","authors":["Junyan Lin","Haoran Chen","Dawei Zhu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.06765v1.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.10344v3","updated":"2024-10-09T10:52:15Z","published":"2024-03-15T14:31:17Z","title":"SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric\n  hybrid solution","summary":"  Neural implicit surface representation methods have recently shown impressive\n3D reconstruction results. However, existing solutions struggle to reconstruct\nurban outdoor scenes due to their large, unbounded, and highly detailed nature.\nHence, to achieve accurate reconstructions, additional supervision data such as\nLiDAR, strong geometric priors, and long training times are required. To tackle\nsuch issues, we present SCILLA, a new hybrid implicit surface learning method\nto reconstruct large driving scenes from 2D images. SCILLA's hybrid\narchitecture models two separate implicit fields: one for the volumetric\ndensity and another for the signed distance to the surface. To accurately\nrepresent urban outdoor scenarios, we introduce a novel volume-rendering\nstrategy that relies on self-supervised probabilistic density estimation to\nsample points near the surface and transition progressively from volumetric to\nsurface representation. Our solution permits a proper and fast initialization\nof the signed distance field without relying on any geometric prior on the\nscene, compared to concurrent methods. By conducting extensive experiments on\nfour outdoor driving datasets, we show that SCILLA can learn an accurate and\ndetailed 3D surface scene representation in various urban scenarios while being\ntwo times faster to train compared to previous state-of-the-art solutions.\n","authors":["Hala Djeghim","Nathan Piasco","Moussab Bennehar","Luis Roldo","Dzmitry Tsishkou","Dsir Sidib"],"pdf_url":"https://arxiv.org/pdf/2403.10344v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17115v2","updated":"2024-10-09T10:43:47Z","published":"2024-06-24T20:08:07Z","title":"Evaluating the Quality of Hallucination Benchmarks for Large\n  Vision-Language Models","summary":"  Despite the rapid progress and outstanding performance of Large\nVision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the\nissue of hallucination, i.e., LVLMs tend to generate responses that are\ninconsistent with the corresponding visual inputs. To evaluate the degree of\nhallucination in LVLMs, previous works have proposed a series of benchmarks\nfeaturing different types of tasks and evaluation metrics. However, we find\nthat the quality of the existing hallucination benchmarks varies, with some\nsuffering from problems, e.g., inconsistent evaluation results under repeated\ntests, and misalignment with human evaluation. To this end, we propose a\nHallucination benchmark Quality Measurement framework (HQM), which leverages\nvarious indicators to assess the reliability and validity of existing\nhallucination benchmarks separately. Specifically, for reliability we explore\ntest-retest reliability and parallel-forms reliability, while for validity we\nexamine criterion validity and coverage of hallucination types. Furthermore,\nbased on the results of our quality measurement, we construct a High-Quality\nHallucination Benchmark (HQH) for LVLMs, which demonstrates superior\nreliability and validity under our HQM framework. We conduct an extensive\nevaluation of over 10 representative LVLMs, including GPT-4o and\nGemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in\nexisting models. Our benchmark is publicly available at\nhttps://github.com/HQHBench/HQHBench.\n","authors":["Bei Yan","Jie Zhang","Zheng Yuan","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06757v1","updated":"2024-10-09T10:41:31Z","published":"2024-10-09T10:41:31Z","title":"Diff-FMT: Diffusion Models for Fluorescence Molecular Tomography","summary":"  Fluorescence molecular tomography (FMT) is a real-time, noninvasive optical\nimaging technology that plays a significant role in biomedical research.\nNevertheless, the ill-posedness of the inverse problem poses huge challenges in\nFMT reconstructions. Previous various deep learning algorithms have been\nextensively explored to address the critical issues, but they remain faces the\nchallenge of high data dependency with poor image quality. In this paper, we,\nfor the first time, propose a FMT reconstruction method based on a denoising\ndiffusion probabilistic model (DDPM), termed Diff-FMT, which is capable of\nobtaining high-quality reconstructed images from noisy images. Specifically, we\nutilize the noise addition mechanism of DDPM to generate diverse training\nsamples. Through the step-by-step probability sampling mechanism in the inverse\nprocess, we achieve fine-grained reconstruction of the image, avoiding issues\nsuch as loss of image detail that can occur with end-to-end deep-learning\nmethods. Additionally, we introduce the fluorescence signals as conditional\ninformation in the model training to sample a reconstructed image that is\nhighly consistent with the input fluorescence signals from the noisy images.\nNumerous experimental results show that Diff-FMT can achieve high-resolution\nreconstruction images without relying on large-scale datasets compared with\nother cutting-edge algorithms.\n","authors":["Qianqian Xue","Peng Zhang","Xingyu Liu","Wenjian Wang","Guanglei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06756v1","updated":"2024-10-09T10:41:08Z","published":"2024-10-09T10:41:08Z","title":"DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh\n  Hybrid Representation","summary":"  Recent advancements in 2D/3D generative techniques have facilitated the\ngeneration of dynamic 3D objects from monocular videos. Previous methods mainly\nrely on the implicit neural radiance fields (NeRF) or explicit Gaussian\nSplatting as the underlying representation, and struggle to achieve\nsatisfactory spatial-temporal consistency and surface appearance. Drawing\ninspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a\nnovel framework combining mesh representation with geometric skinning technique\nto generate high-quality 4D object from a monocular video. Instead of utilizing\nclassical texture map for appearance, we bind Gaussian splats to triangle face\nof mesh for differentiable optimization of both the texture and mesh vertices.\nIn particular, DreamMesh4D begins with a coarse mesh obtained through an\nimage-to-3D generation procedure. Sparse points are then uniformly sampled\nacross the mesh surface, and are used to build a deformation graph to drive the\nmotion of the 3D object for the sake of computational efficiency and providing\nadditional constraint. For each step, transformations of sparse control points\nare predicted using a deformation network, and the mesh vertices as well as the\nsurface Gaussians are deformed via a novel geometric skinning algorithm, which\nis a hybrid approach combining LBS (linear blending skinning) and DQS\n(dual-quaternion skinning), mitigating drawbacks associated with both\napproaches. The static surface Gaussians and mesh vertices as well as the\ndeformation network are learned via reference view photometric loss, score\ndistillation loss as well as other regularizers in a two-stage manner.\nExtensive experiments demonstrate superior performance of our method.\nFurthermore, our method is compatible with modern graphic pipelines, showcasing\nits potential in the 3D gaming and film industry.\n","authors":["Zhiqi Li","Yiming Chen","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06756v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06743v1","updated":"2024-10-09T10:21:45Z","published":"2024-10-09T10:21:45Z","title":"Utilizing Transfer Learning and pre-trained Models for Effective Forest\n  Fire Detection: A Case Study of Uttarakhand","summary":"  Forest fires pose a significant threat to the environment, human life, and\nproperty. Early detection and response are crucial to mitigating the impact of\nthese disasters. However, traditional forest fire detection methods are often\nhindered by our reliability on manual observation and satellite imagery with\nlow spatial resolution. This paper emphasizes the role of transfer learning in\nenhancing forest fire detection in India, particularly in overcoming data\ncollection challenges and improving model accuracy across various regions. We\ncompare traditional learning methods with transfer learning, focusing on the\nunique challenges posed by regional differences in terrain, climate, and\nvegetation. Transfer learning can be categorized into several types based on\nthe similarity between the source and target tasks, as well as the type of\nknowledge transferred. One key method is utilizing pre-trained models for\nefficient transfer learning, which significantly reduces the need for extensive\nlabeled data. We outline the transfer learning process, demonstrating how\nresearchers can adapt pre-trained models like MobileNetV2 for specific tasks\nsuch as forest fire detection. Finally, we present experimental results from\ntraining and evaluating a deep learning model using the Uttarakhand forest fire\ndataset, showcasing the effectiveness of transfer learning in this context.\n","authors":["Hari Prabhat Gupta","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.06743v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.06734v1","updated":"2024-10-09T10:12:37Z","published":"2024-10-09T10:12:37Z","title":"MimicTalk: Mimicking a personalized and expressive 3D talking face in\n  minutes","summary":"  Talking face generation (TFG) aims to animate a target identity's face to\ncreate realistic talking videos. Personalized TFG is a variant that emphasizes\nthe perceptual identity similarity of the synthesized result (from the\nperspective of appearance and talking style). While previous works typically\nsolve this problem by learning an individual neural radiance field (NeRF) for\neach identity to implicitly store its static and dynamic information, we find\nit inefficient and non-generalized due to the per-identity-per-training\nframework and the limited training data. To this end, we propose MimicTalk, the\nfirst attempt that exploits the rich knowledge from a NeRF-based\nperson-agnostic generic model for improving the efficiency and robustness of\npersonalized TFG. To be specific, (1) we first come up with a person-agnostic\n3D TFG model as the base model and propose to adapt it into a specific\nidentity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help\nthe model learn the personalized static appearance and facial dynamic features;\n(3) To generate the facial motion of the personalized talking style, we propose\nan in-context stylized audio-to-motion model that mimics the implicit talking\nstyle provided in the reference video without information loss by an explicit\nstyle representation. The adaptation process to an unseen identity can be\nperformed in 15 minutes, which is 47 times faster than previous\nperson-dependent methods. Experiments show that our MimicTalk surpasses\nprevious baselines regarding video quality, efficiency, and expressiveness.\nSource code and video samples are available at https://mimictalk.github.io .\n","authors":["Zhenhui Ye","Tianyun Zhong","Yi Ren","Ziyue Jiang","Jiawei Huang","Rongjie Huang","Jinglin Liu","Jinzheng He","Chen Zhang","Zehan Wang","Xize Chen","Xiang Yin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.06734v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06733v1","updated":"2024-10-09T10:09:11Z","published":"2024-10-09T10:09:11Z","title":"Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with\n  Situation Puzzles","summary":"  While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking.\n","authors":["Qi Chen","Bowen Zhang","Gang Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.06733v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2311.04069v2","updated":"2024-10-09T10:08:29Z","published":"2023-11-07T15:35:17Z","title":"LISBET: a machine learning model for the automatic segmentation of\n  social behavior motifs","summary":"  Social behavior is crucial for survival in many animal species, and a heavily\ninvestigated research subject. Current analysis methods generally rely on\nmeasuring animal interaction time or annotating predefined behaviors. However,\nthese approaches are time consuming, human biased, and can fail to capture\nsubtle behaviors. Here we introduce LISBET (LISBET Is a Social BEhavior\nTransformer), a machine learning model for detecting and segmenting social\ninteractions. Using self-supervised learning on body tracking data, our model\neliminates the need for extensive human annotation. We tested LISBET in three\nscenarios across multiple datasets in mice: supervised behavior classification,\nunsupervised motifs segmentation, and unsupervised animal phenotyping.\nAdditionally, in vivo electrophysiology revealed distinct neural signatures in\nthe Ventral Tegmental Area corresponding to motifs identified by our model. In\nsummary, LISBET automates data annotation and reduces human bias in social\nbehavior research, offering a promising approach to enhance our understanding\nof behavior and its neural correlates.\n","authors":["Giuseppe Chindemi","Benoit Girard","Camilla Bellone"],"pdf_url":"https://arxiv.org/pdf/2311.04069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06725v1","updated":"2024-10-09T09:46:53Z","published":"2024-10-09T09:46:53Z","title":"Evaluating the Impact of Point Cloud Colorization on Semantic\n  Segmentation Accuracy","summary":"  Point cloud semantic segmentation, the process of classifying each point into\npredefined categories, is essential for 3D scene understanding. While\nimage-based segmentation is widely adopted due to its maturity, methods relying\nsolely on RGB information often suffer from degraded performance due to color\ninaccuracies. Recent advancements have incorporated additional features such as\nintensity and geometric information, yet RGB channels continue to negatively\nimpact segmentation accuracy when errors in colorization occur. Despite this,\nprevious studies have not rigorously quantified the effects of erroneous\ncolorization on segmentation performance. In this paper, we propose a novel\nstatistical approach to evaluate the impact of inaccurate RGB information on\nimage-based point cloud segmentation. We categorize RGB inaccuracies into two\ntypes: incorrect color information and similar color information. Our results\ndemonstrate that both types of color inaccuracies significantly degrade\nsegmentation accuracy, with similar color errors particularly affecting the\nextraction of geometric features. These findings highlight the critical need to\nreassess the role of RGB information in point cloud segmentation and its\nimplications for future algorithm design.\n","authors":["Qinfeng Zhu","Jiaze Cao","Yuanzhi Cai","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2410.06725v1.pdf","comment":"Accepted by 2024 IEEE 8th International Conference on Vision, Image\n  and Signal Processing"},{"id":"http://arxiv.org/abs/2407.08567v2","updated":"2024-10-09T09:46:22Z","published":"2024-07-11T14:57:27Z","title":"Adaptive Parametric Activation","summary":"  The activation function plays a crucial role in model optimisation, yet the\noptimal choice remains unclear. For example, the Sigmoid activation is the\nde-facto activation in balanced classification tasks, however, in imbalanced\nclassification, it proves inappropriate due to bias towards frequent classes.\nIn this work, we delve deeper in this phenomenon by performing a comprehensive\nstatistical analysis in the classification and intermediate layers of both\nbalanced and imbalanced networks and we empirically show that aligning the\nactivation function with the data distribution, enhances the performance in\nboth balanced and imbalanced tasks. To this end, we propose the Adaptive\nParametric Activation (APA) function, a novel and versatile activation function\nthat unifies most common activation functions under a single formula. APA can\nbe applied in both intermediate layers and attention layers, significantly\noutperforming the state-of-the-art on several imbalanced benchmarks such as\nImageNet-LT, iNaturalist2018, Places-LT, CIFAR100-LT and LVIS and balanced\nbenchmarks such as ImageNet1K, COCO and V3DET. The code is available at\nhttps://github.com/kostas1515/AGLU.\n","authors":["Konstantinos Panagiotis Alexandridis","Jiankang Deng","Anh Nguyen","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.08567v2.pdf","comment":"ECCV2024 Oral"},{"id":"http://arxiv.org/abs/2410.06723v1","updated":"2024-10-09T09:45:53Z","published":"2024-10-09T09:45:53Z","title":"Evaluating Computational Pathology Foundation Models for Prostate Cancer\n  Grading under Distribution Shifts","summary":"  Foundation models have recently become a popular research direction within\ncomputational pathology. They are intended to be general-purpose feature\nextractors, promising to achieve good performance on a range of downstream\ntasks. Real-world pathology image data does however exhibit considerable\nvariability. Foundation models should be robust to these variations and other\ndistribution shifts which might be encountered in practice. We evaluate two\ncomputational pathology foundation models: UNI (trained on more than 100,000\nwhole-slide images) and CONCH (trained on more than 1.1 million image-caption\npairs), by utilizing them as feature extractors within prostate cancer grading\nmodels. We find that while UNI and CONCH perform well relative to baselines,\nthe absolute performance can still be far from satisfactory in certain\nsettings. The fact that foundation models have been trained on large and varied\ndatasets does not guarantee that downstream models always will be robust to\ncommon distribution shifts.\n","authors":["Fredrik K. Gustafsson","Mattias Rantalainen"],"pdf_url":"https://arxiv.org/pdf/2410.06723v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2410.06719v1","updated":"2024-10-09T09:43:36Z","published":"2024-10-09T09:43:36Z","title":"Suppress Content Shift: Better Diffusion Features via Off-the-Shelf\n  Generation Techniques","summary":"  Diffusion models are powerful generative models, and this capability can also\nbe applied to discrimination. The inner activations of a pre-trained diffusion\nmodel can serve as features for discriminative tasks, namely, diffusion\nfeature. We discover that diffusion feature has been hindered by a hidden yet\nuniversal phenomenon that we call content shift. To be specific, there are\ncontent differences between features and the input image, such as the exact\nshape of a certain object. We locate the cause of content shift as one inherent\ncharacteristic of diffusion models, which suggests the broad existence of this\nphenomenon in diffusion feature. Further empirical study also indicates that\nits negative impact is not negligible even when content shift is not visually\nperceivable. Hence, we propose to suppress content shift to enhance the overall\nquality of diffusion features. Specifically, content shift is related to the\ninformation drift during the process of recovering an image from the noisy\ninput, pointing out the possibility of turning off-the-shelf generation\ntechniques into tools for content shift suppression. We further propose a\npractical guideline named GATE to efficiently evaluate the potential benefit of\na technique and provide an implementation of our methodology. Despite the\nsimplicity, the proposed approach has achieved superior results on various\ntasks and datasets, validating its potential as a generic booster for diffusion\nfeatures. Our code is available at\nhttps://github.com/Darkbblue/diffusion-content-shift.\n","authors":["Benyuan Meng","Qianqian Xu","Zitai Wang","Zhiyong Yang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06719v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2410.03558"},{"id":"http://arxiv.org/abs/2410.06718v1","updated":"2024-10-09T09:41:34Z","published":"2024-10-09T09:41:34Z","title":"MatMamba: A Matryoshka State Space Model","summary":"  State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}\n","authors":["Abhinav Shukla","Sai Vemprala","Aditya Kusupati","Ashish Kapoor"],"pdf_url":"https://arxiv.org/pdf/2410.06718v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06711v1","updated":"2024-10-09T09:33:48Z","published":"2024-10-09T09:33:48Z","title":"Analysis of different disparity estimation techniques on aerial stereo\n  image datasets","summary":"  With the advent of aerial image datasets, dense stereo matching has gained\ntremendous progress. This work analyses dense stereo correspondence analysis on\naerial images using different techniques. Traditional methods, optimization\nbased methods and learning based methods have been implemented and compared\nhere for aerial images. For traditional methods, we implemented the\narchitecture of Stereo SGBM while using different cost functions to get an\nunderstanding of their performance on aerial datasets. Analysis of most of the\nmethods in standard datasets has shown good performance, however in case of\naerial dataset, not much benchmarking is available. Visual qualitative and\nquantitative analysis has been carried out for two stereo aerial datasets in\norder to compare different cost functions and techniques for the purpose of\ndepth estimation from stereo images. Using existing pre-trained models, recent\nlearning based architectures have also been tested on stereo pairs along with\ndifferent cost functions in SGBM. The outputs and given ground truth are\ncompared using MSE, SSIM and other error metrics.\n","authors":["Ishan Narayan","Shashi Poddar"],"pdf_url":"https://arxiv.org/pdf/2410.06711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02483v2","updated":"2024-10-09T09:33:04Z","published":"2024-09-04T07:20:01Z","title":"TASAR: Transfer-based Attack on Skeletal Action Recognition","summary":"  Skeletal sequences, as well-structured representations of human behaviors,\nplay a vital role in Human Activity Recognition (HAR). The transferability of\nadversarial skeletal sequences enables attacks in real-world HAR scenarios,\nsuch as autonomous driving, intelligent surveillance, and human-computer\ninteractions. However, most existing skeleton-based HAR (S-HAR) attacks are\nprimarily designed for white-box scenarios and exhibit weak adversarial\ntransferability. Therefore, they cannot be considered true transfer-based S-HAR\nattacks. More importantly, the reason for this failure remains unclear. In this\npaper, we study this phenomenon through the lens of loss surface, and find that\nits sharpness contributes to the weak transferability in S-HAR. Inspired by\nthis observation, we assume and empirically validate that smoothening the\nrugged loss landscape could potentially improve adversarial transferability in\nS-HAR. To this end, we propose the first \\textbf{T}ransfer-based\n\\textbf{A}ttack on \\textbf{S}keletal \\textbf{A}ction \\textbf{R}ecognition,\nTASAR. TASAR explores the smoothed model posterior without requiring surrogate\nre-training, which is achieved by a new post-train Dual Bayesian optimization\nstrategy. Furthermore, unlike previous transfer-based attacks that treat each\nframe independently and overlook temporal coherence within sequences, TASAR\nincorporates motion dynamics into the Bayesian attack gradient, effectively\ndisrupting the spatial-temporal coherence of S-HARs. To exhaustively evaluate\nthe effectiveness of existing methods and our method, we build the first\nlarge-scale robust S-HAR benchmark, comprising 7 S-HAR models, 10 attack\nmethods, 3 S-HAR datasets and 2 defense methods. Extensive results demonstrate\nthe superiority of TASAR. Our benchmark enables easy comparisons for future\nstudies, with the code available in the supplementary material.\n","authors":["Yunfeng Diao","Baiqi Wu","Ruixuan Zhang","Ajian Liu","Xingxing Wei","Meng Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16655v2","updated":"2024-10-09T09:08:05Z","published":"2024-07-23T17:17:05Z","title":"MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence","summary":"  Recent advancements in video generation have primarily leveraged diffusion\nmodels for short-duration content. However, these approaches often fall short\nin modeling complex narratives and maintaining character consistency over\nextended periods, which is essential for long-form video production like\nmovies. We propose MovieDreamer, a novel hierarchical framework that integrates\nthe strengths of autoregressive models with diffusion-based rendering to\npioneer long-duration video generation with intricate plot progressions and\nhigh visual fidelity. Our approach utilizes autoregressive models for global\nnarrative coherence, predicting sequences of visual tokens that are\nsubsequently transformed into high-quality video frames through diffusion\nrendering. This method is akin to traditional movie production processes, where\ncomplex stories are factorized down into manageable scene capturing. Further,\nwe employ a multimodal script that enriches scene descriptions with detailed\ncharacter information and visual style, enhancing continuity and character\nidentity across scenes. We present extensive experiments across various movie\ngenres, demonstrating that our approach not only achieves superior visual and\nnarrative quality but also effectively extends the duration of generated\ncontent significantly beyond current capabilities. Homepage:\nhttps://aim-uofa.github.io/MovieDreamer/.\n","authors":["Canyu Zhao","Mingyu Liu","Wen Wang","Weihua Chen","Fan Wang","Hao Chen","Bo Zhang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2407.16655v2.pdf","comment":"30 pages, 22 figures"},{"id":"http://arxiv.org/abs/2410.06699v1","updated":"2024-10-09T09:06:56Z","published":"2024-10-09T09:06:56Z","title":"Break the Visual Perception: Adversarial Attacks Targeting Encoded\n  Visual Tokens of Large Vision-Language Models","summary":"  Large vision-language models (LVLMs) integrate visual information into large\nlanguage models, showcasing remarkable multi-modal conversational capabilities.\nHowever, the visual modules introduces new challenges in terms of robustness\nfor LVLMs, as attackers can craft adversarial images that are visually clean\nbut may mislead the model to generate incorrect answers. In general, LVLMs rely\non vision encoders to transform images into visual tokens, which are crucial\nfor the language models to perceive image contents effectively. Therefore, we\nare curious about one question: Can LVLMs still generate correct responses when\nthe encoded visual tokens are attacked and disrupting the visual information?\nTo this end, we propose a non-targeted attack method referred to as VT-Attack\n(Visual Tokens Attack), which constructs adversarial examples from multiple\nperspectives, with the goal of comprehensively disrupting feature\nrepresentations and inherent relationships as well as the semantic properties\nof visual tokens output by image encoders. Using only access to the image\nencoder in the proposed attack, the generated adversarial examples exhibit\ntransferability across diverse LVLMs utilizing the same image encoder and\ngenerality across different tasks. Extensive experiments validate the superior\nattack performance of the VT-Attack over baseline methods, demonstrating its\neffectiveness in attacking LVLMs with image encoders, which in turn can provide\nguidance on the robustness of LVLMs, particularly in terms of the stability of\nthe visual feature space.\n","authors":["Yubo Wang","Chaohu Liu","Yanqiu Qu","Haoyu Cao","Deqiang Jiang","Linli Xu"],"pdf_url":"https://arxiv.org/pdf/2410.06699v1.pdf","comment":"Accepted to ACMMM 2024"},{"id":"http://arxiv.org/abs/2410.06698v1","updated":"2024-10-09T09:06:37Z","published":"2024-10-09T09:06:37Z","title":"Fourier-based Action Recognition for Wildlife Behavior Quantification\n  with Event Cameras","summary":"  Event cameras are novel bio-inspired vision sensors that measure pixel-wise\nbrightness changes asynchronously instead of images at a given frame rate. They\noffer promising advantages, namely a high dynamic range, low latency, and\nminimal motion blur. Modern computer vision algorithms often rely on artificial\nneural network approaches, which require image-like representations of the data\nand cannot fully exploit the characteristics of event data. We propose\napproaches to action recognition based on the Fourier Transform. The approaches\nare intended to recognize oscillating motion patterns commonly present in\nnature. In particular, we apply our approaches to a recent dataset of breeding\npenguins annotated for \"ecstatic display\", a behavior where the observed\npenguins flap their wings at a certain frequency. We find that our approaches\nare both simple and effective, producing slightly lower results than a deep\nneural network (DNN) while relying just on a tiny fraction of the parameters\ncompared to the DNN (five orders of magnitude fewer parameters). They work well\ndespite the uncontrolled, diverse data present in the dataset. We hope this\nwork opens a new perspective on event-based processing and action recognition.\n","authors":["Friedhelm Hamann","Suman Ghosh","Ignacio Juarez Martinez","Tom Hart","Alex Kacelnik","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2410.06698v1.pdf","comment":"11 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.06694v1","updated":"2024-10-09T09:01:40Z","published":"2024-10-09T09:01:40Z","title":"OmniPose6D: Towards Short-Term Object Pose Tracking in Dynamic Scenes\n  from Monocular RGB","summary":"  To address the challenge of short-term object pose tracking in dynamic\nenvironments with monocular RGB input, we introduce a large-scale synthetic\ndataset OmniPose6D, crafted to mirror the diversity of real-world conditions.\nWe additionally present a benchmarking framework for a comprehensive comparison\nof pose tracking algorithms. We propose a pipeline featuring an\nuncertainty-aware keypoint refinement network, employing probabilistic modeling\nto refine pose estimation. Comparative evaluations demonstrate that our\napproach achieves performance superior to existing baselines on real datasets,\nunderscoring the effectiveness of our synthetic dataset and refinement\ntechnique in enhancing tracking precision in dynamic contexts. Our\ncontributions set a new precedent for the development and assessment of object\npose tracking methodologies in complex scenes.\n","authors":["Yunzhi Lin","Yipu Zhao","Fu-Jen Chu","Xingyu Chen","Weiyao Wang","Hao Tang","Patricio A. Vela","Matt Feiszli","Kevin Liang"],"pdf_url":"https://arxiv.org/pdf/2410.06694v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.06689v1","updated":"2024-10-09T08:51:51Z","published":"2024-10-09T08:51:51Z","title":"Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds","summary":"  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we develop the first PCQA model dedicated to\nTrisoup-Lifting encoded 3D point clouds by analyzing bitstreams without full\ndecoding. Specifically, we investigate the relationship among texture bitrate\nper point (TBPP), texture complexity (TC) and texture quantization parameter\n(TQP) while geometry encoding is lossless. Subsequently, we estimate TC by\nutilizing TQP and TBPP. Then, we establish a texture distortion evaluation\nmodel based on TC, TBPP and TQP. Ultimately, by integrating this texture\ndistortion model with a geometry attenuation factor, a function of\ntrisoupNodeSizeLog2 (tNSL), we acquire a comprehensive NR bitstream-layer PCQA\nmodel named streamPCQ-TL. In addition, this work establishes a database named\nWPC6.0, the first and largest PCQA database dedicated to Trisoup-Lifting\nencoding mode, encompassing 400 distorted point clouds with both 4 geometric\nmultiplied by 5 texture distortion levels. Experiment results on M-PCCD,\nICIP2020 and the proposed WPC6.0 database suggest that the proposed\nstreamPCQ-TL model exhibits robust and notable performance in contrast to\nexisting advanced PCQA metrics, particularly in terms of computational cost.\nThe dataset and source code will be publicly released at\n\\href{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}{\\textit{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}}\n","authors":["Juncheng Long","Honglei Su","Qi Liu","Hui Yuan","Wei Gao","Jiarun Song","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06682v1","updated":"2024-10-09T08:44:47Z","published":"2024-10-09T08:44:47Z","title":"Enhancing Multimodal LLM for Detailed and Accurate Video Captioning\n  using Multi-Round Preference Optimization","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimization (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimized using DPO. To further improve training, we\nintroduce a novel multi-round DPO (mrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initializing the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilize the\nprocess. To address potential catastrophic forgetting of non-captioning\nabilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO\nLLM by using the captions generated by the mrDPO-trained model as supervised\nlabels. Experiments show that mrDPO significantly enhances video-SALMONN 2's\ncaptioning accuracy, reducing global and local error rates by 40\\% and 20\\%,\nrespectively, while decreasing the repetition rate by 35\\%. The final\nvideo-SALMONN 2 model, with just 7 billion parameters, surpasses leading models\nsuch as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining\ncompetitive performance to the state-of-the-art on widely used video\nquestion-answering benchmark among models of similar size. Upon acceptance, we\nwill release the code, model checkpoints, and training and test data. Demos are\navailable at\n\\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zujun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06678v1","updated":"2024-10-09T08:38:21Z","published":"2024-10-09T08:38:21Z","title":"M${}^{3}$Bench: Benchmarking Whole-body Motion Generation for Mobile\n  Manipulation in 3D Scenes","summary":"  We propose M^3Bench, a new benchmark for whole-body motion generation for\nmobile manipulation tasks. Given a 3D scene context, M^3Bench requires an\nembodied agent to understand its configuration, environmental constraints and\ntask objectives, then generate coordinated whole-body motion trajectories for\nobject rearrangement tasks. M^3Bench features 30k object rearrangement tasks\nacross 119 diverse scenes, providing expert demonstrations generated by our\nnewly developed M^3BenchMaker. This automatic data generation tool produces\ncoordinated whole-body motion trajectories from high-level task instructions,\nrequiring only basic scene and robot information. Our benchmark incorporates\nvarious task splits to assess generalization across different dimensions and\nleverages realistic physics simulation for trajectory evaluation. Through\nextensive experimental analyses, we reveal that state-of-the-art models still\nstruggle with coordinated base-arm motion while adhering to environment-context\nand task-specific constraints, highlighting the need to develop new models that\naddress this gap. Through M^3Bench, we aim to facilitate future robotics\nresearch towards more adaptive and capable mobile manipulation in diverse,\nreal-world environments.\n","authors":["Zeyu Zhang","Sixu Yan","Muzhi Han","Zaijin Wang","Xinggang Wang","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06664v1","updated":"2024-10-09T08:19:25Z","published":"2024-10-09T08:19:25Z","title":"Decouple-Then-Merge: Towards Better Training for Diffusion Models","summary":"  Diffusion models are trained by learning a sequence of models that reverse\neach step of noise corruption. Typically, the model parameters are fully shared\nacross multiple timesteps to enhance training efficiency. However, since the\ndenoising tasks differ at each timestep, the gradients computed at different\ntimesteps may conflict, potentially degrading the overall performance of image\ngeneration. To solve this issue, this work proposes a Decouple-then-Merge\n(DeMe) framework, which begins with a pretrained model and finetunes separate\nmodels tailored to specific timesteps. We introduce several improved techniques\nduring the finetuning stage to promote effective knowledge sharing while\nminimizing training interference across timesteps. Finally, after finetuning,\nthese separate models can be merged into a single model in the parameter space,\nensuring efficient and practical inference. Experimental results show\nsignificant generation quality improvements upon 6 benchmarks including Stable\nDiffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN\nBedroom, and CIFAR10.\n","authors":["Qianli Ma","Xuefei Ning","Dongrui Liu","Li Niu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15881v2","updated":"2024-10-09T08:11:32Z","published":"2024-08-28T15:52:23Z","title":"LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation","summary":"  We introduce LLaVA-MoD, a novel framework designed to enable the efficient\ntraining of small-scale Multimodal Language Models (s-MLLM) by distilling\nknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental\nchallenges in MLLM distillation. First, we optimize the network structure of\ns-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the\nlanguage model, striking a balance between computational efficiency and model\nexpressiveness. Second, we propose a progressive knowledge transfer strategy to\nensure comprehensive knowledge migration. This strategy begins with mimic\ndistillation, where we minimize the Kullback-Leibler (KL) divergence between\noutput distributions to enable the student model to emulate the teacher\nnetwork's understanding. Following this, we introduce preference distillation\nvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLM\nas the reference model. During this phase, the s-MLLM's ability to discriminate\nbetween superior and inferior examples is significantly enhanced beyond l-MLLM,\nleading to a better student that surpasses its teacher, particularly in\nhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD\noutperforms existing models across various multimodal benchmarks while\nmaintaining a minimal number of activated parameters and low computational\ncosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses\nQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of\nthe training data and 23% trainable parameters. These results underscore\nLLaVA-MoD's ability to effectively distill comprehensive knowledge from its\nteacher model, paving the way for the development of more efficient MLLMs. The\ncode will be available on: https://github.com/shufangxun/LLaVA-MoD.\n","authors":["Fangxun Shu","Yue Liao","Le Zhuo","Chenning Xu","Lei Zhang","Guanghao Zhang","Haonan Shi","Long Chen","Tao Zhong","Wanggui He","Siming Fu","Haoyuan Li","Bolin Li","Zhelun Yu","Si Liu","Hongsheng Li","Hao Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.15881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12815v2","updated":"2024-10-09T07:58:37Z","published":"2024-08-23T03:21:51Z","title":"Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and\n  Long-Range Dependencies for Structural Crack Segmentation","summary":"  Detecting cracks with pixel-level precision for key structures is a\nsignificant challenge, as existing methods struggle to effectively integrate\nlocal textures and pixel dependencies of cracks. Furthermore, these methods\noften possess numerous parameters and substantial computational requirements,\ncomplicating deployment on edge control devices. In this paper, we propose a\nstaircase cascaded fusion crack segmentation network (CrackSCF) that generates\nhigh-quality crack segmentation maps using minimal computational resources. We\nconstructed a staircase cascaded fusion module that effectively captures local\npatterns of cracks and long-range dependencies of pixels, and it can suppress\nbackground noise well. To reduce the computational resources required by the\nmodel, we introduced a lightweight convolution block, which replaces all\nconvolution operations in the network, significantly reducing the required\ncomputation and parameters without affecting the network's performance. To\nevaluate our method, we created a challenging benchmark dataset called TUT and\nconducted experiments on this dataset and five other public datasets. The\nexperimental results indicate that our method offers significant advantages\nover existing methods, especially in handling background noise interference and\ndetailed crack segmentation. The F1 and mIoU scores on the TUT dataset are\n0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance\nwhile requiring the least computational resources. The code and dataset is\navailable at https://github.com/Karl1109/CrackSCF.\n","authors":["Hui Liu","Chen Jia","Fan Shi","Xu Cheng","Mianzhao Wang","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.12815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06645v1","updated":"2024-10-09T07:57:47Z","published":"2024-10-09T07:57:47Z","title":"Continual Learning in the Frequency Domain","summary":"  Continual learning (CL) is designed to learn new tasks while preserving\nexisting knowledge. Replaying samples from earlier tasks has proven to be an\neffective method to mitigate the forgetting of previously acquired knowledge.\nHowever, the current research on the training efficiency of rehearsal-based\nmethods is insufficient, which limits the practical application of CL systems\nin resource-limited scenarios. The human visual system (HVS) exhibits varying\nsensitivities to different frequency components, enabling the efficient\nelimination of visually redundant information. Inspired by HVS, we propose a\nnovel framework called Continual Learning in the Frequency Domain (CLFD). To\nour knowledge, this is the first study to utilize frequency domain features to\nenhance the performance and efficiency of CL training on edge devices. For the\ninput features of the feature extractor, CLFD employs wavelet transform to map\nthe original input image into the frequency domain, thereby effectively\nreducing the size of input feature maps. Regarding the output features of the\nfeature extractor, CLFD selectively utilizes output features for distinct\nclasses for classification, thereby balancing the reusability and interference\nof output features based on the frequency domain similarity of the classes\nacross various tasks. Optimizing only the input and output features of the\nfeature extractor allows for seamless integration of CLFD with various\nrehearsal-based methods. Extensive experiments conducted in both cloud and edge\nenvironments demonstrate that CLFD consistently improves the performance of\nstate-of-the-art (SOTA) methods in both precision and training efficiency.\nSpecifically, CLFD can increase the accuracy of the SOTA CL method by up to\n6.83% and reduce the training time by 2.6$\\times$.\n","authors":["Ruiqi Liu","Boyu Diao","Libo Huang","Zijia An","Zhulin An","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.06645v1.pdf","comment":"Accepted by NeurlIPS 2024"},{"id":"http://arxiv.org/abs/2410.06626v1","updated":"2024-10-09T07:22:28Z","published":"2024-10-09T07:22:28Z","title":"Open-RGBT: Open-vocabulary RGB-T Zero-shot Semantic Segmentation in\n  Open-world Environments","summary":"  Semantic segmentation is a critical technique for effective scene\nunderstanding. Traditional RGB-T semantic segmentation models often struggle to\ngeneralize across diverse scenarios due to their reliance on pretrained models\nand predefined categories. Recent advancements in Visual Language Models (VLMs)\nhave facilitated a shift from closed-set to open-vocabulary semantic\nsegmentation methods. However, these models face challenges in dealing with\nintricate scenes, primarily due to the heterogeneity between RGB and thermal\nmodalities. To address this gap, we present Open-RGBT, a novel open-vocabulary\nRGB-T semantic segmentation model. Specifically, we obtain instance-level\ndetection proposals by incorporating visual prompts to enhance category\nunderstanding. Additionally, we employ the CLIP model to assess image-text\nsimilarity, which helps correct semantic consistency and mitigates ambiguities\nin category identification. Empirical evaluations demonstrate that Open-RGBT\nachieves superior performance in diverse and challenging real-world scenarios,\neven in the wild, significantly advancing the field of RGB-T semantic\nsegmentation.\n","authors":["Meng Yu","Luojie Yang","Xunjie He","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2410.06626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15011v3","updated":"2024-10-09T07:21:56Z","published":"2024-03-22T07:49:55Z","title":"Cell Tracking according to Biological Needs -- Strong Mitosis-aware\n  Multi-Hypothesis Tracker with Aleatoric Uncertainty","summary":"  Cell tracking and segmentation assist biologists in extracting insights from\nlarge-scale microscopy time-lapse data. Driven by local accuracy metrics,\ncurrent tracking approaches often suffer from a lack of long-term consistency\nand the ability to reconstruct lineage trees correctly. To address this issue,\nwe introduce an uncertainty estimation technique for motion estimation\nframeworks and extend the multi-hypothesis tracking framework. Our uncertainty\nestimation lifts motion representations into probabilistic spatial densities\nusing problem-specific test-time augmentations. Moreover, we introduce a novel\nmitosis-aware assignment problem formulation that allows multi-hypothesis\ntrackers to model cell splits and to resolve false associations and mitosis\ndetections based on long-term conflicts. In our framework, explicit biological\nknowledge is modeled in assignment costs. We evaluate our approach on nine\ncompetitive datasets and demonstrate that we outperform the current\nstate-of-the-art on biologically inspired metrics substantially, achieving\nimprovements by a factor of approximately 6 and uncover new insights into the\nbehavior of motion estimation uncertainty.\n","authors":["Timo Kaiser","Maximilian Schier","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2403.15011v3.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.06625v1","updated":"2024-10-09T07:21:43Z","published":"2024-10-09T07:21:43Z","title":"ETA: Evaluating Then Aligning Safety of Vision Language Models at\n  Inference Time","summary":"  Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.\n","authors":["Yi Ding","Bolian Li","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06625v1.pdf","comment":"27pages"},{"id":"http://arxiv.org/abs/2410.06618v1","updated":"2024-10-09T07:14:49Z","published":"2024-10-09T07:14:49Z","title":"Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video\n  Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06614v1","updated":"2024-10-09T07:09:46Z","published":"2024-10-09T07:09:46Z","title":"Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification\n  for Visual Place Recognition with Vision Transformers","summary":"  In this work we propose a novel joint training method for Visual Place\nRecognition (VPR), which simultaneously learns a global descriptor and a pair\nclassifier for re-ranking. The pair classifier can predict whether a given pair\nof images are from the same place or not. The network only comprises Vision\nTransformer components for both the encoder and the pair classifier, and both\ncomponents are trained using their respective class tokens. In existing VPR\nmethods, typically the network is initialized using pre-trained weights from a\ngeneric image dataset such as ImageNet. In this work we propose an alternative\npre-training strategy, by using Siamese Masked Image Modelling as a\npre-training task. We propose a Place-aware image sampling procedure from a\ncollection of large VPR datasets for pre-training our model, to learn visual\nfeatures tuned specifically for VPR. By re-using the Mask Image Modelling\nencoder and decoder weights in the second stage of training, Pair-VPR can\nachieve state-of-the-art VPR performance across five benchmark datasets with a\nViT-B encoder, along with further improvements in localization recall with\nlarger encoders. The Pair-VPR website is:\nhttps://csiro-robotics.github.io/Pair-VPR.\n","authors":["Stephen Hausler","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2410.06614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06613v1","updated":"2024-10-09T07:09:29Z","published":"2024-10-09T07:09:29Z","title":"ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian\n  Completion","summary":"  Accurate and affordable indoor 3D reconstruction is critical for effective\nrobot navigation and interaction. Traditional LiDAR-based mapping provides high\nprecision but is costly, heavy, and power-intensive, with limited ability for\nnovel view rendering. Vision-based mapping, while cost-effective and capable of\ncapturing visual data, often struggles with high-quality 3D reconstruction due\nto sparse point clouds. We propose ES-Gaussian, an end-to-end system using a\nlow-altitude camera and single-line LiDAR for high-quality 3D indoor\nreconstruction. Our system features Visual Error Construction (VEC) to enhance\nsparse point clouds by identifying and correcting areas with insufficient\ngeometric detail from 2D error maps. Additionally, we introduce a novel 3DGS\ninitialization method guided by single-line LiDAR, overcoming the limitations\nof traditional multi-view setups and enabling effective reconstruction in\nresource-constrained environments. Extensive experimental results on our new\nDreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian\noutperforms existing methods, particularly in challenging scenarios. The\nproject page is available at https://chenlu-china.github.io/ES-Gaussian/.\n","authors":["Lu Chen","Yingfu Zeng","Haoang Li","Zhitao Deng","Jiafu Yan","Zhenjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.06613v1.pdf","comment":"Project page: https://chenlu-china.github.io/ES-Gaussian/"},{"id":"http://arxiv.org/abs/2410.01611v2","updated":"2024-10-09T06:52:54Z","published":"2024-10-02T14:49:05Z","title":"DRUPI: Dataset Reduction Using Privileged Information","summary":"  Dataset reduction (DR) seeks to select or distill samples from large datasets\ninto smaller subsets while preserving performance on target tasks. Existing\nmethods primarily focus on pruning or synthesizing data in the same format as\nthe original dataset, typically the input data and corresponding labels.\nHowever, in DR settings, we find it is possible to synthesize more information\nbeyond the data-label pair as an additional learning target to facilitate model\ntraining. In this paper, we introduce Dataset Reduction Using Privileged\nInformation (DRUPI), which enriches DR by synthesizing privileged information\nalongside the reduced dataset. This privileged information can take the form of\nfeature labels or attention labels, providing auxiliary supervision to improve\nmodel learning. Our findings reveal that effective feature labels must balance\nbetween being overly discriminative and excessively diverse, with a moderate\nlevel proving optimal for improving the reduced dataset's efficacy. Extensive\nexperiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI\nintegrates seamlessly with existing dataset reduction methods, offering\nsignificant performance gains. *The code will be released after the paper is\naccepted.*\n","authors":["Shaobo Wang","Yantai Yang","Shuaiyu Zhang","Chenghao Sun","Weiya Li","Xuming Hu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06600v1","updated":"2024-10-09T06:52:05Z","published":"2024-10-09T06:52:05Z","title":"DDRN:a Data Distribution Reconstruction Network for Occluded Person\n  Re-Identification","summary":"  In occluded person re-identification(ReID), severe occlusions lead to a\nsignificant amount of irrelevant information that hinders the accurate\nidentification of individuals. These irrelevant cues primarily stem from\nbackground interference and occluding interference, adversely affecting the\nfinal retrieval results. Traditional discriminative models, which rely on the\nspecific content and positions of the images, often misclassify in cases of\nocclusion. To address these limitations, we propose the Data Distribution\nReconstruction Network (DDRN), a generative model that leverages data\ndistribution to filter out irrelevant details, enhancing overall feature\nperception ability and reducing irrelevant feature interference. Additionally,\nsevere occlusions lead to the complexity of the feature space. To effectively\nhandle this, we design a multi-center approach through the proposed\nHierarchical SubcenterArcface (HS-Arcface) loss function, which can better\napproximate complex feature spaces. On the Occluded-Duke dataset, we achieved a\nmAP of 62.4\\% (+1.1\\%) and a rank-1 accuracy of 71.3\\% (+0.6\\%), surpassing the\nlatest state-of-the-art methods(FRT) significantly.\n","authors":["Zhaoyong Wang","Yujie Liu","Mingyue Li","Wenxin Zhang","Zongmin Li"],"pdf_url":"https://arxiv.org/pdf/2410.06600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06593v1","updated":"2024-10-09T06:43:19Z","published":"2024-10-09T06:43:19Z","title":"Towards Natural Image Matting in the Wild via Real-Scenario Prior","summary":"  Recent approaches attempt to adapt powerful interactive segmentation models,\nsuch as SAM, to interactive matting and fine-tune the models based on synthetic\nmatting datasets. However, models trained on synthetic data fail to generalize\nto complex and occlusion scenes. We address this challenge by proposing a new\nmatting dataset based on the COCO dataset, namely COCO-Matting. Specifically,\nthe construction of our COCO-Matting includes accessory fusion and\nmask-to-matte, which selects real-world complex images from COCO and converts\nsemantic segmentation masks to matting labels. The built COCO-Matting comprises\nan extensive collection of 38,251 human instance-level alpha mattes in complex\nnatural scenarios. Furthermore, existing SAM-based matting methods extract\nintermediate features and masks from a frozen SAM and only train a lightweight\nmatting decoder by end-to-end matting losses, which do not fully exploit the\npotential of the pre-trained SAM. Thus, we propose SEMat which revamps the\nnetwork architecture and training objectives. For network architecture, the\nproposed feature-aligned transformer learns to extract fine-grained edge and\ntransparency features. The proposed matte-aligned decoder aims to segment\nmatting-specific objects and convert coarse masks into high-precision mattes.\nFor training objectives, the proposed regularization and trimap loss aim to\nretain the prior from the pre-trained model and push the matting logits\nextracted from the mask decoder to contain trimap-based semantic information.\nExtensive experiments across seven diverse datasets demonstrate the superior\nperformance of our method, proving its efficacy in interactive natural image\nmatting. We open-source our code, models, and dataset at\nhttps://github.com/XiaRho/SEMat.\n","authors":["Ruihao Xia","Yu Liang","Peng-Tao Jiang","Hao Zhang","Qianru Sun","Yang Tang","Bo Li","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.06593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06576v1","updated":"2024-10-09T06:18:53Z","published":"2024-10-09T06:18:53Z","title":"On The Relationship between Visual Anomaly-free and Anomalous\n  Representations","summary":"  Anomaly Detection is an important problem within computer vision, having\nvariety of real-life applications. Yet, the current set of solutions to this\nproblem entail known, systematic shortcomings. Specifically, contemporary\nsurface Anomaly Detection task assumes the presence of multiple specific\nanomaly classes e.g. cracks, rusting etc., unlike one-class classification\nmodel of past. However, building a deep learning model in such setup remains a\nchallenge because anomalies arise rarely, and hence anomaly samples are quite\nscarce. Transfer learning has been a preferred paradigm in such situations. But\nthe typical source domains with large dataset sizes e.g. ImageNet, JFT-300M,\nLAION-2B do not correlate well with the domain of surfaces and materials, an\nimportant premise of transfer learning. In this paper, we make an important\nhypothesis and show, by exhaustive experimentation, that the space of\nanomaly-free visual patterns of the normal samples correlates well with each of\nthe various spaces of anomalous patterns of the class-specific anomaly samples.\nThe first results of using this hypothesis in transfer learning have indeed\nbeen quite encouraging. We expect that finding such a simple closeby domain\nthat readily entails large number of samples, and which also oftentimes shows\ninterclass separability though with narrow margins, will be a useful discovery.\nEspecially, it is expected to improve domain adaptation for anomaly detection,\nand few-shot learning for anomaly detection, making in-the-wild anomaly\ndetection realistically possible in future.\n","authors":["Riya Sadrani","Hrishikesh Sharma","Ayush Bachan"],"pdf_url":"https://arxiv.org/pdf/2410.06576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05008v3","updated":"2024-10-09T06:05:53Z","published":"2024-08-09T11:40:20Z","title":"FlowDreamer: Exploring High Fidelity Text-to-3D Generation via Rectified\n  Flow","summary":"  Recent advances in text-to-3D generation have made significant progress. In\nparticular, with the pretrained diffusion models, existing methods\npredominantly use Score Distillation Sampling (SDS) to train 3D models such as\nNeural RaRecent advances in text-to-3D generation have made significant\nprogress. In particular, with the pretrained diffusion models, existing methods\npredominantly use Score Distillation Sampling (SDS) to train 3D models such as\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS). However, a\nhurdle is that they often encounter difficulties with over-smoothing textures\nand over-saturating colors. The rectified flow model -- which utilizes a simple\nordinary differential equation (ODE) to represent a straight trajectory --\nshows promise as an alternative prior to text-to-3D generation. It learns a\ntime-independent vector field, thereby reducing the ambiguity in 3D model\nupdate gradients that are calculated using time-dependent scores in the SDS\nframework. In light of this, we first develop a mathematical analysis to\nseamlessly integrate SDS with rectified flow model, paving the way for our\ninitial framework known as Vector Field Distillation Sampling (VFDS). However,\nempirical findings indicate that VFDS still results in over-smoothing outcomes.\nTherefore, we analyze the grounding reasons for such a failure from the\nperspective of ODE trajectories. On top, we propose a novel framework, named\nFlowDreamer, which yields high fidelity results with richer textual details and\nfaster convergence. The key insight is to leverage the coupling and reversible\nproperties of the rectified flow model to search for the corresponding noise,\nrather than using randomly sampled noise as in VFDS. Accordingly, we introduce\na novel Unique Couple Matching (UCM) loss, which guides the 3D model to\noptimize along the same trajectory.\n","authors":["Hangyu Li","Xiangxiang Chu","Dingyuan Shi","Wang Lin"],"pdf_url":"https://arxiv.org/pdf/2408.05008v3.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2410.06558v1","updated":"2024-10-09T05:28:43Z","published":"2024-10-09T05:28:43Z","title":"Deep Correlated Prompting for Visual Recognition with Missing Modalities","summary":"  Large-scale multimodal models have shown excellent performance over a series\nof tasks powered by the large corpus of paired multimodal training data.\nGenerally, they are always assumed to receive modality-complete inputs.\nHowever, this simple assumption may not always hold in the real world due to\nprivacy constraints or collection difficulty, where models pretrained on\nmodality-complete data easily demonstrate degraded performance on\nmissing-modality cases. To handle this issue, we refer to prompt learning to\nadapt large pretrained multimodal models to handle missing-modality scenarios\nby regarding different missing cases as different types of input. Instead of\nonly prepending independent prompts to the intermediate layers, we present to\nleverage the correlations between prompts and input features and excavate the\nrelationships between different layers of prompts to carefully design the\ninstructions. We also incorporate the complementary semantics of different\nmodalities to guide the prompting design for each modality. Extensive\nexperiments on three commonly-used datasets consistently demonstrate the\nsuperiority of our method compared to the previous approaches upon different\nmissing scenarios. Plentiful ablations are further given to show the\ngeneralizability and reliability of our method upon different modality-missing\nratios and types.\n","authors":["Lianyu Hu","Tongkai Shi","Wei Feng","Fanhua Shang","Liang Wan"],"pdf_url":"https://arxiv.org/pdf/2410.06558v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06551v1","updated":"2024-10-09T05:15:29Z","published":"2024-10-09T05:15:29Z","title":"InstantIR: Blind Image Restoration with Instant Generative Reference","summary":"  Handling test-time unknown degradation is the major challenge in Blind Image\nRestoration (BIR), necessitating high model generalization. An effective\nstrategy is to incorporate prior knowledge, either from human input or\ngenerative model. In this paper, we introduce Instant-reference Image\nRestoration (InstantIR), a novel diffusion-based BIR method which dynamically\nadjusts generation condition during inference. We first extract a compact\nrepresentation of the input via a pre-trained vision encoder. At each\ngeneration step, this representation is used to decode current diffusion latent\nand instantiate it in the generative prior. The degraded image is then encoded\nwith this reference, providing robust generation condition. We observe the\nvariance of generative references fluctuate with degradation intensity, which\nwe further leverage as an indicator for developing a sampling algorithm\nadaptive to input quality. Extensive experiments demonstrate InstantIR achieves\nstate-of-the-art performance and offering outstanding visual quality. Through\nmodulating generative references with textual description, InstantIR can\nrestore extreme degradation and additionally feature creative restoration.\n","authors":["Jen-Yuan Huang","Haofan Wang","Qixun Wang","Xu Bai","Hao Ai","Peng Xing","Jen-Tse Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05249v2","updated":"2024-10-09T05:05:31Z","published":"2024-10-07T17:52:56Z","title":"LoTLIP: Improving Language-Image Pre-training for Long Text\n  Understanding","summary":"  Understanding long text is of great demands in practice but beyond the reach\nof most language-image pre-training (LIP) models. In this work, we empirically\nconfirm that the key reason causing such an issue is that the training images\nare usually paired with short captions, leaving certain tokens easily\novershadowed by salient tokens. Towards this problem, our initial attempt is to\nrelabel the data with long captions, however, directly learning with which may\nlead to performance degradation in understanding short text (e.g., in the image\nclassification task). Then, after incorporating corner tokens to aggregate\ndiverse textual information, we manage to help the model catch up to its\noriginal level of short text understanding yet greatly enhance its capability\nof long text understanding. We further look into whether the model can\ncontinuously benefit from longer captions and notice a clear trade-off between\nthe performance and the efficiency. Finally, we validate the effectiveness of\nour approach using a self-constructed large-scale dataset, which consists of\n100M long caption oriented text-image pairs. It is noteworthy that, on the task\nof long-text image retrieval, we beat the competitor using long captions with\n11.1% improvement (i.e., from 72.62% to 83.72%). We will release the code, the\nmodel, and the new dataset to facilitate the reproducibility and further\nresearch. The project page is available at https://wuw2019.github.io/lot-lip.\n","authors":["Wei Wu","Kecheng Zheng","Shuailei Ma","Fan Lu","Yuxin Guo","Yifei Zhang","Wei Chen","Qingpei Guo","Yujun Shen","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2410.05249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10163v2","updated":"2024-10-09T05:00:16Z","published":"2024-06-14T16:30:25Z","title":"MeshAnything: Artist-Created Mesh Generation with Autoregressive\n  Transformers","summary":"  Recently, 3D assets created via reconstruction and generation have matched\nthe quality of manually crafted assets, highlighting their potential for\nreplacement. However, this potential is largely unrealized because these assets\nalways need to be converted to meshes for 3D industry applications, and the\nmeshes produced by current mesh extraction methods are significantly inferior\nto Artist-Created Meshes (AMs), i.e., meshes created by human artists.\nSpecifically, current mesh extraction methods rely on dense faces and ignore\ngeometric features, leading to inefficiencies, complicated post-processing, and\nlower representation quality. To address these issues, we introduce\nMeshAnything, a model that treats mesh extraction as a generation problem,\nproducing AMs aligned with specified shapes. By converting 3D assets in any 3D\nrepresentation into AMs, MeshAnything can be integrated with various 3D asset\nproduction methods, thereby enhancing their application across the 3D industry.\nThe architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned\ndecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,\nthen train the shape-conditioned decoder-only transformer on this vocabulary\nfor shape-conditioned autoregressive mesh generation. Our extensive experiments\nshow that our method generates AMs with hundreds of times fewer faces,\nsignificantly improving storage, rendering, and simulation efficiencies, while\nachieving precision comparable to previous methods.\n","authors":["Yiwen Chen","Tong He","Di Huang","Weicai Ye","Sijin Chen","Jiaxiang Tang","Xin Chen","Zhongang Cai","Lei Yang","Gang Yu","Guosheng Lin","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10163v2.pdf","comment":"Project Page: https://buaacyw.github.io/mesh-anything/ Code:\n  https://github.com/buaacyw/MeshAnything"},{"id":"http://arxiv.org/abs/2410.06542v1","updated":"2024-10-09T04:36:47Z","published":"2024-10-09T04:36:47Z","title":"MedImageInsight: An Open-Source Embedding Model for General Domain\n  Medical Imaging","summary":"  In this work, we present MedImageInsight, an open-source medical imaging\nembedding model. MedImageInsight is trained on medical images with associated\ntext and labels across a diverse collection of domains, including X-Ray, CT,\nMRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and\nmammography. Rigorous evaluations demonstrate MedImageInsight's ability to\nachieve state-of-the-art (SOTA) or human expert level performance across\nclassification, image-image search, and fine-tuning tasks. Specifically, on\npublic datasets, MedImageInsight achieves SOTA in CT 3D medical image\nretrieval, as well as SOTA in disease classification and search for chest\nX-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves\nhuman expert performance in bone age estimation (on both public and partner\ndata), as well as AUC above 0.9 in most other domains. When paired with a text\ndecoder, MedImageInsight achieves near SOTA level single image report findings\ngeneration with less than 10\\% the parameters of other models. Compared to\nfine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight\noutperforms in clinical metrics, but underperforms on lexical metrics where\nGPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight\ncan generate ROC curves, adjust sensitivity and specificity based on clinical\nneed, and provide evidence-based decision support through image-image search\n(which can also enable retrieval augmented generation). In an independent\nclinical evaluation of image-image search in chest X-ray, MedImageInsight\noutperformed every other publicly available foundation model evaluated by large\nmargins (over 6 points AUC), and significantly outperformed other models in\nterms of AI fairness (across age and gender). We hope releasing MedImageInsight\nwill help enhance collective progress in medical imaging AI research and\ndevelopment.\n","authors":["Noel C. F. Codella","Ying Jin","Shrey Jain","Yu Gu","Ho Hin Lee","Asma Ben Abacha","Alberto Santamaria-Pang","Will Guyman","Naiteek Sangani","Sheng Zhang","Hoifung Poon","Stephanie Hyland","Shruthi Bannur","Javier Alvarez-Valle","Xue Li","John Garrett","Alan McMillan","Gaurav Rajguru","Madhu Maddi","Nilesh Vijayrania","Rehaan Bhimai","Nick Mecklenburg","Rupal Jain","Daniel Holstein","Naveen Gaur","Vijay Aski","Jenq-Neng Hwang","Thomas Lin","Ivan Tarapov","Matthew Lungren","Mu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.06542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14206v2","updated":"2024-10-09T04:30:30Z","published":"2024-05-23T06:04:40Z","title":"LG-VQ: Language-Guided Codebook Learning","summary":"  Vector quantization (VQ) is a key technique in high-resolution and\nhigh-fidelity image synthesis, which aims to learn a codebook to encode an\nimage with a sequence of discrete codes and then generate an image in an\nauto-regression manner. Although existing methods have shown superior\nperformance, most methods prefer to learn a single-modal codebook (\\emph{e.g.},\nimage), resulting in suboptimal performance when the codebook is applied to\nmulti-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due\nto the existence of modal gaps. In this paper, we propose a novel\nlanguage-guided codebook learning framework, called LG-VQ, which aims to learn\na codebook that can be aligned with the text to improve the performance of\nmulti-modal downstream tasks. Specifically, we first introduce pre-trained text\nsemantics as prior knowledge, then design two novel alignment modules\n(\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to\ntransfer such prior knowledge into codes for achieving codebook text alignment.\nIn particular, our LG-VQ method is model-agnostic, which can be easily\nintegrated into existing VQ models. Experimental results show that our method\nachieves superior performance on reconstruction and various multi-modal\ndownstream tasks.\n","authors":["Guotao Liang","Baoquan Zhang","Yaowei Wang","Xutao Li","Yunming Ye","Huaibin Wang","Chuyao Luo","Kola Ye","linfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2405.14206v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.14407v4","updated":"2024-10-09T04:25:34Z","published":"2024-02-22T09:48:47Z","title":"Learning an Actionable Discrete Diffusion Policy via Large-Scale\n  Actionless Video Pre-Training","summary":"  Learning a generalist embodied agent capable of completing multiple tasks\nposes challenges, primarily stemming from the scarcity of action-labeled\nrobotic datasets. In contrast, a vast amount of human videos exist, capturing\nintricate tasks and interactions with the physical world. Promising prospects\narise for utilizing actionless human videos for pre-training and transferring\nthe knowledge to facilitate robot policy learning through limited robot\ndemonstrations. However, it remains a challenge due to the domain gap between\nhumans and robots. Moreover, it is difficult to extract useful information\nrepresenting the dynamic world from human videos, because of its noisy and\nmultimodal data structure. In this paper, we introduce a novel framework to\ntackle these challenges, which leverages a unified discrete diffusion to\ncombine generative pre-training on human videos and policy fine-tuning on a\nsmall number of action-labeled robot videos. We start by compressing both human\nand robot videos into unified video tokens. In the pre-training stage, we\nemploy a discrete diffusion model with a mask-and-replace diffusion strategy to\npredict future video tokens in the latent space. In the fine-tuning stage, we\nharness the imagined future videos to guide low-level action learning with a\nlimited set of robot data. Experiments demonstrate that our method generates\nhigh-fidelity future videos for planning and enhances the fine-tuned policies\ncompared to previous state-of-the-art approaches with superior performance. Our\nproject website is available at https://video-diff.github.io/.\n","authors":["Haoran He","Chenjia Bai","Ling Pan","Weinan Zhang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2402.14407v4.pdf","comment":"Accepted by NeurIPS 2024. 24 pages"},{"id":"http://arxiv.org/abs/2410.06535v1","updated":"2024-10-09T04:18:51Z","published":"2024-10-09T04:18:51Z","title":"Happy: A Debiased Learning Framework for Continual Generalized Category\n  Discovery","summary":"  Constantly discovering novel concepts is crucial in evolving environments.\nThis paper explores the underexplored task of Continual Generalized Category\nDiscovery (C-GCD), which aims to incrementally discover new classes from\nunlabeled data while maintaining the ability to recognize previously learned\nclasses. Although several settings are proposed to study the C-GCD task, they\nhave limitations that do not reflect real-world scenarios. We thus study a more\npractical C-GCD setting, which includes more new classes to be discovered over\na longer period, without storing samples of past classes. In C-GCD, the model\nis initially trained on labeled data of known classes, followed by multiple\nincremental stages where the model is fed with unlabeled data containing both\nold and new classes. The core challenge involves two conflicting objectives:\ndiscover new classes and prevent forgetting old ones. We delve into the\nconflicts and identify that models are susceptible to prediction bias and\nhardness bias. To address these issues, we introduce a debiased learning\nframework namely Happy. For the prediction bias, we first introduce\nclustering-guided initialization to provide robust features. In addition, we\npropose soft entropy regularization to assign appropriate probabilities to new\nclasses, which can significantly enhance the clustering performance of new\nclasses. For the harness bias, we present the hardness-aware prototype\nsampling, which can effectively reduce the forgetting issue for previously seen\nclasses, especially for difficult classes. Experimental results demonstrate our\nmethod proficiently manages the conflicts of C-GCD and achieves remarkable\nperformance across various datasets, e.g., 7.5% overall gains on ImageNet-100.\nOur code is publicly available at https://github.com/mashijie1028/Happy-CGCD.\n","authors":["Shijie Ma","Fei Zhu","Zhun Zhong","Wenzhuo Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06535v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06527v1","updated":"2024-10-09T03:57:13Z","published":"2024-10-09T03:57:13Z","title":"The Sampling-Gaussian for stereo matching","summary":"  The soft-argmax operation is widely adopted in neural network-based stereo\nmatching methods to enable differentiable regression of disparity. However,\nnetwork trained with soft-argmax is prone to being multimodal due to absence of\nexplicit constraint to the shape of the probability distribution. Previous\nmethods leverages Laplacian distribution and cross-entropy for training but\nfailed to effectively improve the accuracy and even compromises the efficiency\nof the network. In this paper, we conduct a detailed analysis of the previous\ndistribution-based methods and propose a novel supervision method for stereo\nmatching, Sampling-Gaussian. We sample from the Gaussian distribution for\nsupervision. Moreover, we interpret the training as minimizing the distance in\nvector space and propose a combined loss of L1 loss and cosine similarity loss.\nAdditionally, we leveraged bilinear interpolation to upsample the cost volume.\nOur method can be directly applied to any soft-argmax-based stereo matching\nmethod without a reduction in efficiency. We have conducted comprehensive\nexperiments to demonstrate the superior performance of our Sampling-Gaussian.\nThe experimental results prove that we have achieved better accuracy on five\nbaseline methods and two datasets. Our method is easy to implement, and the\ncode is available online.\n","authors":["Baiyu Pan","jichao jiao","Bowen Yao","Jianxin Pang","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.06527v1.pdf","comment":"TL;DR: A novel Gaussian distribution-based supervision method for\n  stereo matching. Implemented with five baseline methods and achieves notable\n  improvement. Main content, 10 pages. conference submission"},{"id":"http://arxiv.org/abs/2404.03187v2","updated":"2024-10-09T03:55:07Z","published":"2024-04-04T04:12:30Z","title":"AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying\n  Scales","summary":"  We present AGL-NET, a novel learning-based method for global localization\nusing LiDAR point clouds and satellite maps. AGL-NET tackles two critical\nchallenges: bridging the representation gap between image and points modalities\nfor robust feature matching, and handling inherent scale discrepancies between\nglobal view and local view. To address these challenges, AGL-NET leverages a\nunified network architecture with a novel two-stage matching design. The first\nstage extracts informative neural features directly from raw sensor data and\nperforms initial feature matching. The second stage refines this matching\nprocess by extracting informative skeleton features and incorporating a novel\nscale alignment step to rectify scale variations between LiDAR and map data.\nFurthermore, a novel scale and skeleton loss function guides the network toward\nlearning scale-invariant feature representations, eliminating the need for\npre-processing satellite maps. This significantly improves real-world\napplicability in scenarios with unknown map scales. To facilitate rigorous\nperformance evaluation, we introduce a meticulously designed dataset within the\nCARLA simulator specifically tailored for metric localization training and\nassessment. The code and data can be accessed at\nhttps://github.com/rayguan97/AGL-Net.\n","authors":["Tianrui Guan","Ruiqi Xian","Xijun Wang","Xiyang Wu","Mohamed Elnoor","Daeun Song","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2404.03187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09559v3","updated":"2024-10-09T03:51:45Z","published":"2024-03-14T16:47:25Z","title":"Less is More: High-value Data Selection for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building large vision language\nmodels~(LVLMs), which can greatly improve the task generalization and solving\ncapabilities by learning a mixture of instruction data from diverse visual\ntasks. Previous work mostly collects multiple existing visual instruction\ndatasets via heuristic ways for training (even more than a million\ninstructions), which may introduce data redundancy and enlarge the training\ncost. To investigate this issue, we conduct a series of empirical studies,\nwhich reveal a significant redundancy within the visual instruction datasets,\nand show that greatly reducing the amount of instructions from several tasks\neven do not affect the performance. Based on the findings, we propose a\nhigh-value data selection approach TIVE, to eliminate redundancy within the\nvisual instruction data and reduce the training cost. In TIVE, we first\nestimate the instance influence score on its corresponding task, and the task\ndifficulty score, based on the gradient-based influence functions. Then, we\nleverage the two kinds of scores to determine the task proportion within the\nselected visual instruction subset, and select high-value instances for each\ntask, respectively. Experiments on various LVLMs show that our approach using\nonly about 15% data can achieve comparable average performance to the full-data\nfine-tuned model across eight benchmarks, even surpassing it on four of the\nbenchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.10900v2","updated":"2024-10-09T03:42:22Z","published":"2024-06-16T11:44:43Z","title":"AutoHallusion: Automatic Generation of Hallucination Benchmarks for\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) are prone to hallucinations, where\ncertain contextual cues in an image can trigger the language module to produce\noverconfident and incorrect reasoning about abnormal or hypothetical objects.\nWhile some benchmarks have been developed to investigate LVLM hallucinations,\nthey often rely on hand-crafted corner cases whose failure patterns may not\ngeneralize well. Additionally, fine-tuning on these examples could undermine\ntheir validity. To address this, we aim to scale up the number of cases through\nan automated approach, reducing human bias in crafting such corner cases. This\nmotivates the development of AutoHallusion, the first automated benchmark\ngeneration approach that employs several key strategies to create a diverse\nrange of hallucination examples. Our generated visual-question pairs pose\nsignificant challenges to LVLMs, requiring them to overcome contextual biases\nand distractions to arrive at correct answers. AutoHallusion enables us to\ncreate new benchmarks at the minimum cost and thus overcomes the fragility of\nhand-crafted benchmarks. It also reveals common failure patterns and reasons,\nproviding key insights to detect, avoid, or control hallucinations.\nComprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro\nVision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of\nhallucination induction on synthetic and real-world datasets of AutoHallusion,\npaving the way for a long battle against hallucinations. The codebase and data\ncan be accessed at https://github.com/wuxiyang1996/AutoHallusion.\n","authors":["Xiyang Wu","Tianrui Guan","Dianqi Li","Shuaiyi Huang","Xiaoyu Liu","Xijun Wang","Ruiqi Xian","Abhinav Shrivastava","Furong Huang","Jordan Lee Boyd-Graber","Tianyi Zhou","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03382v3","updated":"2024-10-09T03:31:44Z","published":"2023-05-05T09:27:59Z","title":"Guided Image Synthesis via Initial Image Editing in Diffusion Model","summary":"  Diffusion models have the ability to generate high quality images by\ndenoising pure Gaussian noise images. While previous research has primarily\nfocused on improving the control of image generation through adjusting the\ndenoising process, we propose a novel direction of manipulating the initial\nnoise to control the generated image. Through experiments on stable diffusion,\nwe show that blocks of pixels in the initial latent images have a preference\nfor generating specific content, and that modifying these blocks can\nsignificantly influence the generated image. In particular, we show that\nmodifying a part of the initial image affects the corresponding region of the\ngenerated image while leaving other regions unaffected, which is useful for\nrepainting tasks. Furthermore, we find that the generation preferences of pixel\nblocks are primarily determined by their values, rather than their position. By\nmoving pixel blocks with a tendency to generate user-desired content to\nuser-specified regions, our approach achieves state-of-the-art performance in\nlayout-to-image generation. Our results highlight the flexibility and power of\ninitial image manipulation in controlling the generated image. Project Page:\nhttps://ut-mao.github.io/swap.github.io/\n","authors":["Jiafeng Mao","Xueting Wang","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2305.03382v3.pdf","comment":"ACM MM 23"},{"id":"http://arxiv.org/abs/2312.08872v4","updated":"2024-10-09T03:29:17Z","published":"2023-12-13T03:31:19Z","title":"The Lottery Ticket Hypothesis in Denoising: Towards Semantic-Driven\n  Initialization","summary":"  Text-to-image diffusion models allow users control over the content of\ngenerated images. Still, text-to-image generation occasionally leads to\ngeneration failure requiring users to generate dozens of images under the same\ntext prompt before they obtain a satisfying result. We formulate the lottery\nticket hypothesis in denoising: randomly initialized Gaussian noise images\ncontain special pixel blocks (winning tickets) that naturally tend to be\ndenoised into specific content independently. The generation failure in\nstandard text-to-image synthesis is caused by the gap between optimal and\nactual spatial distribution of winning tickets in initial noisy images. To this\nend, we implement semantic-driven initial image construction creating initial\nnoise from known winning tickets for each concept mentioned in the prompt. We\nconduct a series of experiments that verify the properties of winning tickets\nand demonstrate their generalizability across images and prompts. Our results\nshow that aggregating winning tickets into the initial noise image effectively\ninduce the model to generate the specified object at the corresponding\nlocation. Project Page: https://ut-mao.github.io/noise.github.io\n","authors":["Jiafeng Mao","Xueting Wang","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2312.08872v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2410.06513v1","updated":"2024-10-09T03:27:14Z","published":"2024-10-09T03:27:14Z","title":"MotionRL: Align Text-to-Motion Generation to Human Preferences with\n  Multi-Reward Reinforcement Learning","summary":"  We introduce MotionRL, the first approach to utilize Multi-Reward\nReinforcement Learning (RL) for optimizing text-to-motion generation tasks and\naligning them with human preferences. Previous works focused on improving\nnumerical performance metrics on the given datasets, often neglecting the\nvariability and subjectivity of human feedback. In contrast, our novel approach\nuses reinforcement learning to fine-tune the motion generator based on human\npreferences prior knowledge of the human perception model, allowing it to\ngenerate motions that better align human preferences. In addition, MotionRL\nintroduces a novel multi-objective optimization strategy to approximate Pareto\noptimality between text adherence, motion quality, and human preferences.\nExtensive experiments and user studies demonstrate that MotionRL not only\nallows control over the generated results across different objectives but also\nsignificantly enhances performance across these metrics compared to other\nalgorithms.\n","authors":["Xiaoyang Liu","Yunyao Mao","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.06513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00411v2","updated":"2024-10-09T02:56:46Z","published":"2024-02-01T08:10:39Z","title":"LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through\n  Learnable Multi-hierarchical Threshold Model","summary":"  Compared to traditional Artificial Neural Network (ANN), Spiking Neural\nNetwork (SNN) has garnered widespread academic interest for its intrinsic\nability to transmit information in a more energy-efficient manner. However,\ndespite previous efforts to optimize the learning algorithm of SNNs through\nvarious methods, SNNs still lag behind ANNs in terms of performance. The\nrecently proposed multi-threshold model provides more possibilities for further\nenhancing the learning capability of SNNs. In this paper, we rigorously analyze\nthe relationship among the multi-threshold model, vanilla spiking model and\nquantized ANNs from a mathematical perspective, then propose a novel LM-HT\nmodel, which is an equidistant multi-threshold model that can dynamically\nregulate the global input current and membrane potential leakage on the time\ndimension. The LM-HT model can also be transformed into a vanilla single\nthreshold model through reparameterization, thereby achieving more flexible\nhardware deployment. In addition, we note that the LM-HT model can seamlessly\nintegrate with ANN-SNN Conversion framework under special initialization. This\nnovel hybrid learning framework can effectively improve the relatively poor\nperformance of converted SNNs under low time latency. Extensive experimental\nresults have demonstrated that our model can outperform previous\nstate-of-the-art works on various types of datasets, which promote SNNs to\nachieve a brand-new level of performance comparable to quantized ANNs. Code is\navailable at https://github.com/hzc1208/LMHT_SNN.\n","authors":["Zecheng Hao","Xinyu Shi","Yujia Liu","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2402.00411v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06488v1","updated":"2024-10-09T02:30:24Z","published":"2024-10-09T02:30:24Z","title":"HFH-Font: Few-shot Chinese Font Synthesis with Higher Quality, Faster\n  Speed, and Higher Resolution","summary":"  The challenge of automatically synthesizing high-quality vector fonts,\nparticularly for writing systems (e.g., Chinese) consisting of huge amounts of\ncomplex glyphs, remains unsolved. Existing font synthesis techniques fall into\ntwo categories: 1) methods that directly generate vector glyphs, and 2) methods\nthat initially synthesize glyph images and then vectorize them. However, the\nfirst category often fails to construct complete and correct shapes for complex\nglyphs, while the latter struggles to efficiently synthesize high-resolution\n(i.e., 1024 $\\times$ 1024 or higher) glyph images while preserving local\ndetails. In this paper, we introduce HFH-Font, a few-shot font synthesis method\ncapable of efficiently generating high-resolution glyph images that can be\nconverted into high-quality vector glyphs. More specifically, our method\nemploys a diffusion model-based generative framework with component-aware\nconditioning to learn different levels of style information adaptable to\nvarying input reference sizes. We also design a distillation module based on\nScore Distillation Sampling for 1-step fast inference, and a style-guided\nsuper-resolution module to refine and upscale low-resolution synthesis results.\nExtensive experiments, including a user study with professional font designers,\nhave been conducted to demonstrate that our method significantly outperforms\nexisting font synthesis approaches. Experimental results show that our method\nproduces high-fidelity, high-resolution raster images which can be vectorized\ninto high-quality vector fonts. Using our method, for the first time,\nlarge-scale Chinese vector fonts of a quality comparable to those manually\ncreated by professional font designers can be automatically generated.\n","authors":["Hua Li","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2410.06488v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 (TOG). Code:\n  https://github.com/grovessss/HFH-Font"},{"id":"http://arxiv.org/abs/2410.06483v1","updated":"2024-10-09T02:16:29Z","published":"2024-10-09T02:16:29Z","title":"Deep Learning Ensemble for Predicting Diabetic Macular Edema Onset Using\n  Ultra-Wide Field Color Fundus Image","summary":"  Diabetic macular edema (DME) is a severe complication of diabetes,\ncharacterized by thickening of the central portion of the retina due to\naccumulation of fluid. DME is a significant and common cause of visual\nimpairment in diabetic patients. Center-involved DME (ci-DME) is the highest\nrisk form of disease as fluid extends close to the fovea which is responsible\nfor sharp central vision. Earlier diagnosis or prediction of ci-DME may improve\ntreatment outcomes. Here, we propose an ensemble method to predict ci-DME onset\nwithin a year using ultra-wide-field color fundus photography (UWF-CFP) images\nprovided by the DIAMOND Challenge. We adopted a variety of baseline\nstate-of-the-art classification networks including ResNet, DenseNet,\nEfficientNet, and VGG with the aim of enhancing model robustness. The best\nperforming models were Densenet 121, Resnet 152 and EfficientNet b7, and these\nwere assembled into a definitive predictive model. The final ensemble model\ndemonstrates a strong performance with an Area Under Curve (AUC) of 0.7017, an\nF1 score of 0.6512, and an Expected Calibration Error (ECE) of 0.2057 when\ndeployed on a synthetic dataset. The performance of this ensemble model is\ncomparable to previous studies despite training and testing in a more realistic\nsetting, indicating the potential of UWF-CFP combined with a deep learning\nclassification system to facilitate earlier diagnosis, better treatment\ndecisions, and improved prognostication in ci-DME.\n","authors":["Pengyao Qin","Arun J. Thirunavukarasu","Le Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06478v1","updated":"2024-10-09T02:12:01Z","published":"2024-10-09T02:12:01Z","title":"MaskBlur: Spatial and Angular Data Augmentation for Light Field Image\n  Super-Resolution","summary":"  Data augmentation (DA) is an effective approach for enhancing model\nperformance with limited data, such as light field (LF) image super-resolution\n(SR). LF images inherently possess rich spatial and angular information.\nNonetheless, there is a scarcity of DA methodologies explicitly tailored for LF\nimages, and existing works tend to concentrate solely on either the spatial or\nangular domain. This paper proposes a novel spatial and angular DA strategy\nnamed MaskBlur for LF image SR by concurrently addressing spatial and angular\naspects. MaskBlur consists of spatial blur and angular dropout two components.\nSpatial blur is governed by a spatial mask, which controls where pixels are\nblurred, i.e., pasting pixels between the low-resolution and high-resolution\ndomains. The angular mask is responsible for angular dropout, i.e., selecting\nwhich views to perform the spatial blur operation. By doing so, MaskBlur\nenables the model to treat pixels differently in the spatial and angular\ndomains when super-resolving LF images rather than blindly treating all pixels\nequally. Extensive experiments demonstrate the efficacy of MaskBlur in\nsignificantly enhancing the performance of existing SR methods. We further\nextend MaskBlur to other LF image tasks such as denoising, deblurring,\nlow-light enhancement, and real-world SR. Code is publicly available at\n\\url{https://github.com/chaowentao/MaskBlur}.\n","authors":["Wentao Chao","Fuqing Duan","Yulan Guo","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06478v1.pdf","comment":"accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2409.01003v2","updated":"2024-10-09T02:08:56Z","published":"2024-09-02T07:28:14Z","title":"Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian\n  Splatting for Dynamic Surgical Videos","summary":"  Reconstructing endoscopic videos is crucial for high-fidelity visualization\nand the efficiency of surgical operations. Despite the importance, existing 3D\nreconstruction methods encounter several challenges, including stringent\ndemands for accuracy, imprecise camera positioning, intricate dynamic scenes,\nand the necessity for rapid reconstruction. Addressing these issues, this paper\npresents the first camera-pose-free scene reconstruction framework, Free-DyGS,\ntailored for dynamic surgical videos, leveraging 3D Gaussian splatting\ntechnology. Our approach employs a frame-by-frame reconstruction strategy and\nis delineated into four distinct phases: Scene Initialization, Joint Learning,\nScene Expansion, and Retrospective Learning. We introduce a Generalizable\nGaussians Parameterization module within the Scene Initialization and Expansion\nphases to proficiently generate Gaussian attributes for each pixel from the\nRGBD frames. The Joint Learning phase is crafted to concurrently deduce scene\ndeformation and camera pose, facilitated by an innovative flexible deformation\nmodule. In the scene expansion stage, the Gaussian points gradually grow as the\ncamera moves. The Retrospective Learning phase is dedicated to enhancing the\nprecision of scene deformation through the reassessment of prior frames. The\nefficacy of the proposed Free-DyGS is substantiated through experiments on two\ndatasets: the StereoMIS and Hamlyn datasets. The experimental outcomes\nunderscore that Free-DyGS surpasses conventional baseline models in both\nrendering fidelity and computational efficiency.\n","authors":["Qian Li","Shuojue Yang","Daiyun Shen","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2409.01003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07840v3","updated":"2024-10-09T02:03:26Z","published":"2024-07-10T17:00:29Z","title":"Decompose and Compare Consistency: Measuring VLMs' Answer Reliability\n  via Task-Decomposition Consistency Comparison","summary":"  Despite tremendous advancements, current state-of-the-art Vision-Language\nModels (VLMs) are still far from perfect. They tend to hallucinate and may\ngenerate biased responses. In such circumstances, having a way to assess the\nreliability of a given response generated by a VLM is quite useful. Existing\nmethods, such as estimating uncertainty using answer likelihoods or\nprompt-based confidence generation, often suffer from overconfidence. Other\nmethods use self-consistency comparison but are affected by confirmation\nbiases. To alleviate these, we propose Decompose and Compare Consistency (DeCC)\nfor reliability measurement. By comparing the consistency between the direct\nanswer generated using the VLM's internal reasoning process, and the indirect\nanswers obtained by decomposing the question into sub-questions and reasoning\nover the sub-answers produced by the VLM, DeCC measures the reliability of\nVLM's direct answer. Experiments across six vision-language tasks with three\nVLMs show DeCC's reliability estimation achieves better correlation with task\naccuracy compared to the existing methods.\n","authors":["Qian Yang","Weixiang Yan","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.07840v3.pdf","comment":"Accepted to EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.06475v1","updated":"2024-10-09T02:01:05Z","published":"2024-10-09T02:01:05Z","title":"3D Representation Methods: A Survey","summary":"  The field of 3D representation has experienced significant advancements,\ndriven by the increasing demand for high-fidelity 3D models in various\napplications such as computer graphics, virtual reality, and autonomous\nsystems. This review examines the development and current state of 3D\nrepresentation methods, highlighting their research trajectories, innovations,\nstrength and weakness. Key techniques such as Voxel Grid, Point Cloud, Mesh,\nSigned Distance Function (SDF), Neural Radiance Field (NeRF), 3D Gaussian\nSplatting, Tri-Plane, and Deep Marching Tetrahedra (DMTet) are reviewed. The\nreview also introduces essential datasets that have been pivotal in advancing\nthe field, highlighting their characteristics and impact on research progress.\nFinally, we explore potential research directions that hold promise for further\nexpanding the capabilities and applications of 3D representation methods.\n","authors":["Zhengren Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06475v1.pdf","comment":"Preliminary Draft"},{"id":"http://arxiv.org/abs/2410.01110v2","updated":"2024-10-09T01:57:34Z","published":"2024-10-01T22:39:26Z","title":"RobustEMD: Domain Robust Matching for Cross-domain Few-shot Medical\n  Image Segmentation","summary":"  Few-shot medical image segmentation (FSMIS) aims to perform the limited\nannotated data learning in the medical image analysis scope. Despite the\nprogress has been achieved, current FSMIS models are all trained and deployed\non the same data domain, as is not consistent with the clinical reality that\nmedical imaging data is always across different data domains (e.g. imaging\nmodalities, institutions and equipment sequences). How to enhance the FSMIS\nmodels to generalize well across the different specific medical imaging\ndomains? In this paper, we focus on the matching mechanism of the few-shot\nsemantic segmentation models and introduce an Earth Mover's Distance (EMD)\ncalculation based domain robust matching mechanism for the cross-domain\nscenario. Specifically, we formulate the EMD transportation process between the\nforeground support-query features, the texture structure aware weights\ngeneration method, which proposes to perform the sobel based image gradient\ncalculation over the nodes, is introduced in the EMD matching flow to restrain\nthe domain relevant nodes. Besides, the point set level distance measurement\nmetric is introduced to calculated the cost for the transportation from support\nset nodes to query set nodes. To evaluate the performance of our model, we\nconduct experiments on three scenarios (i.e., cross-modal, cross-sequence and\ncross-institution), which includes eight medical datasets and involves three\nbody regions, and the results demonstrate that our model achieves the SoTA\nperformance against the compared models.\n","authors":["Yazhou Zhu","Minxian Li","Qiaolin Ye","Shidong Wang","Tong Xin","Haofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06468v1","updated":"2024-10-09T01:41:49Z","published":"2024-10-09T01:41:49Z","title":"Does Spatial Cognition Emerge in Frontier Models?","summary":"  Not yet. We present SPACE, a benchmark that systematically evaluates spatial\ncognition in frontier models. Our benchmark builds on decades of research in\ncognitive science. It evaluates large-scale mapping abilities that are brought\nto bear when an organism traverses physical environments, smaller-scale\nreasoning about object shapes and layouts, and cognitive infrastructure such as\nspatial attention and memory. For many tasks, we instantiate parallel\npresentations via text and images, allowing us to benchmark both large language\nmodels and large multimodal models. Results suggest that contemporary frontier\nmodels fall short of the spatial intelligence of animals, performing near\nchance level on a number of classic tests of animal cognition.\n","authors":["Santhosh Kumar Ramakrishnan","Erik Wijmans","Philipp Kraehenbuehl","Vladlen Koltun"],"pdf_url":"https://arxiv.org/pdf/2410.06468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01804v3","updated":"2024-10-09T01:25:35Z","published":"2024-10-02T17:59:09Z","title":"EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis","summary":"  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n","authors":["Alexander Mai","Peter Hedman","George Kopanas","Dor Verbin","David Futschik","Qiangeng Xu","Falko Kuester","Jonathan T. Barron","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01804v3.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/ever"},{"id":"http://arxiv.org/abs/2410.06456v1","updated":"2024-10-09T01:24:04Z","published":"2024-10-09T01:24:04Z","title":"From Generalist to Specialist: Adapting Vision Language Models via\n  Task-Specific Visual Instruction Tuning","summary":"  Large vision language models (VLMs) combine large language models with vision\nencoders, demonstrating promise across various tasks. However, they often\nunderperform in task-specific applications due to domain gaps between\npre-training and fine-tuning. We introduce VITask, a novel framework that\nenhances task-specific adaptability of VLMs by integrating task-specific models\n(TSMs). VITask employs three key strategies: exemplar prompting (EP), response\ndistribution alignment (RDA), and contrastive response tuning (CRT) to improve\nthe task-specific performance of VLMs by adjusting their response\ndistributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to\nadapt without TSMs during inference by learning from exemplar-prompted models.\nCRT further optimizes the ranking of correct image-response pairs, thereby\nreducing the risk of generating undesired responses. Experiments on 12 medical\ndiagnosis datasets across 9 imaging modalities show that VITask outperforms\nboth vanilla instruction-tuned VLMs and TSMs, showcasing its ability to\nintegrate complementary features from both models effectively. Additionally,\nVITask offers practical advantages such as flexible TSM integration and\nrobustness to incomplete instructions, making it a versatile and efficient\nsolution for task-specific VLM tuning. Our code are available at\nhttps://github.com/baiyang4/VITask.\n","authors":["Yang Bai","Yang Zhou","Jun Zhou","Rick Siow Mong Goh","Daniel Shu Wei Ting","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06446v1","updated":"2024-10-09T01:12:07Z","published":"2024-10-09T01:12:07Z","title":"Machine Unlearning in Forgettability Sequence","summary":"  Machine unlearning (MU) is becoming a promising paradigm to achieve the\n\"right to be forgotten\", where the training trace of any chosen data points\ncould be eliminated, while maintaining the model utility on general testing\nsamples after unlearning. With the advancement of forgetting research, many\nfundamental open questions remain unanswered: do different samples exhibit\nvarying levels of difficulty in being forgotten? Further, does the sequence in\nwhich samples are forgotten, determined by their respective difficulty levels,\ninfluence the performance of forgetting algorithms? In this paper, we identify\nkey factor affecting unlearning difficulty and the performance of unlearning\nalgorithms. We find that samples with higher privacy risks are more likely to\nbe unlearning, indicating that the unlearning difficulty varies among different\nsamples which motives a more precise unlearning mode. Built upon this insight,\nwe propose a general unlearning framework, dubbed RSU, which consists of\nRanking module and SeqUnlearn module.\n","authors":["Junjie Chen","Qian Chen","Jian Lou","Xiaoyu Zhang","Kai Wu","Zilong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.04435v4","updated":"2024-10-09T01:02:59Z","published":"2022-01-12T12:09:24Z","title":"Beyond the Visible: A Survey on Cross-spectral Face Recognition","summary":"  Cross-spectral face recognition (CFR) refers to recognizing individuals using\nface images stemming from different spectral bands, such as infrared versus\nvisible. While CFR is inherently more challenging than classical face\nrecognition due to significant variation in facial appearance caused by the\nmodality gap, it is useful in many scenarios including night-vision biometrics\nand detecting presentation attacks. Recent advances in deep neural networks\n(DNNs) have resulted in significant improvement in the performance of CFR\nsystems. Given these developments, the contributions of this survey are\nthree-fold. First, we provide an overview of CFR, by formalizing the CFR\nproblem and presenting related applications. Secondly, we discuss the\nappropriate spectral bands for face recognition and discuss recent CFR methods,\nplacing emphasis on deep neural networks. In particular we describe techniques\nthat have been proposed to extract and compare heterogeneous features emerging\nfrom different spectral bands. We also discuss the datasets that have been used\nfor evaluating CFR methods. Finally, we discuss the challenges and future lines\nof research on this topic.\n","authors":["David Anghelone","Cunjian Chen","Arun Ross","Antitza Dantcheva"],"pdf_url":"https://arxiv.org/pdf/2201.04435v4.pdf","comment":"Accepted by Neurocomputing"},{"id":"http://arxiv.org/abs/2410.03456v2","updated":"2024-10-09T01:01:34Z","published":"2024-10-04T14:14:28Z","title":"Dynamic Diffusion Transformer","summary":"  Diffusion Transformer (DiT), an emerging diffusion model for image\ngeneration, has demonstrated superior performance but suffers from substantial\ncomputational costs. Our investigations reveal that these costs stem from the\nstatic inference paradigm, which inevitably introduces redundant computation in\ncertain diffusion timesteps and spatial regions. To address this inefficiency,\nwe propose Dynamic Diffusion Transformer (DyDiT), an architecture that\ndynamically adjusts its computation along both timestep and spatial dimensions\nduring generation. Specifically, we introduce a Timestep-wise Dynamic Width\n(TDW) approach that adapts model width conditioned on the generation timesteps.\nIn addition, we design a Spatial-wise Dynamic Token (SDT) strategy to avoid\nredundant computation at unnecessary spatial locations. Extensive experiments\non various datasets and different-sized models verify the superiority of DyDiT.\nNotably, with <3% additional fine-tuning iterations, our method reduces the\nFLOPs of DiT-XL by 51%, accelerates generation by 1.73, and achieves a\ncompetitive FID score of 2.07 on ImageNet. The code is publicly available at\nhttps://github.com/NUS-HPC-AI-Lab/ Dynamic-Diffusion-Transformer.\n","authors":["Wangbo Zhao","Yizeng Han","Jiasheng Tang","Kai Wang","Yibing Song","Gao Huang","Fan Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2410.03456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13055v2","updated":"2024-10-09T00:57:02Z","published":"2024-08-23T13:27:27Z","title":"Atlas Gaussians Diffusion for 3D Generation","summary":"  Using the latent diffusion model has proven effective in developing novel 3D\ngeneration techniques. To harness the latent diffusion model, a key challenge\nis designing a high-fidelity and efficient representation that links the latent\nspace and the 3D space. In this paper, we introduce Atlas Gaussians, a novel\nrepresentation for feed-forward native 3D generation. Atlas Gaussians represent\na shape as the union of local patches, and each patch can decode 3D Gaussians.\nWe parameterize a patch as a sequence of feature vectors and design a learnable\nfunction to decode 3D Gaussians from the feature vectors. In this process, we\nincorporate UV-based sampling, enabling the generation of a sufficiently large,\nand theoretically infinite, number of 3D Gaussian points. The large amount of\n3D Gaussians enables the generation of high-quality details. Moreover, due to\nlocal awareness of the representation, the transformer-based decoding procedure\noperates on a patch level, ensuring efficiency. We train a variational\nautoencoder to learn the Atlas Gaussians representation, and then apply a\nlatent diffusion model on its latent space for learning 3D Generation.\nExperiments show that our approach outperforms the prior arts of feed-forward\nnative 3D generation.\n","authors":["Haitao Yang","Yuan Dong","Hanwen Jiang","Dejia Xu","Georgios Pavlakos","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2408.13055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06437v1","updated":"2024-10-09T00:45:02Z","published":"2024-10-09T00:45:02Z","title":"LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality","summary":"  Understanding human locomotion is crucial for AI agents such as robots,\nparticularly in complex indoor home environments. Modeling human trajectories\nin these spaces requires insight into how individuals maneuver around physical\nobstacles and manage social navigation dynamics. These dynamics include subtle\nbehaviors influenced by proxemics - the social use of space, such as stepping\naside to allow others to pass or choosing longer routes to avoid collisions.\nPrevious research has developed datasets of human motion in indoor scenes, but\nthese are often limited in scale and lack the nuanced social navigation\ndynamics common in home environments. To address this, we present LocoVR, a\ndataset of 7000+ two-person trajectories captured in virtual reality from over\n130 different indoor home environments. LocoVR provides full body pose data and\nprecise spatial information, along with rich examples of socially-motivated\nmovement behaviors. For example, the dataset captures instances of individuals\nnavigating around each other in narrow spaces, adjusting paths to respect\npersonal boundaries in living areas, and coordinating movements in high-traffic\nzones like entryways and kitchens. Our evaluation shows that LocoVR\nsignificantly enhances model performance in three practical indoor tasks\nutilizing human trajectories, and demonstrates predicting socially-aware\nnavigation patterns in home environments.\n","authors":["Kojiro Takeyama","Yimeng Liu","Misha Sra"],"pdf_url":"https://arxiv.org/pdf/2410.06437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09136v3","updated":"2024-10-09T00:06:37Z","published":"2024-03-14T07:21:46Z","title":"Biophysics Informed Pathological Regularisation for Brain Tumour\n  Segmentation","summary":"  Recent advances in deep learning have significantly improved brain tumour\nsegmentation techniques; however, the results still lack confidence and\nrobustness as they solely consider image data without biophysical priors or\npathological information. Integrating biophysics-informed regularisation is one\neffective way to change this situation, as it provides an prior regularisation\nfor automated end-to-end learning. In this paper, we propose a novel approach\nthat designs brain tumour growth Partial Differential Equation (PDE) models as\na regularisation with deep learning, operational with any network model. Our\nmethod introduces tumour growth PDE models directly into the segmentation\nprocess, improving accuracy and robustness, especially in data-scarce\nscenarios. This system estimates tumour cell density using a periodic\nactivation function. By effectively integrating this estimation with\nbiophysical models, we achieve better capture of tumour characteristics. This\napproach not only aligns the segmentation closer to actual biological behaviour\nbut also strengthens the model's performance under limited data conditions. We\ndemonstrate the effectiveness of our framework through extensive experiments on\nthe BraTS 2023 dataset, showcasing significant improvements in both precision\nand reliability of tumour segmentation.\n","authors":["Lipei Zhang","Yanqi Cheng","Lihao Liu","Carola-Bibiane Schnlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2403.09136v3.pdf","comment":"11 pages, 4 figures and 1 table. Accepted by MICCAI2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.07022v1","updated":"2024-10-09T16:05:16Z","published":"2024-10-09T16:05:16Z","title":"Exploiting Distribution Constraints for Scalable and Efficient Image\n  Retrieval","summary":"  Image retrieval is crucial in robotics and computer vision, with downstream\napplications in robot place recognition and vision-based product\nrecommendations. Modern retrieval systems face two key challenges: scalability\nand efficiency. State-of-the-art image retrieval systems train specific neural\nnetworks for each dataset, an approach that lacks scalability. Furthermore,\nsince retrieval speed is directly proportional to embedding size, existing\nsystems that use large embeddings lack efficiency. To tackle scalability,\nrecent works propose using off-the-shelf foundation models. However, these\nmodels, though applicable across datasets, fall short in achieving performance\ncomparable to that of dataset-specific models. Our key observation is that,\nwhile foundation models capture necessary subtleties for effective retrieval,\nthe underlying distribution of their embedding space can negatively impact\ncosine similarity searches. We introduce Autoencoders with Strong Variance\nConstraints (AE-SVC), which, when used for projection, significantly improves\nthe performance of foundation models. We provide an in-depth theoretical\nanalysis of AE-SVC. Addressing efficiency, we introduce Single-shot Similarity\nSpace Distillation ((SS)$_2$D), a novel approach to learn embeddings with\nadaptive sizes that offers a better trade-off between size and performance. We\nconducted extensive experiments on four retrieval datasets, including Stanford\nOnline Products (SoP) and Pittsburgh30k, using four different off-the-shelf\nfoundation models, including DinoV2 and CLIP. AE-SVC demonstrates up to a\n$16\\%$ improvement in retrieval performance, while (SS)$_2$D shows a further\n$10\\%$ improvement for smaller embedding sizes.\n","authors":["Mohammad Omama","Po-han Li","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2410.07022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06948v1","updated":"2024-10-09T14:45:43Z","published":"2024-10-09T14:45:43Z","title":"An Overview of zbMATH Open Digital Library","summary":"  Mathematical research thrives on the effective dissemination and discovery of\nknowledge.\n  zbMATH Open has emerged as a pivotal platform in this landscape, offering a\ncomprehensive repository of mathematical literature. Beyond indexing and\nabstracting, it serves as a unified quality-assured infrastructure for finding,\nevaluating, and connecting mathematical information that advances mathematical\nresearch as well as interdisciplinary exploration. zbMATH Open enables\nscientific quality control by post-publication reviews and promotes connections\nbetween researchers, institutions, and research outputs. This paper represents\nthe functionalities of the most significant features of this open-access\nservice, highlighting its role in shaping the future of mathematical\ninformation retrieval.\n","authors":["Madhurima Deb","Isabel Beckenbach","Matteo Petrera","Dariush Ehsani","Marcel Fuhrmann","Yun Hao","Olaf Teschke","Moritz Schubotz"],"pdf_url":"https://arxiv.org/pdf/2410.06948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09666v3","updated":"2024-10-09T11:48:18Z","published":"2022-05-19T16:29:17Z","title":"Personalized Prompt for Sequential Recommendation","summary":"  Pre-training models have shown their power in sequential recommendation.\nRecently, prompt has been widely explored and verified for tuning in NLP\npre-training, which could help to more effectively and efficiently extract\nuseful knowledge from pre-training models for downstream tasks, especially in\ncold-start scenarios. However, it is challenging to bring prompt-tuning from\nNLP to recommendation, since the tokens in recommendation (i.e., items) do not\nhave explicit explainable semantics, and the sequence modeling should be\npersonalized. In this work, we first introduces prompt to recommendation and\npropose a novel Personalized prompt-based recommendation (PPR) framework for\ncold-start recommendation. Specifically, we build the personalized soft prefix\nprompt via a prompt generator based on user profiles and enable a sufficient\ntraining of prompts via a prompt-oriented contrastive learning with both\nprompt- and behavior-based augmentations. We conduct extensive evaluations on\nvarious tasks. In both few-shot and zero-shot recommendation, PPR models\nachieve significant improvements over baselines on various metrics in three\nlarge-scale open datasets. We also conduct ablation tests and sparsity analysis\nfor a better understanding of PPR. Moreover, We further verify PPR's\nuniversality on different pre-training models, and conduct explorations on\nPPR's other promising downstream tasks including cross-domain recommendation\nand user profile prediction.\n","authors":["Yiqing Wu","Ruobing Xie","Yongchun Zhu","Fuzhen Zhuang","Xu Zhang","Leyu Lin","Qing He"],"pdf_url":"https://arxiv.org/pdf/2205.09666v3.pdf","comment":"accepted by TKDE"},{"id":"http://arxiv.org/abs/2405.00469v2","updated":"2024-10-09T08:52:33Z","published":"2024-05-01T12:12:59Z","title":"Exploiting Positional Bias for Query-Agnostic Generative Content in\n  Search","summary":"  In recent years, neural ranking models (NRMs) have been shown to\nsubstantially outperform their lexical counterparts in text retrieval. In\ntraditional search pipelines, a combination of features leads to well-defined\nbehaviour. However, as neural approaches become increasingly prevalent as the\nfinal scoring component of engines or as standalone systems, their robustness\nto malicious text and, more generally, semantic perturbation needs to be better\nunderstood. We posit that the transformer attention mechanism can induce\nexploitable defects through positional bias in search models, leading to an\nattack that could generalise beyond a single query or topic. We demonstrate\nsuch defects by showing that non-relevant text--such as promotional\ncontent--can be easily injected into a document without adversely affecting its\nposition in search results. Unlike previous gradient-based attacks, we\ndemonstrate these biases in a query-agnostic fashion. In doing so, without the\nknowledge of topicality, we can still reduce the negative effects of\nnon-relevant content injection by controlling injection position. Our\nexperiments are conducted with simulated on-topic promotional text\nautomatically generated by prompting LLMs with topical context from target\ndocuments. We find that contextualisation of a non-relevant text further\nreduces negative effects whilst likely circumventing existing content filtering\nmechanisms. In contrast, lexical models are found to be more resilient to such\ncontent injection attacks. We then investigate a simple yet effective\ncompensation for the weaknesses of the NRMs in search, validating our\nhypotheses regarding transformer bias.\n","authors":["Andrew Parry","Sean MacAvaney","Debasis Ganguly"],"pdf_url":"https://arxiv.org/pdf/2405.00469v2.pdf","comment":"8 pages, 4 main figures, 7 appendix pages, 2 appendix figures,\n  Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.11345v4","updated":"2024-10-09T08:50:06Z","published":"2024-08-21T05:09:53Z","title":"Learning Deep Tree-based Retriever for Efficient Recommendation: Theory\n  and Method","summary":"  Although advancements in deep learning have significantly enhanced the\nrecommendation accuracy of deep recommendation models, these methods still\nsuffer from low recommendation efficiency. Recently proposed tree-based deep\nrecommendation models alleviate the problem by directly learning tree structure\nand representations under the guidance of recommendation objectives. To\nguarantee the effectiveness of beam search for recommendation accuracy, these\nmodels strive to ensure that the tree adheres to the max-heap assumption, where\na parent node's preference should be the maximum among its children's\npreferences. However, they employ a one-versus-all strategy, framing the\ntraining task as a series of independent binary classification objectives for\neach node, which limits their ability to fully satisfy the max-heap assumption.\nTo this end, we propose a Deep Tree-based Retriever (DTR for short) for\nefficient recommendation. DTR frames the training task as a softmax-based\nmulti-class classification over tree nodes at the same level, enabling explicit\nhorizontal competition and more discriminative top-k selection among them,\nwhich mimics the beam search behavior during training. To mitigate the\nsuboptimality induced by the labeling of non-leaf nodes, we propose a\nrectification method for the loss function, which further aligns with the\nmax-heap assumption in expectation. As the number of tree nodes grows\nexponentially with the levels, we employ sampled softmax to approximate\noptimization and thereby enhance efficiency. Furthermore, we propose a\ntree-based sampling method to reduce the bias inherent in sampled softmax.\nTheoretical results reveal DTR's generalization capability, and both the\nrectification method and tree-based sampling contribute to improved\ngeneralization. The experiments are conducted on four real-world datasets,\nvalidating the effectiveness of the proposed method.\n","authors":["Ze Liu","Jin Zhang","Chao Feng","Defu Lian","Jie Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.11345v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06096v3","updated":"2024-10-09T08:11:47Z","published":"2024-09-09T22:16:48Z","title":"Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer","summary":"  Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred.\n","authors":["Michele Mancusi","Yurii Halychanskyi","Kin Wai Cheuk","Chieh-Hsin Lai","Stefan Uhlich","Junghyun Koo","Marco A. Martnez-Ramrez","Wei-Hsiang Liao","Giorgio Fabbro","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2409.06096v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06654v1","updated":"2024-10-09T08:06:15Z","published":"2024-10-09T08:06:15Z","title":"Performance Evaluation in Multimedia Retrieval","summary":"  Performance evaluation in multimedia retrieval, as in the information\nretrieval domain at large, relies heavily on retrieval experiments, employing a\nbroad range of techniques and metrics. These can involve human-in-the-loop and\nmachine-only settings for the retrieval process itself and the subsequent\nverification of results. Such experiments can be elaborate and\nuse-case-specific, which can make them difficult to compare or replicate. In\nthis paper, we present a formal model to express all relevant aspects of such\nretrieval experiments, as well as a flexible open-source evaluation\ninfrastructure that implements the model. These contributions intend to make a\nstep towards lowering the hurdles for conducting retrieval experiments and\nimproving their reproducibility.\n","authors":["Loris Sauter","Ralph Gasser","Heiko Schuldt","Abraham Bernstein","Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2410.06654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06628v1","updated":"2024-10-09T07:23:02Z","published":"2024-10-09T07:23:02Z","title":"Does Vec2Text Pose a New Corpus Poisoning Threat?","summary":"  The emergence of Vec2Text -- a method for text embedding inversion -- has\nraised serious privacy concerns for dense retrieval systems which use text\nembeddings. This threat comes from the ability for an attacker with access to\nembeddings to reconstruct the original text. In this paper, we take a new look\nat Vec2Text and investigate how much of a threat it poses to the different\nattacks of corpus poisoning, whereby an attacker injects adversarial passages\ninto a retrieval corpus with the intention of misleading dense retrievers.\nTheoretically, Vec2Text is far more dangerous than previous attack methods\nbecause it does not need access to the embedding model's weights and it can\nefficiently generate many adversarial passages. We show that under certain\nconditions, corpus poisoning with Vec2Text can pose a serious threat to dense\nretriever system integrity and user experience by injecting adversarial\npassaged into top ranked positions. Code and data are made available at\nhttps://github.com/ielab/vec2text-corpus-poisoning\n","authors":["Shengyao Zhuang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2410.06628v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.12784"},{"id":"http://arxiv.org/abs/2410.04927v2","updated":"2024-10-09T07:17:03Z","published":"2024-10-07T11:19:05Z","title":"FELLAS: Enhancing Federated Sequential Recommendation with LLM as\n  External Services","summary":"  Federated sequential recommendation (FedSeqRec) has gained growing attention\ndue to its ability to protect user privacy. Unfortunately, the performance of\nFedSeqRec is still unsatisfactory because the models used in FedSeqRec have to\nbe lightweight to accommodate communication bandwidth and clients' on-device\ncomputational resource constraints. Recently, large language models (LLMs) have\nexhibited strong transferable and generalized language understanding abilities\nand therefore, in the NLP area, many downstream tasks now utilize LLMs as a\nservice to achieve superior performance without constructing complex models.\nInspired by this successful practice, we propose a generic FedSeqRec framework,\nFELLAS, which aims to enhance FedSeqRec by utilizing LLMs as an external\nservice. Specifically, FELLAS employs an LLM server to provide both item-level\nand sequence-level representation assistance. The item-level representation\nservice is queried by the central server to enrich the original ID-based item\nembedding with textual information, while the sequence-level representation\nservice is accessed by each client. However, invoking the sequence-level\nrepresentation service requires clients to send sequences to the external LLM\nserver. To safeguard privacy, we implement dx-privacy satisfied sequence\nperturbation, which protects clients' sensitive data with guarantees.\nAdditionally, a contrastive learning-based method is designed to transfer\nknowledge from the noisy sequence representation to clients' sequential\nrecommendation models. Furthermore, to empirically validate the privacy\nprotection capability of FELLAS, we propose two interacted item inference\nattacks. Extensive experiments conducted on three datasets with two widely used\nsequential recommendation models demonstrate the effectiveness and\nprivacy-preserving capability of FELLAS.\n","authors":["Wei Yuan","Chaoqun Yang","Guanhua Ye","Tong Chen","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.04927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v1","updated":"2024-10-09T07:14:49Z","published":"2024-10-09T07:14:49Z","title":"Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video\n  Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01848v2","updated":"2024-10-09T06:32:41Z","published":"2024-05-03T04:43:24Z","title":"RankSHAP: Shapley Value Based Feature Attributions for Learning to Rank","summary":"  Numerous works propose post-hoc, model-agnostic explanations for learning to\nrank, focusing on ordering entities by their relevance to a query through\nfeature attribution methods. However, these attributions often weakly correlate\nor contradict each other, confusing end users. We adopt an axiomatic\ngame-theoretic approach, popular in the feature attribution community, to\nidentify a set of fundamental axioms that every ranking-based feature\nattribution method should satisfy. We then introduce Rank-SHAP, extending\nclassical Shapley values to ranking. We evaluate the RankSHAP framework through\nextensive experiments on two datasets, multiple ranking methods and evaluation\nmetrics. Additionally, a user study confirms RankSHAP's alignment with human\nintuition. We also perform an axiomatic analysis of existing rank attribution\nalgorithms to determine their compliance with our proposed axioms. Ultimately,\nour aim is to equip practitioners with a set of axiomatically backed feature\nattribution methods for studying IR ranking models, that ensure generality as\nwell as consistency.\n","authors":["Tanya Chowdhury","Yair Zick","James Allan"],"pdf_url":"https://arxiv.org/pdf/2405.01848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06581v1","updated":"2024-10-09T06:26:39Z","published":"2024-10-09T06:26:39Z","title":"Enhancing Legal Case Retrieval via Scaling High-quality Synthetic\n  Query-Candidate Pairs","summary":"  Legal case retrieval (LCR) aims to provide similar cases as references for a\ngiven fact description. This task is crucial for promoting consistent judgments\nin similar cases, effectively enhancing judicial fairness and improving work\nefficiency for judges. However, existing works face two main challenges for\nreal-world applications: existing works mainly focus on case-to-case retrieval\nusing lengthy queries, which does not match real-world scenarios; and the\nlimited data scale, with current datasets containing only hundreds of queries,\nis insufficient to satisfy the training requirements of existing data-hungry\nneural models. To address these issues, we introduce an automated method to\nconstruct synthetic query-candidate pairs and build the largest LCR dataset to\ndate, LEAD, which is hundreds of times larger than existing datasets. This data\nconstruction method can provide ample training signals for LCR models.\nExperimental results demonstrate that model training with our constructed data\ncan achieve state-of-the-art results on two widely-used LCR benchmarks.\nBesides, the construction method can also be applied to civil cases and achieve\npromising results. The data and codes can be found in\nhttps://github.com/thunlp/LEAD.\n","authors":["Cheng Gao","Chaojun Xiao","Zhenghao Liu","Huimin Chen","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.06581v1.pdf","comment":"15 pages, 3 figures, accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06536v1","updated":"2024-10-09T04:20:15Z","published":"2024-10-09T04:20:15Z","title":"Learning Recommender Systems with Soft Target: A Decoupled Perspective","summary":"  Learning recommender systems with multi-class optimization objective is a\nprevalent setting in recommendation. However, as observed user feedback often\naccounts for a tiny fraction of the entire item pool, the standard Softmax loss\ntends to ignore the difference between potential positive feedback and truly\nnegative feedback. To address this challenge, we propose a novel decoupled soft\nlabel optimization framework to consider the objectives as two aspects by\nleveraging soft labels, including target confidence and the latent interest\ndistribution of non-target items. Futhermore, based on our carefully\ntheoretical analysis, we design a decoupled loss function to flexibly adjust\nthe importance of these two aspects. To maximize the performance of the\nproposed method, we additionally present a sensible soft-label generation\nalgorithm that models a label propagation algorithm to explore users' latent\ninterests in unobserved feedback via neighbors. We conduct extensive\nexperiments on various recommendation system models and public datasets, the\nresults demonstrate the effectiveness and generality of the proposed method.\n","authors":["Hao Zhang","Mingyue Cheng","Qi Liu","Yucong Luo","Rui Li","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06536v1.pdf","comment":"Accepted by DASFAA 2024"},{"id":"http://arxiv.org/abs/2406.13941v2","updated":"2024-10-09T04:11:28Z","published":"2024-06-20T02:20:21Z","title":"UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture","summary":"  Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.\n","authors":["Sitian Chen","Haobin Tan","Amelie Chi Zhou","Yusen Li","Pavan Balaji"],"pdf_url":"https://arxiv.org/pdf/2406.13941v2.pdf","comment":"Accepted by DAC 2024"},{"id":"http://arxiv.org/abs/2410.06497v1","updated":"2024-10-09T02:51:27Z","published":"2024-10-09T02:51:27Z","title":"ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System","summary":"  The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.\n","authors":["Fang Zhou","Yaning Huang","Dong Liang","Dai Li","Zhongke Zhang","Kai Wang","Xiao Xin","Abdallah Aboelela","Zheliang Jiang","Yang Wang","Jeff Song","Wei Zhang","Chen Liang","Huayu Li","ChongLin Sun","Hang Yang","Lei Qu","Zhan Shu","Mindi Yuan","Emanuele Maccherani","Taha Hayat","John Guo","Varna Puvvada","Uladzimir Pashkevich"],"pdf_url":"https://arxiv.org/pdf/2410.06497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12218v2","updated":"2024-10-09T02:27:04Z","published":"2023-09-20T14:59:15Z","title":"SR-PredictAO: Session-based Recommendation with High-Capability\n  Predictor Add-On","summary":"  Session-based recommendation, aiming at making the prediction of the user's\nnext item click based on the information in a single session only, even in the\npresence of some random user's behavior, is a complex problem. This complex\nproblem requires a high-capability model of predicting the user's next action.\nMost (if not all) existing models follow the encoder-predictor paradigm where\nall studies focus on how to optimize the encoder module extensively in the\nparadigm, but they overlook how to optimize the predictor module. In this\npaper, we discover the critical issue of the low-capability predictor module\namong existing models. Motivated by this, we propose a novel framework called\n*Session-based Recommendation with Predictor Add-On* (SR-PredictAO). In this\nframework, we propose a high-capability predictor module which could alleviate\nthe effect of random user's behavior for prediction. It is worth mentioning\nthat this framework could be applied to any existing models, which could give\nopportunities for further optimizing the framework. Extensive experiments on\ntwo real-world benchmark datasets for three state-of-the-art models show that\n*SR-PredictAO* out-performs the current state-of-the-art model by up to 2.9% in\nHR@20 and 2.3% in MRR@20. More importantly, the improvement is consistent\nacross almost all the existing models on all datasets, and is statistically\nsignificant, which could be regarded as a significant contribution in the\nfield.\n","authors":["Ruida Wang","Raymond Chi-Wing Wong","Weile Tan"],"pdf_url":"https://arxiv.org/pdf/2309.12218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06443v1","updated":"2024-10-09T01:06:00Z","published":"2024-10-09T01:06:00Z","title":"Categorizing Social Media Screenshots for Identifying Author\n  Misattribution","summary":"  Mis/disinformation is a common and dangerous occurrence on social media.\nMisattribution is a form of mis/disinformation that deals with a false claim of\nauthorship, which means a user is claiming someone said (posted) something they\nnever did. We discuss the difference between misinformation and disinformation\nand how screenshots are used to spread author misattribution on social media\nplatforms. It is important to be able to find the original post of a screenshot\nto determine if the screenshot is being correctly attributed. To do this we\nhave built several tools to aid in automating this search process. The first is\na Python script that aims to categorize Twitter posts based on their structure,\nextract the metadata from a screenshot, and use this data to group all the\nposts within a screenshot together. We tested this process on 75 Twitter posts\ncontaining screenshots collected by hand to determine how well the script\nextracted metadata and grouped the individual posts, F1 = 0.80. The second is a\nseries of scrapers being used to collect a dataset that can train and test a\nmodel to differentiate between various social media platforms. We collected\n16,620 screenshots have been collected from Facebook, Instagram, Truth Social,\nand Twitter. Screenshots were taken by the scrapers of the web version and\nmobile version of each platform in both light and dark mode.\n","authors":["Ashlyn M. Farris","Michael L. Nelson"],"pdf_url":"https://arxiv.org/pdf/2410.06443v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.07177v1","updated":"2024-10-09T17:59:59Z","published":"2024-10-09T17:59:59Z","title":"MM-Ego: Towards Building Egocentric Multimodal LLMs","summary":"  This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we develop a data engine that efficiently generates 7M\nhigh-quality QA samples for egocentric videos ranging from 30 seconds to one\nhour long, based on human-annotated data. This is currently the largest\negocentric QA dataset. Second, we contribute a challenging egocentric QA\nbenchmark with 629 videos and 7,026 questions to evaluate the models' ability\nin recognizing and memorizing visual details across videos of varying lengths.\nWe introduce a new de-biasing evaluation method to help mitigate the\nunavoidable language bias present in the models being evaluated. Third, we\npropose a specialized multimodal architecture featuring a novel \"Memory Pointer\nPrompting\" mechanism. This design includes a global glimpse step to gain an\noverarching understanding of the entire video and identify key visual\ninformation, followed by a fallback step that utilizes the key visual\ninformation to generate responses. This enables the model to more effectively\ncomprehend extended video content. With the data, benchmark, and model, we\nsuccessfully build MM-Ego, an egocentric multimodal LLM that shows powerful\nperformance on egocentric video understanding.\n","authors":["Hanrong Ye","Haotian Zhang","Erik Daxberger","Lin Chen","Zongyu Lin","Yanghao Li","Bowen Zhang","Haoxuan You","Dan Xu","Zhe Gan","Jiasen Lu","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.07177v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.07176v1","updated":"2024-10-09T17:59:58Z","published":"2024-10-09T17:59:58Z","title":"Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models","summary":"  Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.\n","authors":["Fei Wang","Xingchen Wan","Ruoxi Sun","Jiefeng Chen","Sercan . Ark"],"pdf_url":"https://arxiv.org/pdf/2410.07176v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.07174v1","updated":"2024-10-09T17:59:45Z","published":"2024-10-09T17:59:45Z","title":"Neural Circuit Architectural Priors for Quadruped Locomotion","summary":"  Learning-based approaches to quadruped locomotion commonly adopt generic\npolicy architectures like fully connected MLPs. As such architectures contain\nfew inductive biases, it is common in practice to incorporate priors in the\nform of rewards, training curricula, imitation data, or trajectory generators.\nIn nature, animals are born with priors in the form of their nervous system's\narchitecture, which has been shaped by evolution to confer innate ability and\nefficient learning. For instance, a horse can walk within hours of birth and\ncan quickly improve with practice. Such architectural priors can also be useful\nin ANN architectures for AI. In this work, we explore the advantages of a\nbiologically inspired ANN architecture for quadruped locomotion based on neural\ncircuits in the limbs and spinal cord of mammals. Our architecture achieves\ngood initial performance and comparable final performance to MLPs, while using\nless data and orders of magnitude fewer parameters. Our architecture also\nexhibits better generalization to task variations, even admitting deployment on\na physical robot without standard sim-to-real methods. This work shows that\nneural circuits can provide valuable architectural priors for locomotion and\nencourages future work in other sensorimotor skills.\n","authors":["Nikhil X. Bhattasali","Venkatesh Pattabiraman","Lerrel Pinto","Grace W. Lindsay"],"pdf_url":"https://arxiv.org/pdf/2410.07174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07172v1","updated":"2024-10-09T17:59:14Z","published":"2024-10-09T17:59:14Z","title":"Glider: Global and Local Instruction-Driven Expert Router","summary":"  The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to particular domains. This\nhas enabled the creation of powerful and adaptive routing-based \"Model\nMoErging\" methods with the goal of using expert modules to create an aggregate\nsystem with improved performance or generalization. However, existing MoErging\nmethods often prioritize generalization to unseen tasks at the expense of\nperformance on held-in tasks, which limits its practical applicability in\nreal-world deployment scenarios. We observe that current token-level routing\nmechanisms neglect the global semantic context of the input task. This\ntoken-wise independence hinders effective expert selection for held-in tasks,\nas routing decisions fail to incorporate the semantic properties of the task.\nTo address this, we propose, Global and Local Instruction Driven Expert Router\n(GLIDER) that integrates a multi-scale routing mechanism, encompassing a\nsemantic global router and a learned local router. The global router leverages\nLLM's advanced reasoning capabilities for semantic-related contexts to enhance\nexpert selection. Given the input query and LLM, the router generates semantic\ntask instructions that guide the retrieval of the most relevant experts across\nall layers. This global guidance is complemented by a local router that\nfacilitates token-level routing decisions within each module, enabling finer\ncontrol and enhanced performance on unseen tasks. Our experiments using\nT5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves\nsubstantially improved held-in performance while maintaining strong\ngeneralization on held-out tasks. We also perform ablations experiments to dive\ndeeper into the components of GLIDER. Our experiments highlight the importance\nof our multi-scale routing that leverages LLM-driven semantic reasoning for\nMoErging methods.\n","authors":["Pingzhi Li","Prateek Yadav","Jaehong Yoon","Jie Peng","Yi-Lin Sung","Mohit Bansal","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07172v1.pdf","comment":"Our code is available at https://github.com/UNITES-Lab/glider"},{"id":"http://arxiv.org/abs/2410.07170v1","updated":"2024-10-09T17:59:06Z","published":"2024-10-09T17:59:06Z","title":"One Initialization to Rule them All: Fine-tuning via Explained Variance\n  Adaptation","summary":"  Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across model\nweights. Recent works focus on weight-driven initialization or learning of\nadaptive ranks during training. Both approaches have only been investigated in\nisolation, resulting in slow convergence or a uniform rank distribution, in\nturn leading to sub-optimal performance. We propose to enhance LoRA by\ninitializing the new weights in a data-driven manner by computing singular\nvalue decomposition on minibatches of activation vectors. Then, we initialize\nthe LoRA matrices with the obtained right-singular vectors and re-distribute\nranks among all weight matrices to explain the maximal amount of variance and\ncontinue the standard LoRA fine-tuning procedure. This results in our new\nmethod Explained Variance Adaptation (EVA). We apply EVA to a variety of\nfine-tuning tasks ranging from language generation and understanding to image\nclassification and reinforcement learning. EVA exhibits faster convergence than\ncompetitors and attains the highest average score across a multitude of tasks\nper domain.\n","authors":["Fabian Paischer","Lukas Hauzenberger","Thomas Schmied","Benedikt Alkin","Marc Peter Deisenroth","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2410.07170v1.pdf","comment":"10 pages + references and appendix, code available at\n  https://github.com/ml-jku/EVA"},{"id":"http://arxiv.org/abs/2410.07166v1","updated":"2024-10-09T17:59:00Z","published":"2024-10-09T17:59:00Z","title":"Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making","summary":"  We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.\n","authors":["Manling Li","Shiyu Zhao","Qineng Wang","Kangrui Wang","Yu Zhou","Sanjana Srivastava","Cem Gokmen","Tony Lee","Li Erran Li","Ruohan Zhang","Weiyu Liu","Percy Liang","Li Fei-Fei","Jiayuan Mao","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.07166v1.pdf","comment":"Accepted for oral presentation at NeurIPS 2024 in the Datasets and\n  Benchmarks track"},{"id":"http://arxiv.org/abs/2404.01814v2","updated":"2024-10-09T17:58:59Z","published":"2024-04-02T10:16:30Z","title":"A neural network-based approach to hybrid systems identification for\n  control","summary":"  We consider the problem of designing a machine learning-based model of an\nunknown dynamical system from a finite number of (state-input)-successor state\ndata points, such that the model obtained is also suitable for optimal control\ndesign. We adopt a neural network (NN) architecture that, once suitably\ntrained, yields a hybrid system with continuous piecewise-affine (PWA) dynamics\nthat is differentiable with respect to the network's parameters, thereby\nenabling the use of derivative-based training procedures. We show that a\ncareful choice of our NN's weights produces a hybrid system model with\nstructural properties that are highly favorable when used as part of a finite\nhorizon optimal control problem (OCP). Specifically, we rely on available\nresults to establish that optimal solutions with strong local optimality\nguarantees can be computed via nonlinear programming (NLP), in contrast to\nclassical OCPs for general hybrid systems which typically require mixed-integer\noptimization. Besides being well-suited for optimal control design, numerical\nsimulations illustrate that our NN-based technique enjoys very similar\nperformance to state-of-the-art system identification methods for hybrid\nsystems and it is competitive on nonlinear benchmarks.\n","authors":["Filippo Fabiani","Bartolomeo Stellato","Daniele Masti","Paul J. Goulart"],"pdf_url":"https://arxiv.org/pdf/2404.01814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07163v1","updated":"2024-10-09T17:58:12Z","published":"2024-10-09T17:58:12Z","title":"Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning","summary":"  In this work, we address the problem of large language model (LLM)\nunlearning, aiming to remove unwanted data influences and associated model\ncapabilities (e.g., copyrighted data or harmful content generation) while\npreserving essential model utilities, without the need for retraining from\nscratch. Despite the growing need for LLM unlearning, a principled optimization\nframework remains lacking. To this end, we revisit the state-of-the-art\napproach, negative preference optimization (NPO), and identify the issue of\nreference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a\nsimple yet effective unlearning optimization framework, called SimNPO, showing\nthat 'simplicity' in removing the reliance on a reference model (through the\nlens of simple preference optimization) benefits unlearning. We also provide\ndeeper insights into SimNPO's advantages, supported by analysis using mixtures\nof Markov chains. Furthermore, we present extensive experiments validating\nSimNPO's superiority over existing unlearning baselines in benchmarks like TOFU\nand MUSE, and robustness against relearning attacks. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Simple.\n","authors":["Chongyu Fan","Jiancheng Liu","Licong Lin","Jinghan Jia","Ruiqi Zhang","Song Mei","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.07163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07158v1","updated":"2024-10-09T17:56:41Z","published":"2024-10-09T17:56:41Z","title":"Quanda: An Interpretability Toolkit for Training Data Attribution\n  Evaluation and Beyond","summary":"  In recent years, training data attribution (TDA) methods have emerged as a\npromising direction for the interpretability of neural networks. While research\naround TDA is thriving, limited effort has been dedicated to the evaluation of\nattributions. Similar to the development of evaluation metrics for traditional\nfeature attribution approaches, several standalone metrics have been proposed\nto evaluate the quality of TDA methods across various contexts. However, the\nlack of a unified framework that allows for systematic comparison limits trust\nin TDA methods and stunts their widespread adoption. To address this research\ngap, we introduce Quanda, a Python toolkit designed to facilitate the\nevaluation of TDA methods. Beyond offering a comprehensive set of evaluation\nmetrics, Quanda provides a uniform interface for seamless integration with\nexisting TDA implementations across different repositories, thus enabling\nsystematic benchmarking. The toolkit is user-friendly, thoroughly tested,\nwell-documented, and available as an open-source library on PyPi and under\nhttps://github.com/dilyabareeva/quanda.\n","authors":["Dilyara Bareeva","Galip mit Yolcu","Anna Hedstrm","Niklas Schmolenski","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2410.07158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07157v1","updated":"2024-10-09T17:56:15Z","published":"2024-10-09T17:56:15Z","title":"InstructG2I: Synthesizing Images from Multimodal Attributed Graphs","summary":"  In this paper, we approach an overlooked yet critical task Graph2Image:\ngenerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To\naddress these challenges, we propose a graph context-conditioned diffusion\nmodel called InstructG2I. InstructG2I first exploits the graph structure and\nmultimodal information to conduct informative neighbor sampling by combining\npersonalized page rank and re-ranking based on vision-language features. Then,\na Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary\nset of graph prompts to guide the denoising process of diffusion. Finally, we\npropose graph classifier-free guidance, enabling controllable generation by\nvarying the strength of graph guidance and multiple connected edges to a node.\nExtensive experiments conducted on three datasets from different domains\ndemonstrate the effectiveness and controllability of our approach. The code is\navailable at https://github.com/PeterGriffinJin/InstructG2I.\n","authors":["Bowen Jin","Ziqi Pang","Bingjun Guo","Yu-Xiong Wang","Jiaxuan You","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2410.07157v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.07153v1","updated":"2024-10-09T17:55:43Z","published":"2024-10-09T17:55:43Z","title":"CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based\n  Multi-Entity Action Recognition","summary":"  Skeleton-based multi-entity action recognition is a challenging task aiming\nto identify interactive actions or group activities involving multiple diverse\nentities. Existing models for individuals often fall short in this task due to\nthe inherent distribution discrepancies among entity skeletons, leading to\nsuboptimal backbone optimization. To this end, we introduce a Convex Hull\nAdaptive Shift based multi-Entity action recognition method (CHASE), which\nmitigates inter-entity distribution gaps and unbiases subsequent backbones.\nSpecifically, CHASE comprises a learnable parameterized network and an\nauxiliary objective. The parameterized network achieves plausible,\nsample-adaptive repositioning of skeleton sequences through two key components.\nFirst, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new\norigin of the coordinate system is within the skeleton convex hull. Second, the\nCoefficient Learning Block provides a lightweight parameterization of the\nmapping from skeleton sequences to their specific coefficients in convex\ncombinations. Moreover, to guide the optimization of this network for\ndiscrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean\nDiscrepancy as the additional objective. CHASE operates as a sample-adaptive\nnormalization method to mitigate inter-entity distribution discrepancies,\nthereby reducing data bias and improving the subsequent classifier's\nmulti-entity action recognition performance. Extensive experiments on six\ndatasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and\nVolleyball, consistently verify our approach by seamlessly adapting to\nsingle-entity backbones and boosting their performance in multi-entity\nscenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .\n","authors":["Yuhang Wen","Mengyuan Liu","Songtao Wu","Beichen Ding"],"pdf_url":"https://arxiv.org/pdf/2410.07153v1.pdf","comment":"NeurIPS 2024 Camera-ready Version"},{"id":"http://arxiv.org/abs/2410.07149v1","updated":"2024-10-09T17:55:02Z","published":"2024-10-09T17:55:02Z","title":"Towards Interpreting Visual Information Processing in Vision-Language\n  Models","summary":"  Vision-Language Models (VLMs) are powerful tools for processing and\nunderstanding text and images. We study the processing of visual tokens in the\nlanguage model component of LLaVA, a prominent VLM. Our approach focuses on\nanalyzing the localization of object information, the evolution of visual token\nrepresentations across layers, and the mechanism of integrating visual\ninformation for predictions. Through ablation studies, we demonstrated that\nobject identification accuracy drops by over 70\\% when object-specific tokens\nare removed. We observed that visual token representations become increasingly\ninterpretable in the vocabulary space across layers, suggesting an alignment\nwith textual tokens corresponding to image content. Finally, we found that the\nmodel extracts object information from these refined representations at the\nlast token position for prediction, mirroring the process in text-only language\nmodels for factual association tasks. These findings provide crucial insights\ninto how VLMs process and integrate visual information, bridging the gap\nbetween our understanding of language and vision models, and paving the way for\nmore interpretable and controllable multimodal systems.\n","authors":["Clement Neo","Luke Ong","Philip Torr","Mor Geva","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.07149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07145v1","updated":"2024-10-09T17:54:28Z","published":"2024-10-09T17:54:28Z","title":"Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling","summary":"  One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.\n","authors":["Yingfa Chen","Xinrong Zhang","Shengding Hu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.07145v1.pdf","comment":"21 pages, 18 figures"},{"id":"http://arxiv.org/abs/2410.07137v1","updated":"2024-10-09T17:53:06Z","published":"2024-10-09T17:53:06Z","title":"Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates","summary":"  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.\n","authors":["Xiaosen Zheng","Tianyu Pang","Chao Du","Qian Liu","Jing Jiang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.07137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13684v2","updated":"2024-10-09T17:47:01Z","published":"2024-09-20T17:53:03Z","title":"The FIX Benchmark: Extracting Features Interpretable to eXperts","summary":"  Feature-based methods are commonly used to explain model predictions, but\nthese methods often implicitly assume that interpretable features are readily\navailable. However, this is often not the case for high-dimensional data, and\nit can be hard even for domain experts to mathematically specify which features\nare important. Can we instead automatically extract collections or groups of\nfeatures that are aligned with expert knowledge? To address this gap, we\npresent FIX (Features Interpretable to eXperts), a benchmark for measuring how\nwell a collection of features aligns with expert knowledge. In collaboration\nwith domain experts, we propose FIXScore, a unified expert alignment measure\napplicable to diverse real-world settings across cosmology, psychology, and\nmedicine domains in vision, language and time series data modalities. With\nFIXScore, we find that popular feature-based explanation methods have poor\nalignment with expert-specified knowledge, highlighting the need for new\nmethods that can better identify features interpretable to experts.\n","authors":["Helen Jin","Shreya Havaldar","Chaehyeon Kim","Anton Xue","Weiqiu You","Helen Qu","Marco Gatti","Daniel A Hashimoto","Bhuvnesh Jain","Amin Madani","Masao Sako","Lyle Ungar","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2409.13684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07110v1","updated":"2024-10-09T17:45:47Z","published":"2024-10-09T17:45:47Z","title":"Continual Learning: Less Forgetting, More OOD Generalization via\n  Adaptive Contrastive Replay","summary":"  Machine learning models often suffer from catastrophic forgetting of\npreviously learned knowledge when learning new classes. Various methods have\nbeen proposed to mitigate this issue. However, rehearsal-based learning, which\nretains samples from previous classes, typically achieves good performance but\ntends to memorize specific instances, struggling with Out-of-Distribution (OOD)\ngeneralization. This often leads to high forgetting rates and poor\ngeneralization. Surprisingly, the OOD generalization capabilities of these\nmethods have been largely unexplored. In this paper, we highlight this issue\nand propose a simple yet effective strategy inspired by contrastive learning\nand data-centric principles to address it. We introduce Adaptive Contrastive\nReplay (ACR), a method that employs dual optimization to simultaneously train\nboth the encoder and the classifier. ACR adaptively populates the replay buffer\nwith misclassified samples while ensuring a balanced representation of classes\nand tasks. By refining the decision boundary in this way, ACR achieves a\nbalance between stability and plasticity. Our method significantly outperforms\nprevious approaches in terms of OOD generalization, achieving an improvement of\n13.41\\% on Split CIFAR-100, 9.91\\% on Split Mini-ImageNet, and 5.98\\% on Split\nTiny-ImageNet.\n","authors":["Hossein Rezaei","Mohammad Sabokrou"],"pdf_url":"https://arxiv.org/pdf/2410.07110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12108v2","updated":"2024-10-09T17:45:07Z","published":"2024-07-16T18:28:40Z","title":"Private prediction for large-scale synthetic text generation","summary":"  We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.\n","authors":["Kareem Amin","Alex Bie","Weiwei Kong","Alexey Kurakin","Natalia Ponomareva","Umar Syed","Andreas Terzis","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2407.12108v2.pdf","comment":"20 pages; updated figure + some new experiments from EMNLP 2024\n  findings camera-ready"},{"id":"http://arxiv.org/abs/2403.11001v2","updated":"2024-10-09T17:44:14Z","published":"2024-03-16T19:11:57Z","title":"Topologically Faithful Multi-class Segmentation in Medical Images","summary":"  Topological accuracy in medical image segmentation is a highly important\nproperty for downstream applications such as network analysis and flow modeling\nin vessels or cell counting. Recently, significant methodological advancements\nhave brought well-founded concepts from algebraic topology to binary\nsegmentation. However, these approaches have been underexplored in multi-class\nsegmentation scenarios, where topological errors are common. We propose a\ngeneral loss function for topologically faithful multi-class segmentation\nextending the recent Betti matching concept, which is based on induced\nmatchings of persistence barcodes. We project the N-class segmentation problem\nto N single-class segmentation tasks, which allows us to use 1-parameter\npersistent homology, making training of neural networks computationally\nfeasible. We validate our method on a comprehensive set of four medical\ndatasets with highly variant topological characteristics. Our loss formulation\nsignificantly enhances topological correctness in cardiac, cell, artery-vein,\nand Circle of Willis segmentation.\n","authors":["Alexander H. Berger","Nico Stucki","Laurin Lux","Vincent Buergin","Suprosanna Shit","Anna Banaszak","Daniel Rueckert","Ulrich Bauer","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2403.11001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00260v2","updated":"2024-10-09T17:39:59Z","published":"2024-09-30T22:15:58Z","title":"DoPAMine: Domain-specific Pre-training Adaptation from seed-guided data\n  Mining","summary":"  Large Language Models (LLMs) have shown remarkable ability to generalize\neffectively across numerous industry domains while executing a range of tasks.\nMany of these competencies are obtained from the data utilized during the\npre-training phase of the Language Models (LMs). However, these models exhibit\nlimitations when tasked with performing in specialized or low-resource industry\ndomains. More recent approaches use LLMs for generating domain-specific\nsynthetic data but most often they lack in truthfulness and complexity.\nAlternatively, in cases where domain data is available like healthcare and\nfinance most of the LMs are proprietary necessitating the need for a scalable\nmethod to curate real world industry specific pre-training data. In this work,\nwe propose an automated and scalable framework - DoPAMine:Domain-specific\nPre-training Adaptation from seed-guided data Mining, to mine domain specific\ntraining data from a large data corpus for domain adaptation of a LM. The\nframework leverages the parametric knowledge of a LLM to generate diverse and\nrepresentative seed data tailored to a specific domain which is then used to\nmine real world data from a large data corpus like Common Crawl. We evaluated\nour framework's performance in the continual pre-training (CPT) setting by\ntraining two domain specific 7B parameter LMs in healthcare and finance with\ndata mined via DoPAMine. Our experiments show that DoPAMine boosts the\nperformance of pre-trained LLMs on average by 4.9% and 5.1% in zero-shot and\n5-shot settings respectively on healthcare tasks from MMLU, MedQA, MedMCQA and\nPubMedQA datasets, and 2.9% and 6.7% for zero-shot and 5-shot settings\nrespectively on finance tasks from FiQA-SA, FPB and Headlines datasets when\ncompared to the baseline.\n","authors":["Vinayak Arannil","Neha Narwal","Sourav Sanjukta Bhabesh","Sai Nikhil Thirandas","Darren Yow-Bang Wang","Graham Horwood","Alex Anto Chirayath","Gouri Pandeshwar"],"pdf_url":"https://arxiv.org/pdf/2410.00260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07094v1","updated":"2024-10-09T17:34:14Z","published":"2024-10-09T17:34:14Z","title":"An Approach for Auto Generation of Labeling Functions for Software\n  Engineering Chatbots","summary":"  Software engineering (SE) chatbots are increasingly gaining attention for\ntheir role in enhancing development processes. At the core of chatbots are the\nNatural Language Understanding platforms (NLUs), which enable them to\ncomprehend and respond to user queries. Before deploying NLUs, there is a need\nto train them with labeled data. However, acquiring such labeled data for SE\nchatbots is challenging due to the scarcity of high-quality datasets. This\nchallenge arises because training SE chatbots requires specialized vocabulary\nand phrases not found in typical language datasets. Consequently, chatbot\ndevelopers often resort to manually annotating user queries to gather the data\nnecessary for training effective chatbots, a process that is both\ntime-consuming and resource-intensive. Previous studies propose approaches to\nsupport chatbot practitioners in annotating users' posed queries. However,\nthese approaches require human intervention to generate rules, called labeling\nfunctions (LFs), that identify and categorize user queries based on specific\npatterns in the data. To address this issue, we propose an approach to\nautomatically generate LFs by extracting patterns from labeled user queries. We\nevaluate the effectiveness of our approach by applying it to the queries of\nfour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)\nand measure the performance improvement gained from training the NLU on the\nqueries labeled by the generated LFs. We find that the generated LFs\neffectively label data with AUC scores of up to 85.3%, and NLU's performance\nimprovement of up to 27.2% across the studied datasets. Furthermore, our\nresults show that the number of LFs used to generate LFs affects the labeling\nperformance. We believe that our approach can save time and resources in\nlabeling users' queries, allowing practitioners to focus on core chatbot\nfunctionalities.\n","authors":["Ebube Alor","Ahmad Abdellatif","SayedHassan Khatoonabadi","Emad Shihab"],"pdf_url":"https://arxiv.org/pdf/2410.07094v1.pdf","comment":"Submitted to IEEE Transactions on Software Engineering for review"},{"id":"http://arxiv.org/abs/2410.07091v1","updated":"2024-10-09T17:31:41Z","published":"2024-10-09T17:31:41Z","title":"Collusion Detection with Graph Neural Networks","summary":"  Collusion is a complex phenomenon in which companies secretly collaborate to\nengage in fraudulent practices. This paper presents an innovative methodology\nfor detecting and predicting collusion patterns in different national markets\nusing neural networks (NNs) and graph neural networks (GNNs). GNNs are\nparticularly well suited to this task because they can exploit the inherent\nnetwork structures present in collusion and many other economic problems. Our\napproach consists of two phases: In Phase I, we develop and train models on\nindividual market datasets from Japan, the United States, two regions in\nSwitzerland, Italy, and Brazil, focusing on predicting collusion in single\nmarkets. In Phase II, we extend the models' applicability through zero-shot\nlearning, employing a transfer learning approach that can detect collusion in\nmarkets in which training data is unavailable. This phase also incorporates\nout-of-distribution (OOD) generalization to evaluate the models' performance on\nunseen datasets from other countries and regions. In our empirical study, we\nshow that GNNs outperform NNs in detecting complex collusive patterns. This\nresearch contributes to the ongoing discourse on preventing collusion and\noptimizing detection methodologies, providing valuable guidance on the use of\nNNs and GNNs in economic applications to enhance market fairness and economic\nwelfare.\n","authors":["Lucas Gomes","Jannis Kueck","Mara Mattes","Martin Spindler","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2410.07091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07076v1","updated":"2024-10-09T17:19:58Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v1.pdf","comment":"Code and Benchmark are available at\n  https://github.com/ZonglinY/MOOSE-Chem.git"},{"id":"http://arxiv.org/abs/2410.07074v1","updated":"2024-10-09T17:19:12Z","published":"2024-10-09T17:19:12Z","title":"Let's Ask GNN: Empowering Large Language Model for Graph In-Context\n  Learning","summary":"  Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning.\n","authors":["Zhengyu Hu","Yichuan Li","Zhengyu Chen","Jingang Wang","Han Liu","Kyumin Lee","Kaize Ding"],"pdf_url":"https://arxiv.org/pdf/2410.07074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07072v1","updated":"2024-10-09T17:16:11Z","published":"2024-10-09T17:16:11Z","title":"Towards xAI: Configuring RNN Weights using Domain Knowledge for MIMO\n  Receive Processing","summary":"  Deep learning is making a profound impact in the physical layer of wireless\ncommunications. Despite exhibiting outstanding empirical performance in tasks\nsuch as MIMO receive processing, the reasons behind the demonstrated superior\nperformance improvement remain largely unclear. In this work, we advance the\nfield of Explainable AI (xAI) in the physical layer of wireless communications\nutilizing signal processing principles. Specifically, we focus on the task of\nMIMO-OFDM receive processing (e.g., symbol detection) using reservoir computing\n(RC), a framework within recurrent neural networks (RNNs), which outperforms\nboth conventional and other learning-based MIMO detectors. Our analysis\nprovides a signal processing-based, first-principles understanding of the\ncorresponding operation of the RC. Building on this fundamental understanding,\nwe are able to systematically incorporate the domain knowledge of wireless\nsystems (e.g., channel statistics) into the design of the underlying RNN by\ndirectly configuring the untrained RNN weights for MIMO-OFDM symbol detection.\nThe introduced RNN weight configuration has been validated through extensive\nsimulations demonstrating significant performance improvements. This\nestablishes a foundation for explainable RC-based architectures in MIMO-OFDM\nreceive processing and provides a roadmap for incorporating domain knowledge\ninto the design of neural networks for NextG systems.\n","authors":["Shashank Jere","Lizhong Zheng","Karim Said","Lingjia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.07072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07071v1","updated":"2024-10-09T17:15:30Z","published":"2024-10-09T17:15:30Z","title":"Retrieval-Augmented Decision Transformer: External Memory for In-context\n  RL","summary":"  In-context learning (ICL) is the ability of a model to learn a new task by\nobserving a few exemplars in its context. While prevalent in NLP, this\ncapability has recently also been observed in Reinforcement Learning (RL)\nsettings. Prior in-context RL methods, however, require entire episodes in the\nagent's context. Given that complex environments typically lead to long\nepisodes with sparse rewards, these methods are constrained to simple\nenvironments with short episodes. To address these challenges, we introduce\nRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external\nmemory mechanism to store past experiences from which it retrieves only\nsub-trajectories relevant for the current situation. The retrieval component in\nRA-DT does not require training and can be entirely domain-agnostic. We\nevaluate the capabilities of RA-DT on grid-world environments, robotics\nsimulations, and procedurally-generated video games. On grid-worlds, RA-DT\noutperforms baselines, while using only a fraction of their context length.\nFurthermore, we illuminate the limitations of current in-context RL methods on\ncomplex environments and discuss future directions. To facilitate future\nresearch, we release datasets for four of the considered environments.\n","authors":["Thomas Schmied","Fabian Paischer","Vihang Patil","Markus Hofmarcher","Razvan Pascanu","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2410.07071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07069v1","updated":"2024-10-09T17:14:50Z","published":"2024-10-09T17:14:50Z","title":"ReIFE: Re-evaluating Instruction-Following Evaluation","summary":"  The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.\n","authors":["Yixin Liu","Kejian Shi","Alexander R. Fabbri","Yilun Zhao","Peifeng Wang","Chien-Sheng Wu","Shafiq Joty","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2410.07069v1.pdf","comment":"GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result\n  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE"},{"id":"http://arxiv.org/abs/2410.07066v1","updated":"2024-10-09T17:11:22Z","published":"2024-10-09T17:11:22Z","title":"A Gentle Introduction and Tutorial on Deep Generative Models in\n  Transportation Research","summary":"  Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming\nessential tools in various fields due to their ability to learn complex data\ndistributions and generate synthetic data. Their importance in transportation\nresearch is increasingly recognized, particularly for applications like traffic\ndata generation, prediction, and feature extraction. This paper offers a\ncomprehensive introduction and tutorial on DGMs, with a focus on their\napplications in transportation. It begins with an overview of generative\nmodels, followed by detailed explanations of fundamental models, a systematic\nreview of the literature, and practical tutorial code to aid implementation.\nThe paper also discusses current challenges and opportunities, highlighting how\nthese models can be effectively utilized and further developed in\ntransportation research. This paper serves as a valuable reference, guiding\nresearchers and practitioners from foundational knowledge to advanced\napplications of DGMs in transportation research.\n","authors":["Seongjin Choi","Zhixiong Jin","Seungwoo Ham","Jiwon Kim","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.07066v1.pdf","comment":"64 pages, 21 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.03044v2","updated":"2024-10-09T17:07:27Z","published":"2024-06-05T08:15:09Z","title":"Population Transformer: Learning Population-level Representations of\n  Neural Activity","summary":"  We present a self-supervised framework that learns population-level codes for\narbitrary ensembles of neural recordings at scale. We address two key\nchallenges in scaling models with neural time-series data: sparse and variable\nelectrode distribution across subjects and datasets. The Population Transformer\n(PopT) stacks on top of pretrained representations and enhances downstream\ndecoding by enabling learned aggregation of multiple spatially-sparse data\nchannels. The pretrained PopT lowers the amount of data required for downstream\ndecoding experiments, while increasing accuracy, even on held-out subjects and\ntasks. Compared to end-to-end methods, this approach is computationally\nlightweight and more interpretable, while still retaining competitive\nperformance. We further show how our framework is generalizable to multiple\ntime-series embeddings and neural data modalities. Beyond decoding, we\ninterpret the pretrained PopT and fine-tuned models to show how they can be\nused to extract neuroscience insights from massive amounts of data. We release\nour code as well as a pretrained PopT to enable off-the-shelf improvements in\nmulti-channel intracranial data decoding and interpretability.\n","authors":["Geeling Chau","Christopher Wang","Sabera Talukder","Vighnesh Subramaniam","Saraswati Soedarmadji","Yisong Yue","Boris Katz","Andrei Barbu"],"pdf_url":"https://arxiv.org/pdf/2406.03044v2.pdf","comment":"19 pages, 11 figures, submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.07063v1","updated":"2024-10-09T17:05:15Z","published":"2024-10-09T17:05:15Z","title":"InAttention: Linear Context Scaling for Transformers","summary":"  VRAM requirements for transformer models scale quadratically with context\nlength due to the self-attention mechanism. In this paper we modify the\ndecoder-only transformer, replacing self-attention with InAttention, which\nscales linearly with context length during inference by having tokens attend\nonly to initial states. Benchmarking shows that InAttention significantly\nreduces VRAM usage during inference, enabling handling of long sequences on\nconsumer GPUs. We corroborate that fine-tuning extends context length\nefficiently, improving performance on long sequences without high training\ncosts. InAttention offers a scalable solution for long-range dependencies in\ntransformer models, paving the way for further optimization.\n","authors":["Joseph Eisner"],"pdf_url":"https://arxiv.org/pdf/2410.07063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07059v1","updated":"2024-10-09T16:58:36Z","published":"2024-10-09T16:58:36Z","title":"Online Epsilon Net and Piercing Set for Geometric Concepts","summary":"  VC-dimension and $\\varepsilon$-nets are key concepts in Statistical Learning\nTheory. Intuitively, VC-dimension is a measure of the size of a class of sets.\nThe famous $\\varepsilon$-net theorem, a fundamental result in Discrete\nGeometry, asserts that if the VC-dimension of a set system is bounded, then a\nsmall sample exists that intersects all sufficiently large sets.\n  In online learning scenarios where data arrives sequentially, the\nVC-dimension helps to bound the complexity of the set system, and\n$\\varepsilon$-nets ensure the selection of a small representative set. This\nsampling framework is crucial in various domains, including spatial data\nanalysis, motion planning in dynamic environments, optimization of sensor\nnetworks, and feature extraction in computer vision, among others. Motivated by\nthese applications, we study the online $\\varepsilon$-net problem for geometric\nconcepts with bounded VC-dimension. While the offline version of this problem\nhas been extensively studied, surprisingly, there are no known theoretical\nresults for the online version to date. We present the first deterministic\nonline algorithm with an optimal competitive ratio for intervals in\n$\\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal\ncompetitive ratio for axis-aligned boxes in $\\mathbb{R}^d$, for $d\\le 3$.\nFurthermore, we introduce a novel technique to analyze similar-sized objects of\nconstant description complexity in $\\mathbb{R}^d$, which may be of independent\ninterest. Next, we focus on the continuous version of this problem, where\nranges of the set system are geometric concepts in $\\mathbb{R}^d$ arriving in\nan online manner, but the universe is the entire space, and the objective is to\nchoose a small sample that intersects all the ranges.\n","authors":["Sujoy Bhore","Devdan Dey","Satyam Singh"],"pdf_url":"https://arxiv.org/pdf/2410.07059v1.pdf","comment":"18 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2410.07054v1","updated":"2024-10-09T16:51:21Z","published":"2024-10-09T16:51:21Z","title":"Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing","summary":"  Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases.\n","authors":["Weichuan Wang","Zhaoyi Li","Defu Lian","Chen Ma","Linqi Song","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2410.07054v1.pdf","comment":"20 pages, EMNLP'2024 Main Conference"},{"id":"http://arxiv.org/abs/2312.05412v2","updated":"2024-10-09T16:49:58Z","published":"2023-12-08T23:55:19Z","title":"CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional\n  Modeling","summary":"  We introduce a multi-modal diffusion model tailored for the bi-directional\nconditional generation of video and audio. We propose a joint contrastive\ntraining loss to improve the synchronization between visual and auditory\noccurrences. We present experiments on two datasets to evaluate the efficacy of\nour proposed model. The assessment of generation quality and alignment\nperformance is carried out from various angles, encompassing both objective and\nsubjective metrics. Our findings demonstrate that the proposed model\noutperforms the baseline in terms of quality and generation speed through\nintroduction of our novel cross-modal easy fusion architectural block.\nFurthermore, the incorporation of the contrastive loss results in improvements\nin audio-visual alignment, particularly in the high-correlation video-to-audio\ngeneration task.\n","authors":["Ruihan Yang","Hannes Gamper","Sebastian Braun"],"pdf_url":"https://arxiv.org/pdf/2312.05412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05649v3","updated":"2024-10-09T16:32:11Z","published":"2024-07-08T06:21:56Z","title":"Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention","summary":"  Graph Neural Networks (GNNs) have become important tools for machine learning\non graph-structured data. In this paper, we explore the synergistic combination\nof graph encoding, graph rewiring, and graph attention, by introducing Graph\nAttention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS\nutilizes relative random walk probabilities (RRWP) encoding and a novel\ndecomposed variant (D-RRWP) to efficiently capture structural information. It\nrewires the input graph by superimposing a random regular graph to enhance\nlong-range information propagation. It also employs a novel additive attention\nmechanism tailored for graph-structured data. Our empirical evaluations\ndemonstrate that GRASS achieves state-of-the-art performance on multiple\nbenchmark datasets, including a 20.3% reduction in mean absolute error on the\nZINC dataset.\n","authors":["Tongzhou Liao","Barnabs Pczos"],"pdf_url":"https://arxiv.org/pdf/2407.05649v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07041v1","updated":"2024-10-09T16:28:23Z","published":"2024-10-09T16:28:23Z","title":"Emergent properties with repeated examples","summary":"  We study the performance of transformers as a function of the number of\nrepetitions of training examples with algorithmically generated datasets. On\nthree problems of mathematics: the greatest common divisor, modular\nmultiplication, and matrix eigenvalues, we show that for a fixed number of\ntraining steps, models trained on smaller sets of repeated examples outperform\nmodels trained on larger sets of single-use examples. We also demonstrate that\ntwo-set training - repeated use of a small random subset of examples, along\nnormal sampling on the rest of the training set - provides for faster learning\nand better performance. This highlights that the benefits of repetition can\noutweigh those of data diversity. These datasets and problems provide a\ncontrolled setting to shed light on the still poorly understood interplay\nbetween generalization and memorization in deep learning.\n","authors":["Franois Charton","Julia Kempe"],"pdf_url":"https://arxiv.org/pdf/2410.07041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10610v3","updated":"2024-10-09T16:28:15Z","published":"2023-11-17T16:04:31Z","title":"A Poincar Inequality and Consistency Results for Signal Sampling on\n  Large Graphs","summary":"  Large-scale graph machine learning is challenging as the complexity of\nlearning models scales with the graph size. Subsampling the graph is a viable\nalternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.\nExisting graph sampling techniques require not only computing the spectra of\nlarge matrices but also repeating these computations when the graph changes,\ne.g., grows. In this paper, we introduce a signal sampling theory for a type of\ngraph limit -- the graphon. We prove a Poincar\\'e inequality for graphon\nsignals and show that complements of node subsets satisfying this inequality\nare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting\nconnections with spectral clustering and Gaussian elimination, we prove that\nsuch sampling sets are consistent in the sense that unique sampling sets on a\nconvergent graph sequence converge to unique sampling sets on the graphon. We\nthen propose a related graphon signal sampling algorithm for large graphs, and\ndemonstrate its good empirical performance on graph machine learning tasks.\n","authors":["Thien Le","Luana Ruiz","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2311.10610v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.07039v1","updated":"2024-10-09T16:25:01Z","published":"2024-10-09T16:25:01Z","title":"Distributionally Robust Clustered Federated Learning: A Case Study in\n  Healthcare","summary":"  In this paper, we address the challenge of heterogeneous data distributions\nin cross-silo federated learning by introducing a novel algorithm, which we\nterm Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approach\nleverages the Wasserstein distance to construct ambiguity sets around each\nclient's empirical distribution that capture possible distribution shifts in\nthe local data, enabling evaluation of worst-case model performance. We then\npropose a model-agnostic integer fractional program to determine the optimal\ndistributionally robust clustering of clients into coalitions so that possible\nbiases in the local models caused by statistically heterogeneous client\ndatasets are avoided, and analyze our method for linear and logistic regression\nmodels. Finally, we discuss a federated learning protocol that ensures the\nprivacy of client distributions, a critical consideration, for instance, when\nclients are healthcare institutions. We evaluate our algorithm on synthetic and\nreal-world healthcare data.\n","authors":["Xenia Konti","Hans Riess","Manos Giannopoulos","Yi Shen","Michael J. Pencina","Nicoleta J. Economou-Zavlanos","Michael M. Zavlanos"],"pdf_url":"https://arxiv.org/pdf/2410.07039v1.pdf","comment":"8 pages, 3 figures, Accepted to IEEE CDC 2024"},{"id":"http://arxiv.org/abs/2410.07021v1","updated":"2024-10-09T16:04:40Z","published":"2024-10-09T16:04:40Z","title":"Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings\n  from a Large-Scale Benchmark","summary":"  We present unexpected findings from a large-scale benchmark study evaluating\nConditional Average Treatment Effect (CATE) estimation algorithms. By running\n16 modern CATE models across 43,200 datasets, we find that: (a) 62\\% of CATE\nestimates have a higher Mean Squared Error (MSE) than a trivial zero-effect\npredictor, rendering them ineffective; (b) in datasets with at least one useful\nCATE estimate, 80\\% still have higher MSE than a constant-effect model; and (c)\nOrthogonality-based models outperform other models only 30\\% of the time,\ndespite widespread optimism about their performance. These findings expose\nsignificant limitations in current CATE models and suggest ample opportunities\nfor further research.\n  Our findings stem from a novel application of \\textit{observational\nsampling}, originally developed to evaluate Average Treatment Effect (ATE)\nestimates from observational methods with experiment data. To adapt\nobservational sampling for CATE evaluation, we introduce a statistical\nparameter, $Q$, equal to MSE minus a constant and preserves the ranking of\nmodels by their MSE. We then derive a family of sample statistics, collectively\ncalled $\\hat{Q}$, that can be computed from real-world data. We prove that\n$\\hat{Q}$ is a consistent estimator of $Q$ under mild technical conditions.\nWhen used in observational sampling, $\\hat{Q}$ is unbiased and asymptotically\nselects the model with the smallest MSE. To ensure the benchmark reflects\nreal-world heterogeneity, we handpick datasets where outcomes come from field\nrather than simulation. By combining the new observational sampling method, new\nstatistics, and real-world datasets, the benchmark provides a unique\nperspective on CATE estimator performance and uncover gaps in capturing\nreal-world heterogeneity.\n","authors":["Haining Yu","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2410.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14432v4","updated":"2024-10-09T16:04:01Z","published":"2024-05-23T11:00:31Z","title":"The Vital Role of Gradient Clipping in Byzantine-Resilient Distributed\n  Learning","summary":"  Byzantine-resilient distributed machine learning seeks to achieve robust\nlearning performance in the presence of misbehaving or adversarial workers.\nWhile state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD)\nmethods were proven theoretically optimal, their empirical success has often\nrelied on pre-aggregation gradient clipping. However, the currently considered\nstatic clipping strategy exhibits mixed results: improving robustness against\nsome attacks while being ineffective or detrimental against others. We address\nthis gap by proposing a principled adaptive clipping strategy, termed Adaptive\nRobust Clipping (ARC). We show that ARC consistently enhances the empirical\nrobustness of SOTA Robust-DGD methods, while preserving the theoretical\nrobustness guarantees. Our analysis shows that ARC provably improves the\nasymptotic convergence guarantee of Robust-DGD in the case when the model is\nwell-initialized. We validate this theoretical insight through an exhaustive\nset of experiments on benchmark image classification tasks. We observe that the\nimprovement induced by ARC is more pronounced in highly heterogeneous and\nadversarial settings.\n","authors":["Youssef Allouah","Rachid Guerraoui","Nirupam Gupta","Ahmed Jellouli","Geovani Rizk","John Stephan"],"pdf_url":"https://arxiv.org/pdf/2405.14432v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00795v4","updated":"2024-10-09T16:02:13Z","published":"2024-02-01T17:28:10Z","title":"LLMs learn governing principles of dynamical systems, revealing an\n  in-context neural scaling law","summary":"  Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. We study LLMs' ability to\nextrapolate the behavior of dynamical systems whose evolution is governed by\nprinciples of physical interest. Our results show that LLaMA 2, a language\nmodel trained primarily on texts, achieves accurate predictions of dynamical\nsystem time series without fine-tuning or prompt engineering. Moreover, the\naccuracy of the learned physical rules increases with the length of the input\ncontext window, revealing an in-context version of neural scaling law. Along\nthe way, we present a flexible and efficient algorithm for extracting\nprobability density functions of multi-digit numbers directly from LLMs.\n","authors":["Toni J. B. Liu","Nicolas Boull","Raphal Sarfati","Christopher J. Earls"],"pdf_url":"https://arxiv.org/pdf/2402.00795v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07018v1","updated":"2024-10-09T16:00:21Z","published":"2024-10-09T16:00:21Z","title":"Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series\n  OOD Generalization","summary":"  Out-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience\nof machine learning models when faced with new, unseen, and potentially\nadversarial data that significantly diverges from their original training\ndatasets. In this paper, we investigate time series OOD generalization via\npre-trained Large Language Models (LLMs). We first propose a novel\n\\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries\n\\textbf{O}OD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective\nfor formulating and analyzing OOD generalization problem. In addition, we\nprovide a theoretical analysis to justify this method is well motivated. We\nthen develop a stratified localization algorithm tailored for this tri-level\noptimization problem, theoretically demonstrating the guaranteed convergence of\nthe proposed algorithm. Our analysis also reveals that the iteration complexity\nto obtain an $\\epsilon$-stationary point is bounded by\nO($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method.\n","authors":["Chengtao Jian","Kai Yang","Yang Jiao"],"pdf_url":"https://arxiv.org/pdf/2410.07018v1.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.07014v1","updated":"2024-10-09T15:58:06Z","published":"2024-10-09T15:58:06Z","title":"Optimizing Estimators of Squared Calibration Errors in Classification","summary":"  In this work, we propose a mean-squared error-based risk that enables the\ncomparison and optimization of estimators of squared calibration errors in\npractical settings. Improving the calibration of classifiers is crucial for\nenhancing the trustworthiness and interpretability of machine learning models,\nespecially in sensitive decision-making scenarios. Although various calibration\n(error) estimators exist in the current literature, there is a lack of guidance\non selecting the appropriate estimator and tuning its hyperparameters. By\nleveraging the bilinear structure of squared calibration errors, we reformulate\ncalibration estimation as a regression problem with independent and identically\ndistributed (i.i.d.) input pairs. This reformulation allows us to quantify the\nperformance of different estimators even for the most challenging calibration\ncriterion, known as canonical calibration. Our approach advocates for a\ntraining-validation-testing pipeline when estimating a calibration error on an\nevaluation dataset. We demonstrate the effectiveness of our pipeline by\noptimizing existing calibration estimators and comparing them with novel kernel\nridge regression-based estimators on standard image classification tasks.\n","authors":["Sebastian G. Gruber","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2410.07014v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.07013v1","updated":"2024-10-09T15:57:50Z","published":"2024-10-09T15:57:50Z","title":"Causal Representation Learning in Temporal Data via Single-Parent\n  Decoding","summary":"  Scientific research often seeks to understand the causal structure underlying\nhigh-level variables in a system. For example, climate scientists study how\nphenomena, such as El Ni\\~no, affect other climate processes at remote\nlocations across the globe. However, scientists typically collect low-level\nmeasurements, such as geographically distributed temperature readings. From\nthese, one needs to learn both a mapping to causally-relevant latent variables,\nsuch as a high-level representation of the El Ni\\~no phenomenon and other\nprocesses, as well as the causal model over them. The challenge is that this\ntask, called causal representation learning, is highly underdetermined from\nobservational data alone, requiring other constraints during learning to\nresolve the indeterminacies. In this work, we consider a temporal model with a\nsparsity assumption, namely single-parent decoding: each observed low-level\nvariable is only affected by a single latent variable. Such an assumption is\nreasonable in many scientific applications that require finding groups of\nlow-level variables, such as extracting regions from geographically gridded\nmeasurement data in climate research or capturing brain regions from neural\nactivity data. We demonstrate the identifiability of the resulting model and\npropose a differentiable method, Causal Discovery with Single-parent Decoding\n(CDSD), that simultaneously learns the underlying latents and a causal graph\nover them. We assess the validity of our theoretical results using simulated\ndata and showcase the practical validity of our method in an application to\nreal-world data from the climate science field.\n","authors":["Philippe Brouillard","Sbastien Lachapelle","Julia Kaltenborn","Yaniv Gurwicz","Dhanya Sridhar","Alexandre Drouin","Peer Nowack","Jakob Runge","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2410.07013v1.pdf","comment":"33 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.07003v1","updated":"2024-10-09T15:48:56Z","published":"2024-10-09T15:48:56Z","title":"Through the Looking Glass: Mirror Schrdinger Bridges","summary":"  Resampling from a target measure whose density is unknown is a fundamental\nproblem in mathematical statistics and machine learning. A setting that\ndominates the machine learning literature consists of learning a map from an\neasy-to-sample prior, such as the Gaussian distribution, to a target measure.\nUnder this model, samples from the prior are pushed forward to generate a new\nsample on the target measure, which is often difficult to sample from directly.\nIn this paper, we propose a new model for conditional resampling called mirror\nSchr\\\"odinger bridges. Our key observation is that solving the Schr\\\"odinger\nbridge problem between a distribution and itself provides a natural way to\nproduce new samples from conditional distributions, giving in-distribution\nvariations of an input data point. We show how to efficiently solve this\nlargely overlooked version of the Schr\\\"odinger bridge problem. We prove that\nour proposed method leads to significant algorithmic simplifications over\nexisting alternatives, in addition to providing control over in-distribution\nvariation. Empirically, we demonstrate how these benefits can be leveraged to\nproduce proximal samples in a number of application domains.\n","authors":["Leticia Mattos Da Silva","Silvia Selln","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2410.07003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10054v3","updated":"2024-10-09T15:44:36Z","published":"2023-11-16T17:48:55Z","title":"When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models","summary":"  Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.\n","authors":["Mingqian Zheng","Jiaxin Pei","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2311.10054v3.pdf","comment":"Accepted by Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.03043v2","updated":"2024-10-09T15:44:35Z","published":"2024-09-04T19:27:56Z","title":"Can Your Generative Model Detect Out-of-Distribution Covariate Shift?","summary":"  Detecting Out-of-Distribution (OOD) sensory data and covariate distribution\nshift aims to identify new test examples with different high-level image\nstatistics to the captured, normal and In-Distribution (ID) set. Existing OOD\ndetection literature largely focuses on semantic shift with little-to-no\nconsensus over covariate shift. Generative models capture the ID data in an\nunsupervised manner, enabling them to effectively identify samples that deviate\nsignificantly from this learned distribution, irrespective of the downstream\ntask. In this work, we elucidate the ability of generative models to detect and\nquantify domain-specific covariate shift through extensive analyses that\ninvolves a variety of models. To this end, we conjecture that it is sufficient\nto detect most occurring sensory faults (anomalies and deviations in global\nsignals statistics) by solely modeling high-frequency signal-dependent and\nindependent details. We propose a novel method, CovariateFlow, for OOD\ndetection, specifically tailored to covariate heteroscedastic high-frequency\nimage-components using conditional Normalizing Flows (cNFs). Our results on\nCIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate the\neffectiveness of the method by accurately detecting OOD covariate shift. This\nwork contributes to enhancing the fidelity of imaging systems and aiding\nmachine learning models in OOD detection in the presence of covariate shift.\n","authors":["Christiaan Viviers","Amaan Valiuddin","Francisco Caetano","Lemar Abdi","Lena Filatova","Peter de With","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2409.03043v2.pdf","comment":"ECCV 2024, typos corrected"},{"id":"http://arxiv.org/abs/2410.06993v1","updated":"2024-10-09T15:40:04Z","published":"2024-10-09T15:40:04Z","title":"Efficient Distribution Matching of Representations via Noise-Injected\n  Deep InfoMax","summary":"  Deep InfoMax (DIM) is a well-established method for self-supervised\nrepresentation learning (SSRL) based on maximization of the mutual information\nbetween the input and the output of a deep neural network encoder. Despite the\nDIM and contrastive SSRL in general being well-explored, the task of learning\nrepresentations conforming to a specific distribution (i.e., distribution\nmatching, DM) is still under-addressed. Motivated by the importance of DM to\nseveral downstream tasks (including generative modeling, disentanglement,\noutliers detection and other), we enhance DIM to enable automatic matching of\nlearned representations to a selected prior distribution. To achieve this, we\npropose injecting an independent noise into the normalized outputs of the\nencoder, while keeping the same InfoMax training objective. We show that such\nmodification allows for learning uniformly and normally distributed\nrepresentations, as well as representations of other absolutely continuous\ndistributions. Our approach is tested on various downstream tasks. The results\nindicate a moderate trade-off between the performance on the downstream tasks\nand quality of DM.\n","authors":["Ivan Butakov","Alexander Sememenko","Alexander Tolmachev","Andrey Gladkov","Marina Munkhoeva","Alexey Frolov"],"pdf_url":"https://arxiv.org/pdf/2410.06993v1.pdf","comment":"22 pages, 3 fugures"},{"id":"http://arxiv.org/abs/2210.08342v9","updated":"2024-10-09T15:27:08Z","published":"2022-10-15T17:32:49Z","title":"Symbolic Recovery of Differential Equations: The Identifiability Problem","summary":"  Symbolic recovery of differential equations is the ambitious attempt at\nautomating the derivation of governing equations with the use of machine\nlearning techniques. In contrast to classical methods which assume the\nstructure of the equation to be known and focus on the estimation of specific\nparameters, these algorithms aim to learn the structure and the parameters\nsimultaneously. While the uniqueness and, therefore, the identifiability of\nparameters of governing equations are a well-addressed problem in the field of\nparameter estimation, it has not been investigated for symbolic recovery.\nHowever, this problem should be even more present in this field since the\nalgorithms aim to cover larger spaces of governing equations. In this paper, we\ninvestigate under which conditions a solution of a differential equation does\nnot uniquely determine the equation itself. For various classes of differential\nequations, we provide both necessary and sufficient conditions for a function\nto uniquely determine the corresponding differential equation. We then use our\nresults to devise numerical algorithms aiming to determine whether a function\nsolves a differential equation uniquely. Finally, we provide extensive\nnumerical experiments showing that our algorithms can indeed guarantee the\nuniqueness of the learned governing differential equation, without assuming any\nknowledge about the analytic form of function, thereby ensuring the reliability\nof the learned equation.\n","authors":["Philipp Scholl","Aras Bacho","Holger Boche","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2210.08342v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15817v2","updated":"2024-10-09T15:26:25Z","published":"2023-12-25T21:55:00Z","title":"A Unified Generative Framework for Realistic Lidar Simulation in\n  Autonomous Driving Systems","summary":"  Simulation models for perception sensors are integral components of\nautomotive simulators used for the virtual Verification and Validation (V\\&V)\nof Autonomous Driving Systems (ADS). These models also serve as powerful tools\nfor generating synthetic datasets to train deep learning-based perception\nmodels. Lidar is a widely used sensor type among the perception sensors for ADS\ndue to its high precision in 3D environment scanning. However, developing\nrealistic Lidar simulation models is a significant technical challenge. In\nparticular, unrealistic models can result in a large gap between the\nsynthesised and real-world point clouds, limiting their effectiveness in ADS\napplications. Recently, deep generative models have emerged as promising\nsolutions to synthesise realistic sensory data. However, for Lidar simulation,\ndeep generative models have been primarily hybridised with conventional\nalgorithms, leaving unified generative approaches largely unexplored in the\nliterature. Motivated by this research gap, we propose a unified generative\nframework to enhance Lidar simulation fidelity. Our proposed framework projects\nLidar point clouds into depth-reflectance images via a lossless transformation,\nand employs our novel Controllable Lidar point cloud Generative model, CoLiGen,\nto translate the images. We extensively evaluate our CoLiGen model, comparing\nit with the state-of-the-art image-to-image translation models using various\nmetrics to assess the realness, faithfulness, and performance of a downstream\nperception model. Our results show that CoLiGen exhibits superior performance\nacross most metrics. The dataset and source code for this research are\navailable at https://github.com/hamedhaghighi/CoLiGen.git.\n","authors":["Hamed Haghighi","Mehrdad Dianati","Valentina Donzella","Kurt Debattista"],"pdf_url":"https://arxiv.org/pdf/2312.15817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06986v1","updated":"2024-10-09T15:21:53Z","published":"2024-10-09T15:21:53Z","title":"Diffusion Density Estimators","summary":"  We investigate the use of diffusion models as neural density estimators. The\ncurrent approach to this problem involves converting the generative process to\na smooth flow, known as the Probability Flow ODE. The log density at a given\nsample can be obtained by solving the ODE with a black-box solver. We introduce\na new, highly parallelizable method that computes log densities without the\nneed to solve a flow. Our approach is based on estimating a path integral by\nMonte Carlo, in a manner identical to the simulation-free training of diffusion\nmodels. We also study how different training parameters affect the accuracy of\nthe density calculation, and offer insights into how these models can be made\nmore scalable and efficient.\n","authors":["Akhil Premkumar"],"pdf_url":"https://arxiv.org/pdf/2410.06986v1.pdf","comment":"20 pages + references, 7 figures"},{"id":"http://arxiv.org/abs/2410.06981v1","updated":"2024-10-09T15:18:57Z","published":"2024-10-09T15:18:57Z","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","summary":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones. This makes it\ndifficult to disentangle and match features across different models. To address\nthis issue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics like Singular Value Canonical Correlation Analysis to\nanalyze these SAE features across different LLMs. Our experiments reveal\nsignificant similarities in SAE feature spaces across various LLMs, providing\nnew evidence for feature universality.\n","authors":["Michael Lan","Philip Torr","Austin Meek","Ashkan Khakzar","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.06981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06976v1","updated":"2024-10-09T15:15:40Z","published":"2024-10-09T15:15:40Z","title":"AdaRC: Mitigating Graph Structure Shifts during Test-Time","summary":"  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\nto distribution shifts. Recently, test-time adaptation (TTA) has attracted\nattention due to its ability to adapt a pre-trained model to a target domain\nwithout re-accessing the source domain. However, existing TTA algorithms are\nprimarily designed for attribute shifts in vision tasks, where samples are\nindependent. These methods perform poorly on graph data that experience\nstructure shifts, where node connectivity differs between source and target\ngraphs. We attribute this performance gap to the distinct impact of node\nattribute shifts versus graph structure shifts: the latter significantly\ndegrades the quality of node representations and blurs the boundaries between\ndifferent node categories. To address structure shifts in graphs, we propose\nAdaRC, an innovative framework designed for effective and efficient adaptation\nto structure shifts by adjusting the hop-aggregation parameters in GNNs. To\nenhance the representation quality, we design a prediction-informed clustering\nloss to encourage the formation of distinct clusters for different node\ncategories. Additionally, AdaRC seamlessly integrates with existing TTA\nalgorithms, allowing it to handle attribute shifts effectively while improving\noverall performance under combined structure and attribute shifts. We validate\nthe effectiveness of AdaRC on both synthetic and real-world datasets,\ndemonstrating its robustness across various combinations of structure and\nattribute shifts.\n","authors":["Wenxuan Bao","Zhichen Zeng","Zhining Liu","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2410.06976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06974v1","updated":"2024-10-09T15:12:35Z","published":"2024-10-09T15:12:35Z","title":"Diagnosis of Malignant Lymphoma Cancer Using Hybrid Optimized Techniques\n  Based on Dense Neural Networks","summary":"  Lymphoma diagnosis, particularly distinguishing between subtypes, is critical\nfor effective treatment but remains challenging due to the subtle morphological\ndifferences in histopathological images. This study presents a novel hybrid\ndeep learning framework that combines DenseNet201 for feature extraction with a\nDense Neural Network (DNN) for classification, optimized using the Harris Hawks\nOptimization (HHO) algorithm. The model was trained on a dataset of 15,000\nbiopsy images, spanning three lymphoma subtypes: Chronic Lymphocytic Leukemia\n(CLL), Follicular Lymphoma (FL), and Mantle Cell Lymphoma (MCL). Our approach\nachieved a testing accuracy of 99.33\\%, demonstrating significant improvements\nin both accuracy and model interpretability. Comprehensive evaluation using\nprecision, recall, F1-score, and ROC-AUC underscores the model's robustness and\npotential for clinical adoption. This framework offers a scalable solution for\nimproving diagnostic accuracy and efficiency in oncology.\n","authors":["Salah A. Aly","Ali Bakhiet","Mazen Balat"],"pdf_url":"https://arxiv.org/pdf/2410.06974v1.pdf","comment":"6 pages, 5 figures, 4 tables, IEEE ICCA"},{"id":"http://arxiv.org/abs/2409.15107v2","updated":"2024-10-09T15:09:47Z","published":"2024-09-23T15:17:30Z","title":"The BRAVO Semantic Segmentation Challenge Results in UNCV2024","summary":"  We propose the unified BRAVO challenge to benchmark the reliability of\nsemantic segmentation models under realistic perturbations and unknown\nout-of-distribution (OOD) scenarios. We define two categories of reliability:\n(1) semantic reliability, which reflects the model's accuracy and calibration\nwhen exposed to various perturbations; and (2) OOD reliability, which measures\nthe model's ability to detect object classes that are unknown during training.\nThe challenge attracted nearly 100 submissions from international teams\nrepresenting notable research institutions. The results reveal interesting\ninsights into the importance of large-scale pre-training and minimal\narchitectural design in developing robust and reliable semantic segmentation\nmodels.\n","authors":["Tuan-Hung Vu","Eduardo Valle","Andrei Bursuc","Tommie Kerssies","Daan de Geus","Gijs Dubbelman","Long Qian","Bingke Zhu","Yingying Chen","Ming Tang","Jinqiao Wang","Tom Voj","Jan ochman","Ji Matas","Michael Smith","Frank Ferrie","Shamik Basu","Christos Sakaridis","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2409.15107v2.pdf","comment":"ECCV 2024 proceeding paper of the BRAVO challenge 2024, see\n  https://benchmarks.elsa-ai.eu/?ch=1&com=introduction Corrected numbers in\n  Tables 1,3,4,5 and 10"},{"id":"http://arxiv.org/abs/2410.06969v1","updated":"2024-10-09T15:07:53Z","published":"2024-10-09T15:07:53Z","title":"DLGNet: Hyperedge Classification through Directed Line Graphs for\n  Chemical Reactions","summary":"  Graphs and hypergraphs provide powerful abstractions for modeling\ninteractions among a set of entities of interest and have been attracting a\ngrowing interest in the literature thanks to many successful applications in\nseveral fields. In particular, they are rapidly expanding in domains such as\nchemistry and biology, especially in the areas of drug discovery and molecule\ngeneration. One of the areas witnessing the fasted growth is the chemical\nreactions field, where chemical reactions can be naturally encoded as directed\nhyperedges of a hypergraph. In this paper, we address the chemical reaction\nclassification problem by introducing the notation of a Directed Line Graph\n(DGL) associated with a given directed hypergraph. On top of it, we build the\nDirected Line Graph Network (DLGNet), the first spectral-based Graph Neural\nNetwork (GNN) expressly designed to operate on a hypergraph via its DLG\ntransformation. The foundation of DLGNet is a novel Hermitian matrix, the\nDirected Line Graph Laplacian, which compactly encodes the directionality of\nthe interactions taking place within the directed hyperedges of the hypergraph\nthanks to the DLG representation. The Directed Line Graph Laplacian enjoys many\ndesirable properties, including admitting an eigenvalue decomposition and being\npositive semidefinite, which make it well-suited for its adoption within a\nspectral-based GNN. Through extensive experiments on chemical reaction\ndatasets, we show that DGLNet significantly outperforms the existing\napproaches, achieving on a collection of real-world datasets an average\nrelative-percentage-difference improvement of 33.01%, with a maximum\nimprovement of 37.71%.\n","authors":["Stefano Fiorini","Giulia M. Bovolenta","Stefano Coniglio","Michele Ciavotta","Pietro Morerio","Michele Parrinello","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2410.06969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09559v2","updated":"2024-10-09T15:03:38Z","published":"2024-05-02T16:56:09Z","title":"KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from\n  a Smartwatch","summary":"  Accurate extraction of heart rate from photoplethysmography (PPG) signals\nremains challenging due to motion artifacts and signal degradation. Although\ndeep learning methods trained as a data-driven inference problem offer\npromising solutions, they often underutilize existing knowledge from the\nmedical and signal processing community. In this paper, we address three\nshortcomings of deep learning models: motion artifact removal, degradation\nassessment, and physiologically plausible analysis of the PPG signal. We\npropose KID-PPG, a knowledge-informed deep learning model that integrates\nexpert knowledge through adaptive linear filtering, deep probabilistic\ninference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset,\nachieving an average mean absolute error of 2.85 beats per minute, surpassing\nexisting reproducible methods. Our results demonstrate a significant\nperformance improvement in heart rate tracking through the incorporation of\nprior knowledge into deep learning models. This approach shows promise in\nenhancing various biomedical applications by incorporating existing expert\nknowledge in deep learning models.\n","authors":["Christodoulos Kechris","Jonathan Dan","Jose Miranda","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2405.09559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06963v1","updated":"2024-10-09T15:02:08Z","published":"2024-10-09T15:02:08Z","title":"ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling","summary":"  This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}\n","authors":["Deok-Kyeong Jang","Dongseok Yang","Deok-Yun Jang","Byeoli Choi","Donghoon Shin","Sung-hee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.06963v1.pdf","comment":"published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024"},{"id":"http://arxiv.org/abs/2310.18304v3","updated":"2024-10-09T14:55:30Z","published":"2023-10-27T17:53:53Z","title":"A Stability Principle for Learning under Non-Stationarity","summary":"  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory and numerical experiments showcase the adaptivity\nof this approach to unknown non-stationarity. We prove regret bounds that are\nminimax optimal up to logarithmic factors when the population losses are\nstrongly convex, or Lipschitz only. At the heart of our analysis lie two novel\ncomponents: a measure of similarity between functions and a segmentation\ntechnique for dividing the non-stationary data sequence into quasi-stationary\npieces.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18304v3.pdf","comment":"65 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06957v1","updated":"2024-10-09T14:55:19Z","published":"2024-10-09T14:55:19Z","title":"Support Vector Boosting Machine (SVBM): Enhancing Classification\n  Performance with AdaBoost and Residual Connections","summary":"  In traditional boosting algorithms, the focus on misclassified training\nsamples emphasizes their importance based on difficulty during the learning\nprocess. While using a standard Support Vector Machine (SVM) as a weak learner\nin an AdaBoost framework can enhance model performance by concentrating on\nerror samples, this approach introduces significant challenges. Specifically,\nSVMs, characterized by their stability and robustness, may require\ndestabilization to fit the boosting paradigm, which in turn can constrain\nperformance due to reliance on the weighted results from preceding iterations.\nTo address these challenges, we propose the Support Vector Boosting Machine\n(SVBM), which integrates a novel subsampling process with SVM algorithms and\nresidual connection techniques. This method updates sample weights by\nconsidering both the current model's predictions and the outputs from prior\nrounds, allowing for effective sparsity control. The SVBM framework enhances\nthe ability to form complex decision boundaries, thereby improving\nclassification performance. The MATLAB source code for SVBM can be accessed at\nhttps://github.com/junbolian/SVBM.\n","authors":["Junbo Jacob Lian"],"pdf_url":"https://arxiv.org/pdf/2410.06957v1.pdf","comment":"The MATLAB source code for SVBM can be accessed at\n  https://github.com/junbolian/SVBM"},{"id":"http://arxiv.org/abs/2407.00710v3","updated":"2024-10-09T14:51:23Z","published":"2024-06-30T14:21:32Z","title":"Directly Handling Missing Data in Linear Discriminant Analysis for\n  Enhancing Classification Accuracy and Interpretability","summary":"  As the adoption of Artificial Intelligence (AI) models expands into critical\nreal-world applications, ensuring the explainability of these models becomes\nparamount, particularly in sensitive fields such as medicine and finance.\nLinear Discriminant Analysis (LDA) remains a popular choice for classification\ndue to its interpretable nature, derived from its capacity to model class\ndistributions and enhance class separation through linear combinations of\nfeatures. However, real-world datasets often suffer from incomplete data,\nposing substantial challenges for both classification accuracy and model\ninterpretability. In this paper, we introduce a novel and robust classification\nmethod, termed Weighted missing Linear Discriminant Analysis (WLDA), which\nextends LDA to handle datasets with missing values without the need for\nimputation. Our approach innovatively incorporates a weight matrix that\npenalizes missing entries, thereby refining parameter estimation directly on\nincomplete data. This methodology not only preserves the interpretability of\nLDA but also significantly enhances classification performance in scenarios\nplagued by missing data. We conduct an in-depth theoretical analysis to\nestablish the properties of WLDA and thoroughly evaluate its explainability.\nExperimental results across various datasets demonstrate that WLDA consistently\noutperforms traditional methods, especially in challenging environments where\nmissing values are prevalent in both training and test datasets. This\nadvancement provides a critical tool for improving classification accuracy and\nmaintaining model transparency in the face of incomplete data.\n","authors":["Tuan L. Vo","Uyen Dang","Thu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.00710v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06950v1","updated":"2024-10-09T14:47:12Z","published":"2024-10-09T14:47:12Z","title":"Faithful Interpretation for Graph Neural Networks","summary":"  Currently, attention mechanisms have garnered increasing attention in Graph\nNeural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph\nTransformers (GTs). It is not only due to the commendable boost in performance\nthey offer but also its capacity to provide a more lucid rationale for model\nbehaviors, which are often viewed as inscrutable. However, Attention-based GNNs\nhave demonstrated instability in interpretability when subjected to various\nsources of perturbations during both training and testing phases, including\nfactors like additional edges or nodes. In this paper, we propose a solution to\nthis problem by introducing a novel notion called Faithful Graph\nAttention-based Interpretation (FGAI). In particular, FGAI has four crucial\nproperties regarding stability and sensitivity to interpretation and final\noutput distribution. Built upon this notion, we propose an efficient\nmethodology for obtaining FGAI, which can be viewed as an ad hoc modification\nto the canonical Attention-based GNNs. To validate our proposed solution, we\nintroduce two novel metrics tailored for graph interpretation assessment.\nExperimental results demonstrate that FGAI exhibits superior stability and\npreserves the interpretability of attention under various forms of\nperturbations and randomness, which makes FGAI a more faithful and reliable\nexplanation tool.\n","authors":["Lijie Hu","Tianhao Huang","Lu Yu","Wanyu Lin","Tianhang Zheng","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06950v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2410.06940v1","updated":"2024-10-09T14:34:53Z","published":"2024-10-09T14:34:53Z","title":"Representation Alignment for Generation: Training Diffusion Transformers\n  Is Easier Than You Think","summary":"  Recent studies have shown that the denoising process in (generative)\ndiffusion models can induce meaningful (discriminative) representations inside\nthe model, though the quality of these representations still lags behind those\nlearned through recent self-supervised learning methods. We argue that one main\nbottleneck in training large-scale diffusion models for generation lies in\neffectively learning these representations. Moreover, training can be made\neasier by incorporating high-quality external visual representations, rather\nthan relying solely on the diffusion models to learn them independently. We\nstudy this by introducing a straightforward regularization called\nREPresentation Alignment (REPA), which aligns the projections of noisy input\nhidden states in denoising networks with clean image representations obtained\nfrom external, pretrained visual encoders. The results are striking: our simple\nstrategy yields significant improvements in both training efficiency and\ngeneration quality when applied to popular diffusion and flow-based\ntransformers, such as DiTs and SiTs. For instance, our method can speed up SiT\ntraining by over 17.5$\\times$, matching the performance (without\nclassifier-free guidance) of a SiT-XL model trained for 7M steps in less than\n400K steps. In terms of final generation quality, our approach achieves\nstate-of-the-art results of FID=1.42 using classifier-free guidance with the\nguidance interval.\n","authors":["Sihyun Yu","Sangkyung Kwak","Huiwon Jang","Jongheon Jeong","Jonathan Huang","Jinwoo Shin","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2410.06940v1.pdf","comment":"Preprint. Project page: https://sihyun.me/REPA"},{"id":"http://arxiv.org/abs/2407.20271v2","updated":"2024-10-09T14:30:08Z","published":"2024-07-25T07:09:35Z","title":"Learn while Unlearn: An Iterative Unlearning Framework for Generative\n  Language Models","summary":"  Recent advancements in machine learning, particularly in Natural Language\nProcessing (NLP), have led to the development of sophisticated models trained\non extensive datasets, yet raising concerns about the potential leakage of\nsensitive information. In response, regulatory measures such as the European\nUnion's General Data Protection Regulation (GDPR) have driven increasing\ninterest in Machine Unlearning techniques, which enable models to selectively\nforget specific data entries. Early approaches primarily relied on\npre-processing methods, while more recent research has shifted towards\ntraining-based unlearning techniques. Despite their effectiveness, most\nexisting methods require access to the original training data, which is often\ninaccessible. Additionally, directly applying unlearning techniques bear the\ncost of undermining the model's expressive capabilities. To address these\nchallenges, we introduce the Iterative Contrastive Unlearning (ICU) framework,\nwhich consists of three core components: A Knowledge Unlearning Induction\nmodule designed to remove specific knowledge through an unlearning loss; A\nContrastive Learning Enhancement module to preserve the model's expressive\ncapabilities against the pure unlearning goal; And an Iterative Unlearning\nRefinement module that dynamically assess the unlearning extent on specific\ndata pieces and make iterative update. Experimental results demonstrate the\nefficacy of our ICU method in unlearning sensitive information while\nmaintaining the model's overall performance, offering a promising solution for\nprivacy-conscious machine learning applications.\n","authors":["Haoyu Tang","Ye Liu","Xukai Liu","Kai Zhang","Yanghai Zhang","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.20271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06935v1","updated":"2024-10-09T14:29:50Z","published":"2024-10-09T14:29:50Z","title":"Predicting Bitcoin Market Trends with Enhanced Technical Indicator\n  Integration and Classification Models","summary":"  Thanks to the high potential for profit, trading has become increasingly\nattractive to investors as the cryptocurrency and stock markets rapidly expand.\nHowever, because financial markets are intricate and dynamic, accurately\npredicting prices remains a significant challenge. The volatile nature of the\ncryptocurrency market makes it even harder for traders and investors to make\ndecisions. This study presents a machine learning model based on classification\nto forecast the direction of the cryptocurrency market, i.e., whether prices\nwill increase or decrease. The model is trained using historical data and\nimportant technical indicators such as the Moving Average Convergence\nDivergence, the Relative Strength Index, and Bollinger Bands. We illustrate our\napproach with an empirical study of the closing price of Bitcoin. Several\nsimulations, including a confusion matrix and Receiver Operating Characteristic\ncurve, are used to assess the model's performance, and the results show a\nbuy/sell signal accuracy of over 92%. These findings demonstrate how machine\nlearning models can assist investors and traders of cryptocurrencies in making\nwise/informed decisions in a very volatile market.\n","authors":["Abdelatif Hafid","Mohamed Rahouti","Linglong Kong","Maad Ebrahim","Mohamed Adel Serhani"],"pdf_url":"https://arxiv.org/pdf/2410.06935v1.pdf","comment":"12 pages, 8 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2405.19452v2","updated":"2024-10-09T14:27:51Z","published":"2024-05-29T19:02:57Z","title":"Gaitor: Learning a Unified Representation Across Gaits for Real-World\n  Quadruped Locomotion","summary":"  The current state-of-the-art in quadruped locomotion is able to produce a\nvariety of complex motions. These methods either rely on switching between a\ndiscrete set of skills or learn a distribution across gaits using complex\nblack-box models. Alternatively, we present Gaitor, which learns a disentangled\nand 2D representation across locomotion gaits. This learnt representation forms\na planning space for closed-loop control delivering continuous gait transitions\nand perceptive terrain traversal. Gaitor's latent space is readily\ninterpretable and we discover that during gait transitions, novel unseen gaits\nemerge. The latent space is disentangled with respect to footswing heights and\nlengths. This means that these gait characteristics can be varied independently\nin the 2D latent representation. Together with a simple terrain encoding and a\nlearnt planner operating in the latent space, Gaitor can take motion commands\nincluding desired gait type and swing characteristics all while reacting to\nuneven terrain. We evaluate Gaitor in both simulation and the real world on the\nANYmal C platform. To the best of our knowledge, this is the first work\nlearning a unified and interpretable latent space for multiple gaits, resulting\nin continuous blending between different locomotion modes on a real quadruped\nrobot. An overview of the methods and results in this paper is found at\nhttps://youtu.be/eVFQbRyilCA.\n","authors":["Alexander L. Mitchell","Wolfgang Merkt","Aristotelis Papatheodorou","Ioannis Havoutis","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2405.19452v2.pdf","comment":"14 pages, 8 figures, 2 tables, Accepted to CoRL 2024"},{"id":"http://arxiv.org/abs/2406.18293v2","updated":"2024-10-09T14:24:24Z","published":"2024-06-26T12:23:54Z","title":"Combining Automated Optimisation of Hyperparameters and Reward Shape","summary":"  There has been significant progress in deep reinforcement learning (RL) in\nrecent years. Nevertheless, finding suitable hyperparameter configurations and\nreward functions remains challenging even for experts, and performance heavily\nrelies on these design choices. Also, most RL research is conducted on known\nbenchmarks where knowledge about these choices already exists. However, novel\npractical applications often pose complex tasks for which no prior knowledge\nabout good hyperparameters and reward functions is available, thus\nnecessitating their derivation from scratch. Prior work has examined\nautomatically tuning either hyperparameters or reward functions individually.\nWe demonstrate empirically that an RL algorithm's hyperparameter configurations\nand reward function are often mutually dependent, meaning neither can be fully\noptimised without appropriate values for the other. We then propose a\nmethodology for the combined optimisation of hyperparameters and the reward\nfunction. Furthermore, we include a variance penalty as an optimisation\nobjective to improve the stability of learned policies. We conducted extensive\nexperiments using Proximal Policy Optimisation and Soft Actor-Critic on four\nenvironments. Our results show that combined optimisation significantly\nimproves over baseline performance in half of the environments and achieves\ncompetitive performance in the others, with only a minor increase in\ncomputational costs. This suggests that combined optimisation should be best\npractice.\n","authors":["Julian Dierkes","Emma Cramer","Holger H. Hoos","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2406.18293v2.pdf","comment":"Published in the Reinforcement Learning Journal 2024"},{"id":"http://arxiv.org/abs/2410.06927v1","updated":"2024-10-09T14:21:59Z","published":"2024-10-09T14:21:59Z","title":"Spectral and Rhythm Features for Audio Classification with Deep\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) are widely used in computer vision. They\ncan be used not only for conventional digital image material to recognize\npatterns, but also for feature extraction from digital imagery representing\nspectral and rhythm features extracted from time-domain digital audio signals\nfor the acoustic classification of sounds. Different spectral and rhythm\nfeature representations like mel-scaled spectrograms, mel-frequency cepstral\ncoefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)\nchromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams are investigated in terms of the audio\nclassification performance using a deep convolutional neural network. It can be\nclearly shown that the mel-scaled spectrograms and the mel-frequency cepstral\ncoefficients (MFCCs) perform significantly better then the other spectral and\nrhythm features investigated in this research for audio classification tasks\nusing deep CNNs. The experiments were carried out with the aid of the ESC-50\ndataset with 2,000 labeled environmental audio recordings.\n","authors":["Friedrich Wolf-Monheim"],"pdf_url":"https://arxiv.org/pdf/2410.06927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06922v1","updated":"2024-10-09T14:19:33Z","published":"2024-10-09T14:19:33Z","title":"Estimating Exoplanet Mass using Machine Learning on Incomplete Datasets","summary":"  The exoplanet archive is an incredible resource of information on the\nproperties of discovered extrasolar planets, but statistical analysis has been\nlimited by the number of missing values. One of the most informative bulk\nproperties is planet mass, which is particularly challenging to measure with\nmore than 70\\% of discovered planets with no measured value. We compare the\ncapabilities of five different machine learning algorithms that can utilize\nmultidimensional incomplete datasets to estimate missing properties for\nimputing planet mass. The results are compared when using a partial subset of\nthe archive with a complete set of six planet properties, and where all planet\ndiscoveries are leveraged in an incomplete set of six and eight planet\nproperties. We find that imputation results improve with more data even when\nthe additional data is incomplete, and allows a mass prediction for any planet\nregardless of which properties are known. Our favored algorithm is the newly\ndeveloped $k$NN$\\times$KDE, which can return a probability distribution for the\nimputed properties. The shape of this distribution can indicate the algorithm's\nlevel of confidence, and also inform on the underlying demographics of the\nexoplanet population. We demonstrate how the distributions can be interpreted\nwith a series of examples for planets where the discovery was made with either\nthe transit method, or radial velocity method. Finally, we test the generative\ncapability of the $k$NN$\\times$KDE to create a large synthetic population of\nplanets based on the archive, and identify potential categories of planets from\ngroups of properties in the multidimensional space. All codes are Open Source.\n","authors":["Florian Lalande","Elizabeth Tasker","Kenji Doya"],"pdf_url":"https://arxiv.org/pdf/2410.06922v1.pdf","comment":"30 pages, 14 figures, 1 table. Accepted for publication in the Open\n  Journal of Astrophysics"},{"id":"http://arxiv.org/abs/2410.06921v1","updated":"2024-10-09T14:18:52Z","published":"2024-10-09T14:18:52Z","title":"Adversarial Vulnerability as a Consequence of On-Manifold Inseparibility","summary":"  Recent works have shown theoretically and empirically that redundant data\ndimensions are a source of adversarial vulnerability. However, the inverse\ndoesn't seem to hold in practice; employing dimension-reduction techniques\ndoesn't exhibit robustness as expected. In this work, we consider\nclassification tasks and characterize the data distribution as a\nlow-dimensional manifold, with high/low variance features defining the on/off\nmanifold direction. We argue that clean training experiences poor convergence\nin the off-manifold direction caused by the ill-conditioning in widely used\nfirst-order optimizers like gradient descent. The poor convergence then acts as\na source of adversarial vulnerability when the dataset is inseparable in the\non-manifold direction. We provide theoretical results for logistic regression\nand a 2-layer linear network on the considered data distribution. Furthermore,\nwe advocate using second-order methods that are immune to ill-conditioning and\nlead to better robustness. We perform experiments and exhibit tremendous\nrobustness improvements in clean training through long training and the\nemployment of second-order methods, corroborating our framework. Additionally,\nwe find the inclusion of batch-norm layers hinders such robustness gains. We\nattribute this to differing implicit biases between traditional and\nbatch-normalized neural networks.\n","authors":["Rajdeep Haldar","Yue Xing","Qifan Song","Guang Lin"],"pdf_url":"https://arxiv.org/pdf/2410.06921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06912v1","updated":"2024-10-09T14:12:50Z","published":"2024-10-09T14:12:50Z","title":"Compositional Entailment Learning for Hyperbolic Vision-Language Models","summary":"  Image-text representation learning forms a cornerstone in vision-language\nmodels, where pairs of images and textual descriptions are contrastively\naligned in a shared embedding space. Since visual and textual concepts are\nnaturally hierarchical, recent work has shown that hyperbolic space can serve\nas a high-potential manifold to learn vision-language representation with\nstrong downstream performance. In this work, for the first time we show how to\nfully leverage the innate hierarchical nature of hyperbolic embeddings by\nlooking beyond individual image-text pairs. We propose Compositional Entailment\nLearning for hyperbolic vision-language models. The idea is that an image is\nnot only described by a sentence but is itself a composition of multiple object\nboxes, each with their own textual description. Such information can be\nobtained freely by extracting nouns from sentences and using openly available\nlocalized grounding models. We show how to hierarchically organize images,\nimage boxes, and their textual descriptions through contrastive and\nentailment-based objectives. Empirical evaluation on a hyperbolic\nvision-language model trained with millions of image-text pairs shows that the\nproposed compositional learning approach outperforms conventional Euclidean\nCLIP learning, as well as recent hyperbolic alternatives, with better zero-shot\nand retrieval generalization and clearly stronger hierarchical performance.\n","authors":["Avik Pal","Max van Spengler","Guido Maria D'Amely di Melendugno","Alessandro Flaborea","Fabio Galasso","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2410.06912v1.pdf","comment":"23 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.06895v1","updated":"2024-10-09T13:58:41Z","published":"2024-10-09T13:58:41Z","title":"Average Certified Radius is a Poor Metric for Randomized Smoothing","summary":"  Randomized smoothing is a popular approach for providing certified robustness\nguarantees against adversarial attacks, and has become a very active area of\nresearch. Over the past years, the average certified radius (ACR) has emerged\nas the single most important metric for comparing methods and tracking progress\nin the field. However, in this work, we show that ACR is an exceptionally poor\nmetric for evaluating robustness guarantees provided by randomized smoothing.\nWe theoretically show not only that a trivial classifier can have arbitrarily\nlarge ACR, but also that ACR is much more sensitive to improvements on easy\nsamples than on hard ones. Empirically, we confirm that existing training\nstrategies that improve ACR reduce the model's robustness on hard samples.\nFurther, we show that by focusing on easy samples, we can effectively replicate\nthe increase in ACR. We develop strategies, including explicitly discarding\nhard samples, reweighing the dataset with certified radius, and extreme\noptimization for easy samples, to achieve state-of-the-art ACR, although these\nstrategies ignore robustness for the general data distribution. Overall, our\nresults suggest that ACR has introduced a strong undesired bias to the field,\nand better metrics are required to holistically evaluate randomized smoothing.\n","authors":["Chenhao Sun","Yuhao Mao","Mark Niklas Mller","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.06895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04154v2","updated":"2024-10-09T13:56:28Z","published":"2024-10-05T13:29:25Z","title":"Applying Quantum Autoencoders for Time Series Anomaly Detection","summary":"  Anomaly detection is an important problem with applications in various\ndomains such as fraud detection, pattern recognition or medical diagnosis.\nSeveral algorithms have been introduced using classical computing approaches.\nHowever, using quantum computing for solving anomaly detection problems in time\nseries data is a widely unexplored research field.\n  This paper explores the application of quantum autoencoders to time series\nanomaly detection. We investigate two primary techniques for classifying\nanomalies: (1) Analyzing the reconstruction error generated by the quantum\nautoencoder and (2) latent representation analysis. Our simulated experimental\nresults, conducted across various ansaetze, demonstrate that quantum\nautoencoders consistently outperform classical deep learning-based autoencoders\nacross multiple datasets. Specifically, quantum autoencoders achieve superior\nanomaly detection performance while utilizing 60-230 times fewer parameters and\nrequiring five times fewer training iterations. In addition, we implement our\nquantum encoder on real quantum hardware. Our experimental results demonstrate\nthat quantum autoencoders achieve anomaly detection performance on par with\ntheir simulated counterparts.\n","authors":["Robin Frehner","Kurt Stockinger"],"pdf_url":"https://arxiv.org/pdf/2410.04154v2.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2409.07351v2","updated":"2024-10-09T13:55:01Z","published":"2024-09-11T15:37:52Z","title":"Federated Impression for Learning with Distributed Heterogeneous Data","summary":"  Standard deep learning-based classification approaches may not always be\npractical in real-world clinical applications, as they require a centralized\ncollection of all samples. Federated learning (FL) provides a paradigm that can\nlearn from distributed datasets across clients without requiring them to share\ndata, which can help mitigate privacy and data ownership issues. In FL,\nsub-optimal convergence caused by data heterogeneity is common among data from\ndifferent health centers due to the variety in data collection protocols and\npatient demographics across centers. Through experimentation in this study, we\nshow that data heterogeneity leads to the phenomenon of catastrophic forgetting\nduring local training. We propose FedImpres which alleviates catastrophic\nforgetting by restoring synthetic data that represents the global information\nas federated impression. To achieve this, we distill the global model resulting\nfrom each communication round. Subsequently, we use the synthetic data\nalongside the local data to enhance the generalization of local training.\nExtensive experiments show that the proposed method achieves state-of-the-art\nperformance on both the BloodMNIST and Retina datasets, which contain label\nimbalance and domain shift, with an improvement in classification accuracy of\nup to 20%.\n","authors":["Atrin Arya","Sana Ayromlou","Armin Saadat","Purang Abolmaesumi","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2409.07351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04655v2","updated":"2024-10-09T13:46:31Z","published":"2024-10-06T23:55:34Z","title":"Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear\n  Diffusive Parametric PDEs on Multiple Domains","summary":"  Predicting time-dependent dynamics of complex systems governed by non-linear\npartial differential equations (PDEs) with varying parameters and domains is a\nchallenging task motivated by applications across various fields. We introduce\na novel family of neural operators based on our Graph Fourier Neural Kernels,\ndesigned to learn solution generators for nonlinear PDEs in which the\nhighest-order term is diffusive, across multiple domains and parameters. G-FuNK\ncombines components that are parameter- and domain-adapted with others that are\nnot. The domain-adapted components are constructed using a weighted graph on\nthe discretized domain, where the graph Laplacian approximates the\nhighest-order diffusive term, ensuring boundary condition compliance and\ncapturing the parameter and domain-specific behavior. Meanwhile, the learned\ncomponents transfer across domains and parameters using our variant Fourier\nNeural Operators. This approach naturally embeds geometric and directional\ninformation, improving generalization to new test domains without need for\nretraining the network. To handle temporal dynamics, our method incorporates an\nintegrated ODE solver to predict the evolution of the system. Experiments show\nG-FuNK's capability to accurately approximate heat, reaction diffusion, and\ncardiac electrophysiology equations across various geometries and anisotropic\ndiffusivity fields. G-FuNK achieves low relative errors on unseen domains and\nfiber fields, significantly accelerating predictions compared to traditional\nfinite-element solvers.\n","authors":["Shane E. Loeffler","Zan Ahmad","Syed Yusuf Ali","Carolyna Yamamoto","Dan M. Popescu","Alana Yee","Yash Lal","Natalia Trayanova","Mauro Maggioni"],"pdf_url":"https://arxiv.org/pdf/2410.04655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06884v1","updated":"2024-10-09T13:46:08Z","published":"2024-10-09T13:46:08Z","title":"Adaptive Refinement Protocols for Distributed Distribution Estimation\n  under $\\ell^p$-Losses","summary":"  Consider the communication-constrained estimation of discrete distributions\nunder $\\ell^p$ losses, where each distributed terminal holds multiple\nindependent samples and uses limited number of bits to describe the samples. We\nobtain the minimax optimal rates of the problem in most parameter regimes. An\nelbow effect of the optimal rates at $p=2$ is clearly identified. To show the\noptimal rates, we first design estimation protocols to achieve them. The key\ningredient of these protocols is to introduce adaptive refinement mechanisms,\nwhich first generate rough estimate by partial information and then establish\nrefined estimate in subsequent steps guided by the rough estimate. The\nprotocols leverage successive refinement, sample compression and thresholding\nmethods to achieve the optimal rates in different parameter regimes. The\noptimality of the protocols is shown by deriving compatible minimax lower\nbounds.\n","authors":["Deheng Yuan","Tao Guo","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06883v1","updated":"2024-10-09T13:45:54Z","published":"2024-10-09T13:45:54Z","title":"Degree Distribution based Spiking Graph Networks for Domain Adaptation","summary":"  Spiking Graph Networks (SGNs) have garnered significant attraction from both\nresearchers and industry due to their ability to address energy consumption\nchallenges in graph classification. However, SGNs are only effective for\nin-distribution data and cannot tackle out-of-distribution data. In this paper,\nwe first propose the domain adaptation problem in SGNs, and introduce a novel\nframework named Degree-aware Spiking Graph Domain Adaptation for\nClassification. The proposed DeSGDA addresses the spiking graph domain\nadaptation problem by three aspects: node degree-aware personalized spiking\nrepresentation, adversarial feature distribution alignment, and pseudo-label\ndistillation. First, we introduce the personalized spiking representation\nmethod for generating degree-dependent spiking signals. Specifically, the\nthreshold of triggering a spike is determined by the node degree, allowing this\npersonalized approach to capture more expressive information for\nclassification. Then, we propose the graph feature distribution alignment\nmodule that is adversarially trained using membrane potential against a domain\ndiscriminator. Such an alignment module can efficiently maintain high\nperformance and low energy consumption in the case of inconsistent\ndistribution. Additionally, we extract consistent predictions across two spaces\nto create reliable pseudo-labels, effectively leveraging unlabeled data to\nenhance graph classification performance. Extensive experiments on benchmark\ndatasets validate the superiority of the proposed DeSGDA compared with\ncompetitive baselines.\n","authors":["Yingxu Wang","Siwei Liu","Mengzhu Wang","Shangsong Liang","Nan Yin"],"pdf_url":"https://arxiv.org/pdf/2410.06883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06878v1","updated":"2024-10-09T13:43:17Z","published":"2024-10-09T13:43:17Z","title":"Noise is All You Need: Private Second-Order Convergence of Noisy SGD","summary":"  Private optimization is a topic of major interest in machine learning, with\ndifferentially private stochastic gradient descent (DP-SGD) playing a key role\nin both theory and practice. Furthermore, DP-SGD is known to be a powerful tool\nin contexts beyond privacy, including robustness, machine unlearning, etc.\nExisting analyses of DP-SGD either make relatively strong assumptions (e.g.,\nLipschitz continuity of the loss function, or even convexity) or prove only\nfirst-order convergence (and thus might end at a saddle point in the non-convex\nsetting). At the same time, there has been progress in proving second-order\nconvergence of the non-private version of ``noisy SGD'', as well as progress in\ndesigning algorithms that are more complex than DP-SGD and do guarantee\nsecond-order convergence. We revisit DP-SGD and show that ``noise is all you\nneed'': the noise necessary for privacy already implies second-order\nconvergence under the standard smoothness assumptions, even for non-Lipschitz\nloss functions. Hence, we get second-order convergence essentially for free:\nDP-SGD, the workhorse of modern private optimization, under minimal assumptions\ncan be used to find a second-order stationary point.\n","authors":["Dmitrii Avdiukhin","Michael Dinitz","Chenglin Fan","Grigory Yaroslavtsev"],"pdf_url":"https://arxiv.org/pdf/2410.06878v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2409.06927v3","updated":"2024-10-09T13:39:27Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, I extend the idea of\nactive steering with vectors that represent a behavioral direction of interest\nto tuning those vectors directly into the model, obviating the need for online\ncontrol. First, I identify activation vectors related to honesty in an\nopen-source LLM (Llama- 2-13b-chat). Next, I demonstrate that model output can\nbe made more or less honest by adding positive or negative multiples of these\nvectors to residual stream activations during generation. Then, I show that a\nsimilar effect can be achieved by fine-tuning the vectors directly into the\nmodel, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, I compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning; tuned\nmodels are available at https://huggingface.co/collections/cackerman/\nrepresentation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v3.pdf","comment":"9 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.06875v1","updated":"2024-10-09T13:38:59Z","published":"2024-10-09T13:38:59Z","title":"Group Shapley Value and Counterfactual Simulations in a Structural Model","summary":"  We propose a variant of the Shapley value, the group Shapley value, to\ninterpret counterfactual simulations in structural economic models by\nquantifying the importance of different components. Our framework compares two\nsets of parameters, partitioned into multiple groups, and applying group\nShapley value decomposition yields unique additive contributions to the changes\nbetween these sets. The relative contributions sum to one, enabling us to\ngenerate an importance table that is as easily interpretable as a regression\ntable. The group Shapley value can be characterized as the solution to a\nconstrained weighted least squares problem. Using this property, we develop\nrobust decomposition methods to address scenarios where inputs for the group\nShapley value are missing. We first apply our methodology to a simple Roy model\nand then illustrate its usefulness by revisiting two published papers.\n","authors":["Yongchan Kwon","Sokbae Lee","Guillaume A. Pouliot"],"pdf_url":"https://arxiv.org/pdf/2410.06875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05525v2","updated":"2024-10-09T13:31:25Z","published":"2024-02-08T10:05:11Z","title":"Differentially Private Deep Model-Based Reinforcement Learning","summary":"  We address private deep offline reinforcement learning (RL), where the goal\nis to train a policy on standard control tasks that is differentially private\n(DP) with respect to individual trajectories in the dataset. To achieve this,\nwe introduce PriMORL, a model-based RL algorithm with formal differential\nprivacy guarantees. PriMORL first learns an ensemble of trajectory-level DP\nmodels of the environment from offline data. It then optimizes a policy on the\npenalized private model, without any further interaction with the system or\naccess to the dataset. In addition to offering strong theoretical foundations,\nwe demonstrate empirically that PriMORL enables the training of private RL\nagents on offline continuous control tasks with deep function approximations,\nwhereas current methods are limited to simpler tabular and linear Markov\nDecision Processes (MDPs). We furthermore outline the trade-offs involved in\nachieving privacy in this setting.\n","authors":["Alexandre Rio","Merwan Barlier","Igor Colin","Albert Thomas"],"pdf_url":"https://arxiv.org/pdf/2402.05525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06851v1","updated":"2024-10-09T13:14:11Z","published":"2024-10-09T13:14:11Z","title":"Understanding Model Ensemble in Transferable Adversarial Attack","summary":"  Model ensemble adversarial attack has become a powerful method for generating\ntransferable adversarial examples that can target even unknown models, but its\ntheoretical foundation remains underexplored. To address this gap, we provide\nearly theoretical insights that serve as a roadmap for advancing model ensemble\nadversarial attack. We first define transferability error to measure the error\nin adversarial transferability, alongside concepts of diversity and empirical\nmodel ensemble Rademacher complexity. We then decompose the transferability\nerror into vulnerability, diversity, and a constant, which rigidly explains the\norigin of transferability error in model ensemble attack: the vulnerability of\nan adversarial example to ensemble components, and the diversity of ensemble\ncomponents. Furthermore, we apply the latest mathematical tools in information\ntheory to bound the transferability error using complexity and generalization\nterms, contributing to three practical guidelines for reducing transferability\nerror: (1) incorporating more surrogate models, (2) increasing their diversity,\nand (3) reducing their complexity in cases of overfitting. Finally, extensive\nexperiments with 54 models validate our theoretical framework, representing a\nsignificant step forward in understanding transferable model ensemble\nadversarial attacks.\n","authors":["Wei Yao","Zeliang Zhang","Huayi Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02592v2","updated":"2024-10-09T13:13:59Z","published":"2024-10-03T15:34:41Z","title":"IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of\n  Both Driver and Passengers","summary":"  Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.\n","authors":["Zihan Fang","Zheng Lin","Senkang Hu","Hangcheng Cao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.02592v2.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.06848v1","updated":"2024-10-09T13:08:14Z","published":"2024-10-09T13:08:14Z","title":"Forgetting Through Transforming: Enabling Federated Unlearning via\n  Class-Aware Representation Transformation","summary":"  Federated Unlearning (FU) enables clients to selectively remove the influence\nof specific data from a trained federated learning model, addressing privacy\nconcerns and regulatory requirements. However, existing FU methods often\nstruggle to balance effective erasure with model utility preservation,\nespecially for class-level unlearning in non-IID settings. We propose Federated\nUnlearning via Class-aware Representation Transformation (FUCRT), a novel\nmethod that achieves unlearning through class-aware representation\ntransformation. FUCRT employs two key components: (1) a transformation class\nselection strategy to identify optimal forgetting directions, and (2) a\ntransformation alignment technique using dual class-aware contrastive learning\nto ensure consistent transformations across clients. Extensive experiments on\nfour datasets demonstrate FUCRT's superior performance in terms of erasure\nguarantee, model utility preservation, and efficiency. FUCRT achieves complete\n(100\\%) erasure of unlearning classes while maintaining or improving\nperformance on remaining classes, outperforming state-of-the-art baselines\nacross both IID and Non-IID settings. Analysis of the representation space\nreveals FUCRT's ability to effectively merge unlearning class representations\nwith the transformation class from remaining classes, closely mimicking the\nmodel retrained from scratch.\n","authors":["Qi Guo","Zhen Tian","Minghao Yao","Yong Qi","Saiyu Qi","Yun Li","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2410.06848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06847v1","updated":"2024-10-09T13:07:24Z","published":"2024-10-09T13:07:24Z","title":"A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement\n  Learning and Application in UAV Hovering","summary":"  This paper proposes a safety modulator actor-critic (SMAC) method to address\nsafety constraint and overestimation mitigation in model-free safe\nreinforcement learning (RL). A safety modulator is developed to satisfy safety\nconstraints by modulating actions, allowing the policy to ignore safety\nconstraint and focus on maximizing reward. Additionally, a distributional\ncritic with a theoretical update rule for SMAC is proposed to mitigate the\noverestimation of Q-values with safety constraints. Both simulation and\nreal-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering\nconfirm that the SMAC can effectively maintain safety constraints and\noutperform mainstream baseline algorithms.\n","authors":["Qihan Qi","Xinsong Yang","Gang Xia","Daniel W. C. Ho","Pengyang Tang"],"pdf_url":"https://arxiv.org/pdf/2410.06847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06846v1","updated":"2024-10-09T13:06:43Z","published":"2024-10-09T13:06:43Z","title":"Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity","summary":"  Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.\n","authors":["Mutian He","Philip N. Garner"],"pdf_url":"https://arxiv.org/pdf/2410.06846v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.12489v4","updated":"2024-10-09T13:04:29Z","published":"2024-05-21T04:18:57Z","title":"Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks","summary":"  Exploring the loss landscape offers insights into the inherent principles of\ndeep neural networks (DNNs). Recent work suggests an additional asymmetry of\nthe valley beyond the flat and sharp ones, yet without thoroughly examining its\ncauses or implications. Our study methodically explores the factors affecting\nthe symmetry of DNN valleys, encompassing (1) the dataset, network\narchitecture, initialization, and hyperparameters that influence the\nconvergence point; and (2) the magnitude and direction of the noise for 1D\nvisualization. Our major observation shows that the {\\it degree of sign\nconsistency} between the noise and the convergence point is a critical\nindicator of valley symmetry. Theoretical insights from the aspects of ReLU\nactivation and softmax function could explain the interesting phenomenon. Our\ndiscovery propels novel understanding and applications in the scenario of Model\nFusion: (1) the efficacy of interpolating separate models significantly\ncorrelates with their sign consistency ratio, and (2) imposing sign alignment\nduring federated learning emerges as an innovative approach for model parameter\nalignment.\n","authors":["Xin-Chun Li","Jin-Lin Tang","Bo Zhang","Lan Li","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2405.12489v4.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.01408v2","updated":"2024-10-09T12:57:37Z","published":"2024-02-02T13:42:12Z","title":"Counterfactual Concept Bottleneck Models","summary":"  Current deep learning models are not designed to simultaneously address three\nfundamental questions: predict class labels to solve a given classification\ntask (the \"What?\"), simulate changes in the situation to evaluate how this\nimpacts class predictions (the \"How?\"), and imagine how the scenario should\nchange to result in different class predictions (the \"Why not?\"). The inability\nto answer these questions represents a crucial gap in deploying reliable AI\nagents, calibrating human trust, and improving human-machine interaction. To\nbridge this gap, we introduce CounterFactual Concept Bottleneck Models\n(CF-CBMs), a class of models designed to efficiently address the above queries\nall at once without the need to run post-hoc searches. Our experimental results\ndemonstrate that CF-CBMs: achieve classification accuracy comparable to\nblack-box models and existing CBMs (\"What?\"), rely on fewer important concepts\nleading to simpler explanations (\"How?\"), and produce interpretable,\nconcept-based counterfactuals (\"Why not?\"). Additionally, we show that training\nthe counterfactual generator jointly with the CBM leads to two key\nimprovements: (i) it alters the model's decision-making process, making the\nmodel rely on fewer important concepts (leading to simpler explanations), and\n(ii) it significantly increases the causal effect of concept interventions on\nclass predictions, making the model more responsive to these changes.\n","authors":["Gabriele Dominici","Pietro Barbiero","Francesco Giannini","Martin Gjoreski","Giuseppe Marra","Marc Langheinrich"],"pdf_url":"https://arxiv.org/pdf/2402.01408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06833v1","updated":"2024-10-09T12:50:50Z","published":"2024-10-09T12:50:50Z","title":"Dynamic metastability in the self-attention model","summary":"  We consider the self-attention model - an interacting particle system on the\nunit sphere, which serves as a toy model for Transformers, the deep neural\nnetwork architecture behind the recent successes of large language models. We\nprove the appearance of dynamic metastability conjectured in [GLPR23] -\nalthough particles collapse to a single cluster in infinite time, they remain\ntrapped near a configuration of several clusters for an exponentially long\nperiod of time. By leveraging a gradient flow interpretation of the system, we\nalso connect our result to an overarching framework of slow motion of gradient\nflows proposed by Otto and Reznikoff [OR07] in the context of coarsening and\nthe Allen-Cahn equation. We finally probe the dynamics beyond the exponentially\nlong period of metastability, and illustrate that, under an appropriate\ntime-rescaling, the energy reaches its global maximum in finite time and has a\nstaircase profile, with trajectories manifesting saddle-to-saddle-like\nbehavior, reminiscent of recent works in the analysis of training dynamics via\ngradient descent for two-layer neural networks.\n","authors":["Borjan Geshkovski","Hugo Koubbi","Yury Polyanskiy","Philippe Rigollet"],"pdf_url":"https://arxiv.org/pdf/2410.06833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02966v3","updated":"2024-10-09T12:46:40Z","published":"2024-03-05T13:43:58Z","title":"Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering","summary":"  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.\n","authors":["Sungho Ko","Hyunjin Cho","Hyungjoo Chae","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2403.02966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06828v1","updated":"2024-10-09T12:40:31Z","published":"2024-10-09T12:40:31Z","title":"Transfer Learning for a Class of Cascade Dynamical Systems","summary":"  This work considers the problem of transfer learning in the context of\nreinforcement learning. Specifically, we consider training a policy in a\nreduced order system and deploying it in the full state system. The motivation\nfor this training strategy is that running simulations in the full-state system\nmay take excessive time if the dynamics are complex. While transfer learning\nalleviates the computational issue, the transfer guarantees depend on the\ndiscrepancy between the two systems. In this work, we consider a class of\ncascade dynamical systems, where the dynamics of a subset of the state-space\ninfluence the rest of the states but not vice-versa. The reinforcement learning\npolicy learns in a model that ignores the dynamics of these states and treats\nthem as commanded inputs. In the full-state system, these dynamics are handled\nusing a classic controller (e.g., a PID). These systems have vast applications\nin the control literature and their structure allows us to provide transfer\nguarantees that depend on the stability of the inner loop controller. Numerical\nexperiments on a quadrotor support the theoretical findings.\n","authors":["Shima Rabiei","Sandipan Mishra","Santiago Paternain"],"pdf_url":"https://arxiv.org/pdf/2410.06828v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2402.12265v2","updated":"2024-10-09T12:38:26Z","published":"2024-02-19T16:26:40Z","title":"On the Byzantine-Resilience of Distillation-Based Federated Learning","summary":"  Federated Learning (FL) algorithms using Knowledge Distillation (KD) have\nreceived increasing attention due to their favorable properties with respect to\nprivacy, non-i.i.d. data and communication cost. These methods depart from\ntransmitting model parameters and instead communicate information about a\nlearning task by sharing predictions on a public dataset. In this work, we\nstudy the performance of such approaches in the byzantine setting, where a\nsubset of the clients act in an adversarial manner aiming to disrupt the\nlearning process. We show that KD-based FL algorithms are remarkably resilient\nand analyze how byzantine clients can influence the learning process. Based on\nthese insights, we introduce two new byzantine attacks and demonstrate their\nability to break existing byzantine-resilient methods. Additionally, we propose\na novel defence method which enhances the byzantine resilience of KD-based FL\nalgorithms. Finally, we provide a general framework to obfuscate attacks,\nmaking them significantly harder to detect, thereby improving their\neffectiveness. Our findings serve as an important building block in the\nanalysis of byzantine FL, contributing through the development of new attacks\nand new defence mechanisms, further advancing the robustness of KD-based FL\nalgorithms.\n","authors":["Christophe Roux","Max Zimmer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2402.12265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06825v1","updated":"2024-10-09T12:37:12Z","published":"2024-10-09T12:37:12Z","title":"K-SAM: A Prompting Method Using Pretrained U-Net to Improve Zero Shot\n  Performance of SAM on Lung Segmentation in CXR Images","summary":"  In clinical procedures, precise localization of the target area is an\nessential step for clinical diagnosis and screening. For many diagnostic\napplications, lung segmentation of chest X-ray images is an essential first\nstep that significantly reduces the image size to speed up the subsequent\nanalysis. One of the primary difficulties with this task is segmenting the lung\nregions covered by dense abnormalities also known as opacities due to diseases\nlike pneumonia and tuberculosis. SAM has astonishing generalization\ncapabilities for category agnostic segmentation. In this study we propose an\nalgorithm to improve zero shot performance of SAM on lung region segmentation\ntask by automatic prompt selection. Two separate UNet models were trained, one\nfor predicting lung segments and another for heart segment. Though these\npredictions lack fine details around the edges, they provide positive and\nnegative points as prompt for SAM. Using proposed prompting method zero shot\nperformance of SAM is evaluated on two benchmark datasets. ViT-l version of the\nmodel achieved slightly better performance compared to other two versions, ViTh\nand ViTb. It yields an average Dice score of 95.5 percent and 94.9 percent on\nhold out data for two datasets respectively. Though, for most of the images,\nSAM did outstanding segmentation, its prediction was way off for some of the\nimages. After careful inspection it is found that all of these images either\nhad extreme abnormality or distorted shape. Unlike most of the research\nperformed so far on lung segmentation from CXR images using SAM, this study\nproposes a fully automated prompt selection process only from the input image.\nOur finding indicates that using pretrained models for prompt selection can\nutilize SAM impressive generalization capability to its full extent.\n","authors":["Mohamed Deriche","Mohammad Marufur"],"pdf_url":"https://arxiv.org/pdf/2410.06825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16507v3","updated":"2024-10-09T12:34:31Z","published":"2024-05-26T10:15:20Z","title":"Causal Concept Graph Models: Beyond Causal Opacity in Deep Learning","summary":"  Causal opacity denotes the difficulty in understanding the \"hidden\" causal\nstructure underlying the decisions of deep neural network (DNN) models. This\nleads to the inability to rely on and verify state-of-the-art DNN-based\nsystems, especially in high-stakes scenarios. For this reason, circumventing\ncausal opacity in DNNs represents a key open challenge at the intersection of\ndeep learning, interpretability, and causality. This work addresses this gap by\nintroducing Causal Concept Graph Models (Causal CGMs), a class of interpretable\nmodels whose decision-making process is causally transparent by design. Our\nexperiments show that Causal CGMs can: (i) match the generalisation performance\nof causally opaque models, (ii) enable human-in-the-loop corrections to\nmispredicted intermediate reasoning steps, boosting not just downstream\naccuracy after corrections but also the reliability of the explanations\nprovided for specific instances, and (iii) support the analysis of\ninterventional and counterfactual scenarios, thereby improving the model's\ncausal interpretability and supporting the effective verification of its\nreliability and fairness.\n","authors":["Gabriele Dominici","Pietro Barbiero","Mateo Espinosa Zarlenga","Alberto Termine","Martin Gjoreski","Giuseppe Marra","Marc Langheinrich"],"pdf_url":"https://arxiv.org/pdf/2405.16507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17264v2","updated":"2024-10-09T12:34:19Z","published":"2024-05-27T15:22:58Z","title":"On the Noise Robustness of In-Context Learning for Text Generation","summary":"  Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations.\n","authors":["Hongfu Gao","Feipeng Zhang","Wenyu Jiang","Jun Shu","Feng Zheng","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2405.17264v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.06820v1","updated":"2024-10-09T12:28:32Z","published":"2024-10-09T12:28:32Z","title":"Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed\n  Methods","summary":"  Physics-informed deep learning often faces optimization challenges due to the\ncomplexity of solving partial differential equations (PDEs), which involve\nexploring large solution spaces, require numerous iterations, and can lead to\nunstable training. These challenges arise particularly from the\nill-conditioning of the optimization problem, caused by the differential terms\nin the loss function. To address these issues, we propose learning a solver,\ni.e., solving PDEs using a physics-informed iterative algorithm trained on\ndata. Our method learns to condition a gradient descent algorithm that\nautomatically adapts to each PDE instance, significantly accelerating and\nstabilizing the optimization process and enabling faster convergence of\nphysics-aware models. Furthermore, while traditional physics-informed methods\nsolve for a single PDE instance, our approach addresses parametric PDEs.\nSpecifically, our method integrates the physical loss gradient with the PDE\nparameters to solve over a distribution of PDE parameters, including\ncoefficients, initial conditions, or boundary conditions. We demonstrate the\neffectiveness of our method through empirical experiments on multiple datasets,\ncomparing training and test-time optimization performance.\n","authors":["Lise Le Boudec","Emmanuel de Bezenac","Louis Serrano","Ramon Daniel Regueiro-Espino","Yuan Yin","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2410.06820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06818v1","updated":"2024-10-09T12:19:58Z","published":"2024-10-09T12:19:58Z","title":"An Improved Approach for Cardiac MRI Segmentation based on 3D UNet\n  Combined with Papillary Muscle Exclusion","summary":"  Left ventricular ejection fraction (LVEF) is the most important clinical\nparameter of cardiovascular function. The accuracy in estimating this parameter\nis highly dependent upon the precise segmentation of the left ventricle (LV)\nstructure at the end diastole and systole phases. Therefore, it is crucial to\ndevelop robust algorithms for the precise segmentation of the heart structure\nduring different phases. Methodology: In this work, an improved 3D UNet model\nis introduced to segment the myocardium and LV, while excluding papillary\nmuscles, as per the recommendation of the Society for Cardiovascular Magnetic\nResonance. For the practical testing of the proposed framework, a total of\n8,400 cardiac MRI images were collected and analysed from the military hospital\nin Tunis (HMPIT), as well as the popular ACDC public dataset. As performance\nmetrics, we used the Dice coefficient and the F1 score for validation/testing\nof the LV and the myocardium segmentation. Results: The data was split into\n70%, 10%, and 20% for training, validation, and testing, respectively. It is\nworth noting that the proposed segmentation model was tested across three axis\nviews: basal, medio basal and apical at two different cardiac phases: end\ndiastole and end systole instances. The experimental results showed a Dice\nindex of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end\ndiastolic and systolic phases, respectively. Additionally, clinical evaluation\noutcomes revealed a significant difference in the LVEF and other clinical\nparameters when the papillary muscles were included or excluded.\n","authors":["Narjes Benameur","Ramzi Mahmoudi","Mohamed Deriche","Amira fayouka","Imene Masmoudi","Nessrine Zoghlami"],"pdf_url":"https://arxiv.org/pdf/2410.06818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17611v2","updated":"2024-10-09T12:18:37Z","published":"2024-07-24T19:55:08Z","title":"Adaptive Training of Grid-Dependent Physics-Informed Kolmogorov-Arnold\n  Networks","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a robust framework\nfor solving Partial Differential Equations (PDEs) by approximating their\nsolutions via neural networks and imposing physics-based constraints on the\nloss function. Traditionally, Multilayer Perceptrons (MLPs) have been the\nneural network of choice, with significant progress made in optimizing their\ntraining. Recently, Kolmogorov-Arnold Networks (KANs) were introduced as a\nviable alternative, with the potential of offering better interpretability and\nefficiency while requiring fewer parameters. In this paper, we present a fast\nJAX-based implementation of grid-dependent Physics-Informed Kolmogorov-Arnold\nNetworks (PIKANs) for solving PDEs, achieving up to 84 times faster training\ntimes than the original KAN implementation. We propose an adaptive training\nscheme for PIKANs, introducing an adaptive state transition technique to avoid\nloss function peaks between grid extensions, and a methodology for designing\nPIKANs with alternative basis functions. Through comparative experiments, we\ndemonstrate that the adaptive features significantly enhance solution accuracy,\ndecreasing the L^2 error relative to the reference solution by up to 43.02%.\nFor the studied PDEs, our methodology approaches or surpasses the results\nobtained from architectures that utilize up to 8.5 times more parameters,\nhighlighting the potential of adaptive, grid-dependent PIKANs as a superior\nalternative in scientific and engineering applications.\n","authors":["Spyros Rigas","Michalis Papachristou","Theofilos Papadopoulos","Fotios Anagnostopoulos","Georgios Alexandridis"],"pdf_url":"https://arxiv.org/pdf/2407.17611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06960v2","updated":"2024-10-09T12:16:15Z","published":"2023-11-12T20:57:30Z","title":"Robust Regression over Averaged Uncertainty","summary":"  We propose a new formulation of robust regression by integrating all\nrealizations of the uncertainty set and taking an averaged approach to obtain\nthe optimal solution for the ordinary least squares regression problem. We show\nthat this formulation recovers ridge regression exactly and establishes the\nmissing link between robust optimization and the mean squared error approaches\nfor existing regression problems. We further demonstrate that the condition of\nthis equivalence relies on the geometric properties of the defined uncertainty\nset. We provide exact, closed-form, in some cases, analytical solutions to the\nequivalent regularization strength under uncertainty sets induced by $\\ell_p$\nnorm, Schatten $p$-norm, and general polytopes. We then show in synthetic\ndatasets with different levels of uncertainties, a consistent improvement of\nthe averaged formulation over the existing worst-case formulation in\nout-of-sample performance. In real-world regression problems obtained from UCI\ndatasets, similar improvements are seen in the out-of-sample datasets.\n","authors":["Dimitris Bertsimas","Yu Ma"],"pdf_url":"https://arxiv.org/pdf/2311.06960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06816v1","updated":"2024-10-09T12:14:24Z","published":"2024-10-09T12:14:24Z","title":"Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex\n  Relaxation","summary":"  Neural work certification has established itself as a crucial tool for\nensuring the robustness of neural networks. Certification methods typically\nrely on convex relaxations of the feasible output set to provide sound bounds.\nHowever, complete certification requires exact bounds, which strongly limits\nthe expressivity of ReLU networks: even for the simple ``$\\max$'' function in\n$\\mathbb{R}^2$, there does not exist a ReLU network that expresses this\nfunction and can be exactly bounded by single-neuron relaxation methods. This\nraises the question whether there exists a convex relaxation that can provide\nexact bounds for general continuous piecewise linear functions in\n$\\mathbb{R}^n$. In this work, we answer this question affirmatively by showing\nthat (layer-wise) multi-neuron relaxation provides complete certification for\ngeneral ReLU networks. Based on this novel result, we show that the\nexpressivity of ReLU networks is no longer limited under multi-neuron\nrelaxation. To the best of our knowledge, this is the first positive result on\nthe completeness of convex relaxations, shedding light on the practice of\ncertified robustness.\n","authors":["Yuhao Mao","Yani Zhang","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.06816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06815v1","updated":"2024-10-09T12:14:06Z","published":"2024-10-09T12:14:06Z","title":"Shap-Select: Lightweight Feature Selection Using SHAP Values and\n  Regression","summary":"  Feature selection is an essential process in machine learning, especially\nwhen dealing with high-dimensional datasets. It helps reduce the complexity of\nmachine learning models, improve performance, mitigate overfitting, and\ndecrease computation time. This paper presents a novel feature selection\nframework, shap-select. The framework conducts a linear or logistic regression\nof the target on the Shapley values of the features, on the validation set, and\nuses the signs and significance levels of the regression coefficients to\nimplement an efficient heuristic for feature selection in tabular regression\nand classification tasks. We evaluate shap-select on the Kaggle credit card\nfraud dataset, demonstrating its effectiveness compared to established methods\nsuch as Recursive Feature Elimination (RFE), HISEL (a mutual information-based\nfeature selection method), Boruta and a simpler Shapley value-based method. Our\nfindings show that shap-select combines interpretability, computational\nefficiency, and performance, offering a robust solution for feature selection.\n","authors":["Egor Kraev","Baran Koseoglu","Luca Traverso","Mohammed Topiwalla"],"pdf_url":"https://arxiv.org/pdf/2410.06815v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.06814v1","updated":"2024-10-09T12:13:49Z","published":"2024-10-09T12:13:49Z","title":"Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning","summary":"  Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off.\n","authors":["Qiang Hu","Hengxiang Zhang","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2410.06814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01121v4","updated":"2024-10-09T12:10:38Z","published":"2024-03-02T08:05:03Z","title":"OpenGraph: Towards Open Graph Foundation Models","summary":"  Graph learning has become essential in various domains, including\nrecommendation systems and social network analysis. Graph Neural Networks\n(GNNs) have emerged as promising techniques for encoding structural information\nand improving performance in tasks like link prediction and node\nclassification. However, a key challenge remains: the difficulty of\ngeneralizing to unseen graph data with different properties. In this work, we\npropose a novel graph foundation model, called OpenGraph, to address this\nchallenge. Our approach tackles several technical obstacles. Firstly, we\nenhance data augmentation using a large language model (LLM) to overcome data\nscarcity in real-world scenarios. Secondly, we introduce a unified graph\ntokenizer that enables the model to generalize effectively to diverse graph\ndata, even when encountering unseen properties during training. Thirdly, our\ndeveloped scalable graph transformer captures node-wise dependencies within the\nglobal topological context. Extensive experiments validate the effectiveness of\nour framework. By adapting OpenGraph to new graph characteristics and\ncomprehending diverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings. We release the model\nimplementation at https://github.com/HKUDS/OpenGraph.\n","authors":["Lianghao Xia","Ben Kao","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2403.01121v4.pdf","comment":"Accepted by EMNLP'2024"},{"id":"http://arxiv.org/abs/2410.06804v1","updated":"2024-10-09T12:00:56Z","published":"2024-10-09T12:00:56Z","title":"The Clear Sky Corridor: Insights Towards Aerosol Formation in Exoplanets\n  Using An AI-based Survey of Exoplanet Atmospheres","summary":"  Producing optimized and accurate transmission spectra of exoplanets from\ntelescope data has traditionally been a manual and labor-intensive procedure.\nHere we present the results of the first attempt to improve and standardize\nthis procedure using artificial intelligence (AI) based processing of light\ncurves and spectroscopic data from transiting exoplanets observed with the\nHubble Space Telescope's (HST) Wide Field Camera 3 (WFC3) instrument. We\nimplement an AI-based parameter optimizer that autonomously operates the Eureka\npipeline to produce homogeneous transmission spectra of publicly available HST\nWFC3 datasets, spanning exoplanet types from hot Jupiters to sub-Neptunes.\nSurveying 43 exoplanets with temperatures between 280 and 2580 Kelvin, we\nconfirm modeled relationships between the amplitude of the water band at 1.4um\nin hot Jupiters and their equilibrium temperatures. We also identify a similar,\nnovel trend in Neptune/sub-Neptune atmospheres, but shifted to cooler\ntemperatures. Excitingly, a planet mass versus equilibrium temperature diagram\nreveals a \"Clear Sky Corridor,\" where planets between 700 and 1700 Kelvin\n(depending on the mass) show stronger 1.4um H2O band measurements. This novel\ntrend points to metallicity as a potentially important driver of aerosol\nformation. As we unveil and include these new discoveries into our\nunderstanding of aerosol formation, we enter a thrilling future for the study\nof exoplanet atmospheres. With HST sculpting this foundational understanding\nfor aerosol formation in various exoplanet types, ranging from Jupiters to\nsub-Neptunes, we present a compelling platform for the James Webb Space\nTelescope (JWST) to discover similar atmospheric trends for more planets across\na broader wavelength range.\n","authors":["Reza Ashtari","Kevin B. Stevenson","David Sing","Mercedes Lopez-Morales","Munazza K. Alam","Nikolay K. Nikolov","Thomas M. Evans-Soma"],"pdf_url":"https://arxiv.org/pdf/2410.06804v1.pdf","comment":"Accepted to AJ. 14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.06800v1","updated":"2024-10-09T11:54:33Z","published":"2024-10-09T11:54:33Z","title":"Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for\n  Sequential Deep Learning","summary":"  Efficiently learning a sequence of related tasks, such as in continual\nlearning, poses a significant challenge for neural nets due to the delicate\ntrade-off between catastrophic forgetting and loss of plasticity. We address\nthis challenge with a grounded framework for sequentially learning related\ntasks based on Bayesian inference. Specifically, we treat the model's\nparameters as a nonlinear Gaussian state-space model and perform efficient\ninference using Gaussian filtering and smoothing. This general formalism\nsubsumes existing continual learning approaches, while also offering a clearer\nconceptual understanding of its components. Leveraging Laplace approximations\nduring filtering, we construct Gaussian posterior measures on the weight space\nof a neural network for each task. We use it as an efficient regularizer by\nexploiting the structure of the generalized Gauss-Newton matrix (GGN) to\nconstruct diagonal plus low-rank approximations. The dynamics model allows\ntargeted control of the learning process and the incorporation of\ndomain-specific knowledge, such as modeling the type of shift between tasks.\nAdditionally, using Bayesian approximate smoothing can enhance the performance\nof task-specific models without needing to re-access any data.\n","authors":["Joanna Sliwa","Frank Schneider","Nathanael Bosch","Agustinus Kristiadi","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2410.06800v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.06796v1","updated":"2024-10-09T11:51:08Z","published":"2024-10-09T11:51:08Z","title":"Diffuse or Confuse: A Diffusion Deepfake Speech Dataset","summary":"  Advancements in artificial intelligence and machine learning have\nsignificantly improved synthetic speech generation. This paper explores\ndiffusion models, a novel method for creating realistic synthetic speech. We\ncreate a diffusion dataset using available tools and pretrained models.\nAdditionally, this study assesses the quality of diffusion-generated deepfakes\nversus non-diffusion ones and their potential threat to current deepfake\ndetection systems. Findings indicate that the detection of diffusion-based\ndeepfakes is generally comparable to non-diffusion deepfakes, with some\nvariability based on detector architecture. Re-vocoding with diffusion vocoders\nshows minimal impact, and the overall speech quality is comparable to\nnon-diffusion methods.\n","authors":["Anton Firc","Kamil Malinka","Petr Hanek"],"pdf_url":"https://arxiv.org/pdf/2410.06796v1.pdf","comment":"Presented at International Conference of the Biometrics Special\n  Interest Group (BIOSIG 2024)"},{"id":"http://arxiv.org/abs/2410.00428v3","updated":"2024-10-09T11:40:31Z","published":"2024-10-01T06:23:17Z","title":"LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management","summary":"  The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.\n","authors":["Yi Xiong","Hao Wu","Changxu Shao","Ziqing Wang","Rui Zhang","Yuhong Guo","Junping Zhao","Ke Zhang","Zhenxuan Pan"],"pdf_url":"https://arxiv.org/pdf/2410.00428v3.pdf","comment":"11 pages, 7 figures, 1 table"},{"id":"http://arxiv.org/abs/2311.13584v4","updated":"2024-10-09T11:38:01Z","published":"2023-11-22T18:40:45Z","title":"On diffusion-based generative models and their error bounds: The\n  log-concave case with full convergence estimates","summary":"  We provide full theoretical guarantees for the convergence behaviour of\ndiffusion-based generative models under the assumption of strongly log-concave\ndata distributions while our approximating class of functions used for score\nestimation is made of Lipschitz continuous functions avoiding any Lipschitzness\nassumption on the score function. We demonstrate via a motivating example,\nsampling from a Gaussian distribution with unknown mean, the powerfulness of\nour approach. In this case, explicit estimates are provided for the associated\noptimization problem, i.e. score approximation, while these are combined with\nthe corresponding sampling estimates. As a result, we obtain the best known\nupper bound estimates in terms of key quantities of interest, such as the\ndimension and rates of convergence, for the Wasserstein-2 distance between the\ndata distribution (Gaussian with unknown mean) and our sampling algorithm.\n  Beyond the motivating example and in order to allow for the use of a diverse\nrange of stochastic optimizers, we present our results using an $L^2$-accurate\nscore estimation assumption, which crucially is formed under an expectation\nwith respect to the stochastic optimizer and our novel auxiliary process that\nuses only known information. This approach yields the best known convergence\nrate for our sampling algorithm.\n","authors":["Stefano Bruno","Ying Zhang","Dong-Young Lim","mer Deniz Akyildiz","Sotirios Sabanis"],"pdf_url":"https://arxiv.org/pdf/2311.13584v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06786v1","updated":"2024-10-09T11:37:09Z","published":"2024-10-09T11:37:09Z","title":"Deep End-to-End Survival Analysis with Temporal Consistency","summary":"  In this study, we present a novel Survival Analysis algorithm designed to\nefficiently handle large-scale longitudinal data. Our approach draws\ninspiration from Reinforcement Learning principles, particularly the Deep\nQ-Network paradigm, extending Temporal Learning concepts to Survival\nRegression. A central idea in our method is temporal consistency, a hypothesis\nthat past and future outcomes in the data evolve smoothly over time. Our\nframework uniquely incorporates temporal consistency into large datasets by\nproviding a stable training signal that captures long-term temporal\nrelationships and ensures reliable updates. Additionally, the method supports\narbitrarily complex architectures, enabling the modeling of intricate temporal\ndependencies, and allows for end-to-end training. Through numerous experiments\nwe provide empirical evidence demonstrating our framework's ability to exploit\ntemporal consistency across datasets of varying sizes. Moreover, our algorithm\noutperforms benchmarks on datasets with long sequences, demonstrating its\nability to capture long-term patterns. Finally, ablation studies show how our\nmethod enhances training stability.\n","authors":["Mariana Vargas Vieyra","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2410.06786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02833v2","updated":"2024-10-09T11:28:41Z","published":"2024-10-02T09:43:43Z","title":"Asymmetry of the Relative Entropy in the Regularization of Empirical\n  Risk Minimization","summary":"  The effect of relative entropy asymmetry is analyzed in the context of\nempirical risk minimization (ERM) with relative entropy regularization\n(ERM-RER). Two regularizations are considered: $(a)$ the relative entropy of\nthe measure to be optimized with respect to a reference measure (Type-I\nERM-RER); or $(b)$ the relative entropy of the reference measure with respect\nto the measure to be optimized (Type-II ERM-RER). The main result is the\ncharacterization of the solution to the Type-II ERM-RER problem and its key\nproperties. By comparing the well-understood Type-I ERM-RER with Type-II\nERM-RER, the effects of entropy asymmetry are highlighted. The analysis shows\nthat in both cases, regularization by relative entropy forces the solution's\nsupport to collapse into the support of the reference measure, introducing a\nstrong inductive bias that can overshadow the evidence provided by the training\ndata. Finally, it is shown that Type-II regularization is equivalent to Type-I\nregularization with an appropriate transformation of the empirical risk\nfunction.\n","authors":["Francisco Daunas","Iaki Esnaola","Samir M. Perlaza","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2410.02833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17478v2","updated":"2024-10-09T11:23:14Z","published":"2024-05-24T06:01:09Z","title":"ROSE: Register Assisted General Time Series Forecasting with Decomposed\n  Frequency Learning","summary":"  With the increasing collection of time series data from various domains,\nthere arises a strong demand for general time series forecasting models\npre-trained on a large number of time-series datasets to support a variety of\ndownstream prediction tasks. Enabling general time series forecasting faces two\nchallenges: how to obtain unified representations from multi-domian time series\ndata, and how to capture domain-specific features from time series data across\nvarious domains for adaptive transfer in downstream tasks. To address these\nchallenges, we propose a Register Assisted General Time Series Forecasting\nModel with Decomposed Frequency Learning (ROSE), a novel pre-trained model for\ntime series forecasting. ROSE employs Decomposed Frequency Learning for the\npre-training task, which decomposes coupled semantic and periodic information\nin time series with frequency-based masking and reconstruction to obtain\nunified representations across domains. We also equip ROSE with a Time Series\nRegister, which learns to generate a register codebook to capture\ndomain-specific representations during pre-training and enhances\ndomain-adaptive transfer by selecting related register tokens on downstream\ntasks. After pre-training on large-scale time series data, ROSE achieves\nstate-of-the-art forecasting performance on 8 real-world benchmarks.\nRemarkably, even in few-shot scenarios, it demonstrates competitive or superior\nperformance compared to existing methods trained with full data.\n","authors":["Yihang Wang","Yuying Qiu","Peng Chen","Kai Zhao","Yang Shu","Zhongwen Rao","Lujia Pan","Bin Yang","Chenjuan Guo"],"pdf_url":"https://arxiv.org/pdf/2405.17478v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09023v2","updated":"2024-10-09T11:15:05Z","published":"2024-06-13T11:56:20Z","title":"Schur's Positive-Definite Network: Deep Learning in the SPD cone with\n  structure","summary":"  Estimating matrices in the symmetric positive-definite (SPD) cone is of\ninterest for many applications ranging from computer vision to graph learning.\nWhile there exist various convex optimization-based estimators, they remain\nlimited in expressivity due to their model-based approach. The success of deep\nlearning motivates the use of learning-based approaches to estimate SPD\nmatrices with neural networks in a data-driven fashion. However, designing\neffective neural architectures for SPD learning is challenging, particularly\nwhen the task requires additional structural constraints, such as element-wise\nsparsity. Current approaches either do not ensure that the output meets all\ndesired properties or lack expressivity. In this paper, we introduce SpodNet, a\nnovel and generic learning module that guarantees SPD outputs and supports\nadditional structural constraints. Notably, it solves the challenging task of\nlearning jointly SPD and sparse matrices. Our experiments illustrate the\nversatility and relevance of SpodNet layers for such applications.\n","authors":["Can Pouliquen","Mathurin Massias","Titouan Vayer"],"pdf_url":"https://arxiv.org/pdf/2406.09023v2.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.06771v1","updated":"2024-10-09T11:04:15Z","published":"2024-10-09T11:04:15Z","title":"Safe and High-Performance Learning of Model Predicitve Control using\n  Kernel-Based Interpolation","summary":"  We present a method, which allows efficient and safe approximation of model\npredictive controllers using kernel interpolation. Since the computational\ncomplexity of the approximating function scales linearly with the number of\ndata points, we propose to use a scoring function which chooses the most\npromising data. To further reduce the complexity of the approximation, we\nrestrict our considerations to the set of closed-loop reachable states. That\nis, the approximating function only has to be accurate within this set. This\nmakes our method especially suited for systems, where the set of initial\nconditions is small. In order to guarantee safety and high performance of the\ndesigned approximated controller, we use reachability analysis based on Monte\nCarlo methods.\n","authors":["Alexander Rose","Philipp Schaub","Rolf Findeisen"],"pdf_url":"https://arxiv.org/pdf/2410.06771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06746v1","updated":"2024-10-09T10:30:01Z","published":"2024-10-09T10:30:01Z","title":"Cluster-wise Graph Transformer with Dual-granularity Kernelized\n  Attention","summary":"  In the realm of graph learning, there is a category of methods that\nconceptualize graphs as hierarchical structures, utilizing node clustering to\ncapture broader structural information. While generally effective, these\nmethods often rely on a fixed graph coarsening routine, leading to overly\nhomogeneous cluster representations and loss of node-level information. In this\npaper, we envision the graph as a network of interconnected node sets without\ncompressing each cluster into a single embedding. To enable effective\ninformation transfer among these node sets, we propose the Node-to-Cluster\nAttention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple\nKernel Learning into the kernelized attention framework, effectively capturing\ninformation at both node and cluster levels. We then devise an efficient form\nfor N2C-Attn using the cluster-wise message-passing framework, achieving linear\ntime complexity. We further analyze how N2C-Attn combines bi-level feature maps\nof queries and keys, demonstrating its capability to merge dual-granularity\ninformation. The resulting architecture, Cluster-wise Graph Transformer\n(Cluster-GT), which uses node clusters as tokens and employs our proposed\nN2C-Attn module, shows superior performance on various graph-level tasks. Code\nis available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.\n","authors":["Siyuan Huang","Yunchong Song","Jiayue Zhou","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2410.06746v1.pdf","comment":"Accepted as NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.06743v1","updated":"2024-10-09T10:21:45Z","published":"2024-10-09T10:21:45Z","title":"Utilizing Transfer Learning and pre-trained Models for Effective Forest\n  Fire Detection: A Case Study of Uttarakhand","summary":"  Forest fires pose a significant threat to the environment, human life, and\nproperty. Early detection and response are crucial to mitigating the impact of\nthese disasters. However, traditional forest fire detection methods are often\nhindered by our reliability on manual observation and satellite imagery with\nlow spatial resolution. This paper emphasizes the role of transfer learning in\nenhancing forest fire detection in India, particularly in overcoming data\ncollection challenges and improving model accuracy across various regions. We\ncompare traditional learning methods with transfer learning, focusing on the\nunique challenges posed by regional differences in terrain, climate, and\nvegetation. Transfer learning can be categorized into several types based on\nthe similarity between the source and target tasks, as well as the type of\nknowledge transferred. One key method is utilizing pre-trained models for\nefficient transfer learning, which significantly reduces the need for extensive\nlabeled data. We outline the transfer learning process, demonstrating how\nresearchers can adapt pre-trained models like MobileNetV2 for specific tasks\nsuch as forest fire detection. Finally, we present experimental results from\ntraining and evaluating a deep learning model using the Uttarakhand forest fire\ndataset, showcasing the effectiveness of transfer learning in this context.\n","authors":["Hari Prabhat Gupta","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.06743v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.06742v1","updated":"2024-10-09T10:20:54Z","published":"2024-10-09T10:20:54Z","title":"Inference over Unseen Entities, Relations and Literals on Knowledge\n  Graphs","summary":"  In recent years, knowledge graph embedding models have been successfully\napplied in the transductive setting to tackle various challenging tasks\nincluding link prediction, and query answering. Yet, the transductive setting\ndoes not allow for reasoning over unseen entities, relations, let alone\nnumerical or non-numerical literals. Although increasing efforts are put into\nexploring inductive scenarios, inference over unseen entities, relations, and\nliterals has yet to come. This limitation prohibits the existing methods from\nhandling real-world dynamic knowledge graphs involving heterogeneous\ninformation about the world. Here, we propose a remedy to this limitation. We\npropose the attentive byte-pair encoding layer (BytE) to construct a triple\nembedding from a sequence of byte-pair encoded subword units of entities and\nrelations. Compared to the conventional setting, BytE leads to massive feature\nreuse via weight tying, since it forces a knowledge graph embedding model to\nlearn embeddings for subword units instead of entities and relations directly.\nConsequently, the size of the embedding matrices are not anymore bound to the\nunique number of entities and relations of a knowledge graph. Experimental\nresults show that BytE improves the link prediction performance of 4 knowledge\ngraph embedding models on datasets where the syntactic representations of\ntriples are semantically meaningful. However, benefits of training a knowledge\ngraph embedding model with BytE dissipate on knowledge graphs where entities\nand relations are represented with plain numbers or URIs. We provide an open\nsource implementation of BytE to foster reproducible research.\n","authors":["Caglar Demir","N'Dah Jean Kouagou","Arnab Sharma","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2410.06742v1.pdf","comment":"8 pages, 4 figures, ECAI 2024 Workshops (CompAI)"},{"id":"http://arxiv.org/abs/2410.06741v1","updated":"2024-10-09T10:20:32Z","published":"2024-10-09T10:20:32Z","title":"CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models","summary":"  Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.\n","authors":["Zi Gong","Hang Yu","Cong Liao","Bingchang Liu","Chaoyu Chen","Jianguo Li"],"pdf_url":"https://arxiv.org/pdf/2410.06741v1.pdf","comment":"15 pages, main conference of EMNLP 2024"},{"id":"http://arxiv.org/abs/2401.13421v3","updated":"2024-10-09T10:19:49Z","published":"2024-01-24T12:32:08Z","title":"Federated learning with distributed fixed design quantum chips and\n  quantum channels","summary":"  The privacy in classical federated learning can be breached through the use\nof local gradient results combined with engineered queries to the clients.\nHowever, quantum communication channels are considered more secure because a\nmeasurement on the channel causes a loss of information, which can be detected\nby the sender. Therefore, the quantum version of federated learning can be used\nto provide better privacy. Additionally, sending an $N$-dimensional data vector\nthrough a quantum channel requires sending $\\log N$ entangled qubits, which can\npotentially provide efficiency if the data vector is utilized as quantum\nstates.\n  In this paper, we propose a quantum federated learning model in which fixed\ndesign quantum chips are operated based on the quantum states sent by a\ncentralized server. Based on the incoming superposition states, the clients\ncompute and then send their local gradients as quantum states to the server,\nwhere they are aggregated to update parameters. Since the server does not send\nmodel parameters, but instead sends the operator as a quantum state, the\nclients are not required to share the model. This allows for the creation of\nasynchronous learning models. In addition, the model is fed into client-side\nchips directly as a quantum state; therefore, it does not require measurements\non the incoming quantum state to obtain model parameters in order to compute\ngradients. This can provide efficiency over models where the parameter vector\nis sent via classical or quantum channels and local gradients are obtained\nthrough the obtained values these parameters.\n","authors":["Ammar Daskin"],"pdf_url":"https://arxiv.org/pdf/2401.13421v3.pdf","comment":"a few typos are corrected and contribution and discussion sections\n  are edited"},{"id":"http://arxiv.org/abs/2311.04069v2","updated":"2024-10-09T10:08:29Z","published":"2023-11-07T15:35:17Z","title":"LISBET: a machine learning model for the automatic segmentation of\n  social behavior motifs","summary":"  Social behavior is crucial for survival in many animal species, and a heavily\ninvestigated research subject. Current analysis methods generally rely on\nmeasuring animal interaction time or annotating predefined behaviors. However,\nthese approaches are time consuming, human biased, and can fail to capture\nsubtle behaviors. Here we introduce LISBET (LISBET Is a Social BEhavior\nTransformer), a machine learning model for detecting and segmenting social\ninteractions. Using self-supervised learning on body tracking data, our model\neliminates the need for extensive human annotation. We tested LISBET in three\nscenarios across multiple datasets in mice: supervised behavior classification,\nunsupervised motifs segmentation, and unsupervised animal phenotyping.\nAdditionally, in vivo electrophysiology revealed distinct neural signatures in\nthe Ventral Tegmental Area corresponding to motifs identified by our model. In\nsummary, LISBET automates data annotation and reduces human bias in social\nbehavior research, offering a promising approach to enhance our understanding\nof behavior and its neural correlates.\n","authors":["Giuseppe Chindemi","Benoit Girard","Camilla Bellone"],"pdf_url":"https://arxiv.org/pdf/2311.04069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06731v1","updated":"2024-10-09T10:00:56Z","published":"2024-10-09T10:00:56Z","title":"Gridded Transformer Neural Processes for Large Unstructured\n  Spatio-Temporal Data","summary":"  Many important problems require modelling large-scale spatio-temporal\ndatasets, with one prevalent example being weather forecasting. Recently,\ntransformer-based approaches have shown great promise in a range of weather\nforecasting problems. However, these have mostly focused on gridded data\nsources, neglecting the wealth of unstructured, off-the-grid data from\nobservational measurements such as those at weather stations. A promising\nfamily of models suitable for such tasks are neural processes (NPs), notably\nthe family of transformer neural processes (TNPs). Although TNPs have shown\npromise on small spatio-temporal datasets, they are unable to scale to the\nquantities of data used by state-of-the-art weather and climate models. This\nlimitation stems from their lack of efficient attention mechanisms. We address\nthis shortcoming through the introduction of gridded pseudo-token TNPs which\nemploy specialised encoders and decoders to handle unstructured observations\nand utilise a processor containing gridded pseudo-tokens that leverage\nefficient attention mechanisms. Our method consistently outperforms a range of\nstrong baselines on various synthetic and real-world regression tasks involving\nlarge-scale data, while maintaining competitive computational efficiency. The\nreal-life experiments are performed on weather data, demonstrating the\npotential of our approach to bring performance and computational benefits when\napplied at scale in a weather modelling pipeline.\n","authors":["Matthew Ashman","Cristiana Diaconu","Eric Langezaal","Adrian Weller","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2410.06731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05468v3","updated":"2024-10-09T09:56:32Z","published":"2023-10-09T07:24:04Z","title":"Enhancing Interpretability and Generalizability in Extended Isolation\n  Forests","summary":"  Anomaly Detection (AD) focuses on identifying unusual behaviors in complex\ndatasets. Machine Learning (ML) algorithms and Decision Support Systems (DSSs)\nprovide effective solutions for AD, but detecting anomalies alone may not be\nenough, especially in engineering, where diagnostics and maintenance are\ncrucial. Users need clear explanations to support root cause analysis and build\ntrust in the model. The unsupervised nature of AD, however, makes\ninterpretability a challenge. This paper introduces Extended Isolation Forest\nFeature Importance (ExIFFI), a method that explains predictions made by\nExtended Isolation Forest (EIF) models, which split data using hyperplanes.\nExIFFI provides explanations at both global and local levels by leveraging\nfeature importance.\n  We also present an improved version, Enhanced Extended Isolation Forest\n(EIF+), designed to enhance the model's ability to detect unseen anomalies\nthrough a revised splitting strategy. Using five synthetic and eleven\nreal-world datasets, we conduct a comparative analysis, evaluating unsupervised\nAD methods with the Average Precision metric. EIF+ consistently outperforms EIF\nacross all datasets when trained without anomalies, demonstrating better\ngeneralization.\n  To assess ExIFFI's interpretability, we introduce the Area Under the Curve of\nFeature Selection (AUC\\_FS), a novel metric using feature selection as a proxy\ntask. ExIFFI outperforms other unsupervised interpretability methods on 8 of 11\nreal-world datasets and successfully identifies anomalous features in synthetic\ndatasets. When trained only on inliers, ExIFFI also outperforms competing\nmodels on real-world data and accurately detects anomalous features in\nsynthetic datasets. We provide open-source code to encourage further research\nand reproducibility.\n","authors":["Alessio Arcudi","Davide Frizzo","Chiara Masiero","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2310.05468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06726v1","updated":"2024-10-09T09:50:06Z","published":"2024-10-09T09:50:06Z","title":"Sharp Bounds of the Causal Effect Under MNAR Confounding","summary":"  We report bounds for any contrast between the probabilities of the\ncounterfactual outcome under exposure and non-exposure when the confounders are\nmissing not at random. We assume that the missingness mechanism is\noutcome-independent, and prove that our bounds are arbitrarily sharp, i.e.,\npractically attainable or logically possible.\n","authors":["Jose M. Pea"],"pdf_url":"https://arxiv.org/pdf/2410.06726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06725v1","updated":"2024-10-09T09:46:53Z","published":"2024-10-09T09:46:53Z","title":"Evaluating the Impact of Point Cloud Colorization on Semantic\n  Segmentation Accuracy","summary":"  Point cloud semantic segmentation, the process of classifying each point into\npredefined categories, is essential for 3D scene understanding. While\nimage-based segmentation is widely adopted due to its maturity, methods relying\nsolely on RGB information often suffer from degraded performance due to color\ninaccuracies. Recent advancements have incorporated additional features such as\nintensity and geometric information, yet RGB channels continue to negatively\nimpact segmentation accuracy when errors in colorization occur. Despite this,\nprevious studies have not rigorously quantified the effects of erroneous\ncolorization on segmentation performance. In this paper, we propose a novel\nstatistical approach to evaluate the impact of inaccurate RGB information on\nimage-based point cloud segmentation. We categorize RGB inaccuracies into two\ntypes: incorrect color information and similar color information. Our results\ndemonstrate that both types of color inaccuracies significantly degrade\nsegmentation accuracy, with similar color errors particularly affecting the\nextraction of geometric features. These findings highlight the critical need to\nreassess the role of RGB information in point cloud segmentation and its\nimplications for future algorithm design.\n","authors":["Qinfeng Zhu","Jiaze Cao","Yuanzhi Cai","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2410.06725v1.pdf","comment":"Accepted by 2024 IEEE 8th International Conference on Vision, Image\n  and Signal Processing"},{"id":"http://arxiv.org/abs/2407.08567v2","updated":"2024-10-09T09:46:22Z","published":"2024-07-11T14:57:27Z","title":"Adaptive Parametric Activation","summary":"  The activation function plays a crucial role in model optimisation, yet the\noptimal choice remains unclear. For example, the Sigmoid activation is the\nde-facto activation in balanced classification tasks, however, in imbalanced\nclassification, it proves inappropriate due to bias towards frequent classes.\nIn this work, we delve deeper in this phenomenon by performing a comprehensive\nstatistical analysis in the classification and intermediate layers of both\nbalanced and imbalanced networks and we empirically show that aligning the\nactivation function with the data distribution, enhances the performance in\nboth balanced and imbalanced tasks. To this end, we propose the Adaptive\nParametric Activation (APA) function, a novel and versatile activation function\nthat unifies most common activation functions under a single formula. APA can\nbe applied in both intermediate layers and attention layers, significantly\noutperforming the state-of-the-art on several imbalanced benchmarks such as\nImageNet-LT, iNaturalist2018, Places-LT, CIFAR100-LT and LVIS and balanced\nbenchmarks such as ImageNet1K, COCO and V3DET. The code is available at\nhttps://github.com/kostas1515/AGLU.\n","authors":["Konstantinos Panagiotis Alexandridis","Jiankang Deng","Anh Nguyen","Shan Luo"],"pdf_url":"https://arxiv.org/pdf/2407.08567v2.pdf","comment":"ECCV2024 Oral"},{"id":"http://arxiv.org/abs/2410.06723v1","updated":"2024-10-09T09:45:53Z","published":"2024-10-09T09:45:53Z","title":"Evaluating Computational Pathology Foundation Models for Prostate Cancer\n  Grading under Distribution Shifts","summary":"  Foundation models have recently become a popular research direction within\ncomputational pathology. They are intended to be general-purpose feature\nextractors, promising to achieve good performance on a range of downstream\ntasks. Real-world pathology image data does however exhibit considerable\nvariability. Foundation models should be robust to these variations and other\ndistribution shifts which might be encountered in practice. We evaluate two\ncomputational pathology foundation models: UNI (trained on more than 100,000\nwhole-slide images) and CONCH (trained on more than 1.1 million image-caption\npairs), by utilizing them as feature extractors within prostate cancer grading\nmodels. We find that while UNI and CONCH perform well relative to baselines,\nthe absolute performance can still be far from satisfactory in certain\nsettings. The fact that foundation models have been trained on large and varied\ndatasets does not guarantee that downstream models always will be robust to\ncommon distribution shifts.\n","authors":["Fredrik K. Gustafsson","Mattias Rantalainen"],"pdf_url":"https://arxiv.org/pdf/2410.06723v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2410.06722v1","updated":"2024-10-09T09:45:01Z","published":"2024-10-09T09:45:01Z","title":"Scaling Laws for Mixed quantization in Large Language Models","summary":"  Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms.\n","authors":["Zeyu Cao","Cheng Zhang","Pedro Gimenes","Jianqiao Lu","Jianyi Cheng","Yiren Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.06722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06718v1","updated":"2024-10-09T09:41:34Z","published":"2024-10-09T09:41:34Z","title":"MatMamba: A Matryoshka State Space Model","summary":"  State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}\n","authors":["Abhinav Shukla","Sai Vemprala","Aditya Kusupati","Ashish Kapoor"],"pdf_url":"https://arxiv.org/pdf/2410.06718v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.06717v1","updated":"2024-10-09T09:41:28Z","published":"2024-10-09T09:41:28Z","title":"Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural\n  networks","summary":"  We analyze the problem of storing random pattern-label associations using two\nclasses of continuous non-convex weights models, namely the perceptron with\nnegative margin and an infinite width two layer neural network with\nnon-overlapping receptive fields and generic activation function. Using a\nfull-RSB ansatz we compute the exact value of the SAT/UNSAT transition.\nFurthermore, in the case of the negative perceptron model we show that,\ndepending on the value of the margin and the constrained density, there is a\nline separating a phase in which the distribution of overlaps of typical states\ndoes not possess a gap from one in which it does. Our results show that the\nhypothesis underlying some recently developed theorems claiming that\nApproximate Message Passing (AMP) based algorithms are able to reach capacity,\ndoes not hold in general. Finally, we show that Gradient Descent is not able to\nreach the maximal capacity both in cases where there is and there is not a\nnon-overlap gap phase for the typical states. This, similarly to what occurs in\nbinary weight models, suggests that gradient-based algorithms are biased\ntowards highly atypical states, whose inaccessibility determines the\nalgorithmic threshold.\n","authors":["Brandon L. Annesi","Enrico M. Malatesta","Francesco Zamponi"],"pdf_url":"https://arxiv.org/pdf/2410.06717v1.pdf","comment":"38 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.06711v1","updated":"2024-10-09T09:33:48Z","published":"2024-10-09T09:33:48Z","title":"Analysis of different disparity estimation techniques on aerial stereo\n  image datasets","summary":"  With the advent of aerial image datasets, dense stereo matching has gained\ntremendous progress. This work analyses dense stereo correspondence analysis on\naerial images using different techniques. Traditional methods, optimization\nbased methods and learning based methods have been implemented and compared\nhere for aerial images. For traditional methods, we implemented the\narchitecture of Stereo SGBM while using different cost functions to get an\nunderstanding of their performance on aerial datasets. Analysis of most of the\nmethods in standard datasets has shown good performance, however in case of\naerial dataset, not much benchmarking is available. Visual qualitative and\nquantitative analysis has been carried out for two stereo aerial datasets in\norder to compare different cost functions and techniques for the purpose of\ndepth estimation from stereo images. Using existing pre-trained models, recent\nlearning based architectures have also been tested on stereo pairs along with\ndifferent cost functions in SGBM. The outputs and given ground truth are\ncompared using MSE, SSIM and other error metrics.\n","authors":["Ishan Narayan","Shashi Poddar"],"pdf_url":"https://arxiv.org/pdf/2410.06711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01794v2","updated":"2024-10-09T09:28:33Z","published":"2024-07-01T20:44:48Z","title":"Probabilistic Conformal Prediction with Approximate Conditional Validity","summary":"  We develop a new method for generating prediction sets that combines the\nflexibility of conformal methods with an estimate of the conditional\ndistribution $P_{Y \\mid X}$. Existing methods, such as conformalized quantile\nregression and probabilistic conformal prediction, usually provide only a\nmarginal coverage guarantee. In contrast, our approach extends these frameworks\nto achieve approximately conditional coverage, which is crucial for many\npractical applications. Our prediction sets adapt to the behavior of the\npredictive distribution, making them effective even under high\nheteroscedasticity. While exact conditional guarantees are infeasible without\nassumptions on the underlying data distribution, we derive non-asymptotic\nbounds that depend on the total variation distance of the conditional\ndistribution and its estimate. Using extensive simulations, we show that our\nmethod consistently outperforms existing approaches in terms of conditional\ncoverage, leading to more reliable statistical inference in a variety of\napplications.\n","authors":["Vincent Plassier","Alexander Fishkov","Mohsen Guizani","Maxim Panov","Eric Moulines"],"pdf_url":"https://arxiv.org/pdf/2407.01794v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2212.12675v2","updated":"2024-10-09T09:23:34Z","published":"2022-12-24T07:15:26Z","title":"Iterative regularization in classification via hinge loss diagonal\n  descent","summary":"  Iterative regularization is a classic idea in regularization theory, that has\nrecently become popular in machine learning. On the one hand, it allows to\ndesign efficient algorithms controlling at the same time numerical and\nstatistical accuracy. On the other hand it allows to shed light on the learning\ncurves observed while training neural networks. In this paper, we focus on\niterative regularization in the context of classification. After contrasting\nthis setting with that of linear inverse problems, we develop an iterative\nregularization approach based on the use of the hinge loss function. More\nprecisely we consider a diagonal approach for a family of algorithms for which\nwe prove convergence as well as rates of convergence and stability results for\na suitable classification noise model. Our approach compares favorably with\nother alternatives, as confirmed by numerical simulations.\n","authors":["Vassilis Apidopoulos","Tomaso Poggio","Lorenzo Rosasco","Silvia Villa"],"pdf_url":"https://arxiv.org/pdf/2212.12675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06704v1","updated":"2024-10-09T09:16:25Z","published":"2024-10-09T09:16:25Z","title":"PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs","summary":"  In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.\n","authors":["Krishna Kanth Nakka","Ahmed Frikha","Ricardo Mendes","Xue Jiang","Xuebing Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.06704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12046v2","updated":"2024-10-09T09:11:02Z","published":"2024-05-20T14:13:22Z","title":"Energy-Efficient Federated Edge Learning with Streaming Data: A Lyapunov\n  Optimization Approach","summary":"  Federated learning (FL) has received significant attention in recent years\nfor its advantages in efficient training of machine learning models across\ndistributed clients without disclosing user-sensitive data. Specifically, in\nfederated edge learning (FEEL) systems, the time-varying nature of wireless\nchannels introduces inevitable system dynamics in the communication process,\nthereby affecting training latency and energy consumption. In this work, we\nfurther consider a streaming data scenario where new training data samples are\nrandomly generated over time at edge devices. Our goal is to develop a dynamic\nscheduling and resource allocation algorithm to address the inherent randomness\nin data arrivals and resource availability under long-term energy constraints.\nTo achieve this, we formulate a stochastic network optimization problem and use\nthe Lyapunov drift-plus-penalty framework to obtain a dynamic resource\nmanagement design. Our proposed algorithm makes adaptive decisions on device\nscheduling, computational capacity adjustment, and allocation of bandwidth and\ntransmit power in every round. We provide convergence analysis for the\nconsidered setting with heterogeneous data and time-varying objective\nfunctions, which supports the rationale behind our proposed scheduling design.\nThe effectiveness of our scheme is verified through simulation results,\ndemonstrating improved learning performance and energy efficiency as compared\nto baseline schemes.\n","authors":["Chung-Hsuan Hu","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2405.12046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06699v1","updated":"2024-10-09T09:06:56Z","published":"2024-10-09T09:06:56Z","title":"Break the Visual Perception: Adversarial Attacks Targeting Encoded\n  Visual Tokens of Large Vision-Language Models","summary":"  Large vision-language models (LVLMs) integrate visual information into large\nlanguage models, showcasing remarkable multi-modal conversational capabilities.\nHowever, the visual modules introduces new challenges in terms of robustness\nfor LVLMs, as attackers can craft adversarial images that are visually clean\nbut may mislead the model to generate incorrect answers. In general, LVLMs rely\non vision encoders to transform images into visual tokens, which are crucial\nfor the language models to perceive image contents effectively. Therefore, we\nare curious about one question: Can LVLMs still generate correct responses when\nthe encoded visual tokens are attacked and disrupting the visual information?\nTo this end, we propose a non-targeted attack method referred to as VT-Attack\n(Visual Tokens Attack), which constructs adversarial examples from multiple\nperspectives, with the goal of comprehensively disrupting feature\nrepresentations and inherent relationships as well as the semantic properties\nof visual tokens output by image encoders. Using only access to the image\nencoder in the proposed attack, the generated adversarial examples exhibit\ntransferability across diverse LVLMs utilizing the same image encoder and\ngenerality across different tasks. Extensive experiments validate the superior\nattack performance of the VT-Attack over baseline methods, demonstrating its\neffectiveness in attacking LVLMs with image encoders, which in turn can provide\nguidance on the robustness of LVLMs, particularly in terms of the stability of\nthe visual feature space.\n","authors":["Yubo Wang","Chaohu Liu","Yanqiu Qu","Haoyu Cao","Deqiang Jiang","Linli Xu"],"pdf_url":"https://arxiv.org/pdf/2410.06699v1.pdf","comment":"Accepted to ACMMM 2024"},{"id":"http://arxiv.org/abs/2302.01310v2","updated":"2024-10-09T08:50:08Z","published":"2023-02-02T18:33:34Z","title":"Knowledge Gradient for Multi-Objective Bayesian Optimization with\n  Decoupled Evaluations","summary":"  Multi-objective Bayesian optimization aims to find the Pareto front of\ntrade-offs between a set of expensive objectives while collecting as few\nsamples as possible. In some cases, it is possible to evaluate the objectives\nseparately, and a different latency or evaluation cost can be associated with\neach objective. This decoupling of the objectives presents an opportunity to\nlearn the Pareto front faster by avoiding unnecessary, expensive evaluations.\nWe propose a scalarization based knowledge gradient acquisition function which\naccounts for the different evaluation costs of the objectives. We prove\nasymptotic consistency of the estimator of the optimum for an arbitrary,\nD-dimensional, real compact search space and show empirically that the\nalgorithm performs comparably with the state of the art and significantly\noutperforms versions which always evaluate both objectives.\n","authors":["Jack M. Buckingham","Sebastian Rojas Gonzalez","Juergen Branke"],"pdf_url":"https://arxiv.org/pdf/2302.01310v2.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.06678v1","updated":"2024-10-09T08:38:21Z","published":"2024-10-09T08:38:21Z","title":"M${}^{3}$Bench: Benchmarking Whole-body Motion Generation for Mobile\n  Manipulation in 3D Scenes","summary":"  We propose M^3Bench, a new benchmark for whole-body motion generation for\nmobile manipulation tasks. Given a 3D scene context, M^3Bench requires an\nembodied agent to understand its configuration, environmental constraints and\ntask objectives, then generate coordinated whole-body motion trajectories for\nobject rearrangement tasks. M^3Bench features 30k object rearrangement tasks\nacross 119 diverse scenes, providing expert demonstrations generated by our\nnewly developed M^3BenchMaker. This automatic data generation tool produces\ncoordinated whole-body motion trajectories from high-level task instructions,\nrequiring only basic scene and robot information. Our benchmark incorporates\nvarious task splits to assess generalization across different dimensions and\nleverages realistic physics simulation for trajectory evaluation. Through\nextensive experimental analyses, we reveal that state-of-the-art models still\nstruggle with coordinated base-arm motion while adhering to environment-context\nand task-specific constraints, highlighting the need to develop new models that\naddress this gap. Through M^3Bench, we aim to facilitate future robotics\nresearch towards more adaptive and capable mobile manipulation in diverse,\nreal-world environments.\n","authors":["Zeyu Zhang","Sixu Yan","Muzhi Han","Zaijin Wang","Xinggang Wang","Song-Chun Zhu","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03191v2","updated":"2024-10-09T08:29:54Z","published":"2024-10-04T07:06:02Z","title":"Nested Deep Learning Model Towards A Foundation Model for Brain Signal\n  Data","summary":"  Epilepsy affects over 50 million people globally, with EEG/MEG-based spike\ndetection playing a crucial role in diagnosis and treatment. Manual spike\nidentification is time-consuming and requires specialized training, limiting\nthe number of professionals available to analyze EEG/MEG data. To address this,\nvarious algorithmic approaches have been developed. However, current methods\nface challenges in handling varying channel configurations and in identifying\nthe specific channels where spikes originate. This paper introduces a novel\nNested Deep Learning (NDL) framework designed to overcome these limitations.\nNDL applies a weighted combination of signals across all channels, ensuring\nadaptability to different channel setups, and allows clinicians to identify key\nchannels more accurately. Through theoretical analysis and empirical validation\non real EEG/MEG datasets, NDL demonstrates superior accuracy in spike detection\nand channel localization compared to traditional methods. The results show that\nNDL improves prediction accuracy, supports cross-modality data integration, and\ncan be fine-tuned for various neurophysiological applications.\n","authors":["Fangyi Wei","Jiajie Mo","Kai Zhang","Haipeng Shen","Srikantan Nagarajan","Fei Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.03191v2.pdf","comment":"43 pages; title modified; typo corrected"},{"id":"http://arxiv.org/abs/2311.16984v3","updated":"2024-10-09T08:29:40Z","published":"2023-11-28T17:35:38Z","title":"FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings","summary":"  External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA provides evidence for a differential\neffect between two drugs that would have otherwise go unnoticed. By sharing our\ncode, we hope FedECA will foster the creation of federated research networks\nand thus accelerate drug development.\n","authors":["Jean Ogier du Terrail","Quentin Klopfenstein","Honghao Li","Imke Mayer","Nicolas Loiseau","Mohammad Hallal","Michael Debouver","Thibault Camalon","Thibault Fouqueray","Jorge Arellano Castro","Zahia Yanes","Laetitia Dahan","Julien Taeb","Pierre Laurent-Puig","Jean-Baptiste Bachet","Shulin Zhao","Remy Nicolle","Jrome Cros","Daniel Gonzalez","Robert Carreras-Torres","Adelaida Garcia Velasco","Kawther Abdilleh","Sudheer Doss","Flix Balazard","Mathieu Andreux"],"pdf_url":"https://arxiv.org/pdf/2311.16984v3.pdf","comment":"code available at: https://github.com/owkin/fedeca, bug in SMD\n  computation present in v1 and v2 has been fixed, many experiments on real\n  data have been added"},{"id":"http://arxiv.org/abs/2410.06671v1","updated":"2024-10-09T08:27:26Z","published":"2024-10-09T08:27:26Z","title":"GLA-DA: Global-Local Alignment Domain Adaptation for Multivariate Time\n  Series","summary":"  Unlike images and natural language tokens, time series data is highly\nsemantically sparse, resulting in labor-intensive label annotations.\nUnsupervised and Semi-supervised Domain Adaptation (UDA and SSDA) have\ndemonstrated efficiency in addressing this issue by utilizing pre-labeled\nsource data to train on unlabeled or partially labeled target data. However, in\ndomain adaptation methods designed for downstream classification tasks,\ndirectly adapting labeled source samples with unlabelled target samples often\nresults in similar distributions across various classes, thereby compromising\nthe performance of the target classification task.\n  To tackle this challenge, we proposed a Global-Local Alignment Domain\nAdaptation (GLA-DA) method for multivariate time series data. Data from two\ndomains were initially encoded to align in an intermediate feature space\nadversarially, achieving Global Feature Alignment (GFA). Subsequently, GLA-DA\nleveraged the consistency between similarity-based and deep learning-based\nmodels to assign pseudo labels to unlabeled target data. This process aims to\npreserve differences among data with distinct labels by aligning the samples\nwith the same class labels together, achieving Local Class Alignment (LCA). We\nimplemented GLA-DA in both UDA and SSDA scenarios, showcasing its superiority\nover state-of-the-art methods through extensive experiments on various public\ndatasets. Ablation experiments underscored the significance of key components\nwithin GLA-DA.\n","authors":["Gang Tu","Dan Li","Bingxin Lin","Zibin Zheng","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2410.06671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15267v2","updated":"2024-10-09T08:24:52Z","published":"2024-03-22T15:06:31Z","title":"Parametric PDE Control with Deep Reinforcement Learning and\n  Differentiable L0-Sparse Polynomial Policies","summary":"  Optimal control of parametric partial differential equations (PDEs) is\ncrucial in many applications in engineering and science. In recent years, the\nprogress in scientific machine learning has opened up new frontiers for the\ncontrol of parametric PDEs. In particular, deep reinforcement learning (DRL)\nhas the potential to solve high-dimensional and complex control problems in a\nlarge variety of applications. Most DRL methods rely on deep neural network\n(DNN) control policies. However, for many dynamical systems, DNN-based control\npolicies tend to be over-parametrized, which means they need large amounts of\ntraining data, show limited robustness, and lack interpretability. In this\nwork, we leverage dictionary learning and differentiable L$_0$ regularization\nto learn sparse, robust, and interpretable control policies for parametric\nPDEs. Our sparse policy architecture is agnostic to the DRL method and can be\nused in different policy-gradient and actor-critic DRL algorithms without\nchanging their policy-optimization procedure. We test our approach on the\nchallenging tasks of controlling parametric Kuramoto-Sivashinsky and\nconvection-diffusion-reaction PDEs. We show that our method (1) outperforms\nbaseline DNN-based DRL policies, (2) allows for the derivation of interpretable\nequations of the learned optimal control laws, and (3) generalizes to unseen\nparameters of the PDE without retraining the policies.\n","authors":["Nicol Botteghi","Urban Fasel"],"pdf_url":"https://arxiv.org/pdf/2403.15267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06665v1","updated":"2024-10-09T08:19:31Z","published":"2024-10-09T08:19:31Z","title":"Revisiting Multi-Permutation Equivariance through the Lens of\n  Irreducible Representations","summary":"  This paper explores the characterization of equivariant linear layers for\nrepresentations of permutations and related groups. Unlike traditional\napproaches, which address these problems using parameter-sharing, we consider\nan alternative methodology based on irreducible representations and Schur's\nlemma. Using this methodology, we obtain an alternative derivation for existing\nmodels like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space\n(DWS) networks. The derivation for DWS networks is significantly simpler than\nthat of previous results.\n  Next, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant\nlayers are Siamese. In contrast, we give a full characterization of layers in\nthis case and show that there is a vast number of additional non-Siamese layers\nin some settings. We also show empirically that these additional non-Siamese\nlayers can improve performance in tasks like graph anomaly detection, weight\nspace alignment, and learning Wasserstein distances. Our code is available at\n\\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.\n","authors":["Yonatan Sverdlov","Ido Springer","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2410.06665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19013v2","updated":"2024-10-09T08:16:44Z","published":"2024-09-23T23:43:43Z","title":"Improving Academic Skills Assessment with NLP and Ensemble Learning","summary":"  This study addresses the critical challenges of assessing foundational\nacademic skills by leveraging advancements in natural language processing\n(NLP). Traditional assessment methods often struggle to provide timely and\ncomprehensive feedback on key cognitive and linguistic aspects, such as\ncoherence, syntax, and analytical reasoning. Our approach integrates multiple\nstate-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,\nwithin an ensemble learning framework. These models are combined through\nstacking techniques using LightGBM and Ridge regression to enhance predictive\naccuracy. The methodology involves detailed data preprocessing, feature\nextraction, and pseudo-label learning to optimize model performance. By\nincorporating sophisticated NLP techniques and ensemble learning, this study\nsignificantly improves the accuracy and efficiency of assessments, offering a\nrobust solution that surpasses traditional methods and opens new avenues for\neducational technology research focused on enhancing core academic\ncompetencies.\n","authors":["Zhengpei Cheng","Yingyi Wu","Danyang Zhang","Jiacheng Hu","Yujian Long"],"pdf_url":"https://arxiv.org/pdf/2409.19013v2.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.03951v2","updated":"2024-10-09T08:16:18Z","published":"2024-07-04T14:08:50Z","title":"Uncertainty-Guided Optimization on Large Language Model Search Trees","summary":"  Tree search algorithms such as greedy and beam search are the standard when\nit comes to finding sequences of maximum likelihood in the decoding processes\nof large language models (LLMs). However, they are myopic since they do not\ntake the complete root-to-leaf path into account. Moreover, they are agnostic\nto prior knowledge available about the process: For example, it does not\nconsider that the objective being maximized is a probability and thereby has\nspecific properties like being bound in the unit interval. Taking a\nprobabilistic approach, we define prior beliefs over LLMs' transition\nprobabilities and obtain posterior beliefs over the most promising paths in\neach iteration. These beliefs are useful for defining a sample-based,\nnon-myopic acquisition function that allows for a more data-efficient\nexploration scheme than standard search algorithms on LLMs. Crucially, unlike\nexpensive simulation-based non-myopic methods like the Monte Carlo tree search,\nour method only requires samples from the beliefs. Our formulation thus views\nLLM decoding as Bayesian optimization on trees. We discuss how to select the\nprior and the acquisition function, and demonstrate in experiments with various\nLLMs that our method achieves higher efficiency than recent baselines: Our\nmethod achieves the same or a higher likelihood while expanding fewer nodes.\n","authors":["Julia Grosse","Ruotian Wu","Ahmad Rashid","Philipp Hennig","Pascal Poupart","Agustinus Kristiadi"],"pdf_url":"https://arxiv.org/pdf/2407.03951v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.06656v1","updated":"2024-10-09T08:07:29Z","published":"2024-10-09T08:07:29Z","title":"WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning","summary":"  When optimizing transportation systems, anticipating traffic flows is a\ncentral element. Yet, computing such traffic equilibria remains computationally\nexpensive. Against this background, we introduce a novel combinatorial\noptimization augmented neural network architecture that allows for fast and\naccurate traffic flow predictions. We propose WardropNet, a neural network that\ncombines classical layers with a subsequent equilibrium layer: the first ones\ninform the latter by predicting the parameterization of the equilibrium\nproblem's latency functions. Using supervised learning we minimize the\ndifference between the actual traffic flow and the predicted output. We show\nhow to leverage a Bregman divergence fitting the geometry of the equilibria,\nwhich allows for end-to-end learning. WardropNet outperforms pure\nlearning-based approaches in predicting traffic equilibria for realistic and\nstylized traffic scenarios. On realistic scenarios, WardropNet improves on\naverage for time-invariant predictions by up to 72% and for time-variant\npredictions by up to 23% over pure learning-based approaches.\n","authors":["Kai Jungel","Dario Paccagnan","Axel Parmentier","Maximilian Schiffer"],"pdf_url":"https://arxiv.org/pdf/2410.06656v1.pdf","comment":"40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.06652v1","updated":"2024-10-09T08:04:48Z","published":"2024-10-09T08:04:48Z","title":"Task-oriented Time Series Imputation Evaluation via Generalized\n  Representers","summary":"  Time series analysis is widely used in many fields such as power energy,\neconomics, and transportation, including different tasks such as forecasting,\nanomaly detection, classification, etc. Missing values are widely observed in\nthese tasks, and often leading to unpredictable negative effects on existing\nmethods, hindering their further application. In response to this situation,\nexisting time series imputation methods mainly focus on restoring sequences\nbased on their data characteristics, while ignoring the performance of the\nrestored sequences in downstream tasks. Considering different requirements of\ndownstream tasks (e.g., forecasting), this paper proposes an efficient\ndownstream task-oriented time series imputation evaluation approach. By\ncombining time series imputation with neural network models used for downstream\ntasks, the gain of different imputation strategies on downstream tasks is\nestimated without retraining, and the most favorable imputation value for\ndownstream tasks is given by combining different imputation strategies\naccording to the estimated gain.\n","authors":["Zhixian Wang","Linxiao Yang","Liang Sun","Qingsong Wen","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06652v1.pdf","comment":"22 pages, 9 figures, 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.06651v1","updated":"2024-10-09T08:04:06Z","published":"2024-10-09T08:04:06Z","title":"Toward Physics-guided Time Series Embedding","summary":"  In various scientific and engineering fields, the primary research areas have\nrevolved around physics-based dynamical systems modeling and data-driven time\nseries analysis. According to the embedding theory, dynamical systems and time\nseries can be mutually transformed using observation functions and physical\nreconstruction techniques. Based on this, we propose Embedding Duality Theory,\nwhere the parameterized embedding layer essentially provides a linear\nestimation of the non-linear time series dynamics. This theory enables us to\nbypass the parameterized embedding layer and directly employ physical\nreconstruction techniques to acquire a data embedding representation. Utilizing\nphysical priors results in a 10X reduction in parameters, a 3X increase in\nspeed, and maximum performance boosts of 18% in expert, 22% in few-shot, and\n53\\% in zero-shot tasks without any hyper-parameter tuning. All methods are\nencapsulated as a plug-and-play module\n","authors":["Jiaxi Hu","Bowen Zhang","Qingsong Wen","Fugee Tsung","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.06651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06648v1","updated":"2024-10-09T08:00:12Z","published":"2024-10-09T08:00:12Z","title":"Q-WSL:Leveraging Dynamic Programming for Weighted Supervised Learning in\n  Goal-conditioned RL","summary":"  A novel class of advanced algorithms, termed Goal-Conditioned Weighted\nSupervised Learning (GCWSL), has recently emerged to tackle the challenges\nposed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL\nconsistently delivers strong performance across a diverse set of goal-reaching\ntasks due to its simplicity, effectiveness, and stability. However, GCWSL\nmethods lack a crucial capability known as trajectory stitching, which is\nessential for learning optimal policies when faced with unseen skills during\ntesting. This limitation becomes particularly pronounced when the replay buffer\nis predominantly filled with sub-optimal trajectories. In contrast, traditional\nTD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do\nnot face this issue but often experience instability due to the inherent\ndifficulties in value function approximation. In this paper, we propose\nQ-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to\novercome the limitations of GCWSL by incorporating the strengths of Dynamic\nProgramming found in Q-learning. Q-WSL leverages Dynamic Programming results to\noutput the optimal action of (state, goal) pairs across different trajectories\nwithin the replay buffer. This approach synergizes the strengths of both\nQ-learning and GCWSL, effectively mitigating their respective weaknesses and\nenhancing overall performance. Empirical evaluations on challenging\ngoal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned\napproaches in terms of both performance and sample efficiency. Additionally,\nQ-WSL exhibits notable robustness in environments characterized by binary\nreward structures and environmental stochasticity.\n","authors":["Xing Lei","Xuetao Zhang","Zifeng Zhuang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05413v3","updated":"2024-10-09T07:53:10Z","published":"2024-07-07T15:37:13Z","title":"SBoRA: Low-Rank Adaptation with Regional Weight Updates","summary":"  This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA reduces the number of trainable parameters by half or doubles\nthe rank with the similar number of trainable parameters as LoRA, while\nimproving learning performance. By utilizing orthogonal standard basis vectors\nto initialize one of the low-rank matrices (either $\\mathbf{A}$ or\n$\\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient\nfine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only\none of the matrices is updated, leading to a sparse update matrix\n$\\mathrm{\\Delta} \\mathbf{W}$ with predominantly zero rows or columns.\nConsequently, most of the fine-tuned model's weights\n$(\\mathbf{W}_0+\\mathrm{\\Delta} \\mathbf{W})$ remain unchanged from the\npre-trained weights, akin to the modular organization of the human brain, which\nefficiently adapts to new tasks. Our empirical results demonstrate the\nsuperiority of SBoRA-FA over LoRA in various fine-tuning tasks, including\ncommonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the\neffectiveness of QSBoRA on quantized LLaMA models of varying scales,\nhighlighting its potential for efficient adaptation to new tasks. Code is\navailable at https://github.com/cityuhkai/SBoRA\n","authors":["Lai-Man Po","Yuyang Liu","Haoxuan Wu","Tianqi Zhang","Wing-Yin Yu","Zhuohan Wang","Zeyu Jiang","Kun Li"],"pdf_url":"https://arxiv.org/pdf/2407.05413v3.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.17819v2","updated":"2024-10-09T07:34:33Z","published":"2024-06-25T08:29:32Z","title":"Automatically Adaptive Conformal Risk Control","summary":"  Science and technology have a growing need for effective mechanisms that\nensure reliable, controlled performance from black-box machine learning\nalgorithms. These performance guarantees should ideally hold conditionally on\nthe input-that is the performance guarantees should hold, at least\napproximately, no matter what the input. However, beyond stylized discrete\ngroupings such as ethnicity and gender, the right notion of conditioning can be\ndifficult to define. For example, in problems such as image segmentation, we\nwant the uncertainty to reflect the intrinsic difficulty of the test sample,\nbut this may be difficult to capture via a conditioning event. Building on the\nrecent work of Gibbs et al. [2023], we propose a methodology for achieving\napproximate conditional control of statistical risks-the expected value of loss\nfunctions-by adapting to the difficulty of test samples. Our framework goes\nbeyond traditional conditional risk control based on user-provided conditioning\nevents to the algorithmic, data-driven determination of appropriate function\nclasses for conditioning. We apply this framework to various regression and\nsegmentation tasks, enabling finer-grained control over model performance and\ndemonstrating that by continuously monitoring and adjusting these parameters,\nwe can achieve superior precision compared to conventional risk-control\nmethods.\n","authors":["Vincent Blot","Anastasios N Angelopoulos","Michael I Jordan","Nicolas J-B Brunel"],"pdf_url":"https://arxiv.org/pdf/2406.17819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09574v2","updated":"2024-10-09T07:21:30Z","published":"2024-06-13T20:25:52Z","title":"Online Bandit Learning with Offline Preference Data","summary":"  Reinforcement Learning with Human Feedback (RLHF) is at the core of\nfine-tuning methods for generative AI models for language and images. Such\nfeedback is often sought as rank or preference feedback from human raters, as\nopposed to eliciting scores since the latter tends to be noisy. On the other\nhand, RL theory and algorithms predominantly assume that a reward feedback is\navailable. In particular, approaches for online learning that can be helpful in\nadaptive data collection via active learning cannot incorporate offline\npreference data. In this paper, we adopt a finite-armed linear bandit model as\na prototypical model of online learning. We consider an offline preference\ndataset to be available generated by an expert of unknown 'competence'. We\npropose $\\texttt{warmPref-PS}$, a posterior sampling algorithm for online\nlearning that can be warm-started with an offline dataset with noisy preference\nfeedback. We show that by modeling the 'competence' of the expert that\ngenerated it, we are able to use such a dataset most effectively. We support\nour claims with novel theoretical analysis of its Bayesian regret, as well as,\nextensive empirical evaluation of an approximate loss function that optimizes\nfor infinitely many arms, and performs substantially better ($25$ to $50\\%$\nregret reduction) than baselines.\n","authors":["Akhil Agnihotri","Rahul Jain","Deepak Ramachandran","Zheng Wen"],"pdf_url":"https://arxiv.org/pdf/2406.09574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06621v1","updated":"2024-10-09T07:19:16Z","published":"2024-10-09T07:19:16Z","title":"Effective Exploration Based on the Structural Information Principles","summary":"  Traditional information theory provides a valuable foundation for\nReinforcement Learning, particularly through representation learning and\nentropy maximization for agent exploration. However, existing methods primarily\nconcentrate on modeling the uncertainty associated with RL's random variables,\nneglecting the inherent structure within the state and action spaces. In this\npaper, we propose a novel Structural Information principles-based Effective\nExploration framework, namely SI2E. Structural mutual information between two\nvariables is defined to address the single-variable limitation in structural\ninformation, and an innovative embedding principle is presented to capture\ndynamics-relevant state-action representations. The SI2E analyzes value\ndifferences in the agent's policy between state-action pairs and minimizes\nstructural entropy to derive the hierarchical state-action structure, referred\nto as the encoding tree. Under this tree structure, value-conditional\nstructural entropy is defined and maximized to design an intrinsic reward\nmechanism that avoids redundant transitions and promotes enhanced coverage in\nthe state-action space. Theoretical connections are established between SI2E\nand classical information-theoretic methodologies, highlighting our framework's\nrationality and advantage. Comprehensive evaluations in the MiniGrid,\nMetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E\nsignificantly outperforms state-of-the-art exploration baselines regarding\nfinal performance and sample efficiency, with maximum improvements of 37.63%\nand 60.25%, respectively.\n","authors":["Xianghua Zeng","Hao Peng","Angsheng Li"],"pdf_url":"https://arxiv.org/pdf/2410.06621v1.pdf","comment":"10 pages in main paper and 15 pages in appendix"},{"id":"http://arxiv.org/abs/2410.06615v1","updated":"2024-10-09T07:12:24Z","published":"2024-10-09T07:12:24Z","title":"$$-calibration of Language Model Confidence Scores for Generative\n  QA","summary":"  To use generative question-and-answering (QA) systems for decision-making and\nin any critical application, these systems need to provide well-calibrated\nconfidence scores that reflect the correctness of their answers. Existing\ncalibration methods aim to ensure that the confidence score is on average\nindicative of the likelihood that the answer is correct. We argue, however,\nthat this standard (average-case) notion of calibration is difficult to\ninterpret for decision-making in generative QA. To address this, we generalize\nthe standard notion of average calibration and introduce $\\beta$-calibration,\nwhich ensures calibration holds across different question-and-answer groups. We\nthen propose discretized posthoc calibration schemes for achieving\n$\\beta$-calibration.\n","authors":["Putra Manggala","Atalanti Mastakouri","Elke Kirschbaum","Shiva Prasad Kasiviswanathan","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2410.06615v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.05412v2","updated":"2024-10-09T16:49:58Z","published":"2023-12-08T23:55:19Z","title":"CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional\n  Modeling","summary":"  We introduce a multi-modal diffusion model tailored for the bi-directional\nconditional generation of video and audio. We propose a joint contrastive\ntraining loss to improve the synchronization between visual and auditory\noccurrences. We present experiments on two datasets to evaluate the efficacy of\nour proposed model. The assessment of generation quality and alignment\nperformance is carried out from various angles, encompassing both objective and\nsubjective metrics. Our findings demonstrate that the proposed model\noutperforms the baseline in terms of quality and generation speed through\nintroduction of our novel cross-modal easy fusion architectural block.\nFurthermore, the incorporation of the contrastive loss results in improvements\nin audio-visual alignment, particularly in the high-correlation video-to-audio\ngeneration task.\n","authors":["Ruihan Yang","Hannes Gamper","Sebastian Braun"],"pdf_url":"https://arxiv.org/pdf/2312.05412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06729v1","updated":"2024-10-09T09:55:51Z","published":"2024-10-09T09:55:51Z","title":"Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds","summary":"  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT\nencoding mode. First, to address the issue that existing PCQA databases have a\nsmall scale and limited distortion levels, we establish the WPC5.0 database\nwhich is the first one dedicated to Octree-RAHT encoding mode with a scale of\n400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude\ndistortion levels. Then, we propose the first PCQA model dedicated to\nOctree-RAHT encoding mode by parsing PC bitstreams without full decoding. The\nmodel introduces texture bitrate (TBPP) to predict texture complexity (TC) and\nfurther derives the texture distortion factor. In addition, the Geometric\nQuantization Parameter (PQS) is used to estimate the geometric distortion\nfactor, which is then integrated into the model along with the texture\ndistortion factor to obtain the proposed PCQA model named streamPCQ-OR. The\nproposed model has been compared with other advanced PCQA methods on the\nWPC5.0, BASICS and M-PCCD databases, and experimental results show that our\nmodel has excellent performance while having very low computational complexity,\nproviding a reliable choice for time-critical applications. To facilitate\nsubsequent research, the database and source code will be publicly released at\nhttps://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.\n","authors":["Dongshuai Duan","Honglei Su","Qi Liu","Hui Yuan","Wei Gao","Jiarun Song","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06725v1","updated":"2024-10-09T09:46:53Z","published":"2024-10-09T09:46:53Z","title":"Evaluating the Impact of Point Cloud Colorization on Semantic\n  Segmentation Accuracy","summary":"  Point cloud semantic segmentation, the process of classifying each point into\npredefined categories, is essential for 3D scene understanding. While\nimage-based segmentation is widely adopted due to its maturity, methods relying\nsolely on RGB information often suffer from degraded performance due to color\ninaccuracies. Recent advancements have incorporated additional features such as\nintensity and geometric information, yet RGB channels continue to negatively\nimpact segmentation accuracy when errors in colorization occur. Despite this,\nprevious studies have not rigorously quantified the effects of erroneous\ncolorization on segmentation performance. In this paper, we propose a novel\nstatistical approach to evaluate the impact of inaccurate RGB information on\nimage-based point cloud segmentation. We categorize RGB inaccuracies into two\ntypes: incorrect color information and similar color information. Our results\ndemonstrate that both types of color inaccuracies significantly degrade\nsegmentation accuracy, with similar color errors particularly affecting the\nextraction of geometric features. These findings highlight the critical need to\nreassess the role of RGB information in point cloud segmentation and its\nimplications for future algorithm design.\n","authors":["Qinfeng Zhu","Jiaze Cao","Yuanzhi Cai","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2410.06725v1.pdf","comment":"Accepted by 2024 IEEE 8th International Conference on Vision, Image\n  and Signal Processing"},{"id":"http://arxiv.org/abs/2410.06654v1","updated":"2024-10-09T08:06:15Z","published":"2024-10-09T08:06:15Z","title":"Performance Evaluation in Multimedia Retrieval","summary":"  Performance evaluation in multimedia retrieval, as in the information\nretrieval domain at large, relies heavily on retrieval experiments, employing a\nbroad range of techniques and metrics. These can involve human-in-the-loop and\nmachine-only settings for the retrieval process itself and the subsequent\nverification of results. Such experiments can be elaborate and\nuse-case-specific, which can make them difficult to compare or replicate. In\nthis paper, we present a formal model to express all relevant aspects of such\nretrieval experiments, as well as a flexible open-source evaluation\ninfrastructure that implements the model. These contributions intend to make a\nstep towards lowering the hurdles for conducting retrieval experiments and\nimproving their reproducibility.\n","authors":["Loris Sauter","Ralph Gasser","Heiko Schuldt","Abraham Bernstein","Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2410.06654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v1","updated":"2024-10-09T07:14:49Z","published":"2024-10-09T07:14:49Z","title":"Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video\n  Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v1.pdf","comment":null}]},"2024-10-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.06428v1","updated":"2024-10-08T23:49:31Z","published":"2024-10-08T23:49:31Z","title":"Stress Detection on Code-Mixed Texts in Dravidian Languages using\n  Machine Learning","summary":"  Stress is a common feeling in daily life, but it can affect mental well-being\nin some situations, the development of robust detection models is imperative.\nThis study introduces a methodical approach to the stress identification in\ncode-mixed texts for Dravidian languages. The challenge encompassed two\ndatasets, targeting Tamil and Telugu languages respectively. This proposal\nunderscores the importance of using uncleaned text as a benchmark to refine\nfuture classification methodologies, incorporating diverse preprocessing\ntechniques. Random Forest algorithm was used, featuring three textual\nrepresentations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams\nof characters. The approach achieved a good performance for both linguistic\ncategories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu,\noverpassing results achieved with different complex techniques such as FastText\nand Transformer models. The results underscore the value of uncleaned data for\nmental state detection and the challenges classifying code-mixed texts for\nstress, indicating the potential for improved performance through cleaning\ndata, other preprocessing techniques, or more complex models.\n","authors":["L. Ramos","M. Shahiki-Tash","Z. Ahani","A. Eponon","O. Kolesnikova","H. Calvo"],"pdf_url":"https://arxiv.org/pdf/2410.06428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06427v1","updated":"2024-10-08T23:46:56Z","published":"2024-10-08T23:46:56Z","title":"NLP Case Study on Predicting the Before and After of the Ukraine-Russia\n  and Hamas-Israel Conflicts","summary":"  We propose a method to predict toxicity and other textual attributes through\nthe use of natural language processing (NLP) techniques for two recent events:\nthe Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis\nfor exploration in future conflicts with hopes to mitigate risk through the\nanalysis of social media before and after a conflict begins. Our work compiles\nseveral datasets from Twitter and Reddit for both conflicts in a before and\nafter separation with an aim of predicting a future state of social media for\navoidance. More specifically, we show that: (1) there is a noticeable\ndifference in social media discussion leading up to and following a conflict\nand (2) social media discourse on platforms like Twitter and Reddit is useful\nin identifying future conflicts before they arise. Our results show that\nthrough the use of advanced NLP techniques (both supervised and unsupervised)\ntoxicity and other attributes about language before and after a conflict is\npredictable with a low error of nearly 1.2 percent for both conflicts.\n","authors":["Jordan Miner","John E. Ortega"],"pdf_url":"https://arxiv.org/pdf/2410.06427v1.pdf","comment":"The clusters created using topic modeling can be viewed at\n  https://naturallang.com/conflict/conflict.html"},{"id":"http://arxiv.org/abs/2410.06420v1","updated":"2024-10-08T23:14:24Z","published":"2024-10-08T23:14:24Z","title":"ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language\n  Models in Hospital Environments","summary":"  The global shortage of healthcare workers has demanded the development of\nsmart healthcare assistants, which can help monitor and alert healthcare\nworkers when necessary. We examine the healthcare knowledge of existing Large\nVision Language Models (LVLMs) via the Visual Question Answering (VQA) task in\nhospital settings through expert annotated open-ended questions. We introduce\nthe Emergency Room Visual Question Answering (ERVQA) dataset, consisting of\n<image, question, answer> triplets covering diverse emergency room scenarios, a\nseminal benchmark for LVLMs. By developing a detailed error taxonomy and\nanalyzing answer trends, we reveal the nuanced nature of the task. We benchmark\nstate-of-the-art open-source and closed LVLMs using traditional and adapted VQA\nmetrics: Entailment Score and CLIPScore Confidence. Analyzing errors across\nmodels, we infer trends based on properties like decoder type, model size, and\nin-context examples. Our findings suggest the ERVQA dataset presents a highly\ncomplex task, highlighting the need for specialized, domain-specific solutions.\n","authors":["Sourjyadip Ray","Kushal Gupta","Soumi Kundu","Payal Arvind Kasat","Somak Aditya","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2410.06420v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.18988v3","updated":"2024-10-08T22:18:59Z","published":"2024-04-29T17:36:58Z","title":"Markovian Transformers for Informative Language Modeling","summary":"  Chain-of-Thought (CoT) reasoning holds great promise for explaining the\noutputs of language models, but recent studies have highlighted significant\nchallenges in its practical application for interpretability. We propose to\naddress this issue via two key components: a technique to factor next-token\nprediction through intermediate CoT text, ensuring the CoT is causally\nload-bearing, and a reinforcement learning approach to train CoT to predict\nfuture tokens independently of other context. This results in \"Markovian\"\nlanguage models, where CoT serves as a fixed-size state for future token\nprediction. Our approach optimizes for \"informativeness\" -- the improvement in\nnext-token predictions using a trained CoT compared to a baseline. We\ndemonstrate our method's effectiveness using Proximal Policy Optimization (PPO)\non arithmetic problems and achieve an 11% performance boost on the GSM8K\nbenchmark using Mistral 7B Inst V2. The increased sensitivity of model\nperformance to CoT perturbations provides strong evidence of CoT reliance. This\nwork advances the development of more transparent and interpretable language\nmodels, potentially enabling their extension to arbitrarily long contexts and\nenhancing AI reasoning capabilities across various domains.\n","authors":["Scott Viteri","Max Lamparth","Peter Chatain","Clark Barrett"],"pdf_url":"https://arxiv.org/pdf/2404.18988v3.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.06396v1","updated":"2024-10-08T21:59:31Z","published":"2024-10-08T21:59:31Z","title":"MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks","summary":"  Language models are now capable of solving tasks that require dealing with\nlong sequences consisting of hundreds of thousands of tokens. However, they\noften fail on tasks that require repetitive use of simple rules, even on\nsequences that are much shorter than those seen during training. For example,\nstate-of-the-art LLMs can find common items in two lists with up to 20 items\nbut fail when lists have 80 items. In this paper, we introduce MLissard, a\nmultilingual benchmark designed to evaluate models' abilities to process and\ngenerate texts of varied lengths and offers a mechanism for controlling\nsequence complexity.\n  Our evaluation of open-source and proprietary models show a consistent\ndecline in performance across all models and languages as the complexity of the\nsequence increases. Surprisingly, the use of in-context examples in languages\nother than English helps increase extrapolation performance significantly. The\ndatasets and code are available at https://github.com/unicamp-dl/Lissard\n","authors":["Mirelle Bueno","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2410.06396v1.pdf","comment":"GenBench Workshop by EMNLP 2024: Camera-ready version"},{"id":"http://arxiv.org/abs/2410.06392v1","updated":"2024-10-08T21:53:07Z","published":"2024-10-08T21:53:07Z","title":"Counterfactual Causal Inference in Natural Language with Large Language\n  Models","summary":"  Causal structure discovery methods are commonly applied to structured data\nwhere the causal variables are known and where statistical testing can be used\nto assess the causal relationships. By contrast, recovering a causal structure\nfrom unstructured natural language data such as news articles contains numerous\nchallenges due to the absence of known variables or counterfactual data to\nestimate the causal links. Large Language Models (LLMs) have shown promising\nresults in this direction but also exhibit limitations. This work investigates\nLLM's abilities to build causal graphs from text documents and perform\ncounterfactual causal inference. We propose an end-to-end causal structure\ndiscovery and causal inference method from natural language: we first use an\nLLM to extract the instantiated causal variables from text data and build a\ncausal graph. We merge causal graphs from multiple data sources to represent\nthe most exhaustive set of causes possible. We then conduct counterfactual\ninference on the estimated graph. The causal graph conditioning allows\nreduction of LLM biases and better represents the causal estimands. We use our\nmethod to show that the limitations of LLMs in counterfactual causal reasoning\ncome from prediction errors and propose directions to mitigate them. We\ndemonstrate the applicability of our method on real-world news articles.\n","authors":["Gal Gendron","Joe M. Roanec","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2410.06392v1.pdf","comment":"22 pages, 10 pages for the main paper, 12 pages for the references\n  and appendix, 5 figures"},{"id":"http://arxiv.org/abs/2410.06384v1","updated":"2024-10-08T21:31:42Z","published":"2024-10-08T21:31:42Z","title":"Validation of the Scientific Literature via Chemputation Augmented by\n  Large Language Models","summary":"  Chemputation is the process of programming chemical robots to do experiments\nusing a universal symbolic language, but the literature can be error prone and\nhard to read due to ambiguities. Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains, including natural language\nprocessing, robotic control, and more recently, chemistry. Despite significant\nadvancements in standardizing the reporting and collection of synthetic\nchemistry data, the automatic reproduction of reported syntheses remains a\nlabour-intensive task. In this work, we introduce an LLM-based chemical\nresearch agent workflow designed for the automatic validation of synthetic\nliterature procedures. Our workflow can autonomously extract synthetic\nprocedures and analytical data from extensive documents, translate these\nprocedures into universal XDL code, simulate the execution of the procedure in\na hardware-specific setup, and ultimately execute the procedure on an\nXDL-controlled robotic system for synthetic chemistry. This demonstrates the\npotential of LLM-based workflows for autonomous chemical synthesis with\nChemputers. Due to the abstraction of XDL this approach is safe, secure, and\nscalable since hallucinations will not be chemputable and the XDL can be both\nverified and encrypted. Unlike previous efforts, which either addressed only a\nlimited portion of the workflow, relied on inflexible hard-coded rules, or\nlacked validation in physical systems, our approach provides four realistic\nexamples of syntheses directly executed from synthetic literature. We\nanticipate that our workflow will significantly enhance automation in\nrobotically driven synthetic chemistry research, streamline data extraction,\nimprove the reproducibility, scalability, and safety of synthetic and\nexperimental chemistry.\n","authors":["Sebastian Pagel","Michael Jirasek","Leroy Cronin"],"pdf_url":"https://arxiv.org/pdf/2410.06384v1.pdf","comment":"22 pages, 7 figures, 34 references"},{"id":"http://arxiv.org/abs/2410.06370v1","updated":"2024-10-08T21:08:13Z","published":"2024-10-08T21:08:13Z","title":"HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting\n  Humanitarian Aid","summary":"  Humanitarian organizations can enhance their effectiveness by analyzing data\nto discover trends, gather aggregated insights, manage their security risks,\nsupport decision-making, and inform advocacy and funding proposals. However,\ndata about violent incidents with direct impact and relevance for humanitarian\naid operations is not readily available. An automatic data collection and\nNLP-backed classification framework aligned with humanitarian perspectives can\nhelp bridge this gap. In this paper, we present HumVI - a dataset comprising\nnews articles in three languages (English, French, Arabic) containing instances\nof different types of violent incidents categorized by the humanitarian sector\nthey impact, e.g., aid security, education, food security, health, and\nprotection. Reliable labels were obtained for the dataset by partnering with a\ndata-backed humanitarian organization, Insecurity Insight. We provide multiple\nbenchmarks for the dataset, employing various deep learning architectures and\ntechniques, including data augmentation and mask loss, to address different\ntask-related challenges, e.g., domain expansion. The dataset is publicly\navailable at https://github.com/dataminr-ai/humvi-dataset.\n","authors":["Hemank Lamba","Anton Abilov","Ke Zhang","Elizabeth M. Olson","Henry k. Dambanemuya","Joo c. Brcia","David S. Batista","Christina Wille","Aoife Cahill","Joel Tetreault","Alex Jaimes"],"pdf_url":"https://arxiv.org/pdf/2410.06370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04617v2","updated":"2024-10-08T20:40:59Z","published":"2024-09-06T21:00:57Z","title":"Sparse Rewards Can Self-Train Dialogue Agents","summary":"  Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)\nagents, especially in multi-turn dialogue tasks, have been primarily driven by\nsupervised fine-tuning and high-quality human feedback. However, as base LLM\nmodels continue to improve, acquiring meaningful human feedback has become\nincreasingly challenging and costly. In certain domains, base LLM agents may\neventually exceed human capabilities, making traditional feedback-driven\nmethods impractical. In this paper, we introduce a novel self-improvement\nparadigm that empowers LLM agents to autonomously enhance their performance\nwithout external human feedback. Our method, Juxtaposed Outcomes for Simulation\nHarvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward\nsimulation environment to extract ideal behaviors and further train the LLM on\nits own outputs. We present ToolWOZ, a sparse reward tool-calling simulation\nenvironment derived from MultiWOZ. We demonstrate that models trained with\nJOSH, both small and frontier, significantly improve tool-based interactions\nwhile preserving general model capabilities across diverse benchmarks. Our code\nand data are publicly available on GitHub at\nhttps://github.com/asappresearch/josh-llm-simulation-training\n","authors":["Barrett Martin Lattimer","Varun Gangal","Ryan McDonald","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.04617v2.pdf","comment":"Minor but nontrivial changes likely"},{"id":"http://arxiv.org/abs/2402.01920v2","updated":"2024-10-08T20:32:15Z","published":"2024-02-02T21:45:24Z","title":"Preference Poisoning Attacks on Reward Model Learning","summary":"  Learning reward models from pairwise comparisons is a fundamental component\nin a number of domains, including autonomous control, conversational agents,\nand recommendation systems, as part of a broad goal of aligning automated\ndecisions with user preferences. These approaches entail collecting preference\ninformation from people, with feedback often provided anonymously. Since\npreferences are subjective, there is no gold standard to compare against; yet,\nreliance of high-impact systems on preference learning creates a strong\nmotivation for malicious actors to skew data collected in this fashion to their\nends. We investigate the nature and extent of this vulnerability by considering\nan attacker who can flip a small subset of preference comparisons to either\npromote or demote a target outcome. We propose two classes of algorithmic\napproaches for these attacks: a gradient-based framework, and several variants\nof rank-by-distance methods. Next, we evaluate the efficacy of best attacks in\nboth these classes in successfully achieving malicious goals on datasets from\nthree domains: autonomous control, recommendation system, and textual\nprompt-response preference learning. We find that the best attacks are often\nhighly successful, achieving in the most extreme case 100\\% success rate with\nonly 0.3\\% of the data poisoned. However, \\emph{which} attack is best can vary\nsignificantly across domains. In addition, we observe that the simpler and more\nscalable rank-by-distance approaches are often competitive with, and on\noccasion significantly outperform, gradient-based methods. Finally, we show\nthat state-of-the-art defenses against other classes of poisoning attacks\nexhibit limited efficacy in our setting.\n","authors":["Junlin Wu","Jiongxiao Wang","Chaowei Xiao","Chenguang Wang","Ning Zhang","Yevgeniy Vorobeychik"],"pdf_url":"https://arxiv.org/pdf/2402.01920v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.06424v1","updated":"2024-10-08T23:39:34Z","published":"2024-10-08T23:39:34Z","title":"Restructuring Vector Quantization with the Rotation Trick","summary":"  Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress\na continuous input to a discrete latent space and reconstruct it with minimal\ndistortion. They operate by maintaining a set of vectors -- often referred to\nas the codebook -- and quantizing each encoder output to the nearest vector in\nthe codebook. However, as vector quantization is non-differentiable, the\ngradient to the encoder flows around the vector quantization layer rather than\nthrough it in a straight-through approximation. This approximation may be\nundesirable as all information from the vector quantization operation is lost.\nIn this work, we propose a way to propagate gradients through the vector\nquantization layer of VQ-VAEs. We smoothly transform each encoder output into\nits corresponding codebook vector via a rotation and rescaling linear\ntransformation that is treated as a constant during backpropagation. As a\nresult, the relative magnitude and angle between encoder output and codebook\nvector becomes encoded into the gradient as it propagates through the vector\nquantization layer and back to the encoder. Across 11 different VQ-VAE training\nparadigms, we find this restructuring improves reconstruction metrics, codebook\nutilization, and quantization error. Our code is available at\nhttps://github.com/cfifty/rotation_trick.\n","authors":["Christopher Fifty","Ronald G. Junkins","Dennis Duan","Aniketh Iger","Jerry W. Liu","Ehsan Amid","Sebastian Thrun","Christopher R"],"pdf_url":"https://arxiv.org/pdf/2410.06424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00251v2","updated":"2024-10-08T23:30:47Z","published":"2024-04-30T23:49:26Z","title":"Semantically Consistent Video Inpainting with Conditional Diffusion\n  Models","summary":"  Current state-of-the-art methods for video inpainting typically rely on\noptical flow or attention-based approaches to inpaint masked regions by\npropagating visual information across frames. While such approaches have led to\nsignificant progress on standard benchmarks, they struggle with tasks that\nrequire the synthesis of novel content that is not present in other frames. In\nthis paper, we reframe video inpainting as a conditional generative modeling\nproblem and present a framework for solving such problems with conditional\nvideo diffusion models. We introduce inpainting-specific sampling schemes which\ncapture crucial long-range dependencies in the context, and devise a novel\nmethod for conditioning on the known pixels in incomplete frames. We highlight\nthe advantages of using a generative approach for this task, showing that our\nmethod is capable of generating diverse, high-quality inpaintings and\nsynthesizing new content that is spatially, temporally, and semantically\nconsistent with the provided context.\n","authors":["Dylan Green","William Harvey","Saeid Naderiparizi","Matthew Niedoba","Yunpeng Liu","Xiaoxuan Liang","Jonathan Lavington","Ke Zhang","Vasileios Lioutas","Setareh Dabiri","Adam Scibior","Berend Zwartsenberg","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2405.00251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06420v1","updated":"2024-10-08T23:14:24Z","published":"2024-10-08T23:14:24Z","title":"ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language\n  Models in Hospital Environments","summary":"  The global shortage of healthcare workers has demanded the development of\nsmart healthcare assistants, which can help monitor and alert healthcare\nworkers when necessary. We examine the healthcare knowledge of existing Large\nVision Language Models (LVLMs) via the Visual Question Answering (VQA) task in\nhospital settings through expert annotated open-ended questions. We introduce\nthe Emergency Room Visual Question Answering (ERVQA) dataset, consisting of\n<image, question, answer> triplets covering diverse emergency room scenarios, a\nseminal benchmark for LVLMs. By developing a detailed error taxonomy and\nanalyzing answer trends, we reveal the nuanced nature of the task. We benchmark\nstate-of-the-art open-source and closed LVLMs using traditional and adapted VQA\nmetrics: Entailment Score and CLIPScore Confidence. Analyzing errors across\nmodels, we infer trends based on properties like decoder type, model size, and\nin-context examples. Our findings suggest the ERVQA dataset presents a highly\ncomplex task, highlighting the need for specialized, domain-specific solutions.\n","authors":["Sourjyadip Ray","Kushal Gupta","Soumi Kundu","Payal Arvind Kasat","Somak Aditya","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2410.06420v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.06418v1","updated":"2024-10-08T23:12:33Z","published":"2024-10-08T23:12:33Z","title":"MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual\n  Learning on Point Clouds via Shape Model construction","summary":"  In this paper, we introduce a novel framework for memory-efficient and\nprivacy-preserving continual learning in 3D object classification. Unlike\nconventional memory-based approaches in continual learning that require storing\nnumerous exemplars, our method constructs a compact shape model for each class,\nretaining only the mean shape along with a few key modes of variation. This\nstrategy not only enables the generation of diverse training samples while\ndrastically reducing memory usage but also enhances privacy by eliminating the\nneed to store original data. To further improve model robustness against input\nvariations, an issue common in 3D domains due to the absence of strong\nbackbones and limited training data, we incorporate Gradient Mode\nRegularization. This technique enhances model stability and broadens\nclassification margins, resulting in accuracy improvements. We validate our\napproach through extensive experiments on the ModelNet40, ShapeNet, and ScanNet\ndatasets, where we achieve state-of-the-art performance. Notably, our method\nconsumes only 15% of the memory required by competing methods on the ModelNet40\nand ShapeNet, while achieving comparable performance on the challenging ScanNet\ndataset with just 8.5% of the memory. These results underscore the scalability,\neffectiveness, and privacy-preserving strengths of our framework for 3D object\nclassification.\n","authors":["Hossein Resani","Behrooz Nasihatkon"],"pdf_url":"https://arxiv.org/pdf/2410.06418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01835v2","updated":"2024-10-08T23:01:58Z","published":"2024-09-22T22:50:27Z","title":"EgoAvatar: Egocentric View-Driven and Photorealistic Full-body Avatars","summary":"  Immersive VR telepresence ideally means being able to interact and\ncommunicate with digital avatars that are indistinguishable from and precisely\nreflect the behaviour of their real counterparts. The core technical challenge\nis two fold: Creating a digital double that faithfully reflects the real human\nand tracking the real human solely from egocentric sensing devices that are\nlightweight and have a low energy consumption, e.g. a single RGB camera. Up to\ndate, no unified solution to this problem exists as recent works solely focus\non egocentric motion capture, only model the head, or build avatars from\nmulti-view captures. In this work, we, for the first time in literature,\npropose a person-specific egocentric telepresence approach, which jointly\nmodels the photoreal digital avatar while also driving it from a single\negocentric video. We first present a character model that is animatible, i.e.\ncan be solely driven by skeletal motion, while being capable of modeling\ngeometry and appearance. Then, we introduce a personalized egocentric motion\ncapture component, which recovers full-body motion from an egocentric video.\nFinally, we apply the recovered pose to our character model and perform a\ntest-time mesh refinement such that the geometry faithfully projects onto the\negocentric view. To validate our design choices, we propose a new and\nchallenging benchmark, which provides paired egocentric and dense multi-view\nvideos of real humans performing various motions. Our experiments demonstrate a\nclear step towards egocentric and photoreal telepresence as our method\noutperforms baselines as well as competing methods. For more details, code, and\ndata, we refer to our project page.\n","authors":["Jianchun Chen","Jian Wang","Yinda Zhang","Rohit Pandey","Thabo Beeler","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2410.01835v2.pdf","comment":"Project Page: https://vcai.mpi-inf.mpg.de/projects/EgoAvatar/"},{"id":"http://arxiv.org/abs/2410.06410v1","updated":"2024-10-08T22:45:53Z","published":"2024-10-08T22:45:53Z","title":"BEVLoc: Cross-View Localization and Matching via Birds-Eye-View\n  Synthesis","summary":"  Ground to aerial matching is a crucial and challenging task in outdoor\nrobotics, particularly when GPS is absent or unreliable. Structures like\nbuildings or large dense forests create interference, requiring GNSS\nreplacements for global positioning estimates. The true difficulty lies in\nreconciling the perspective difference between the ground and air images for\nacceptable localization. Taking inspiration from the autonomous driving\ncommunity, we propose a novel framework for synthesizing a birds-eye-view (BEV)\nscene representation to match and localize against an aerial map in off-road\nenvironments. We leverage contrastive learning with domain specific hard\nnegative mining to train a network to learn similar representations between the\nsynthesized BEV and the aerial map. During inference, BEVLoc guides the\nidentification of the most probable locations within the aerial map through a\ncoarse-to-fine matching strategy. Our results demonstrate promising initial\noutcomes in extremely difficult forest environments with limited semantic\ndiversity. We analyze our model's performance for coarse and fine matching,\nassessing both the raw matching capability of our model and its performance as\na GNSS replacement. Our work delves into off-road map localization while\nestablishing a foundational baseline for future developments in localization.\nOur code is available at: https://github.com/rpl-cmu/bevloc\n","authors":["Christopher Klammer","Michael Kaess"],"pdf_url":"https://arxiv.org/pdf/2410.06410v1.pdf","comment":"8 pages, 6 figures, Conference: IROS 2024"},{"id":"http://arxiv.org/abs/2410.06405v1","updated":"2024-10-08T22:25:34Z","published":"2024-10-08T22:25:34Z","title":"Tackling the Abstraction and Reasoning Corpus with Vision Transformers:\n  the Importance of 2D Representation, Positions, and Objects","summary":"  The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on\nvisual reasoning in the evaluation of Artificial Intelligence systems. In its\noriginal framing, an ARC task requires solving a program synthesis problem over\nsmall 2D images using a few input-output training pairs. In this work, we adopt\nthe recently popular data-driven approach to the ARC and ask whether a Vision\nTransformer (ViT) can learn the implicit mapping, from input image to output\nimage, that underlies the task. We show that a ViT -- otherwise a\nstate-of-the-art model for images -- fails dramatically on most ARC tasks even\nwhen trained on one million examples per task. This points to an inherent\nrepresentational deficiency of the ViT architecture that makes it incapable of\nuncovering the simple structured mappings underlying the ARC tasks. Building on\nthese insights, we propose ViTARC, a ViT-style architecture that unlocks some\nof the visual reasoning capabilities required by the ARC. Specifically, we use\na pixel-level input representation, design a spatially-aware tokenization\nscheme, and introduce a novel object-based positional encoding that leverages\nautomatic segmentation, among other enhancements. Our task-specific ViTARC\nmodels achieve a test solve rate close to 100% on more than half of the 400\npublic ARC tasks strictly through supervised learning from input-output grids.\nThis calls attention to the importance of imbuing the powerful (Vision)\nTransformer with the correct inductive biases for abstract visual reasoning\nthat are critical even when the training data is plentiful and the mapping is\nnoise-free. Hence, ViTARC provides a strong foundation for future research in\nvisual reasoning using transformer-based architectures.\n","authors":["Wenhao Li","Yudong Xu","Scott Sanner","Elias Boutros Khalil"],"pdf_url":"https://arxiv.org/pdf/2410.06405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20320v2","updated":"2024-10-08T21:40:13Z","published":"2024-05-30T17:56:04Z","title":"Improving the Training of Rectified Flows","summary":"  Diffusion models have shown great promise for image and video generation, but\nsampling from state-of-the-art models requires expensive numerical integration\nof a generative ODE. One approach for tackling this problem is rectified flows,\nwhich iteratively learn smooth ODE paths that are less susceptible to\ntruncation error. However, rectified flows still require a relatively large\nnumber of function evaluations (NFEs). In this work, we propose improved\ntechniques for training rectified flows, allowing them to compete with\n\\emph{knowledge distillation} methods even in the low NFE setting. Our main\ninsight is that under realistic settings, a single iteration of the Reflow\nalgorithm for training rectified flows is sufficient to learn nearly straight\ntrajectories; hence, the current practice of using multiple Reflow iterations\nis unnecessary. We thus propose techniques to improve one-round training of\nrectified flows, including a U-shaped timestep distribution and LPIPS-Huber\npremetric. With these techniques, we improve the FID of the previous\n2-rectified flow by up to 75\\% in the 1 NFE setting on CIFAR-10. On ImageNet\n64$\\times$64, our improved rectified flow outperforms the state-of-the-art\ndistillation methods such as consistency distillation and progressive\ndistillation in both one-step and two-step settings and rivals the performance\nof improved consistency training (iCT) in FID. Code is available at\nhttps://github.com/sangyun884/rfpp.\n","authors":["Sangyun Lee","Zinan Lin","Giulia Fanti"],"pdf_url":"https://arxiv.org/pdf/2405.20320v2.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.06385v1","updated":"2024-10-08T21:33:02Z","published":"2024-10-08T21:33:02Z","title":"Skin Cancer Machine Learning Model Tone Bias","summary":"  Background: Many open-source skin cancer image datasets are the result of\nclinical trials conducted in countries with lighter skin tones. Due to this\ntone imbalance, machine learning models derived from these datasets can perform\nwell at detecting skin cancer for lighter skin tones. Any tone bias in these\nmodels could introduce fairness concerns and reduce public trust in the\nartificial intelligence health field.\n  Methods: We examine a subset of images from the International Skin Imaging\nCollaboration (ISIC) archive that provide tone information. The subset has a\nsignificant tone imbalance. These imbalances could explain a model's tone bias.\nTo address this, we train models using the imbalanced dataset and a balanced\ndataset to compare against. The datasets are used to train a deep convolutional\nneural network model to classify the images as malignant or benign. We then\nevaluate the models' disparate impact, based on selection rate, relative to\ndark or light skin tone.\n  Results: Using the imbalanced dataset, we found that the model is\nsignificantly better at detecting malignant images in lighter tone resulting in\na disparate impact of 0.577. Using the balanced dataset, we found that the\nmodel is also significantly better at detecting malignant images in lighter\nversus darker tones with a disparate impact of 0.684. Using the imbalanced or\nbalanced dataset to train the model still results in a disparate impact well\nbelow the standard threshold of 0.80 which suggests the model is biased with\nrespect to skin tone.\n  Conclusion: The results show that typical skin cancer machine learning models\ncan be tone biased. These results provide evidence that diagnosis or tone\nimbalance is not the cause of the bias. Other techniques will be necessary to\nidentify and address the bias in these models, an area of future investigation.\n","authors":["James Pope","Md Hassanuzzaman","Mingmar Sherpa","Omar Emara","Ayush Joshi","Nirmala Adhikari"],"pdf_url":"https://arxiv.org/pdf/2410.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06380v1","updated":"2024-10-08T21:26:22Z","published":"2024-10-08T21:26:22Z","title":"Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception\n  Under Adverse Weather Conditions","summary":"  Adverse weather conditions pose a significant challenge to the widespread\nadoption of Autonomous Vehicles (AVs) by impacting sensors like LiDARs and\ncameras. Even though Collaborative Perception (CP) improves AV perception in\ndifficult conditions, existing CP datasets lack adverse weather conditions. To\naddress this, we introduce Adver-City, the first open-source synthetic CP\ndataset focused on adverse weather conditions. Simulated in CARLA with OpenCDA,\nit contains over 24 thousand frames, over 890 thousand annotations, and 110\nunique scenarios across six different weather conditions: clear weather, soft\nrain, heavy rain, fog, foggy heavy rain and, for the first time in a synthetic\nCP dataset, glare. It has six object categories including pedestrians and\ncyclists, and uses data from vehicles and roadside units featuring LiDARs, RGB\nand semantic segmentation cameras, GNSS, and IMUs. Its scenarios, based on real\ncrash reports, depict the most relevant road configurations for adverse weather\nand poor visibility conditions, varying in object density, with both dense and\nsparse scenes, allowing for novel testing conditions of CP models. Benchmarks\nrun on the dataset show that weather conditions created challenging conditions\nfor perception models, reducing multi-modal object detection performance by up\nto 19%, while object density affected LiDAR-based detection by up to 29%. The\ndataset, code and documentation are available at\nhttps://labs.cs.queensu.ca/quarrg/datasets/adver-city/.\n","authors":["Mateus Karvat","Sidney Givigi"],"pdf_url":"https://arxiv.org/pdf/2410.06380v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.06373v1","updated":"2024-10-08T21:14:23Z","published":"2024-10-08T21:14:23Z","title":"Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation\n  Learning","summary":"  This paper delves into the interplay between vision backbones and optimizers,\nunvealing an inter-dependent phenomenon termed\n\\textit{\\textbf{b}ackbone-\\textbf{o}ptimizer \\textbf{c}oupling \\textbf{b}ias}\n(BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a\nmarked co-dependency with SGD families, while recent architectures like ViTs\nand ConvNeXt share a tight coupling with the adaptive learning rate ones. We\nfurther show that BOCB can be introduced by both optimizers and certain\nbackbone designs and may significantly impact the pre-training and downstream\nfine-tuning of vision models. Through in-depth empirical analysis, we summarize\ntakeaways on recommended optimizers and insights into robust vision backbone\narchitectures. We hope this work can inspire the community to question\nlong-held assumptions on backbones and optimizers, stimulate further\nexplorations, and thereby contribute to more robust vision systems. The source\ncode and models are publicly available at https://bocb-ai.github.io/.\n","authors":["Siyuan Li","Juanxi Tian","Zedong Wang","Luyuan Zhang","Zicheng Liu","Weiyang Jin","Yang Liu","Baigui Sun","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2410.06373v1.pdf","comment":"Preprint V1. Online project at https://bocb-ai.github.io/"},{"id":"http://arxiv.org/abs/2405.18654v2","updated":"2024-10-08T21:01:08Z","published":"2024-05-28T23:36:00Z","title":"Data-augmented phrase-level alignment for mitigating object\n  hallucination","summary":"  Despite their significant advancements, Multimodal Large Language Models\n(MLLMs) often generate factually inaccurate information, referred to as\nhallucination. In this work, we address object hallucinations in MLLMs, where\ninformation is generated about an object not present in the input image. We\nintroduce Data-augmented Phrase-level Alignment (DPA), a novel loss which can\nbe applied to instruction-tuned off-the-shelf MLLMs to mitigate hallucinations,\nwhile preserving their general vision-language capabilities. To fine-tune MLLMs\nwith DPA, we first generate a set of `hallucinated' and `correct' response\npairs through generative data augmentation by selectively altering the\nground-truth information of the correct responses at a phrase level. The DPA\nloss is then used to train MLLMs to reduce the likelihood of hallucinated\nphrases compared to the correct ones. Our thorough evaluation on various\nbenchmarks confirms the effectiveness of DPA in mitigating hallucination while\nretaining the out-of-the-box performance of the MLLMs on general tasks. For\ninstance, MLLMs finetuned with DPA, which we refer to as Hallucination\nAttenuated Language and Vision Assistant (HALVA), improve F1 by up to 13.4% on\nhallucination visual question-answering and reduce the hallucination rate by up\nto 4.2% on image description tasks.\n","authors":["Pritam Sarkar","Sayna Ebrahimi","Ali Etemad","Ahmad Beirami","Sercan . Ark","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2405.18654v2.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2410.06353v1","updated":"2024-10-08T20:42:51Z","published":"2024-10-08T20:42:51Z","title":"Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal\n  Action Segmentation","summary":"  Skeleton-based Temporal Action Segmentation involves the dense action\nclassification of variable-length skeleton sequences. Current approaches\nprimarily apply graph-based networks to extract framewise, whole-body-level\nmotion representations, and use one-hot encoded labels for model optimization.\nHowever, whole-body motion representations do not capture fine-grained\npart-level motion representations and the one-hot encoded labels neglect the\nintrinsic semantic relationships within the language-based action definitions.\nTo address these limitations, we propose a novel method named Language-assisted\nHuman Part Motion Representation Learning (LPL), which contains a Disentangled\nPart Motion Encoder (DPE) to extract dual-level (i.e., part and whole-body)\nmotion representations and a Language-assisted Distribution Alignment (LDA)\nstrategy for optimizing spatial relations within representations. Specifically,\nafter part-aware skeleton encoding via DPE, LDA generates dual-level action\ndescriptions to construct a textual embedding space with the help of a\nlarge-scale language model. Then, LDA motivates the alignment of the embedding\nspace between text descriptions and motions. This alignment allows LDA not only\nto enhance intra-class compactness but also to transfer the language-encoded\nsemantic correlations among actions to skeleton-based motion learning.\nMoreover, we propose a simple yet efficient Semantic Offset Adapter to smooth\nthe cross-domain misalignment. Our experiments indicate that LPL achieves\nstate-of-the-art performance across various datasets (e.g., +4.4\\% Accuracy,\n+5.6\\% F1 on the PKU-MMD dataset). Moreover, LDA is compatible with existing\nmethods and improves their performance (e.g., +4.8\\% Accuracy, +4.3\\% F1 on the\nLARa dataset) without additional inference costs.\n","authors":["Bowen Chen","Haoyu Ji","Zhiyong Wang","Benjamin Filtjens","Chunzhuo Wang","Weihong Ren","Bart Vanrumste","Honghai Liu"],"pdf_url":"https://arxiv.org/pdf/2410.06353v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.06384v1","updated":"2024-10-08T21:31:42Z","published":"2024-10-08T21:31:42Z","title":"Validation of the Scientific Literature via Chemputation Augmented by\n  Large Language Models","summary":"  Chemputation is the process of programming chemical robots to do experiments\nusing a universal symbolic language, but the literature can be error prone and\nhard to read due to ambiguities. Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains, including natural language\nprocessing, robotic control, and more recently, chemistry. Despite significant\nadvancements in standardizing the reporting and collection of synthetic\nchemistry data, the automatic reproduction of reported syntheses remains a\nlabour-intensive task. In this work, we introduce an LLM-based chemical\nresearch agent workflow designed for the automatic validation of synthetic\nliterature procedures. Our workflow can autonomously extract synthetic\nprocedures and analytical data from extensive documents, translate these\nprocedures into universal XDL code, simulate the execution of the procedure in\na hardware-specific setup, and ultimately execute the procedure on an\nXDL-controlled robotic system for synthetic chemistry. This demonstrates the\npotential of LLM-based workflows for autonomous chemical synthesis with\nChemputers. Due to the abstraction of XDL this approach is safe, secure, and\nscalable since hallucinations will not be chemputable and the XDL can be both\nverified and encrypted. Unlike previous efforts, which either addressed only a\nlimited portion of the workflow, relied on inflexible hard-coded rules, or\nlacked validation in physical systems, our approach provides four realistic\nexamples of syntheses directly executed from synthetic literature. We\nanticipate that our workflow will significantly enhance automation in\nrobotically driven synthetic chemistry research, streamline data extraction,\nimprove the reproducibility, scalability, and safety of synthetic and\nexperimental chemistry.\n","authors":["Sebastian Pagel","Michael Jirasek","Leroy Cronin"],"pdf_url":"https://arxiv.org/pdf/2410.06384v1.pdf","comment":"22 pages, 7 figures, 34 references"},{"id":"http://arxiv.org/abs/2410.06371v1","updated":"2024-10-08T21:09:55Z","published":"2024-10-08T21:09:55Z","title":"Improved Estimation of Ranks for Learning ItemRecommenders with Negative\n  Sampling","summary":"  In recommendation systems, there has been a growth in the num-ber of\nrecommendable items (# of movies, music, products). Whenthe set of\nrecommendable items is large, training and evaluationof item recommendation\nmodels becomes computationally expen-sive. To lower this cost, it has become\ncommon to sample negativeitems. However, the recommendation quality can suffer\nfrom biasesintroduced by traditional negative sampling mechanisms.In this work,\nwe demonstrate the benefits from correcting thebias introduced by sampling of\nnegatives. We first provide sampledbatch version of the well-studied WARP and\nLambdaRank methods.Then, we present how these methods can benefit from\nimprovedranking estimates. Finally, we evaluate the recommendation qualityas a\nresult of correcting rank estimates and demonstrate that WARPand LambdaRank can\nbe learned efficiently with negative samplingand our proposed correction\ntechnique.\n","authors":["Anushya Subbiah","Steffen Rendle","Vikram Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2410.06371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06311v1","updated":"2024-10-08T19:43:37Z","published":"2024-10-08T19:43:37Z","title":"A Comparative Study of Hybrid Models in Health Misinformation Text\n  Classification","summary":"  This study evaluates the effectiveness of machine learning (ML) and deep\nlearning (DL) models in detecting COVID-19-related misinformation on online\nsocial networks (OSNs), aiming to develop more effective tools for countering\nthe spread of health misinformation during the pan-demic. The study trained and\ntested various ML classifiers (Naive Bayes, SVM, Random Forest, etc.), DL\nmodels (CNN, LSTM, hybrid CNN+LSTM), and pretrained language models\n(DistilBERT, RoBERTa) on the \"COVID19-FNIR DATASET\". These models were\nevaluated for accuracy, F1 score, recall, precision, and ROC, and used\npreprocessing techniques like stemming and lemmatization. The results showed\nSVM performed well, achieving a 94.41% F1-score. DL models with Word2Vec\nembeddings exceeded 98% in all performance metrics (accuracy, F1 score, recall,\nprecision & ROC). The CNN+LSTM hybrid models also exceeded 98% across\nperformance metrics, outperforming pretrained models like DistilBERT and\nRoBERTa. Our study concludes that DL and hybrid DL models are more effective\nthan conventional ML algorithms for detecting COVID-19 misinformation on OSNs.\nThe findings highlight the importance of advanced neural network approaches and\nlarge-scale pretraining in misinformation detection. Future research should\noptimize these models for various misinformation types and adapt to changing\nOSNs, aiding in combating health misinformation.\n","authors":["Mkululi Sikosana","Oluwaseun Ajao","Sean Maudsley-Barton"],"pdf_url":"https://arxiv.org/pdf/2410.06311v1.pdf","comment":"8 pages, 4 tables presented at the OASIS workshop of the ACM\n  Hypertext and Social Media Conference 2024"},{"id":"http://arxiv.org/abs/2409.10309v2","updated":"2024-10-08T19:30:34Z","published":"2024-09-16T14:15:42Z","title":"beeFormer: Bridging the Gap Between Semantic and Interaction Similarity\n  in Recommender Systems","summary":"  Recommender systems often use text-side information to improve their\npredictions, especially in cold-start or zero-shot recommendation scenarios,\nwhere traditional collaborative filtering approaches cannot be used. Many\napproaches to text-mining side information for recommender systems have been\nproposed over recent years, with sentence Transformers being the most prominent\none. However, these models are trained to predict semantic similarity without\nutilizing interaction data with hidden patterns specific to recommender\nsystems. In this paper, we propose beeFormer, a framework for training sentence\nTransformer models with interaction data. We demonstrate that our models\ntrained with beeFormer can transfer knowledge between datasets while\noutperforming not only semantic similarity sentence Transformers but also\ntraditional collaborative filtering methods. We also show that training on\nmultiple datasets from different domains accumulates knowledge in a single\nmodel, unlocking the possibility of training universal, domain-agnostic\nsentence Transformer models to mine text representations for recommender\nsystems. We release the source code, trained models, and additional details\nallowing replication of our experiments at\nhttps://github.com/recombee/beeformer.\n","authors":["Vojtch Vanura","Pavel Kordk","Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2409.10309v2.pdf","comment":"Accepted to RecSys 2024"},{"id":"http://arxiv.org/abs/2310.13001v4","updated":"2024-10-08T17:39:36Z","published":"2023-10-06T12:31:05Z","title":"Conversational Factor Information Retrieval Model (ConFIRM)","summary":"  This paper introduces the Conversational Factor Information Retrieval Method\n(ConFIRM), a novel approach to fine-tuning large language models (LLMs) for\ndomain-specific retrieval tasks. ConFIRM leverages the Five-Factor Model of\npersonality to generate synthetic datasets that accurately reflect target\npopulation characteristics, addressing data scarcity in specialized domains. We\ndemonstrate ConFIRM's effectiveness through a case study in the finance sector,\nfine-tuning a Llama-2-7b model using personality-aligned data from the\nPolyU-Asklora Fintech Adoption Index. The resulting model achieved 91% accuracy\nin classifying financial queries, with an average inference time of 0.61\nseconds on an NVIDIA A100 GPU. ConFIRM shows promise for creating more accurate\nand personalized AI-driven information retrieval systems across various\ndomains, potentially mitigating issues of hallucinations and outdated\ninformation in LLMs deployed\n","authors":["Stephen Choi","William Gazeley","Siu Ho Wong","Tingting Li"],"pdf_url":"https://arxiv.org/pdf/2310.13001v4.pdf","comment":"8 pages, 2 figures, 2 tables, 2 appendices"},{"id":"http://arxiv.org/abs/2406.14783v2","updated":"2024-10-08T15:10:42Z","published":"2024-06-20T23:20:34Z","title":"Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework","summary":"  Challenges in the automated evaluation of Retrieval-Augmented Generation\n(RAG) Question-Answering (QA) systems include hallucination problems in\ndomain-specific knowledge and the lack of gold standard benchmarks for company\ninternal tasks. This results in difficulties in evaluating RAG variations, like\nRAG-Fusion (RAGF), in the context of a product QA task at Infineon\nTechnologies. To solve these problems, we propose a comprehensive evaluation\nframework, which leverages Large Language Models (LLMs) to generate large\ndatasets of synthetic queries based on real user queries and in-domain\ndocuments, uses LLM-as-a-judge to rate retrieved documents and answers,\nevaluates the quality of answers, and ranks different variants of\nRetrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based\ncompetition. LLM-as-a-judge rating of a random sample of synthetic queries\nshows a moderate, positive correlation with domain expert scoring in relevance,\naccuracy, completeness, and precision. While RAGF outperformed RAG in Elo\nscore, a significance analysis against expert annotations also shows that RAGF\nsignificantly outperforms RAG in completeness, but underperforms in precision.\nIn addition, Infineon's RAGF assistant demonstrated slightly higher performance\nin document relevance based on MRR@5 scores. We find that RAGElo positively\naligns with the preferences of human annotators, though due caution is still\nrequired. Finally, RAGF's approach leads to more complete answers based on\nexpert annotations and better answers overall based on RAGElo's evaluation\ncriteria.\n","authors":["Zackary Rackauckas","Arthur Cmara","Jakub Zavrel"],"pdf_url":"https://arxiv.org/pdf/2406.14783v2.pdf","comment":"Accepted to LLM4Eval @ SIGIR24"},{"id":"http://arxiv.org/abs/2409.18427v3","updated":"2024-10-08T14:23:25Z","published":"2024-09-27T03:28:11Z","title":"Neural Collaborative Filtering to Detect Anomalies in Human Semantic\n  Trajectories","summary":"  Human trajectory anomaly detection has become increasingly important across a\nwide range of applications, including security surveillance and public health.\nHowever, existing trajectory anomaly detection methods are primarily focused on\nvehicle-level traffic, while human-level trajectory anomaly detection remains\nunder-explored. Since human trajectory data is often very sparse, machine\nlearning methods have become the preferred approach for identifying complex\npatterns. However, concerns regarding potential biases and the robustness of\nthese models have intensified the demand for more transparent and explainable\nalternatives. In response to these challenges, our research focuses on\ndeveloping a lightweight anomaly detection model specifically designed to\ndetect anomalies in human trajectories. We propose a Neural Collaborative\nFiltering approach to model and predict normal mobility. Our method is designed\nto model users' daily patterns of life without requiring prior knowledge,\nthereby enhancing performance in scenarios where data is sparse or incomplete,\nsuch as in cold start situations. Our algorithm consists of two main modules.\nThe first is the collaborative filtering module, which applies collaborative\nfiltering to model normal mobility of individual humans to places of interest.\nThe second is the neural module, responsible for interpreting the complex\nspatio-temporal relationships inherent in human trajectory data. To validate\nour approach, we conducted extensive experiments using simulated and real-world\ndatasets comparing to numerous state-of-the-art trajectory anomaly detection\napproaches.\n","authors":["Yueyang Liu","Lance Kennedy","Hossein Amiri","Andreas Zfle"],"pdf_url":"https://arxiv.org/pdf/2409.18427v3.pdf","comment":"Accepted for publication in the 1st ACM SIGSPATIAL International\n  Workshop on Geospatial Anomaly Detection (GeoAnomalies'24)"},{"id":"http://arxiv.org/abs/2410.06062v1","updated":"2024-10-08T14:09:12Z","published":"2024-10-08T14:09:12Z","title":"LLM-based SPARQL Query Generation from Natural Language over Federated\n  Knowledge Graphs","summary":"  We introduce a Retrieval-Augmented Generation (RAG) system for translating\nuser questions into accurate federated SPARQL queries over bioinformatics\nknowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance\naccuracy and reduce hallucinations in query generation, our system utilises\nmetadata from the KGs, including query examples and schema information, and\nincorporates a validation step to correct generated queries. The system is\navailable online at chat.expasy.org.\n","authors":["Vincent Emonet","Jerven Bolleman","Severine Duvaud","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06043v1","updated":"2024-10-08T13:41:35Z","published":"2024-10-08T13:41:35Z","title":"KwicKwocKwac, a tool for rapidly generating concordances and marking up\n  a literary text","summary":"  This paper introduces KwicKwocKwac 1.0 (KwicKK), a web application designed\nto enhance the annotation and enrichment of digital texts in the humanities.\nKwicKK provides a user-friendly interface that enables scholars and researchers\nto perform semi-automatic markup of textual documents, facilitating the\nidentification of relevant entities such as people, organizations, and\nlocations. Key functionalities include the visualization of annotated texts\nusing KeyWord in Context (KWIC), KeyWord Out Of Context (KWOC), and KeyWord\nAfter Context (KWAC) methodologies, alongside automatic disambiguation of\ngeneric references and integration with Wikidata for Linked Open Data\nconnections. The application supports metadata input and offers multiple\ndownload formats, promoting accessibility and ease of use. Developed primarily\nfor the National Edition of Aldo Moro's works, KwicKK aims to lower the\ntechnical barriers for users while fostering deeper engagement with digital\nscholarly resources. The architecture leverages contemporary web technologies,\nensuring scalability and reliability. Future developments will explore user\nexperience enhancements, collaborative features, and integration of additional\ndata sources.\n","authors":["Sebastian Barzaghi","Francesco Paolucci","Francesca Tomasi","Fabio Vitali"],"pdf_url":"https://arxiv.org/pdf/2410.06043v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.05165v2","updated":"2024-10-08T13:33:52Z","published":"2024-10-07T16:23:36Z","title":"Efficient Inference for Large Language Model-based Generative\n  Recommendation","summary":"  Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5 speedup under relaxed\nsampling verification. The codes and datasets will be released in the near\nfuture.\n","authors":["Xinyu Lin","Chaoqun Yang","Wenjie Wang","Yongqi Li","Cunxiao Du","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.05165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06010v1","updated":"2024-10-08T13:08:07Z","published":"2024-10-08T13:08:07Z","title":"A large collection of bioinformatics question-query pairs over federated\n  knowledge graphs: methodology and applications","summary":"  Background. In the last decades, several life science resources have\nstructured data using the same framework and made these accessible using the\nsame query language to facilitate interoperability. Knowledge graphs have seen\nincreased adoption in bioinformatics due to their advantages for representing\ndata in a generic graph format. For example, yummydata.org catalogs more than\n60 knowledge graphs accessible through SPARQL, a technical query language.\nAlthough SPARQL allows powerful, expressive queries, even across physically\ndistributed knowledge graphs, formulating such queries is a challenge for most\nusers. Therefore, to guide users in retrieving the relevant data, many of these\nresources provide representative examples. These examples can also be an\nimportant source of information for machine learning, if a sufficiently large\nnumber of examples are provided and published in a common, machine-readable and\nstandardized format across different resources.\n  Findings. We introduce a large collection of human-written natural language\nquestions and their corresponding SPARQL queries over federated bioinformatics\nknowledge graphs (KGs) collected for several years across different research\ngroups at the SIB Swiss Institute of Bioinformatics. The collection comprises\nmore than 1000 example questions and queries, including 65 federated queries.\nWe propose a methodology to uniformly represent the examples with minimal\nmetadata, based on existing standards. Furthermore, we introduce an extensive\nset of open-source applications, including query graph visualizations and smart\nquery editors, easily reusable by KG maintainers who adopt the proposed\nmethodology.\n  Conclusions. We encourage the community to adopt and extend the proposed\nmethodology, towards richer KG metadata and improved Semantic Web services.\n","authors":["Jerven Bolleman","Vincent Emonet","Adrian Altenhoff","Amos Bairoch","Marie-Claude Blatter","Alan Bridge","Severine Duvaud","Elisabeth Gasteiger","Dmitry Kuznetsov","Sebastien Moretti","Pierre-Andre Michel","Anne Morgat","Marco Pagni","Nicole Redaschi","Monique Zahn-Zabal","Tarcisio Mendes de Farias","Ana Claudia Sima"],"pdf_url":"https://arxiv.org/pdf/2410.06010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05939v1","updated":"2024-10-08T11:42:37Z","published":"2024-10-08T11:42:37Z","title":"RLRF4Rec: Reinforcement Learning from Recsys Feedback for Enhanced\n  Recommendation Reranking","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains, prompting researchers to explore their potential for use in\nrecommendation systems. Initial attempts have leveraged the exceptional\ncapabilities of LLMs, such as rich knowledge and strong generalization through\nIn-context Learning, which involves phrasing the recommendation task as\nprompts. Nevertheless, the performance of LLMs in recommendation tasks remains\nsuboptimal due to a substantial disparity between the training tasks for LLMs\nand recommendation tasks and inadequate recommendation data during\npre-training. This paper introduces RLRF4Rec, a novel framework integrating\nReinforcement Learning from Recsys Feedback for Enhanced Recommendation\nReranking(RLRF4Rec) with LLMs to address these challenges. Specifically, We\nfirst have the LLM generate inferred user preferences based on user interaction\nhistory, which is then used to augment traditional ID-based sequence\nrecommendation models. Subsequently, we trained a reward model based on\nknowledge augmentation recommendation models to evaluate the quality of the\nreasoning knowledge from LLM. We then select the best and worst responses from\nthe N samples to construct a dataset for LLM tuning. Finally, we design a\nstructure alignment strategy with Direct Preference Optimization(DPO). We\nvalidate the effectiveness of RLRF4Rec through extensive experiments,\ndemonstrating significant improvements in recommendation re-ranking metrics\ncompared to baselines. This demonstrates that our approach significantly\nimproves the capability of LLMs to respond to instructions within recommender\nsystems.\n","authors":["Chao Sun","Yaobo Liang","Yaming Yang","Shilin Xu","Tianmeng Yang","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2410.05939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18462v2","updated":"2024-10-08T10:31:29Z","published":"2024-03-27T11:20:48Z","title":"Decoy Effect In Search Interaction: Understanding User Behavior and\n  Measuring System Vulnerability","summary":"  This study examines the decoy effect's underexplored influence on user search\ninteractions and methods for measuring information retrieval (IR) systems'\nvulnerability to this effect. It explores how decoy results alter users'\ninteractions on search engine result pages, focusing on metrics like\nclick-through likelihood, browsing time, and perceived document usefulness. By\nanalyzing user interaction logs from multiple datasets, the study demonstrates\nthat decoy results significantly affect users' behavior and perceptions.\nFurthermore, it investigates how different levels of task difficulty and user\nknowledge modify the decoy effect's impact, finding that easier tasks and lower\nknowledge levels lead to higher engagement with target documents. In terms of\nIR system evaluation, the study introduces the DEJA-VU metric to assess\nsystems' susceptibility to the decoy effect, testing it on specific retrieval\ntasks. The results show differences in systems' effectiveness and\nvulnerability, contributing to our understanding of cognitive biases in search\nbehavior and suggesting pathways for creating more balanced and bias-aware IR\nevaluations.\n","authors":["Nuo Chen","Jiqun Liu","Hanpei Fang","Yuankai Luo","Tetsuya Sakai","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05877v1","updated":"2024-10-08T10:06:45Z","published":"2024-10-08T10:06:45Z","title":"MDAP: A Multi-view Disentangled and Adaptive Preference Learning\n  Framework for Cross-Domain Recommendation","summary":"  Cross-domain Recommendation systems leverage multi-domain user interactions\nto improve performance, especially in sparse data or new user scenarios.\nHowever, CDR faces challenges such as effectively capturing user preferences\nand avoiding negative transfer. To address these issues, we propose the\nMulti-view Disentangled and Adaptive Preference Learning (MDAP) framework. Our\nMDAP framework uses a multiview encoder to capture diverse user preferences.\nThe framework includes a gated decoder that adaptively combines embeddings from\ndifferent views to generate a comprehensive user representation. By\ndisentangling representations and allowing adaptive feature selection, our\nmodel enhances adaptability and effectiveness. Extensive experiments on\nbenchmark datasets demonstrate that our method significantly outperforms\nstate-of-the-art CDR and single-domain models, providing more accurate\nrecommendations and deeper insights into user behavior across different\ndomains.\n","authors":["Junxiong Tong","Mingjia Yin","Hao Wang","Qiushi Pan","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05877v1.pdf","comment":"The International Web Information Systems Engineering conference"},{"id":"http://arxiv.org/abs/2410.05863v1","updated":"2024-10-08T09:53:10Z","published":"2024-10-08T09:53:10Z","title":"Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework","summary":"  Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.\n","authors":["Yunfei Yang","Zhenghao Qi","Honghuan Wu","Qi Song","Tieyao Zhang","Hao Li","Yimin Tu","Kaiqiao Zhan","Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2410.05863v1.pdf","comment":"CIKM 2024 applied research track, 7 pages"},{"id":"http://arxiv.org/abs/2008.01377v4","updated":"2024-10-08T09:01:08Z","published":"2020-08-04T07:21:36Z","title":"Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued\n  Prediction","summary":"  Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.\n","authors":["Stefan Heid","Marcel Wever","Eyke Hllermeier"],"pdf_url":"https://arxiv.org/pdf/2008.01377v4.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05806v1","updated":"2024-10-08T08:39:15Z","published":"2024-10-08T08:39:15Z","title":"A Parameter Update Balancing Algorithm for Multi-task Ranking Models in\n  Recommendation Systems","summary":"  Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2410.05806v1.pdf","comment":"Accepted by ICDM'24"},{"id":"http://arxiv.org/abs/2410.05779v1","updated":"2024-10-08T08:00:12Z","published":"2024-10-08T08:00:12Z","title":"LightRAG: Simple and Fast Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.\n","authors":["Zirui Guo","Lianghao Xia","Yanhua Yu","Tu Ao","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2410.05779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05763v1","updated":"2024-10-08T07:41:01Z","published":"2024-10-08T07:41:01Z","title":"Information Discovery in e-Commerce","summary":"  Electronic commerce, or e-commerce, is the buying and selling of goods and\nservices, or the transmitting of funds or data online. E-commerce platforms\ncome in many kinds, with global players such as Amazon, Airbnb, Alibaba,\nBooking.com, eBay, JD.com and platforms targeting specific geographic regions\nsuch as Bol.com and Flipkart.com.Information retrieval has a natural role to\nplay in e-commerce, especially in connecting people to goods and services.\nInformation discovery in e-commerce concerns different types of search (e.g.,\nexploratory search vs. lookup tasks), recommender systems, and natural language\nprocessing in e-commerce portals. The rise in popularity of e-commerce sites\nhas made research on information discovery in e-commerce an increasingly active\nresearch area. This is witnessed by an increase in publications and dedicated\nworkshops in this space. Methods for information discovery in e-commerce\nlargely focus on improving the effectiveness of e-commerce search and\nrecommender systems, on enriching and using knowledge graphs to support\ne-commerce, and on developing innovative question answering and bot-based\nsolutions that help to connect people to goods and services. In this survey, an\noverview is given of the fundamental infrastructure, algorithms, and technical\nsolutions for information discovery in e-commerce. The topics covered include\nuser behavior and profiling, search, recommendation, and language technology in\ne-commerce.\n","authors":["Zhaochun Ren","Xiangnan He","Dawei Yin","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2410.05763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05752v1","updated":"2024-10-08T07:28:17Z","published":"2024-10-08T07:28:17Z","title":"Exploring the Meaningfulness of Nearest Neighbor Search in\n  High-Dimensional Space","summary":"  Dense high dimensional vectors are becoming increasingly vital in fields such\nas computer vision, machine learning, and large language models (LLMs), serving\nas standard representations for multimodal data. Now the dimensionality of\nthese vector can exceed several thousands easily. Despite the nearest neighbor\nsearch (NNS) over these dense high dimensional vectors have been widely used\nfor retrieval augmented generation (RAG) and many other applications, the\neffectiveness of NNS in such a high-dimensional space remains uncertain, given\nthe possible challenge caused by the \"curse of dimensionality.\" To address\nabove question, in this paper, we conduct extensive NNS studies with different\ndistance functions, such as $L_1$ distance, $L_2$ distance and\nangular-distance, across diverse embedding datasets, of varied types,\ndimensionality and modality. Our aim is to investigate factors influencing the\nmeaningfulness of NNS. Our experiments reveal that high-dimensional text\nembeddings exhibit increased resilience as dimensionality rises to higher\nlevels when compared to random vectors. This resilience suggests that text\nembeddings are less affected to the \"curse of dimensionality,\" resulting in\nmore meaningful NNS outcomes for practical use. Additionally, the choice of\ndistance function has minimal impact on the relevance of NNS. Our study shows\nthe effectiveness of the embedding-based data representation method and can\noffer opportunity for further optimization of dense vector-related\napplications.\n","authors":["Zhonghan Chen","Ruiyuan Zhang","Xi Zhao","Xiaojun Cheng","Xiaofang Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.05752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05731v1","updated":"2024-10-08T06:48:46Z","published":"2024-10-08T06:48:46Z","title":"Enhancing SPARQL Generation by Triplet-order-sensitive Pre-training","summary":"  Semantic parsing that translates natural language queries to SPARQL is of\ngreat importance for Knowledge Graph Question Answering (KGQA) systems.\nAlthough pre-trained language models like T5 have achieved significant success\nin the Text-to-SPARQL task, their generated outputs still exhibit notable\nerrors specific to the SPARQL language, such as triplet flips. To address this\nchallenge and further improve the performance, we propose an additional\npre-training stage with a new objective, Triplet Order Correction (TOC), along\nwith the commonly used Masked Language Modeling (MLM), to collectively enhance\nthe model's sensitivity to triplet order and SPARQL syntax. Our method achieves\nstate-of-the-art performances on three widely-used benchmarks.\n","authors":["Chang Su","Jiexing Qi","He Yan","Kai Zou","Zhouhan Lin"],"pdf_url":"https://arxiv.org/pdf/2410.05731v1.pdf","comment":"accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2410.05672v1","updated":"2024-10-08T04:02:06Z","published":"2024-10-08T04:02:06Z","title":"Embedding derivatives and derivative Area operators of Hardy spaces into\n  Lebesgue spaces","summary":"  We characterize the compactness of embedding derivatives from Hardy space\n$H^p$ into Lebesgue space $L^q(\\mu)$. We also completely characterize the\nboundedness and compactness of derivative area operators from $H^p$ into\n$L^q(\\mathbb{S}_n)$, $0<p, q<\\infty$. Some of the tools used in the proof of\nthe one-dimensional case are not available in higher dimensions, such as the\nstrong factorization of Hardy spaces. Therefore, we need the theory of tent\nspaces which was established by Coifman, Mayer and Stein in 1985.\n","authors":["Xiaosong Liu","Zengjian Lou","Zixing Yuan","Ruhan Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.05672v1.pdf","comment":"28pages"},{"id":"http://arxiv.org/abs/2212.06933v3","updated":"2024-10-08T03:29:14Z","published":"2022-12-13T23:06:20Z","title":"Paraphrase Identification with Deep Learning: A Review of Datasets and\n  Methods","summary":"  The rapid progress of Natural Language Processing (NLP) technologies has led\nto the widespread availability and effectiveness of text generation tools such\nas ChatGPT and Claude. While highly useful, these technologies also pose\nsignificant risks to the credibility of various media forms if they are\nemployed for paraphrased plagiarism -- one of the most subtle forms of content\nmisuse in scientific literature and general text media. Although automated\nmethods for paraphrase identification have been developed, detecting this type\nof plagiarism remains challenging due to the inconsistent nature of the\ndatasets used to train these methods. In this article, we examine traditional\nand contemporary approaches to paraphrase identification, investigating how the\nunder-representation of certain paraphrase types in popular datasets, including\nthose used to train Large Language Models (LLMs), affects the ability to detect\nplagiarism. We introduce and validate a new refined typology for paraphrases\n(ReParaphrased, REfined PARAPHRASE typology definitions) to better understand\nthe disparities in paraphrase type representation. Lastly, we propose new\ndirections for future research and dataset development to enhance AI-based\nparaphrase detection.\n","authors":["Chao Zhou","Cheng Qiu","Lizhen Liang","Daniel E. Acuna"],"pdf_url":"https://arxiv.org/pdf/2212.06933v3.pdf","comment":"45 pages, 6 figures, 7 tables, 143 references"},{"id":"http://arxiv.org/abs/2403.18227v4","updated":"2024-10-08T02:24:25Z","published":"2024-03-27T03:31:05Z","title":"One Backpropagation in Two Tower Recommendation Models","summary":"  Recent years have witnessed extensive researches on developing two tower\nrecommendation models for relieving information overload. Four building modules\ncan be identified in such models, namely, user-item encoding, negative\nsampling, loss computing and back-propagation updating. To the best of our\nknowledge, existing algorithms have researched only on the first three modules,\nyet neglecting the backpropagation module. They all adopt a kind of two\nbackpropagation strategy, which are based on an implicit assumption of equally\ntreating users and items in the training phase. In this paper, we challenge\nsuch an equal training assumption and propose a novel one backpropagation\nupdating strategy, which keeps the normal gradient backpropagation for the item\nencoding tower, but cuts off the backpropagation for the user encoding tower.\nInstead, we propose a moving-aggregation updating strategy to update a user\nencoding in each training epoch. Except the proposed backpropagation updating\nmodule, we implement the other three modules with the most straightforward\nchoices. Experiments on four public datasets validate the effectiveness and\nefficiency of our model in terms of improved recommendation performance and\nreduced computation overload over the state-of-the-art competitors.\n","authors":["Erjia Chen","Bang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18227v4.pdf","comment":"14 pages, 7 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.06221v1","updated":"2024-10-08T17:29:14Z","published":"2024-10-08T17:29:14Z","title":"POLIPHONE: A Dataset for Smartphone Model Identification from Audio\n  Recordings","summary":"  When dealing with multimedia data, source attribution is a key challenge from\na forensic perspective. This task aims to determine how a given content was\ncaptured, providing valuable insights for various applications, including legal\nproceedings and integrity investigations. The source attribution problem has\nbeen addressed in different domains, from identifying the camera model used to\ncapture specific photographs to detecting the synthetic speech generator or\nmicrophone model used to create or record given audio tracks. Recent\nadvancements in this area rely heavily on machine learning and data-driven\ntechniques, which often outperform traditional signal processing-based methods.\n  However, a drawback of these systems is their need for large volumes of\ntraining data, which must reflect the latest technological trends to produce\naccurate and reliable predictions. This presents a significant challenge, as\nthe rapid pace of technological progress makes it difficult to maintain\ndatasets that are up-to-date with real-world conditions. For instance, in the\ntask of smartphone model identification from audio recordings, the available\ndatasets are often outdated or acquired inconsistently, making it difficult to\ndevelop solutions that are valid beyond a research environment. In this paper\nwe present POLIPHONE, a dataset for smartphone model identification from audio\nrecordings. It includes data from 20 recent smartphones recorded in a\ncontrolled environment to ensure reproducibility and scalability for future\nresearch. The released tracks contain audio data from various domains (i.e.,\nspeech, music, environmental sounds), making the corpus versatile and\napplicable to a wide range of use cases. We also present numerous experiments\nto benchmark the proposed dataset using a state-of-the-art classifier for\nsmartphone model identification from audio recordings.\n","authors":["Davide Salvi","Daniele Ugo Leonzio","Antonio Giganti","Claudio Eutizi","Sara Mandelli","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2410.06221v1.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2410.06149v1","updated":"2024-10-08T15:48:34Z","published":"2024-10-08T15:48:34Z","title":"Toward Scalable Image Feature Compression: A Content-Adaptive and\n  Diffusion-Based Approach","summary":"  Traditional image codecs emphasize signal fidelity and human perception,\noften at the expense of machine vision tasks. Deep learning methods have\ndemonstrated promising coding performance by utilizing rich semantic embeddings\noptimized for both human and machine vision. However, these compact embeddings\nstruggle to capture fine details such as contours and textures, resulting in\nimperfect reconstructions. Furthermore, existing learning-based codecs lack\nscalability. To address these limitations, this paper introduces a\ncontent-adaptive diffusion model for scalable image compression. The proposed\nmethod encodes fine textures through a diffusion process, enhancing perceptual\nquality while preserving essential features for machine vision tasks. The\napproach employs a Markov palette diffusion model combined with widely used\nfeature extractors and image generators, enabling efficient data compression.\nBy leveraging collaborative texture-semantic feature extraction and\npseudo-label generation, the method accurately captures texture information. A\ncontent-adaptive Markov palette diffusion model is then applied to represent\nboth low-level textures and high-level semantic content in a scalable manner.\nThis framework offers flexible control over compression ratios by selecting\nintermediate diffusion states, eliminating the need for retraining deep\nlearning models at different operating points. Extensive experiments\ndemonstrate the effectiveness of the proposed framework in both image\nreconstruction and downstream machine vision tasks such as object detection,\nsegmentation, and facial landmark detection, achieving superior perceptual\nquality compared to state-of-the-art methods.\n","authors":["Sha Guo","Zhuo Chen","Yang Zhao","Ning Zhang","Xiaotong Li","Lingyu Duan"],"pdf_url":"https://arxiv.org/pdf/2410.06149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06068v1","updated":"2024-10-08T14:14:46Z","published":"2024-10-08T14:14:46Z","title":"Resolution limit of the eye: how many pixels can we see?","summary":"  As large engineering efforts go towards improving the resolution of mobile,\nAR and VR displays, it is important to know the maximum resolution at which\nfurther improvements bring no noticeable benefit. This limit is often referred\nto as the \"retinal resolution\", although the limiting factor may not\nnecessarily be attributed to the retina. To determine the ultimate resolution\nat which an image appears sharp to our eyes with no perceivable blur, we\ncreated an experimental setup with a sliding display, which allows for\ncontinuous control of the resolution. The lack of such control was the main\nlimitation of the previous studies. We measure achromatic (black-white) and\nchromatic (red-green and yellow-violet) resolution limits for foveal vision,\nand at two eccentricities (10 and 20 deg). Our results demonstrate that the\nresolution limit is higher than what was previously believed, reaching 94\npixels-per-degree (ppd) for foveal achromatic vision, 89 ppd for red-green\npatterns, and 53 ppd for yellow-violet patterns. We also observe a much larger\ndrop in the resolution limit for chromatic patterns (red-green and\nyellow-violet) than for achromatic. Our results set the north star for display\ndevelopment, with implications for future imaging, rendering and video coding\ntechnologies.\n","authors":["Maliha Ashraf","Alexandre Chapiro","Rafa K. Mantiuk"],"pdf_url":"https://arxiv.org/pdf/2410.06068v1.pdf","comment":"Main document: 12 pages, 4 figures, 1 table. Supplementary: 14 pages,\n  12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.05935v1","updated":"2024-10-08T11:38:13Z","published":"2024-10-08T11:38:13Z","title":"Learning Gaussian Data Augmentation in Feature Space for One-shot Object\n  Detection in Manga","summary":"  We tackle one-shot object detection in Japanese Manga. The rising global\npopularity of Japanese manga has made the object detection of character faces\nincreasingly important, with potential applications such as automatic\ncolorization. However, obtaining sufficient data for training conventional\nobject detectors is challenging due to copyright restrictions. Additionally,\nnew characters appear every time a new volume of manga is released, making it\nimpractical to re-train object detectors each time to detect these new\ncharacters. Therefore, one-shot object detection, where only a single query\n(reference) image is required to detect a new character, is an essential task\nin the manga industry. One challenge with one-shot object detection in manga is\nthe large variation in the poses and facial expressions of characters in target\nimages, despite having only one query image as a reference. Another challenge\nis that the frequency of character appearances follows a long-tail\ndistribution. To overcome these challenges, we propose a data augmentation\nmethod in feature space to increase the variation of the query. The proposed\nmethod augments the feature from the query by adding Gaussian noise, with the\nnoise variance at each channel learned during training. The experimental\nresults show that the proposed method improves the performance for both seen\nand unseen classes, surpassing data augmentation methods in image space.\n","authors":["Takara Taniguchi","Ryosuke Furuta"],"pdf_url":"https://arxiv.org/pdf/2410.05935v1.pdf","comment":"Accepted to ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2410.05767v1","updated":"2024-10-08T07:48:34Z","published":"2024-10-08T07:48:34Z","title":"Grounding is All You Need? Dual Temporal Grounding for Video Dialog","summary":"  In the realm of video dialog response generation, the understanding of video\ncontent and the temporal nuances of conversation history are paramount. While a\nsegment of current research leans heavily on large-scale pretrained\nvisual-language models and often overlooks temporal dynamics, another delves\ndeep into spatial-temporal relationships within videos but demands intricate\nobject trajectory pre-extractions and sidelines dialog temporal dynamics. This\npaper introduces the Dual Temporal Grounding-enhanced Video Dialog model\n(DTGVD), strategically designed to merge the strengths of both dominant\napproaches. It emphasizes dual temporal relationships by predicting dialog\nturn-specific temporal regions, filtering video content accordingly, and\ngrounding responses in both video and dialog contexts. One standout feature of\nDTGVD is its heightened attention to chronological interplay. By recognizing\nand acting upon the dependencies between different dialog turns, it captures\nmore nuanced conversational dynamics. To further bolster the alignment between\nvideo and dialog temporal dynamics, we've implemented a list-wise contrastive\nlearning strategy. Within this framework, accurately grounded turn-clip\npairings are designated as positive samples, while less precise pairings are\ncategorized as negative. This refined classification is then funneled into our\nholistic end-to-end response generation mechanism. Evaluations using\nAVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our\nmethodology.\n","authors":["You Qin","Wei Ji","Xinze Lan","Hao Fei","Xun Yang","Dan Guo","Roger Zimmermann","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2410.05767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05650v1","updated":"2024-10-08T02:59:08Z","published":"2024-10-08T02:59:08Z","title":"SIA-OVD: Shape-Invariant Adapter for Bridging the Image-Region Gap in\n  Open-Vocabulary Detection","summary":"  Open-vocabulary detection (OVD) aims to detect novel objects without\ninstance-level annotations to achieve open-world object detection at a lower\ncost. Existing OVD methods mainly rely on the powerful open-vocabulary\nimage-text alignment capability of Vision-Language Pretrained Models (VLM) such\nas CLIP. However, CLIP is trained on image-text pairs and lacks the perceptual\nability for local regions within an image, resulting in the gap between image\nand region representations. Directly using CLIP for OVD causes inaccurate\nregion classification. We find the image-region gap is primarily caused by the\ndeformation of region feature maps during region of interest (RoI) extraction.\nTo mitigate the inaccurate region classification in OVD, we propose a new\nShape-Invariant Adapter named SIA-OVD to bridge the image-region gap in the OVD\ntask. SIA-OVD learns a set of feature adapters for regions with different\nshapes and designs a new adapter allocation mechanism to select the optimal\nadapter for each region. The adapted region representations can align better\nwith text representations learned by CLIP. Extensive experiments demonstrate\nthat SIA-OVD effectively improves the classification accuracy for regions by\naddressing the gap between images and regions caused by shape deformation.\nSIA-OVD achieves substantial improvements over representative methods on the\nCOCO-OVD benchmark. The code is available at\nhttps://github.com/PKU-ICST-MIPL/SIA-OVD_ACMMM2024.\n","authors":["Zishuo Wang","Wenhao Zhou","Jinglin Xu","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2410.05650v1.pdf","comment":"9 pages, 7 figures"}]}}